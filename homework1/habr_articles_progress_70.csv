title,author,publish_date,hubs,text,views,comments,rating,reading_time,url,text_length,word_count,hubs_count
"Один в поле не воин: как собрать команду для мероприятия, чтобы всё прошло гладко (и вы не сошли с ума)",kseniaevent,2025-11-13T06:04:31.000Z,"['Developer Relations *', 'Конференции', 'Управление персоналом *', 'Управление проектами *']","kseniaevent 5 часов назадОдин в поле не воин: как собрать команду для мероприятия, чтобы всё прошло гладко (и вы не сошли с ума)Уровень сложностиПростойВремя на прочтение8 минКоличество просмотров100Developer Relations * КонференцииУправление персоналом * Управление проектами * ТуториалПривет! Продолжаем говорить про организацию мероприятий для непрофессиональных ивентеров. HR, ассистенты, руководители проектов, маркетологи, пиарщики и все те, на чью долю выпало в нагрузку к основным рабочим обязанностям сделать мероприятие, эта статья для вас.Мы уже обсудили с чего начать подготовку к мероприятию, его упаковку, CJM участников, выбор площадки, организацию питания, как собрать программу, фото-/видеосъемку, организацию трансляции и спонсорские интеграции. Пришло время поговорить про команду. Как мы уже говорили ранее, в случае мероприятия один в поле - не воин. Это не только довольно тяжело физически и морально, но и опасно, так как если организатор делает все сам, то он представляет из себя страшный bus-фактор и если с ним что-то случится, то мероприятие может сорваться. Поэтому в этом разделе мы рассмотрим основные роли, которые могут быть в команде. В зависимости от масштаба мероприятия некоторые из этих ролей могут не пригождаться. Но я верю в то, что данный список поможет вам охватить контролем все зоны мероприятия и не взять на себя лишнего. Ключевые роли и их функцииКаждый член команды играет важную роль, и от их профессионализма, ответственности и способности работать вместе зависит общий успех. При формировании команды важно учитывать компетенции участников, четко распределять обязанности и зоны ответственности, корректно и понятно ставить задачи и контролировать их выполнение.Ключевые роли и их функцииМенеджер проекта - руководитель команды и основной координатор всех процессов. В его обязанности входит:Планирование всех этапов мероприятия.Контроль сроков, бюджета и ресурсов.Управление рисками и решение непредвиденных ситуаций.Он несет ответственность за все мероприятие в целом.Менеджер по продвижению - отвечает за привлечение участников и создание информационного поля вокруг мероприятия. На этой роли могут быть маркетологи, smm-менеджеры, контент-менеджеры и даже менеджеры по внутренним коммуникациям в зависимости от специфики вашего мероприятия и маркетинговой стратегииОсновные задачи:Разработка стратегии продвижения.Подготовка и размещение рекламных материалов.Работа с соцсетями и аналитикой.Дизайнер - тот человек, который сделает вам красиво. Задачи дизайнера минимально включают:Создание визуальной айдентики мероприятияРазработку логотипов, баннеров, презентаций и иных брендированных материалов.Подготовку макетов для печати.Веб-разработчик или команда - обеспечивают создание и техническую поддержку сайта мероприятия. Копирайтер - создает все тексты, которые будут использованы в рамках мероприятия, за исключением сценария разве что, хотя и там копирайтер может приложить свою руку к отдельным частям. Может также быть представлен в роли smm-менеджера. Задачи:Написание пресс-релизов, постов, анонсов.Подготовка текстов для ведущих и спикеров.Вычитка текстов от СМИРежиссер-сценарист - это может быть как один человек, который совмещает в себе обе роли, так и отдельные люди. Суть от этого не меняется, задача сформировать и реализовать программу. Обычно речь все же идет про шоу-программу. Кстати, режиссер мероприятия и режиссер трансляции - это точно 2 разных человека, если у вас гибридное мероприятие.Задачи: Составление сценария.Координация тайминга шоу. Постановка номеров.Организация репетиций.Программный директор и/или комитет - тоже работают над программой, но уже деловых событий и конференций, там, где встречаются доклады. Задачи: Наполнение программы в соответствии с тематикой мероприятия.Выбор подходящих докладов и их доработка.Репетиции с выступающими. Иногда они также занимаются модерацией и контролем за проведением сессий и выступлений.Юрист - проверяет все договоры, подписываемые с подрядчиками, партнерами и спонсорами, консультирует по вопросам соблюдения прав гостей (например, согласие на обработку персональных данных или съемку), разрабатывает договоры с сотрудниками и временным персоналом.Бухгалтер - отвечает за все финансовые операции: расчеты с подрядчиками, оплата услуг и закупок, организует прием оплаты от участников, если мероприятие платное. Иногда, если планируется продажа чего-либо в день мероприятия, то помогает с организацией приема оплаты на площадке. Банкетный менеджер - в некоторых случаях вам придется нанять и его, если площадка сама не справляется (см. подробнее в главе «Питание»). Отвечает за весь сервис связанные с питанием гостей и персонала, помогает составить меню, следит за его реализацией, контролирует выносы и объемы потребления. Хорошо бы, чтобы он имел доступ на кухню на случай кризисных ситуаций, а для этого нужна мед.книжка и предварительное согласование с кухней.Персонал на площадке в день мероприятияДля оперативной работы на площадке важно заранее определить и заказать или найти среди сотрудников необходимый персонал. Ниже очень важные люди, про которых очень часто забываютПерсонал на площадке в день мероприятияРегистраторы - встречают гостей, выдают бейджи и материалы. Мы говорили о них в блоке про организацию площадки и работу с гостямиХостесс - помогают с навигацией и отвечают на вопросы участников. Обычно приглашаются на более статичные мероприятия. На мероприятиях попроще могут быть заменены на волонтеров.Монтажно-демонтажная группа (АХО) - отвечает за установку оборудования и декораций.Охрана - обеспечивает безопасность гостей и защищает имущество. Подробнее в блоке про охрануГардеробщики - заботятся о хранении верхней одежды гостей.Клинеры - поддерживают чистоту и порядок на протяжении мероприятия.Информационная стойка - предоставляет информацию и оперативно решает вопросы. Расположена на площадке мероприятияКонсьерж - помогает гостям с заказом такси, билетов, навигацией по городу. Обычно встречается на ВИП-мероприятихКолл-центр или саппорт - отвечают на вопросы гостей в чате / боте / по телефонуВолонтеры — незаменимые помощники, особенно на больших мероприятиях.  Их основные задачи встреча гостей и помощь с навигацией, помощь в функциональных зонах (нетворкинг, фуд-корт, мастер-классы), быстрое реагирование на запросы организаторов (дай-подай-принеси). Супервайзер волонтеров - обучает и координирует работу команды волонтеров, а также контролирует их в день мероприятия, решает возникающие проблемы и следит за соблюдением расписания.Хелперы - диапазон их задач довольно велик, я для себя делю их на технические - помощь с застройкой, перетаскиванием и прочими физическими работами, которые не связаны с взаимодействием с гостями, и административные - это своего рода платные волонтеры, возможно, несколько более опытные, которые представляют собой запасной человеческий ресурс. Ими можно усилить регистрацию, потом перекинуть на навигацию, при необходимости попросить встретить спикера и т.д.Курьеры - их тоже часто забывают заложить в бюджет, поэтому напомню об этой статье тоже - доставляют материалы, оборудование и еду. Если в процессе организации вы можете пользоваться приложениями, то в день мероприятия, если оно напряженное и вы понимаете, что вам нужно оказывать сервис превосходящий ожидания, то хорошо бы иметь дежурного курьера во время всего мероприятия. Вообще это просто может быть водитель с машиной, который сможет съездить по вашему поручению. Сотрудники для функциональных зон - координируют работу зон нетворкинга, фотозон, интерактивных площадок, залов и проч. Они следят за выполнением запланированных активностей и решают возникающие вопросы. В зависимости от формата мероприятия важно предусмотреть персонал для контроля всех важных зон. На небольших мероприятиях допускается совмещение. Подбор временного персонала и его обучениеЧасто для проведения мероприятий нанимается временный персонал, это нормальная практика. Его правильная подготовка и управление являются залогом успеха и хорошего клиентского опыта ваших гостей. Рассмотрим основные этапы подготовкиПодборСейчас есть специализированные агентства, которые предоставляют временные персонал для мероприятия. Обычно это самый безопасный и надежный способ поиска. Выбирайте активных, коммуникабельных и ответственных людей. Просите отзывы и опыт работы на других мероприятиях. Проводите интервью с теми, кто будет много коммуницировать с гостями не по стандартному протоколу.Согласование условий работыЗаранее согласуйте график работы и обязанности временного персонала.Согласуйте оплату и форму одежды персонала в день мероприятия. Не стесняйтесь запрашивать фото, если персонал будет в своей одежде. Если вы предполагаете бронированную одежду, то соберите размеры и закажите ее. Мой совет, берите что-то в стиле one size, так как часть персонала может «отвалиться» в процессе подготовки и будет заменена другими людьми.Обеспечьте удобное место для отдыха между сменами, а также питание в случае длительных смен.Предусмотрите технические перерывы и человека, который будет на замене на время перерываНазначьте супервайзера для контроля временного персонала. Если у вас много временного персонала, то берите супервайзеров для каждой категории. Супервайзер должен быть связующим звеном между персоналом и организаторами, обеспечивать своевременную ротацию людей, чтобы избежать их перегрузки и быстро реагировать на вопросы и проблемы.Обучение Проведите вводный инструктаж на площадке за несколько дней до мероприятия, если есть такая возможность. Хотя бы покажите заранее площадку супервайзерам, чтобы в день мероприятия они смогли сами расставить подконтрольный им персонал и проинструктировать. Перед инструктажем на площадке рекомендую выслать письменную инструкцию со всеми правилами, чтобы во время очной встречи ответить на вопросы. Также эта инструкция будет шпаргалкой для вас, чтобы ничего не забыть. Разъясняйте обязанности, моделируйте ситуации и смотрите за реакцией. Обучите заранее работе с оборудованием и навигацией.Распределение зон ответственности Назначайте задачи в зависимости от интересов и компетенций персонала. Особенно это касается волонтеров и хелперов, которые в целом многофункциональны, но обычно имеют сильные стороны и предпочтения.Контроль в день мероприятияВ процессе мероприятия вы должны контролировать работу временного персонала. Если у вас есть супервайзер, то это происходит через него. Ег задача присылать вам периодически фото того, что персонал в нужном количестве находится в своем месте и выполняет обязанности. Также супервайзер следит за общением с гостями, если оно предусмотрено и корректирует его при необходимости. Благодарность после мероприятияНе забудьте после мероприятия поблагодарить весь персонал. Если не успеваете лично, то хотя бы передайте благодарности через супервайзера. При необходимости будьте готовы написать благодарственные письма. Если у вас работают волонтеры на безвозмездной основе, то подарите им фирменный мерч или иным способом порадуйте ребят. Кстати, у меня было несколько кейсов, когда встречались настолько хорошие волонтеры, что мы обменивались контактами и продолжали работать на других мероприятиях уже на платных условиях. Некоторые даже работали в штате компании какое-то время. Так что, не упускайте хорошие кадры.  Организация работы внутренней командыДопустим, вы добились того, что ваши коллеги будут вам помогать в процессе организации мероприятия и у них даже выделены время и ресурс на это. Для эффективной работы команды необходимо выстроить четкую систему внутренней коммуникации. Минимальный чек-листНазначьте ответственных за различные направления, чтобы минимизировать хаос в коммуникациях и понимать, с кого спрашивать результат.Создайте общий чат в мессенджере для оперативного решения вопросов. Возможно, будет даже несколько чатов по разным направлениям мероприятия. Проводите регулярные встречи для обсуждения задач и решения проблем. Будьте на связи и выстраивайте открытую коммуникацию, чтобы сотрудники не боялись задавать вам уточняющие вопросы и просить помощи в решении проблем. Ничего не замалчивайте.Организуйте перерывы для отдыха и питания, особенно во время интенсивных дней подготовки и самого мероприятия.Контролируйте работу в день мероприятия и просите регулярно (периодичность установите сами) присылать вам уведомления, все ли идет хорошо в их зоне ответственностиПредусмотрите бонусы или подарки по итогам мероприятия.Мой принцип в коммуникации с командой ивента: демократия в процессе подготовки и деспотизм на площадке. В процессе подготовки мы всегда могли обсудить разные варианты решения задач и я всегда рада выслушать мнение команды, но на площадке на демократию часто не остается времени, особенно в ситуации форс-мажора, я ответственна за весь процесс и поэтому придется принимать мои решения, даже если они кажутся неверными.  Ксения КазаковаHR-проекты / Employer Brand / DevRelТеги:мероприятияконференцииуправление людьмиуправление проектамиуправление командойХабы:Developer RelationsКонференцииУправление персоналомУправление проектами",100,0,0,8 мин,https://habr.com/ru/articles/964316/,13025,1685,4
Очереди сообщений в Postgres Pro: отказ от внешних брокеров ради транзакционной надёжности,slonik_pg,2025-11-12T15:32:34.000Z,"['Блог компании Postgres Professional', 'SQL *', 'PostgreSQL *', 'Базы данных *', 'Серверное администрирование *']","slonik_pg 19 часов назадОчереди сообщений в Postgres Pro: отказ от внешних брокеров ради транзакционной надёжностиУровень сложностиПростойВремя на прочтение7 минКоличество просмотров3.3KБлог компании Postgres ProfessionalSQL * PostgreSQL * Базы данных * Серверное администрирование * ОбзорВ эпоху распределённых систем, где каждая компонента должна быть не только эффективной, но и предсказуемой, вопрос надёжности обмена данными становится критическим. Представьте: пользователь нажимает кнопку «Сгенерировать отчёт», и в этот момент должны синхронизироваться десятки процессов — от создания документа до его отправки по email. Но что если почтовый сервер временно недоступен? Или обработчик задач падает, не завершив операцию? Именно здесь на сцену выходят очередь сообщений — механизм, который превращает хаотичные запросы в управляемый поток, гарантируя, что ни одна задача не потеряется в пути.История создания расширения встроенных очередей в PostgreSQL началась с простой боли: внешние брокеры вроде RabbitMQ или Kafka, хотя и мощные, добавляли слои сложности. Управление ими требует выделенных ресурсов: отдельные серверы, настройка кластеров, мониторинг доступности. В enterprise-среде, где системы развертываются тысячами экземпляров, каждый новый компонент увеличивает риски и административную нагрузку.Зачем подключать отдельный брокер, если очередь уже встроена в базу данных и работает «из коробки»? Это не только экономит время, но и исключает проблемы согласованности: если транзакция откатывается, сообщение автоматически возвращается в очередь для повторной попытки, без единой строчки дополнительного кода.Два подхода к очередям: Log-based и AMQP/JMSВ мире распределенных систем сформировались два подхода к обработке сообщений:Log-based-очереди (например, Kafka). Работают как непрерывный журнал событий. Данные записываются строго последовательно, и потребители читают их в том же порядке. Это идеально для синхронизации данных между микросервисами или репликации баз данных. Однако их сила — в линейности — становится слабостью, когда нужна гибкость: выборка сообщений по приоритету или фильтрам здесь невозможна.AMQP/JMS-брокеры (например, RabbitMQ). Позволяют не просто передавать сообщения, но и управлять их жизненным циклом: устанавливать приоритеты, фильтровать по условиям, обрабатывать ошибки. Такие очереди позволяют имитировать асинхронное RPC, где задача может быть повторена при сбоях. Но их главная слабость — фундаментальная проблема, общая для любого внешнего брокера, включая Kafka: невозможно гарантировать транзакционную целостность с базой данных.Проблемы внешних брокеров: транзакционность и сложностьИспользование внешних брокеров, таких как Kafka или RabbitMQ, в enterprise-средах часто сопряжено со скрытыми сложностями:Рассогласование данных. Представьте сценарий: приложение отправляет сообщение в RabbitMQ и начинает транзакцию в базе данных. Сообщение уже ушло в брокер, но в момент коммита транзакции в БД происходит сбой. Результат — рассогласование: задача будет обработана, но данные для неё не сохранены. Попытки гарантировать атомарность через двухфазный коммит (2PC) усложняют архитектуру, требуя дополнительного координатора транзакций, а также заметно снижают производительность.Административная нагрузка. Внешние брокеры требуют отдельной инсталляции, настройки, мониторинга и резервного копирования. Это создаёт дополнительные точки отказа (например, сетевые сбои между сервером приложений и брокером) и усложняет поддержку, особенно если за СУБД и брокер отвечают разные команды.Postgres Pro Enterprise Queues: транзакционность и автоматизация с pgpro_queueНовое расширение pgpro_queue — это ответ на «боль» разработчиков, уставших бороться с рассогласованием данных. Интеграция очередей непосредственно в СУБД устраняет необходимость во внешних компонентах. Сообщения хранятся в обычных таблицах, реплицируются через стандартные механизмы PostgreSQL и участвуют в тех же транзакциях, что и бизнес-логика приложения.Установка и настройкаИнтеграция pgpro_queue начинается с простых шагов, знакомых любому DBA:Добавление в shared_preload_libraries. В файле postgresql.conf необходимо добавить расширение в список предварительно загружаемых библиотек:shared_preload_libraries = 'pgpro_queue'Создание расширения. В нужной базе данных выполняется команда:CREATE EXTENSION pgpro_queue;Инициализация. Для хранения служебных объектов (таблиц метаданных, очередей) создаётся отдельная схема pgpro_queue_data. Это делается с помощью функции:SELECT queue_initialize();Такое разделение обеспечивает корректную работу pg_dump и репликации.Ключевые возможности и их реализация1. Retry-on-rollback (автоматический повтор при откате).Это главная особенность pgpro_queue. Если транзакция, в которой было прочитано сообщение, откатывается (например, из-за недоступности внешнего сервиса), сообщение не теряется, а автоматически возвращается в очередь для повторной обработки.Управление. Поведение повторов настраивается как на уровне очереди, так и для каждого сообщения. При создании очереди с помощью CREATE_QUEUE можно задать параметры q_retries (максимальное количество попыток) и q_retrydelay (задержка в секундах перед следующей попыткой).Важный параметр. Для работы механизма повторов необходимо явно указать базу данных в конфигурационном файле postgresql.conf:pgpro_queue.database_with_managed_retry = 'имя_вашей_бд'Без этой настройки сообщения при откате транзакции будут удаляться, а не ставиться на повтор.2. Фильтрация и приоритеты.pgpro_queue позволяет гибко управлять потоком сообщений:Приоритеты. При вставке сообщения с помощью INSERT_MESSAGE можно указать q_msg_priority. Чем ниже значение, тем выше приоритет. Сообщения с более высоким приоритетом обрабатываются в первую очередь.Фильтрация. Функции чтения READ_MESSAGE и READ_MESSAGE_XML принимают параметры q_msg_hfilter и q_msg_pfilter, позволяя извлекать сообщения по содержимому их заголовков или свойств.3. Поддержка форматов JSON и XML.Расширение предоставляет отдельные функции для работы с разными форматами данных, что упрощает интеграцию с различными системами:INSERT_MESSAGE и READ_MESSAGE для JSONB;INSERT_MESSAGE_XML и READ_MESSAGE_XML для XML;READ_MESSAGE_ANY для чтения сообщений любого формата из одной очереди.4. Отложенная обработка.При вставке сообщения можно указать параметр q_msg_enable_time, чтобы задача стала доступна для обработки только при наступлении указанного времени.Что мы положили под капот?В основе расширения pgpro_queue лежит простой и надёжный принцип: сообщения хранятся в обычных таблицах PostgreSQL. Такой подход позволяет использовать всю мощь стандартных механизмов СУБД, включая репликацию и восстановление через WAL-файлы. Каждое сообщение, попавшее в очередь, записывается в WAL, что гарантирует его восстановление даже после аварийного отключения сервера. Ключевая особенность pgpro_queue — это глубокая интеграция с транзакционной моделью PostgreSQL, реализующая механизм retry-on-rollback. Он работает как страховка: если транзакция, обработавшая сообщение, откатывается из-за любой ошибки, pgpro_queue автоматически возвращает это сообщение в очередь для повторной попытки. Это гарантирует, что задача будет считаться выполненной только тогда, когда вся связанная с ней работа успешно зафиксирована в базе данных.Ещё одна сильная сторона pgpro_queue — синергия со встроенным в Postgres Pro Enterprise планировщиком фоновых задач (Scheduler). Этот компонент, работающий как cron внутри базы данных, может запускать периодические родительские задачи, которые, в свою очередь, наполняют очередь подзадачами для асинхронного выполнения. Например, массовую генерацию отчётов для клиентов из разных часовых поясов можно разбить на этапы: основная задача в планировщике создаёт подзадачи для каждого региона, а те, в свою очередь, помещают сообщения в очередь с учётом локального времени.Внешние брокеры, такие как Kafka, справедливо славятся своей «непотопляемостью»: они действительно спроектированы выдерживать серьёзные сбои. Однако эта надёжность относится к самому брокеру в изоляции. В реальной системе слабое звено появляется в точках интеграции, и общая стабильность оказывается зависима от внешних факторов: сети, конфигурации, транзакционной согласованности с базой данных. Но pgpro_queue исключает эти риски: очереди живут в той же экосистеме, что и данные приложения. Сообщения не теряются при сбоях, так как их сохранность гарантируется механизмами репликации и восстановления СУБД. Администраторам больше не нужно настраивать отдельные системы мониторинга для очередей: всё управляется через знакомые инструменты PostgreSQL.Важные технические особенностиДля корректной работы с pgpro_queue специалистам следует учитывать несколько моментов:Уровень изоляции. Расширение может использоваться только в транзакциях с уровнем изоляции READ COMMITTED.Подготовленные транзакции. При использовании двухфазного коммита (2PC) есть нюансы. Если транзакция с READ_MESSAGE() подготавливается через PREPARE TRANSACTION, сообщение блокируется до выполнения COMMIT PREPARED или ROLLBACK PREPARED. При этом ROLLBACK PREPARED только разблокирует сообщение, но не активирует логику повторных попыток.Что ждет встроенные очереди: дорожная картаРазвитие pgpro_queue сфокусировано на расширении сценариев использования. Уже в ближайших версиях появятся две ключевые возможности:Система подписок (Pub/Sub). Будет реализован механизм, аналогичный Exchange в RabbitMQ. Продюсер сможет отправлять сообщение в «тему», а не в конкретную очередь. Все потребители, подписанные на эту тему, получат свою копию сообщения. Это открывает путь к созданию полноценных событийно-ориентированных архитектур.Callback-уведомления. Появится возможность настроить «обратный вызов» — HTTP-запрос к внешнему сервису при появлении сообщения в очереди. Это позволит СУБД самой инициировать обработку, а не ждать, пока приложение опросит очередь.Функциональность Dead Letter Queue (DLQ) для «отравленных» сообщений также остаётся в планах, но на более долгосрочную перспективу.ЗаключениеДля корпоративных клиентов ключевое преимущество — предсказуемость. Каждое сообщение обрабатывается в рамках транзакций СУБД, что исключает рассогласование данных. Инфраструктура становится проще: вместо набора разнородных сервисов — единая база данных, где очереди, бизнес-логика и планировщик работают как части одного организма.Такие решения идеальны для проектов, где цена ошибки высока: банковские транзакции, медицинские системы, государственные реестры. Если вашему приложению критически важны атомарность операций и минимизация ручного вмешательства, встроенные очереди PostgreSQL — не выбор, а необходимость. Они не просто обрабатывают сообщения, а становятся страховкой от хаоса в мире распределённых систем.Теги:postgresqlkafkakafka apacheброкер сообщенийброкеры сообщенийбазы данныхpostgres propostgres pro enterpriseХабы:Блог компании Postgres ProfessionalSQLPostgreSQLБазы данныхСерверное администрирование",3300,0,0,7 мин,https://habr.com/ru/companies/postgrespro/articles/965632/,10931,1281,5
"Культура «AI-First»: как перестроить мышление команды, чтобы не отстать от рынка",Michael_Jerlis,2025-11-12T22:37:26.000Z,"['Искусственный интеллект', 'Читальный зал', 'Управление проектами *', 'Машинное обучение *', 'Управление персоналом *']","Michael_Jerlis 12 часов назадКультура «AI-First»: как перестроить мышление команды, чтобы не отстать от рынкаУровень сложностиСреднийВремя на прочтение4 минКоличество просмотров683Искусственный интеллектЧитальный залУправление проектами * Машинное обучение * Управление персоналом * МнениеПока одни компании разочаровываются в искусственном интеллекте, другие строят на его основе бизнес-империи. В чем их секрет? Не в деньгах и не в доступе к технологиям, а в особой культуре.Разберемся, как перестать просто использовать ИИ в работе и перейти к мышлению в стиле AI-First, и почему это единственный способ не превратиться в динозавра.Сколько стоит промедлениеВ прошлой статье я рассказал, почему многие компании, которые внедрили ИИ, разочаровались. Это не кликбейт, а суровая реальность: по некоторым оценкам, до 80% ИИ-проектов проваливаются — это вдвое чаще, чем обычные IT-проекты.Проблема не в том, что ИИ не работает. Но если встроить двигатель от гиперкара в телегу, она точно не поедет. Технология требует не просто инвестиций, а ментального сдвига всей команды, от CEO до стажера.Чтобы избежать участи «динозавров» рынка, которые выбывают из гонки из-за медленного внедрения, нужно перестать воспринимать ИИ как инструмент и сделать его фундаментом всех бизнес-процессов.От инструмента к фундаменту: что значит быть AI-FirstТермин «AI-First» ввел в оборот CEO Google Сундар Пичаи в 2016 году, когда объявил о стратегическом развороте компании. Быть AI-First — значит не просто «прикручивать» ИИ к рабочим процессам, а проектировать продукты, операции и принимать решения вокруг возможностей, которые дает искусственный интеллект.Вместо вопроса «как нам использовать ИИ в этом проекте?», стоит задуматься «как ИИ может полностью изменить способ решения этой задачи?». Этот подход помогает перестроить мышление.Исследование RAND выявило 5 ключевых причин провала ИИ-проектов, и почти все они — культурные, а не технические:Непонимание бизнес-проблемы;Нет качественных данных;Фокус на технологии, а не на решении;Слабая инфраструктура;Попытка решить слишком сложные для ИИ задачи.Культура AI-First решает эти проблемы, выстраивая организацию на трех столпах.Три Столпа Культуры AI-FirstСтолп 1: Данные как главный активСтрогие стандарты управления данными (data governance) — это не бюрократия, а необходимое условие для выживания.Посмотри на Netflix: их система рекомендаций, подбор обложек для фильмов и даже решения о съемках новых сериалов — результат глубокого анализа данных. Сервис построил бизнес-модель на данных, и остается на вершине.Компании, которые пренебрегли данными, столкнулись с проблемой «мусор на входе — мусор на выходе», когда ИИ-модели, обученные на некачественных данных, начали принимать ошибочные решения, стоившие миллионы.Столп 2: Поощрение экспериментовПередовые компании создают «ИИ-песочницы» — изолированные среды, где команды могут тестировать гипотезы и прототипы без риска «сломать» основные бизнес-процессы. Так можно быстро проверять идеи, отбрасывать нерабочие и масштабировать успешные. Подход не только ускоряет инновации, но и повышает мотивацию сотрудников, которые видят, что их идеи не уходят «в стол». Если же попытаться внедрить ИИ-решения сразу в боевую среду, это может привести к дорогостоящим провалам и демотивации команды.Столп 3: ИИ-грамотность для всехСамая большая ошибка — считать, что в ИИ должны разбираться только инженеры. В AI-First компании базовая грамотность должна быть у всех: менеджеров, юристов, маркетологов, HR.Крупные компании уже осознали это и запускают массовые программы обучения:КомпанияИнициативаОхватIKEAПрограмма ИИ-грамотности, включая этику ИИ30,000+ сотрудниковJPMorgan ChaseОбязательное обучение prompt engineering для всех новых сотрудниковВсе новые сотрудникиMasterCard8-часовой курс по принципам ответственного ИИ (справедливость, прозрачность)Все сотрудникиВ EMCD каждый новый сотрудник проходит вводный ИИ-тренинг, а раз в неделю можно посещать лекции и воркшопы, чтобы освоить конкретные инструменты, облегчить рутину, автоматизировать отчеты и упростить коммуникацию.Когда вся команда говорит на одном ИИ-языке, рождаются самые сильные идеи. Маркетолог может предложить новый способ сегментации аудитории, юрист — вовремя заметить риски в использовании данных.Рынок труда и AI-FirstAI-First культура — это не только про эффективность, но и про найм самых талантливых кадров. Зарплаты в сфере ИИ на 67% выше, чем у традиционных разработчиков, а дефицит огромный — до 68% компаний говорят, что им не хватает кадров.Лучшие специалисты не пойдут в компанию, где им придется месяцами выбивать доступ к данным, где боятся экспериментов, а руководство не понимает, чем они занимаются. Они ищут среду, где можно реализоваться. Они ищут AI-First культуру.И здесь — важный нюанс. Мы в EMCD ищем не просто кодеров, а людей, которые умеют решать конкретные бизнес-задачи с помощью ИИ. Поделюсь случаем.Один из наших сервисов постоянно проседал под нагрузкой, и команда долго пыталась найти причину. Стандартные методы не давали результата.Новый разработчик попробовал вместо ручного анализа скормить данные ИИ-модели. Она выявила повторяющиеся паттерны, которые всегда предшествовали сбоям. После этого осталось добавить механизм предиктивного распределения нагрузки, и проблема исчезла полностью.Парень не был «сильнее» как программист, но мыслил в парадигме AI-First.Не стань динозавромЕсли не в этом, то в следующем году компании разделятся на два типа: те, кто смог перестроить культуру и сделал ИИ частью ДНК, и все остальные.Культура AI-First про здравый смысл: учить людей, пересобирать процессы, поощрять эксперименты и фиксировать лучший опыт внутри команды. Чем раньше начать, тем больше шансов не остаться вне игры через несколько лет.Поделись, как внедряешь ИИ в своей команде, и какие еще есть способы перейти на AI-First мышление?Теги:ai-firstии-стартапии в бизнесецифровая трансформацияцифровая трансформация бизнесаавтоматизация процессовИИ-песочницыобучение ииdata governanceИИ в бизнес-процессахХабы:Искусственный интеллектЧитальный залУправление проектамиМашинное обучениеУправление персоналом",683,0,0,4 мин,https://habr.com/ru/articles/965874/,6098,780,5
Эластик и проблемы хранения ленты операций,shoshana_kv,2025-11-12T15:35:02.000Z,"['Блог компании ОТП Банк', 'Финансы в IT']","shoshana_kv 19 часов назадЭластик и проблемы хранения ленты операцийУровень сложностиСреднийВремя на прочтение11 минКоличество просмотров475Блог компании ОТП БанкФинансы в ITКейсПривет, меня зовут Екатерина, я работаю в ОТП Банке на позиции Senior-разработчика в одном из трайбов. В продолжение предыдущей статьи мы вместе с Александром, главным solution-архитектором, расскажем о вызовах, с которыми столкнулись при внедрении нереляционного хранилища в наше ДБО.В ОТП в первую очередь думают об удобстве клиента. Использование современных отказоустойчивых систем, таких как Elasticsearch, — один из важных трейд-оффов: с одной стороны, мы повышаем скорость и качество клиентского опыта, с другой — усложняем архитектуру и сталкиваемся с проблемами, которых не было бы при более простом подходе. Все это работает на эту цель.В этой статье мы поделимся частью таких кейсов и расскажем, как наша команда их решала.Задача: современная лента операцийПеред нами стояла задача создать ленту операций, которая соответствовала бы трем ключевым требованиям:  Высокая производительность: Быстрый доступ к клиентским транзакциям при больших объемах данных.Гибкий поиск: Поддержка полнотекстового поиска и фильтрации.Масштабируемость по глубине данных: Возможность работы с разной глубиной истории операций.Мы рассмотрели несколько архитектурных подходов и проанализировали подходящие для них технологии.Вариант 1: Реляционная база данных с кешем Суть подхода: Классическая связка базы данных (например, PostgreSQL) и горячего кеша в памяти для оперативных данных.Почему отказались: Главная проблема — глубина поиска данных. Глубина поиска для выписок может составлять от нескольких дней до нескольких лет. Хранить многолетний кеш для всех клиентов крайне ресурсоёмко с точки зрения оперативной памяти. При горизонтальном масштабировании пришлось бы либо использовать отдельное распределенное кеш-хранилище (например, Redis), что усложняет архитектуру, либо механизм sticky sessions, от которого мы хотели уйти. Этот подход хорошо справляется с недавними данными, но не подходит для эффективного поиска по полной истории.Кроме того, по результатам множества бенчмарков, системы вроде Elasticsearch показывают значительно более высокую скорость выполнения сложных поисковых запросов по большим datasets по сравнению с реляционными СУБД.Вариант 2: ClickHouse Суть подхода: Использование колон��чной аналитической СУБД, оптимизированной для быстрого чтения.Почему отказались: ClickHouse отлично подходит для аналитических отчетов, но оказался неудобен для нашей онлайн-системы по нескольким причинам:Сложность эксплуатации: Требует глубоких знаний для поддержки и тонкой настройки.Ограничения на обновления: Отсутствие привычной поддержки точечных UPDATE. Клиентские операции часто обновляются (например, баланс карты может меняться многократно за день), что в ClickHouse приводит к необходимости замены всей записи. Это ведет к высокой фрагментации данных и снижению производительности.Вариант 3: ElasticsearchСуть подхода: Использование поискового движка, заточенного под работу с большими объемами неструктурированных данных.Почему выбрали: Этот вариант наилучшим образом соответствовал нашим требованиям:Скорость поискаВстроенная морфология и поддержка нечеткого (fuzzy) поискаПроизводительность на больших данных: Система стабильно работает с миллиардами документов, в то время как запросы аналогичной сложности в SQL-базах могут выполняться неприемлемо долго.Гибкость схемы: Данные хранятся в виде JSON-документов, возможность создавать динамический маппингМасштабируемость: Поддержка шардирования и репликации реализована на уровне платформы, что избавляет от необходимости разработки кастомных решений.Итог: После анализа мы остановились на Elasticsearch как на наиболее сбалансированном решении. Стоит отметить, что на момент принятия решения у команды не было практического опыта работы с этой технологией. Поэтому процесс внедрения сопровождался как успешными находками, так и преодолением трудностей, о которых мы подробнее расскажем в следующих разделах.Проблема 1. Производительность Elasticsearch и количество индексовВ Elasticsearch данные организованы в индексы, которые, в свою очередь, делятся на шарды. Изначально мы выбрали простую и логичную схему: выделить отдельный индекс для каждой организации (клиента). На старте, при небольшом количестве клиентов, система работала идеально, обеспечивая мгновенный отклик.Однако с ростом числа клиентов мы столкнулись с фундаментальным ограничением Elasticsearch: производительность критически падает при большом количестве индексов и шард. Согласно официальной документации, для одного узла не рекомендуется превышать лимит в 1000 шард [источник]. Хотя этот лимит можно увеличить, сама компания Elastic предупреждает, что это временное решение, и стабильная работа не гарантируется [источник].На практике деградация проявилась быстро: время операций (например, реиндексации документов) выросло до ~200 мс. Для высоконагруженной системы с SLA в десятки миллисекунд такие показатели стали неприемлемыми.Расчет допустимого количества индексовЧтобы найти решение, мы рассчитали верхнюю границу количества индексов для нашей инфраструктуры. Исходили из следующей логики:Конфигурация одного индекса: 8 шард (для горизонтального масштабирования) × 4 реплики (для отказоустойчивости) = 32 шарда на индекс.Общий лимит для нашего кластера из 4 узлов: ~4000 шард.С учетом запаса на миграции и техническое обслуживание мы задействовали только треть от общего лимита: ≈1300 шард.Поделив 1300 на 32, мы получили максимум ~40 индексов. Стало очевидно, что первоначальная схема «один индекс на организацию» нежизнеспособна на долгосрочную перспективу.Поиск оптимальной стратегииМы рассмотрели несколько путей оптимизации структуры индексов:Объединение мелких организаций в общие индексы.Почему отказались: Невозможно надежно прогнозировать объем транзакций компании. Маленький клиент сегодня мог стать крупным завтра, создавая дисбаланс нагрузки в общем индексе.Разбивка данных по временным интервалам (например, по месяцам).Почему отказались: Этот подход усложняет архитектуру, так как поиск по истории операций потребовал бы отправки множественных запросов (multisearch) к нескольким индексам. Это сделало бы время отклика непредсказуемым и увеличило бы нагрузку на кластер.Разбивка по регионам.Почему отказались: Деление оказалось слишком крупным и неравномерным, что не решало проблему эффективного распределения нагрузки.В итоге мы выбрали стратегию равномерного распределения организаций по фиксированному пулу индексов. Этот подход позволил нам жестко контролировать общее количество индексов (в пределах рассчитанного лимита в 40 штук), обеспечивая предсказуемую производительность, отказоустойчивость и простоту масштабирования.Скрытый текстpublic void init() {
       final var indexes = indexRepository.findAll().stream()
           .collect(Collectors.toMap(OrganizationIndex::getOrganizationId, Function.identity()));
       INDEX_BY_ORGANIZATION_ID_MAP.putAll(indexes);
   }    
 
    /**
    * При запросе на регистрацию присваиваем организации индекс (по очереди)
    */
   @Transactional
   public void initializeOrganization(final UUID organizationId) {
       init();
       // горячий кеш с данными индексов и организаций
       if (INDEX_BY_ORGANIZATION_ID_MAP.containsKey(organizationId)) {
           return;
       }
       final int organizationsCount = (int) indexRepository.count();
       // номер индекса - остаток от деления на количество организаций
       final var indexName = getIndexName(organizationsCount % indexCount + 1);
       final var index = indexRepository.save(OrganizationIndex.of(organizationId, indexName, OrganizationIndexStatus.READY));
       INDEX_BY_ORGANIZATION_ID_MAP.put(organizationId, index);
       log.info(""Add organization to index {}"", index);
   }Проблема 2. Дублирование данных и конкурентные обновленияКлиентские операции в нашей системе могут создаваться через несколько независимых каналов:Обращение в офис;Новое ДБО (дистанционное банковское обслуживание);Старое ДБО;Внутренние операции.После создания событие проходит через цепочку промежуточных сервисов и финализируется в АБС. Критически важным моментом стало то, что обновление одной и той же операции могло быть инициировано разными системами параллельно. На практике это вылилось в ситуацию, когда обновления для одного документа в Elasticsearch приходили одновременно из разных источников — через REST-API и несколько Kafka-топиков.Почему стандартные механизмы Elasticsearch не подходятПопытка обновлять данные «в лоб» привела к потере изменений. Причина в том, что Elasticsearch не гарантирует строгой консистентности (immediate consistency) на уровне отдельных документов. Даже принудительное обновление индекса после записи с помощью withRefreshPolicy(RefreshPolicy.IMMEDIATE) не защищает от race condition (гонки состояний) между параллельными запросами.В отличие от реляционных СУБД, в Elasticsearch отсутствуют механизмы строгой блокировки на уровне записи (row-level locking) в рамках транзакции. В результате конкурентные операции update могут выполняться на устаревшей версии документа, затирая изменения друг друга.Этап 1: Оптимистическая блокировка (Optimistic Concurrency Control)В качестве первого решения мы реализовали оптимистическую блокировку средствами самого Elasticsearch, используя механизм версий документов (_version). Однако этот метод не сработал в наших условиях. Из-за высокой задержки индексации, вызванной проблемой с большим количеством индексов (см. предыдущий раздел), обновления приходили быстрее, чем успевала обновляться версия документа в индексе. Это приводило к лавине ошибок конфликта, и большая часть обновлений не применялась.Этап 2: Специализированный сервис-агрегатор и синхронизация через RedisМы пришли к выводу, что нужен централизованный механизм синхронизации, вынесенный за пределы Elasticsearch. Для этого был разработан отдельный сервис-агрегатор, который выполняет следующие функции:Принимает события из всех источников (REST, Kafka-топики).Приводит их к единому формату данных.Определяет приоритет обработки для событий из разных каналов.Вводит небольшие искусственные задержки для выравнивания нагрузки.Таким образом, разнородные операции превращались в упорядоченный поток событий с единым идентификатором.Для синхронизации параллельных обращений к одному и тому же идентификатору операции и сохранения stateless-архитектуры самого агрегатора мы использовали Redis в качестве распределенного кэша и легковесного координатора:Ключ: Идентификатор операции.Значение: Временная метка последнего принятого обновления.Скрытый текстpublic void handle(final TransactionUpdate transactionUpdate) {
    final var startTime = System.currentTimeMillis();
    // transactionUpdate - это класс wrapper, который оперирует идентификаторами операций
    transactionUpdate.message().getUpdateIds().forEach(updateId -> {
        try {
            // делаем лок в redis c максимальным временем обработки  - 1 минута
            redisLockRegistry.executeLocked(
                updateId,
                Duration.ofMillis(lockTimeoutMs),
                () -> {
                     // в связи с тем, что нужно дать время elasticsearch обновить индексы, то вычисляем время ожидания для операций, которые одновременно обновляются из разных источников
                    final long timeToWait = getTimeToWaitAndUpdateCache(updateId);
                    if (timeToWait <= 0) {
                        process(transactionUpdate, updateId);
                        return;
                    }
                    Thread.sleep(timeToWait);
                    process(transactionUpdate, updateId);
                });
        } catch (Exception e) {
            log.error(""Error while processing update transaction {}, in: {}"", updateId, timeDurationFrom(startTime), e);
            throw new UnexpectedException(""Error while processing update transaction"");
        }
    });
    log.info(""Transaction message was processed in: {}"", timeDurationFrom(startTime));
}
 
public long getTimeToWaitAndUpdateCache(final String updateId) {
    // ищем в редисе последнее время обновления операции
    final var savedLastUpdateTime = transactionLastUpdateRepository.findById(updateId)
        .map(TransactionLastUpdate::getLastUpdateTime)
        .orElse(null);
    // получаем время для начала обновления
    final var newLastUpdateTime = calculateUpdateTime(savedLastUpdateTime);
    // обновляем данные в кеше redis
    transactionLastUpdateRepository.save(TransactionLastUpdate.builder()
        .updateId(updateId)
        .lastUpdateTime(newLastUpdateTime)
        .ttl(lastUpdateCacheTtlSec)
        .build());
    long timeToWait = newLastUpdateTime - System.currentTimeMillis();
    log.debug(""Calculate time to wait: id {}, new update time {}, time to wait {}"",
        updateId, newLastUpdateTime, timeToWait);
    return timeToWait;
}
 
private long calculateUpdateTime(@Nullable final Long lastUpdateTime) {
    // операцию еще не обновляли или обновляли в прошлом (раньше, чем delayMilliSeconds назад), значит можно обновлять сразу
    if (isReadyForProcess(lastUpdateTime)) {
        return System.currentTimeMillis();
    }
    // в ином случае надо ожидать
    return lastUpdateTime + delayMilliSeconds;
}
 
private boolean isReadyForProcess(@Nullable final Long lastUpdateTime) {
    return lastUpdateTime == null || lastUpdateTime < System.currentTimeMillis() - delayMilliSeconds;
}
 
 
 private void process(final TransactionUpdate transactionUpdate, final String updateId) {
    // по типу источника информации понимаем, какой пришел формат данных, и применяем необходимые операции к нему
    switch (transactionUpdate.type()) {
        case TYPE_1 -> service1.process(transactionUpdate);
        case TYPE_2 - > service1.process(transactionUpdate, updateId);
        .....
        default -> throw new UnsupportedOperationException(""Unknown update type "" + transactionUpdate.type());
    }
}
Это решение позволило эффективно справляться с параллельными обновлениями и стало временным решением проблемы.Этап 3: Гарантия порядка с помощью Kafka и унификация форматаРешение с сервисом-агрегатором и Redis было работоспособным, но добавляло сложность: необходимость поддерживать отдельный сервис и инфраструктуру кэша. Мы решили пересмотреть архитектуру потоков данных, чтобы найти более фундаментальное и простое решение.Проблемы предыдущего подхода:Разнородность форматов: Источники продолжали генерировать события в разных структурах данных.Нарушение последовательности: Не было гарантии, что обновления одной операции придут в агрегатор в правильном хронологическом порядке.Архитектурные измененияМы провели ряд ключевых изменений, переведя взаимодействие на Apache Kafka:Kafka с ключом сообщенияВ качестве ключа каждого сообщения стал использоваться идентификатор операции. Это фундаментальное изменение, так как Kafka гарантирует порядок доставки сообщений с одним и тем же ключом в рамках одной партиции. Это решило проблему конкурентных обновлений на транспортном уровне, устранив саму возможность нарушения последовательности.Единый унифицированный форматМы разработали и внедрили единый контракт для событий об операциях. Все системы-источники стали публиковать сообщения только в этом формате. Это позволило упростить логику агрегатора, который теперь мог напрямую десериализовать и обрабатывать входящие события без дополнительных преобразований.Консолидация топиковВместо множества специализированных топиков для каждого сервиса мы ввели единый топик для событий, связанных с операциями. Это сместило фокус: теперь не каждый сервис диктовал свой формат, а агрегатор определял единый контракт для всего потока данных.РезультатБлагодаря этим изменениям нам удалось полностью отказаться от механизма синхронизации через Redis. Новый подход обеспечил:Предсказуемость: Порядок обработки гарантирован средствами Kafka.Упрощение архитектуры: Исчезла необходимость в поддержке отдельного кэша и логики разрешения конфликтов.Повышение надежности: Система стала менее зависимой от дополнительных компонентов.Минимальная компонентная архитектура разверткиВ качестве итога приведем минимальную рабочую, по нашему мнению, архитектуру кластера Elasticsearch, разработанную для следующих условий:Инфраструктура: 2 основных ЦОДа с кластерами Kubernetes и микросервисами.Ограничение: 1 дополнительный ЦОД для кворума с ограниченными ресурсами.Схематичное изображение финальной архитектуры представлено ниже.Для начала кратко обозначим роли нод, которые мы использовали:Мастер-нода (Master Node): Формирует кворум, управляет состоянием кластера (метаданные, шардирование), обеспечивает консистентность и защиту от split-brain. Не хранит данные и не обрабатывает пользовательские запросы.Координирующая нода (Coordinating Node): Принимает запросы от микросервисов на чтение и запись, маршрутизирует их к нужным data-нодам и агрегирует результаты. Является входной точкой в кластер.Data-нода (Data Node): Хранит данные, выполняет поиск, агрегацию и другие операции непосредственно с индексами.Выбранная конфигурацияИсходя из требований отказоустойчивости и производительности, мы развернули кластер следующей конфигурации:Координирующие ноды: По 1 ноде в каждом из 2 основных ЦОДов. Это обеспечивает минимальные задержки для микросервисов, которые обращаются к координатору в своем ЦОДе.Мастер-ноды: 3 ноды, распределенные по трем ЦОДам. Это гарантирует работоспособность кворума (требуется большинство, т.е. 2 из 3) даже при полном отказе одного из основных ЦОДов. В случае сетевого разрыва кластер запретит запись на изолированном участке, но сохранит доступность данных для чтения.Data-ноды: 4 ноды в основных ЦОДах. Это минимальное количество для эффективного шардирования, которое можно легко увеличить для горизонтального масштабирования.ИтогПредставленная архитектура позволила нам построить отказоустойчивую и надежную систему, которая соответствует строгим требованиям бизнеса к доступности и производительности. Она устойчива к сбоям на уровне ЦОДа и предоставляет четкий путь для дальнейшего масштабирования хранилища данных.Теги:elasticsearchjavaarchitectureфинтехинтеграцииХабы:Блог компании ОТП БанкФинансы в IT",475,0,8,11 мин,https://habr.com/ru/companies/otpbank/articles/965420/,18221,1996,2
"Нечёткий поиск при пересечении множеств, или Как выжать все соки из Хэширования по сигнатуре",VGoren,2025-11-13T07:16:06.000Z,"['SQL *', '.NET *', 'C# *', 'Microsoft SQL Server *', 'Алгоритмы *']","VGoren 4 часа назадНечёткий поиск при пересечении множеств, или Как выжать все соки из Хэширования по сигнатуреУровень сложностиСреднийВремя на прочтение23 минКоличество просмотров265SQL * .NET * C# * Microsoft SQL Server * Алгоритмы * Из песочницыСлияние рек Солимоэнс (верхняя Амазонка) и Риу-Негру в БразилииНа просторах интернета легко можно найти материалы по реализации нечёткого поиска, в которых предполагается поиск одной строки в множестве строк M. Но что если возникнет необходимость реализовать нечёткое сравнение множества M₁ с множеством M₂? При классическом подходе нам придется выполнить  сравнений - при линейном росте этих множеств, сложность задачи будет расти экспоненциально, в плане производительности это решение никуда не годиться!Предложенное ниже решение для БД SQL реализовано с помощью хэширования строк по сигнатуре. Оно максимально эффективно выполняет данный поиск в пределах одной ошибки (по расстоянию Левенштейна), но его можно адаптировать и под поиск в пределах какого угодно количества ошибок, но увеличение допуска экспоненциально усложняет алгоритм.Размер строк также не ограничен, но нечёткий поиск очень большого текста в пределах одной, двух, трёх ошибок можно сравнить со ""сферический конём в вакууме"" (не могу вообразить, зачем это может понадобится), в этом случае гораздо эффективнее воспользоваться другими методами хэширования - например, SimHash(SimilarityHash)[1][2][3], MinHash[1][2]. Поэтому предложенный алгоритм наиболее уместен для поиска средних, малых строк - ФИ, ФИО, VIN, маркировка, спецификация техники, серийные номера, штрихкоды, хэшсуммы и т.д.Цели:Ознакомить с концепциейДать конкретный пример интеграции в БД SQL(MSSQL)Ознакомить с возможностями на базе практической реализацииПример результатов при тестировании пересечения 5000 customers с 5000 employees по условию нечёткого поиска в пределах 2-ух ошибок по расстоянию ЛевенштейнаОсновная концепцияХэш по сигнатуре В целях упрощения восприятия, здесь и далее будем разбираться на примере 32-битного хэша, хотя его размер может быть любым. Кратко описываю алгоритм получения хэша(в практической реализации я буду делать так же):  №  Операция                                                                                                                                                                                                               Результат                                           1  Берем за вводные хэша ASCII код каждой буквы                                                                                                                         значения в диапазоне байта - (1-255)                2  Пропускаем значения через рандомайзер (в моей реализации на C# это Алгоритм генератор случайных чисел D.E. Knuth)  значения в диапазоне int'а - (0-4294967295)  3  Применяем простейший остаток от деления на размер хэша(x32)                                                                                                                                                     значения в диапазоне - (1-32)                       4  В пустой битовой(x32) маске ""включаем"" биты по индексу полученных значений                                                                                                                                             значения в диапазоне байта - (1-255)               Правило 2-ух ошибокСреди всех нечётких хэшей он обладает уникальной особенностью, на которой будет строиться вся дальнейшая работа - он подчиняется ""правилу 2-ух ошибок""(для удобства изложения пришлось придумать название самостоятельно, в статье Бойцова Л.М. это свойство описано, но названия не получило). Формулировка: Если расстояние Левенштейна между строками А и B = 1, то расстояние Хэмминга между хэшами от А и Б меньше или равно 2, если это была замена, либо меньше или равно 1, если это была вставка/удалениеИными словами:    1 вставка/удаление     не может изменить хэш более чем на 1 ошибку         1 замена               не может изменить хэш более чем на 2 ошибки         ↓                      ↓                                               1 ошибка не может изменить хэш более чем на 2 ошибки Например, получим сигнатурные хэши от 2-ух одинаковых строк с одной ошибкой: СтрокаХэшAЧумаков Максим Глебович00011011 00001100 00101011 01100110BЧубаков Максим Глебович00011011 00001101 00001011 01100110В строке A была вводная м, которая, очевидно, включала бит №19 В строке B пропала вводная м, и появилась вводная б, которая, очевидно, включила бит №16Правило 2-ух ошибок позволяет сделать поиск детерменированным - мы можем быть уверены, что точно не пропустим строки с заданным количеством ошибок. Например, очень популярный и действительно неплохо работающий даже на таких небольших строках, как ФИО, SimHash все равно остается вероятностным - 1 ошибка скорее всего поменяет мáлую часть хэша, но всегда остается вероятность того, что хэш изменится кардинально. MinHash тоже вероятностный. Они лучше подходят для больших текстов, например, при сравнении на антиплагиат - особенно, если предварительно обработать их, исключив связующие предлоги, союзы, артикли и т.д.HEngineHEngine[1][2] является алгоритмом ускорения поиска запрашиваемой бинарной строки в множестве таких строк в пределах заданного расстояния Хэмминга. Сейчас попробуем применить этот алгоритм для поиска в пределах 2-ух ошибок на нашем примере, попутно раскрывая его суть:Если разделить 32-битные хэши A и B на 4 равные части(chunk'и) по 8 бит с сохранением информации об их порядковом номере, то как минимум по двум из них A и B совпадут:ichunkichunkmatchA000011011B000011011✓A100001100B100001101✗A200101011B200001011✗A301100110B301100110✓То есть необязательно проверять сплошняком каждый бит. Если мы ищем похожие на А строки в множестве М, мы можем таким же образом дробить хэши на 4, 8, 16 частей и искать match'и соответственно по 2/4, 6/8, 14/16, 30/32(собственно расстояние Хэмминга). Постоянно сужая выборку на текущем этапе ""предрасчёта"" расстояния Хэмминга, мы будем компенсировать возрастающую стоимость поиска match'ей на следующем этапе (очевидно, что проверить 2/4 легче, чем 30/32).Более того, если мы ищем строки из M₁, похожие на строки M₂ с расстоянием Левенштейна = 1, после первого самого дешевого этапа (2/4), мы уже получаем не просто сокращенные множества от M₁ и M₂, а суженный набор пар M₁.id-M₂.id, которые теоретически могут быть искомыми. Таким образом уже после первого этапа предрасчёта расстояния Хэмминга перед нами стоит задача не экспоненциальной сложности, а линейной.Специфика SQLВот и весь алгоритм - хэшируем по сигнатуре множества M₁, M₂ и ищем совпадения по HEngine (перебирая 2 массива вложенными циклами). Теоретической новизны в проекте практически нет. Осталось написать пару примеров реализации, и на этом можно было бы и закончить, если бы мы программировали на обычном императивном C-подобном языке программирования. Но, как правило, на практике для хранения/выборки данных используются БД SQL. Внедрение этого поиска в SQL оказалось достаточно сложной задачей, в ходе которой сам алгоритм претерпел много изменений, хотя в сущности концепция осталась прежней.Предоставление конкретной реализации нечёткого поиска является одной из целей этой статьи. Реализовано на примере MSSQL. Подобное решение можно портировать на любую СУБД, имеющую возможность интеграции стороннего кода(SQLCLR, PL/Java, PL/Python, MySQL Connector/C++ и др.), без каких-либо ограничений.Необходимость интеграции стороннего кодаСледующие функции очень проблематично реализовать силами процедурного SQL (T-SQL) - производительность будет на плачевном уровне. Поэтому они реализованы через интеграцию стороннего кода (C# SQLCLR)ФункцияНазначениеLevenshteinDistanceString()Принимает 2 строки,       возвращает расстояние Левенштейна между нимиHammingDistanceX32()Принимает 2 int'а по x32, возвращает расстояние Хэмминга между ними, прерывает подсчет расстояния, если оно превысит лимитGetSignatureHash()Возвращает сигнатурный хэш от строкиSplitSignatureHash()Разбивает сигнатурный хэш на chunk'и по x8В SQL нельзя эффективно реализовать алгоритм HEngine полностьюПосле первого самого дешёвого этапа 2/4, перед нами будет множество пар из M₁.id-M₂.id. Если мы начнем дальше ""копать"" согласно этому алгоритму (проходить этапы 6/8, 14/16), то в каждой итерации на каждую пару, нужно будет вызывать дважды(на M₁.id и M₂.id) функцию разбивки хэша SplitSignatureHash(), затем полученные множества придется join'ить. Операционная стоимость даже одной SplitSignatureHash() в любом случае больше, чем одной HammingDistanceX32(), потому что она примитивна и работает через битовый XOR. Очевидно, что проходить этапы 6/8, 14/16 в SQL для дальнейшего сужения выборки бессмысленно - эффективнее сразу вызывать HammingDistanceX32().Базово описываем алгоритм: №ОперацияРезультат1На этапе 2/4 вызываем SplitSignatureHash() для M₁ и M₂линейно растущие множества 2MEGRE/HASH JOIN'им с помощью стандартного SQLэкспоненциально растущее множество 3Агрегируем значения (GROUP BY M₁.Id, M₂.Id HAVING COUNT() >= 2)сокращенное множество комбинаций 4На остаток пар вызываем HammingDistanceX32()сокращенное множество комбинаций 5На остаток пар вызываем LevenshteinDistanceString()искомые комбинации Ускорение с помощью k-комбинаций chunk'ов.6 k-комбинаций 2/4При классическом исполнении алгоритма HEngine, БД тратит огромный ресурс на сложное агрегирование данных(3 этап). Эта операция настолько затратна, что оптимизатор запросов SQLServer'а предпочитает сначала выполнять 4-ый этап, производя много лишних запусков HammingDistanceX32() на несгруппированном множестве, но сокращая множество комбинаций, а затем - 3-ий, потому что так дешевле!Вернемся к нашему примеру. Если A находится в множестве M₁, а B - в M₂. То на 2-ом этапе  с комбинации А-Б в множестве комбинаций M₁.id-M₂.id мы получили 2 строки по 2-ум совпадающим парам chunk'ов по индексам 1 и 4. Но помимо интересной нам комбинации, в полученном несгруппированном множестве находятся как интересные нам комбинаций, которые заjoin'ились более чем по 2-ум chunk'ам, так и НЕинтересные, которые заjoin'ились только лишь по 1-му chunk'у - вот поэтому нам необходим 3-ий этап.Для избежания затратного агрегирования введем функцию стороннего кода SplitSignatureHashKComb() - возвращает все возможные k-комбинации 2/4 chunk'ов по 8 бит:Во-первых, будем возвращать не 2 поля (i, chunk), а сконкатенированное 2-байтовое (i_chunk). Да, для диапазона значений 0-3 достаточно 2 бит, но байт(8 бит) – это минимальная ячейка адресации. На примере A-B это будет выглядеть следующим образом.ii_chunkii_chunkmatchA000000000 00011011B000000000 00011011✓A100000001 00001100B100000001 00001101✗A200000010 00101011B200000010 00001011✗A300000011 01100110B300000011 01100110✓А теперь будем возвращать все возможные k-комбинации i_chunk'ов:k_combi_chunk_k_combii_chunk_k_combmatchA01__00000000 00011011        00000001 00001100B01__00000000 00011011        00000001 00001101✗A0_2_00000000 00011011        00000010 00101011B0_2_00000000 00011011        00000010 00001011✗A0__300000000 00011011        00000011 01100110B0__300000000 00011011        00000011 01100110✓A_12_00000001 00001100 00000010 00101011B_12_00000001 00001101 00000010 00001011✗A_1_300000001 00001100 00000011 01100110B_1_300000001 00001101 00000011 01100110✗A__2300000010 00101011 00000011 01100110B__2300000010 00001011 00000011 01100110✗-: Увеличение веса и количества возвращаемых строк. Раньше на каждый хэш приходилось по  байт, теперь  байт. +: Не нужно агрегировать комбинации, это дает несопоставимо бóльший выигрыш в производительностиОписываем ускоренный окончательный алгоритм, которым будем пользоваться (теперь можно оценить, как далеко мы ушли от первоначального HEngine): №ОперацияРезультат1На этапе 2/4 вызываем SplitSignatureHashKComb() для M₁ и M₂линейно растущие множества 2merge/hash join'им с помощью стандартного SQL, с чем он неплохо справляетсяэкспоненциально растущее множество  исключительно интересных нам комбинаций M₁.Id-M₂.Id3Агрегируем значения (GROUP BY M₁.Id, M₂.Id или DISTINCT)сокращенное множество комбинаций 4На остаток пар вызываем HammingDistanceX32()сокращенное множество комбинаций 5На остаток пар вызываем LevenshteinDistanceString()искомые комбинации Стоит отметить, что:На практике оптимизатор запросов SQLServer'а всё равно предпочитает менять 4-ый и - 3-ий этап местами, но сама агрегация гораздо дешевлеДо 5-го этапа алгоритм обладает свойством резистивности к перестановке слов в строке - порядок в строках не важен(ФИО!=ИФО). Чтобы сохранить это свойство, нам придётся разбивать 5-ый шаг на несколько однотипных расчётов расстояния Левенштейна для каждой комбинации слов в шаблоне(в случае ФИО - это факториал  ). Учитывая, что к 5-му этапу выборка уже максимально сужена, это не несёт слишком больших затрат, хотя, конечно, количество слов в шаблоне усложняет 5-ый этап экспоненциально.DDL vs расчёт на местеРасчёт на местеSQLServer на этапе 2/4 сортирует chunk'и и использует MERGE JOIN, т.е. план адекватныйDDLфактически экономим время на расчет хэшей и сортировку(индексацию)меньше дергаем tempDB(больше свободной ОЗУ)Более предсказуемый план выполнения, т.к. оптимизатор SQLServer'а неадекватно оценивает стоимость SQLCLR-функций - они для него словно ""закрытый ящик"". Например, более дешёвый HammingDistanceX32() имеет бóльший Estimated Operator Cost, чем у LevenshteinDistanceBytes(), потому что имеет больше параметров, несмотря на то, что он примитивнее:HammingDistanceX32() ""дороже"" LevenshteinDistanceBytes()Код запроса:SELECT *
FROM       (SELECT * FROM dbo.employees WHERE dbo.HammingDistanceX32      (id, 25, 32) <= 2) AS t1
INNER JOIN (SELECT * FROM dbo.employees WHERE dbo.LevenshteinDistanceBytes(id, 25)     <= 2) AS t2 ON t1.id = t2.id
Специфика SQL(необязательно к прочтению)Индексные таблицы для 2, 3, 4... этапов HEngineВыше я утверждал, что стоимость SplitSignatureHash() больше одной HammingDistanceX32(), чтобы доказать бессмысленность промежуточных этапов HEngine, но что если мы дополнительно создадим индексированные 6/8, 14/16 таблицы для M₁ и M₂?Если мы будем использовать базовый алгоритм, то всё равно на каждом этапе нам придется выполнять дорогую агрегациюЕсли мы будем использовать ускоренный алгоритм, то количество и размер этих k-комбинаций будет расти экспоненциально, как и затраты на обслуживание индекса.ЭтапКоличество строкВес строки(байт)Вес индекса на строку(байт)2/4646/8281214/1612028В подтверждение словSELECT COUNT(*) FROM SplitSignatureHashKComb(123456, 8, 2)
SELECT COUNT(*) FROM SplitSignatureHashKComb(123456, 4, 6)
SELECT COUNT(*) FROM SplitSignatureHashKComb(123456, 2, 14)
Как минимум, по расходу памяти это неэффективно - алгоритм из п.3 окончательный.super_function()Если мы положим абсолютно всю логику по сравнению хэшей в одну воображаемую super_function() из стороннего кода, то сравнивая M₁.id-M₂.id приемлемой производительности достигнуть не получится, потому что, как уже было сказано, для SQLServer'а эта функция - ""закрытый ящик"". Оптимизатор никак не сможет построить оптимальный план - он будет вызывать super_function()  раз.Если бы мы искали, к примеру, одинаковую длину строк через стандартный LEN(), SQLServer скорее всего:№Операция1посчитает все M₁.id.LEN(), M₂.id.LEN()2отсортирует полученные множества3заmerge join'ит их булевой сортировкой, или накрайняк(если их млрд) заhash join'ит4↓5экспоненциальную задачу  он сведет к линейной Здесь простейший запрос, который невозможно свести к линейной сложности, на котором можно ""прикинуть"" длительность выполнения при использовании super_function(): xor32() - возвращает побитовый XOR двух чисел, это простейшая функция. Она отрабатывает медленнее встроенного ABS() - видимо, дополнительные затраты идут на приведение SQLServer типов к SQLCLR C# типамЗапросDECLARE @start int = 0
DECLARE @end   int = 2000
DECLARE @step  int = 1

;WITH x AS
(
   SELECT n FROM (VALUES (0),(1),(2),(3),(4),(5),(6),(7),(8),(9)) v(n)
),
series AS
(
    SELECT TOP (ROUND((@end- @start) / @step, 0) + 1)
           @start - @step + ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) * @step AS n
    FROM
       x as x1, --1           - 10
       x as x2, --11          - 100
       x as x3, --101         - 1000
       x as x4, --1001        - 10 000
       x as x5, --10 001      - 100 000
       x as x6, --100 001     - 1 000 000
       x as x7, --1 000 001   - 10 000 000
       x as x8, --10 000 001  - 100 000 000
       x as x9  --100 000 001 - 1 000 000 000
)
SELECT *
FROM       series s1
CROSS JOIN series s2
WHERE --ABS(s1.n*s2.n - 305)  = 4056/*random int*/
      dbo.xor32(s1.n, s2.n) = 4056/*random int*/
То есть фактически бóльшая часть экономии стоимости при практической реализации алгоритма достигается за счёт того, что количество вызовов функций SplitSignatureHash() линейно, а задача экспоненциальной сложности - первичный (2/4) предрасчёт расстояния Хэмминга - ложится на плечи SQL, с чем он неплохо справляется с помощью своих инструментов - индексов, merge/hash join'ов и т.д.Почему мы не можем передать внутрь SQLCLR M₁ и M₂ целиком?Таблицы в SQLServer не передаются, придется передавать через XMLСчитаем предположительный вес этой переменной: M₁.N = 100 0001 строка содержит Вес переменной XML как минимум Далее её нужно распарсить и превратить в массивы struct'ур, пригодных для циклической обработки.С M₂.N нужно проделать то же самоеОперирование такими большими переменными будет вынуждать пользоваться кучей и следить за стекомSQLServer сам включает многопоток, в C# придется прописывать вручнуюMEGRE/HASH JOIN не используется, нужно будет внутри сортироватьВ общем слишком много трудозатрат для решения с очень сомнительной производительностью, реализовывать не пробовалСолениеСолениеВ этом разделе возможно есть немного теоретической новизны(по крайней мере для русскоязычного интернета). При классическом хэшировании по сигнатуре, если строки разные, но имеют одинаковый набор уникальных букв, случится коллизия.Например Строка                          Уникальные вводные  Чумаков Максим Глебович  Ч,у,м,...м,...б     Чубаков Максим Глебович  Ч,у,б,...м,...б    Хэш должен быть качественным - обладать свойствами устойчивости к коллизиям, которые гарантируются нормальным распределением битов в битовой маске. Иначе на этапе 2/4 будет много лишних join'ов, что негативно скажется на производительности. Чтобы повысить его качество были предприняты попытки его дополнительно посолить(добавить в хэш больше уникальной информации о строке). Пробовались следующие соли, которые применялись к каждой вводной(букве) хэша:Нарушающие правило 2-ух ошибокКоличество повторений буквы во всей строкеЗамена повторяющейся буквы не может изменить хэш более чем на 4 ошибкиНапример Строка                          Уникальные вводные      Чумаков Максим Глебович  Ч*1,у*1,м*2,...б*1,...  Чубаков Максим Глебович  Ч*1,у*1,б*2,...м*1,... Количество повторений буквы в словеЗамена повторяющейся буквы в пределах слова, если нет такой же буквы с таким же кол-вом повторений в другом слове(нет такой же вводной) не может изменить хэш более чем на 4 ошибкиНапример Строка                               Уникальные вводные  Колокольцев Максим Глебович   ...л*2.........     Корокольцев Максим Глебович   ...р*1...л*1...     Колокольцев Максим Гарриевич  ...л*2...р*2...     Корокольцев Максим Гарриевич  ...р*1...р*2...    N-gramm'ыЗамена повторяющейся буквы не может изменить хэш более чем на  ошибки.Например Строка                          Уникальные вводные по 2-gramm'ам  Чумаков Максим Глебович  ...ум, ма...                      Чубаков Максим Глебович  ...уб, ба...                     Вывод:Эти соли усиливают селективность хэша, но увеличение предела допустимых ошибок в хэше экспоненциально негативно влияет на производительность:Количество ошибокЭтапКоличество строкВес строки(байт)Вес индекса на строку(байт)12/4642416/82812336114/1612028336024/8708560212/161820244368032/8284112310/16800820160160В подтверждение слов          SELECT '1' AS lev_dist,'2/4' AS stage, COUNT(*) AS cnt, MAX(LEN(iChunkKComb)) AS kComb_size, COUNT(*) * MAX(LEN(iChunkKComb)) AS row_size FROM dbo.SplitSignatureHashKComb(123456, 8, 2)  
UNION ALL SELECT '1',            '6/8',          COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 4, 6)  
UNION ALL SELECT '1',            '14/16',        COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 2, 14) 
UNION ALL SELECT '2',            '4/8',          COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 4, 4)  
UNION ALL SELECT '2',            '12/16',        COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 2, 12) 
UNION ALL SELECT '3',            '2/8',          COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 4, 2)  
UNION ALL SELECT '3',            '10/16',        COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 2, 10) 
Но, тем не менее, они могут пригодиться, если мы хотим осуществить ""поверхностный/быстрый"" поиск. Эти соли значительно ускоряют алгоритм, но становится возможным пропуск искомых совпадений. Появляется уязвимость - можно целенаправленно изменить строку так, чтобы получить ложноотрицательный результат совпадения.Сохраняющие правило 2-ух ошибокОтбирать буквы определенного количества повторений.Замена повторяющейся буквы не может изменить хэш более чем на 2 ошибки, но теряет информацию.Например Строка                              Уникальные вводные с кол-вом 2        Колокольцев Максим Глебович  ...л*3...к*2...е*2, в*2...и*2  Корокольцев Максим Глебович  ...   л*2    ...к*2...е*2, в*2...и*2  Колокольцев Максим Глебович  ...л*3...   к*2                Кококольцев Максим Глебович  ...   л*2    ...к*3 Есть идея использовать как двойной хэш, например, хэш от букв с кол-вом 1 + хэш от букв с кол-вом 2. Они вместе дают неплохую селективность, но затраты на поиск по двойному хэшу тоже значительны - придётся выборку от хэша №1 intersect'ить с выборкой от хэша №2.Индекс словаЗамена повторяющейся буквы не может изменить хэш более чем на 2 ошибки, но теряется свойство резистивности к перестановке слов в строке.Например Строка                              Уникальные вводные с кол-вом 2  Колокольцев Максим Глебович  ...л*1...л*1...и*2...и*3        Корокольцев Максим Глебович  ...р*1...л*1...и*2...и*3       Колокольцев Максим Глебович  ...К*1...                      Сококольцев Максим Глебович  ...С*1...                      Вывод: Если нам НЕ нужна резистивность к перестановке слов в строке, то соль по индексу слова считаю наиболее перспективной.Погружение в кодКак вы уже поняли из всего вышеизложенного материала, я достаточно много ""игрался"" с размером хэша, k-комбинаций, солями. Объём DDL, который мне пришлось бы писать вручную параллельно дорабатывая ""шаблон"" создания индекса изначально меня насторожил. Поэтому я сразу прибег к динамическому SQL, который в конечном счёте получился достаточно громоздким, страшным, но работающим. В конце концов, для меня главное - дать пример реализации и представление о производительности интегрированного в SQL алгоритма. Шаблон не умеет реализовывать:резистивность к перестановке слов в строке - это перегрузило бы и без того перегруженный шаблонизатор DDL'а, это можно дописать вручную при необходимости на готовый шаблон. Все нижележащие тесты проводятся без реализации этого свойства, но это не помешает сравнительному анализу хэшей.двойной/тройной хэш - это перегрузило бы и без того перегруженный шаблонизатор DDL'а, это можно дописать вручную при необходимости на готовый шаблон. Вручную дописанный в тестах показал себя неудачно.Шаблон умеет:создавать индекс с несколькими однотипными солями:classic - без солиsalt_cnt - количество повторений буквы во всей строкеsalt_cnt_per_word - количество повторений буквы в словеsalt_i_word - индекс словасоздавать индекс с несколькими хэшами с разными фильтрами вводных - для соли с отбором букв определенного количества повторенийЯ не буду здесь объяснять, как именно работает инструмент - не вижу смысла лезть в дебри генератора SQL'а - я базово опишу, что он делает и покажу как им пользоваться:Допустим, нам нужно осуществить нечёткий поиск между двумя однотипными таблицами customers и employees по шаблону first_name + '%' + patronomyc_name + '%' + last_namecustomers, employeesCREATE TABLE dbo.customers
(
    id              int IDENTITY(1, 1) PRIMARY KEY,
    first_name      nvarchar(100),
    patronomyc_name nvarchar(100),
    last_name       nvarchar(100),
    gender          nchar(1)
)
CREATE TABLE dbo.employees
(
    id              int IDENTITY(1, 1) PRIMARY KEY,
    first_name      nvarchar(100),
    patronomyc_name nvarchar(100),
    last_name       nvarchar(100),
    gender          nchar(1)
)
Для этого нам понадобятся 2 процедуры - create_SH_fuzzy_search_index и create_SH_fuzzy_search_joincreate_SH_fuzzy_search_indexПредназначена для создания индекса на конкретную таблицу. На примере customers 1-граммный 32-битный индекс этапа 2/4 (8-битные chunk'и) без солей(classic) состоит из:Таблицы для хэша с внешним ключом на customers[ix].[customers_full_name_H_1gram_x32]CREATE TABLE [ix].[customers_full_name_H_1gram_x32]
(
     [row_num] int NOT NULL
    ,[classic] varbinary(4)
    ,CONSTRAINT [PK_customers_full_name_H_1gram_x32]                      PRIMARY KEY ([row_num])
    ,CONSTRAINT [FK_customers_full_name_H_1gram_x32.row_num-customers.id] FOREIGN KEY ([row_num]) REFERENCES [dbo].[customers]([id]) ON DELETE CASCADE
)
Таблицы для k-комбинаций chunk'ов от хэша с внешним ключом на таблицу для хэша и индексами - это основная таблица, по которой будет идти первичный поиск этапа 2/4[ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4]CREATE TABLE [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4]
(
     [row_num] int NOT NULL
    ,[classic] varbinary(4)
    ,CONSTRAINT [FK_customers_full_name_H_1gram_x32_C_x8_K_2/4.row_num-customers_full_name_H_1gram_x32.row_num] FOREIGN KEY ([row_num]) REFERENCES [ix].[customers_full_name_H_1gram_x32]([row_num]) ON DELETE CASCADE
)
CREATE CLUSTERED INDEX [customers_full_name_H_1gram_x32_C_x8_K_2/4.row_num] ON [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4] ([row_num])
CREATE INDEX [customers_full_name_H_1gram_x32_C_x8_K_2/4.classic] ON [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4] ([classic])
3.Триггер на customers для заполнения таблицы для хэша. Стоит отметить, что он достаточно умный:не триггериться на строки, у которых template не изменился, чтобы не замедлять update'ы в таблице, если они не меняют templateне включает в индекс строки, у которых пустой template - множество таких строк замедляет работу алгоритма, т.к. хэши от таких template'ов одинаковы -> появляются неуникальные значения -> селективность падает[dbo].[TR_customers_full_name_H_1gram_x32]-- generated in dbo.p_create_SH_fuzzy_search_index
ALTER TRIGGER [dbo].[TR_customers_full_name_H_1gram_x32]
   ON [dbo].[customers]
   AFTER INSERT, UPDATE
AS
BEGIN
    SET NOCOUNT ON;

    DELETE FROM [ix].[customers_full_name_H_1gram_x32]
    WHERE row_num IN
    (
    SELECT I.id
    FROM      (SELECT id, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS template FROM Inserted) I
    LEFT JOIN (SELECT id, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS template FROM Deleted)  D ON D.id        = I.id
                                                                                                                                               AND D.template <> I.template
    WHERE D.id IS NOT NULL -- changed rows
    )

    INSERT INTO [ix].[customers_full_name_H_1gram_x32] ([row_num], [classic])
    SELECT I.id
          ,dbo.GetSignatureHash(I.template, 1, '%', 65001, 0/*classic*/, 0, 32)
    FROM      (SELECT id, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS template FROM Inserted) AS I
    LEFT JOIN (SELECT id, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS template FROM Deleted)  AS D ON D.id        = I.id
                                                                                                                                                  AND D.template <> I.template
    LEFT JOIN [ix].[customers_full_name_H_1gram_x32]                                                                                          AS H ON H.row_num   = I.id
    WHERE (   D.id       IS NOT NULL  -- changed rows
           OR H.row_num  IS     NULL) -- haven't been inserted yet
      AND REPLACE(I.template, '%', '') <> ''
END
4.Триггер на таблицу для хэша для заполнения таблицы k-комбинаций хэша[ix].[TR_customers_full_name_H_1gram_x32_C_x8_K_2/4]-- generated in dbo.p_create_SH_fuzzy_search_index
ALTER TRIGGER [ix].[TR_customers_full_name_H_1gram_x32_C_x8_K_2/4]
   ON [ix].[customers_full_name_H_1gram_x32]
   AFTER INSERT, UPDATE
AS
BEGIN
    SET NOCOUNT ON;

    DELETE FROM [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4] WHERE row_num IN (SELECT row_num FROM Inserted)

    INSERT INTO [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4] ([row_num], [classic])
    SELECT [row_num], [classic].iChunkKComb
    FROM Inserted
    CROSS APPLY dbo.SplitSignatureHashKComb([classic], 8, 2) AS [classic] WHERE  [classic].i = [classic].i 
END
После всех манипуляций нам остается лишь ""дёрнуть"" за любое поле customers следующим образом UPDATE dbo.customers SET Gender = Gender WHERE 1 = 1 и индекс заполнится по цепочке: 1.UPDATE 2.Триггер на customers  заполнит таблицу для хэша 3.Триггер таблицы для хэша заполнит таблицу k-комбинаций хэшаТо же самое динамическим SQL:create_SH_fuzzy_search_indexEXEC dbo.create_SH_fuzzy_search_index
    @schema_name            = 'dbo',
    @table_name             = 'customers', @table_id_field_name = 'id',
    --@table_name             = 'employees', @table_id_field_name = 'id',
    @table_mock_field_name  = 'Gender',
    @postfix                = 'full_name',
    @template               = 'ISNULL(first_name, '''') + ''%'' + ISNULL(patronomyc_name, '''') + ''%'' + ISNULL(last_name, '''')',
    @delimiter              = '%',
                            
    @h_table                = '',
    @hc_name                = '',
    @hc_table               = '',
    @h_table_join           = '',
    @hc_table_join          = '',
    @hc_table_case_col      = '',
    @hc_table_case_col_name = '',
    @row_size               = NULL,
                            
    @h_schema_name          = 'ix',
    @codepage               = '65001',
    @n                      = '1',
    @hashSize               = '32',
    @hashChunkSize          = '8',
    @kCombCount             = '2',
    @nGramHashModes         = '0',
    @salt_filter            = '0',
                            
    @is_del                 = 1,
    @top                    = 999999,
    @DEBUG                  = 0
create_SH_fuzzy_search_joinСоздаёт полноценный связочный индекс между двумя таблицами и процедуры, функции для работы с ним. Возвращаясь к нашей задаче:Запускает create_SH_fuzzy_search_index для customersЗапускает create_SH_fuzzy_search_index для employeesСоздаёт основную TVF, в которой хранится весь алгоритм поиска[ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4]/*
Generated in create_SH_fuzzy_search_join
-- usage
SELECT * FROM [ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4](1, 1) AS SH_search
*/
ALTER FUNCTION [ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4]
(
    @col_num    int = 1,
    @only_fuzzy int = 1
)
RETURNS table AS RETURN
(
 	--DECLARE @col_num int = 1, @only_fuzzy int = 1
    WITH
    residual_chunks AS
    (
        SELECT hc1.row_num AS rn1,
               hc2.row_num AS rn2
        FROM       [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4] AS hc1
        INNER JOIN [ix].[employees_full_name_H_1gram_x32_C_x8_K_2/4] AS hc2 ON (@col_num = 1 AND hc1.classic = hc2.classic)
    ),
    residual_ham_dist AS
    (
        SELECT DISTINCT
               rn1, rn2
        FROM             residual_chunks AS residual
        INNER JOIN [ix].[customers_full_name_H_1gram_x32] AS h1 ON residual.rn1 = h1.row_num
        INNER JOIN [ix].[employees_full_name_H_1gram_x32] AS h2 ON residual.rn2 = h2.row_num
        WHERE (@col_num = 1 AND dbo.HammingDistanceX32(h1.classic, h2.classic, 2) <= 2)
    ),
    residual_lev_dist AS -- query optimizer leaves this CTE for last, because it's necessary to JOIN the templates, which is what we want
    (
        SELECT residual.rn1, residual.rn2, t1.templ1, t2.templ2
        FROM       residual_ham_dist AS residual
        INNER JOIN (SELECT id AS rn1, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS templ1 FROM dbo.customers) AS t1 ON t1.rn1 = residual.rn1
        INNER JOIN (SELECT id AS rn2, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS templ2 FROM dbo.employees) AS t2 ON t2.rn2 = residual.rn2
        WHERE (@only_fuzzy = 0 AND dbo.LevenshteinDistanceString(t1.templ1, t2.templ2) <= 1)
           OR (@only_fuzzy = 1 AND dbo.LevenshteinDistanceString(t1.templ1, t2.templ2)  = 1)
    )
    SELECT * FROM residual_lev_dist
)

Создаёт вспомогательную процедуру [ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4(filter)] для удобной работы с индексом, позволяет join'ить другие таблицы и навешивать предикатыСоздаёт вспомогательную процедуру [ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4(index_size)] для анализа размера индексаСоздаёт вспомогательную процедуру [ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4(stat) для анализа селективности и заполненности хэшаТо же самое динамическим SQLcreate_SH_fuzzy_search_joinEXEC dbo.create_SH_fuzzy_search_join
    @schema_name_1           = 'dbo',
    @table_name_1            = 'customers', 
    @table_id_field_name_1   = 'id',
    @table_mock_field_name_1 = 'Gender',
    @postfix_1               = 'full_name',
    @template_1              = 'ISNULL(first_name, '''') + ''%'' + ISNULL(patronomyc_name, '''') + ''%'' + ISNULL(last_name, '''')',
    @delimiter_1             = '%',

    @schema_name_2           = 'dbo',
    @table_name_2            = 'employees', 
    @table_id_field_name_2   = 'id',
    @table_mock_field_name_2 = 'Gender',
    @postfix_2               = 'full_name',
    @template_2              = 'ISNULL(first_name, '''') + ''%'' + ISNULL(patronomyc_name, '''') + ''%'' + ISNULL(last_name, '''')',
    @delimiter_2             = '%',

    @h_schema_name           = 'ix',
    @codepage                = '65001',
    @n                       = '1',
    @hashSize                = '32',
    @hashChunkSize           = '8',
    @kCombCount              = '2',
    @nGramHashModes          = '0',
    @salt_filter             = '0',
    @lev_dist                = '1',
                 
    @is_del                  = 0,
    @top                     = 999999,
    @DEBUG                   = 0
ТестированиеКак пройти по моим стопам описано в github.com/VGoren/SHFuzzySearch/README.md. На моей машине с Intel Core i3 7gen Windows 11 x64 все тесты отрабатывают примерно за 1 час. Если у вас железо помощнее, то можно увеличить переменные @top_stat(для анализа индекса) и @top_srch(для замера производительности индекса). @top_stat меньше, т.к. вспомогательная аналитическая процедура (с постфиксом stat) достаточно затратная(не использует многопоток), в то же время данные однородны - увеличение выборки не влияет на аналитические показатели индекса.Итак, если всё сделали верно, то получите следующий результат:РезультатыОсновные поля таблицы результатов Поле                 описание                                                                                                   rn                   порядковый номер теста                                                                                     hash_name            название хэша                                                                                              salt_name            название соли                                                                                              lev_dist             расстояние Левенштейна                                                                                     data(KB)             размер данных индексных таблиц без учёта индекса(индекс весит гораздо меньше и его размер пропорционален)  data_calculated(KB)  расчётный размер данных индексных таблиц без учёта индекса                                                 time_create          время создания индекса                                                                                     combs                количество комбинаций при тестовом поиске                                                                  found                количество найденных совпадений                                                                            time_srch            время поиска                                                                                               stat_combs           количество выборочных комбинаций для анализа                                                               residual_chunks      количество комбинаций после 1, 2, 3-го этапа алгоритма  residual_chunks_%    в процентном выражении                                                                                     residual_ham_dist    количество комбинаций после 4-го этапа алгоритма  residual_ham_dist_%  в процентном выражении                                                                                     residual_lev_dist    количество комбинаций после 5-го этапа алгоритма  residual_lev_dist_%  в процентном выражении                                                                                     med_fullness         медианна заполненности хэша                                                                                med_fullness_%       в процентном выражении                                                                                     avg_fullness         среднее арифметическое заполненности хэша                                                                  avg_fullness_%       в процентном выражении                                                                                    Проанализируем результаты на примере выборки 5000 customers с 5000 employees:Анализ основных солейЕсли требуется реализовать нечёткий поиск БЕЗ резистивности к перестановке слов, то:salt_i_word - показывал себя наилучшим образом2gram + salt_i_word несплошной(2/4) - можно использовать в качестве беглого поискаЕсли требуется реализовать нечёткий поиск С резистивностью к перестановке слов, то salt_i_word использовать нельзя, поэтому практически все соли могут найти своё применение:salt_cnt_per_word - беглый поискsalt_cnt - более беглый поиск2gram несплошной(2/4) - наиболее беглый поиск2gram сплошной(4/8) - отрабатывает лучше classic'а, но весит большеДвойной хэш classic_1_2 везде проигрывает в производительности, хотя его селективность(residual_chunks_%)  слегка выше, чем у classic'а. Он теряет информацию о буквах с 3, 4, 5... повторениями. Поиск по тройному, четверному... хэшу потребует дополнительных intersect'ов, что повлечёт за собой увеличение операционной стоимости.У беглых поисков уменьшение селективности(residual_chunks_%) - не всегда обозначает   снижение истинной селективности, это может быть связано с уменьшением уязвимости к ложноотрицательным результатам - просто поиск становится более тщательным. Особенности анализа беглых поисковНа малых размерах индекса (размер хэша/количество строк в исходной таблице) прослеживается, что предельное увеличение размера хэша(hash_size) меньше предельного увеличения размера данных индексных таблиц(data(KB)) - SQLServer резервирует минимальное место под хранение метаданных, таких как информация о таблицах, столбцах, индексах и ограничениях Кривая размера индекса от размера хэшаРасчётный размер данных индексных таблиц data_calculated(KB) меньше, чем data(KB) - SQLServer оставляет свободные ""страницы"" с определённым fill-factor'ом таблицы (не путать с fill-factor'ом индекса) в файле данных для оптимизации операций с таблицей.Кривая скорости поиска от размера хэшаГрафик зависимости размера хэша(hash_size) от фактического времени поиска(time_srch) представляет из себя кривую параболического типа, на неё с двух сторон влияют следующие основные негативные факторы:При уменьшении размера хэша:Поиск по переполненному хэшу сопровождается множеством коллизионных join'ов переполненных chunk'овВероятность коллизии разных вводных внутри хэша - 1/hash_size(одна и та же буква может занять один и тот же бит). Чем сильнее селекивность(residual_chunks_%) хэша, тем губительнее для него этот фактор, поэтому максимальная скорость classic'а раскрывается на x512, а salt_i_word'а - на x1024.При увеличении размера хэша:Увеличение размера хэша, очевидно, увеличивает операционную стоимость join'ов. На слишком малых размерах хэшей проявляется слабо, потому что как SQLServer в частности, так и .NET в целом ""под капотом"" зачастую приводят значения к бóльшему типу данных - в конце концов, в современной 64-битной ОС процессор обрабатывает 64 бита за один такт.Поиск по слишком недозаполненному хэшу сопровождается множеством коллизионных join'ов вообще пустых chunk'ов. Проявляется на слишком недозаполненных хэшах.Кривая смещена по горизонтали влево. Влияние негативных факторов при уменьшении размера хэша усиливается сильнее, чем у негативных факторов при увеличении размера хэша - недозаполненный хэш лучше переполненного.Помимо основных факторов, на графике не отображены, но продолжают воздействовать дополнительные факторы: величина шаблона, уникальность данных, лингвистический аспект, количество букв в алфавите, рандом и др. Поэтому не стоит удивляться, что хэш x128 выбивается из общего ряда в худшую сторону. Возможно, на другом наборе данных он отработал бы лучше.Тестирование изменения размера текста не проводилось, но очевидно, что с его увеличением при заполнении сигнатурного хэша, мы неизбежно упрёмся в количество уникальных символов в алфавите, а при использовании солей - в статистическое сглаживание. В этом случае остается лишь одно - за вводные брать большие n-граммы, но они пропорционально увеличивают допуск ошибок в хэше. Для очень больших текстов этот поиск - не лучший выборИтого, увеличивать размера хэша(hash_size) стоит до тех пор, пока:Вас устраивает размер индексаНе перестанет увеличиваться селективность(уменьшаться residual_chunks_%)Не перестанет уменьшаться фактическое время поиска(time_srch)Да, инструмент узкоспециализированный... Надеюсь, кому-то пригодится. Спасибо за внимание!Теги:нечёткое сравнение строкрасстояние хэммингарасстояние левенштейнаhengineхэширование по сигнатуреsqlинтеграция стороннего кода в SQLmssqlsqlclrc#.netХабы:SQL.NETC#Microsoft SQL ServerАлгоритмы",265,0,0,23 мин,https://habr.com/ru/articles/965934/,43272,4656,5
GFS2 — файловая система для новой виртуализации: наш опыт интеграции в SpaceVM,SpaceVM,2025-11-13T08:00:18.000Z,"['Блог компании Space', 'Виртуализация *', 'Хранение данных *']","SpaceVM 3 часа назадGFS2 — файловая система для новой виртуализации: наш опыт интеграции в SpaceVMУровень сложностиСреднийВремя на прочтение13 минКоличество просмотров259Блог компании SpaceВиртуализация * Хранение данных * ТуториалПривет, Хабр! Меня зовут Сергей Алексанков, я технический директор Space. В этой статье расскажу о нашем опыте внедрения файловой системы GFS2 в платформу виртуализации SpaceVM.Облачные среды, отказоустойчивые кластеры и платформы виртуализации требуют от хранилищ не только надежности, но и поддержки одновременного доступа. В этих условиях традиционные файловые системы (EXT4, NTFS, XFS и др.) оказываются недостаточными — они не рассчитаны на работу с общими блочными устройствами между несколькими узлами. Одним из решений может стать кластерная файловая система, и одной из самых зрелых в этом классе является GFS2 (Global File System 2). Современные ИТ-инфраструктуры часто строятся вокруг виртуализации и облаков, где несколько серверов одновременно обращаются к одним и тем же данным. В таких системах ключевым становится не просто объем или скорость хранилища, а способ доступа к данным — общий или локальный, файловый или блочный. От того, как именно организовано взаимодействие с хранилищем, зависит архитектура всего решения: от производительности виртуальных машин до отказоустойчивости кластера.Локальные хранилища привычны для одиночных серверов: диск или массив принадлежит конкретному узлу, который управляет им напрямую. Общие (shared) хранилища, напротив, предоставляют единое пространство данных для нескольких серверов. Именно они лежат в основе высокодоступных кластеров и виртуализационных платформ, где важно, чтобы виртуальные машины могли мигрировать между узлами без потери доступа к своим дискам.Но общий доступ — это не только вопрос архитектуры, но и способа взаимодействия с данными. Файловые протоколы (NFS, SMB и др.) дают возможность работать с файлами на уровне операционной системы, но вносят дополнительные задержки и ограничения. Блочные протоколы (iSCSI, Fibre Channel) предоставляют более низкоуровневый доступ — сервер видит удаленное устройство как локальный диск. Однако при этом возникает другая проблема: как синхронизировать работу нескольких узлов с одним и тем же блочным устройством, не разрушив файловую систему?Ответ на этот вызов дают кластерные файловые системы, специально разработанные для совместного блочного доступа. Одна из самых зрелых и функциональных среди них — GFS2 (Global File System 2). В нашем опыте ее интеграция в собственный продукт - платформу виртуализации SpaceVM - позволила приблизиться к созданию устойчивой, масштабируемой и по-настоящему отказоустойчивой среды.SpaceVM — собственная разработка компании, платформа виртуализации на базе гипервизора KVM. Своего рода аналог vSphere от VMware, адаптированный под отечественные реалии и высокую степень автоматизации, это программный комплекс для управления вычислительными кластерами, автоматизации развертывания виртуальных машин и балансировки ресурсов между узлами. В его основе — идея унифицированного управления инфраструктурой: от гипервизора до сетевых и дисковых подсистем, с акцентом на прозрачность процессов и возможность интеграции с существующими ИТ-ландшафтами.С использованием GFS2 достигается эффект, сопоставимый по удобству и стабильности с использованием VMFS в продуктах VMware — с открытым исходным кодом и возможностью глубокой адаптации под нужды отечественных заказчиков.В архитектуре платформы стояла задача обеспечить общий доступ к блочному хранилищу между узлами кластера — без потери целостности данных и с возможностью одновременной работы виртуальных машин. При этом многие заказчики ожидали получить функциональность, сопоставимую с VMFS, используемой в продуктах VMware. Эти два требования сошлись в одной точке: для реализации такой модели доступа мы выбрали GFS2 — кластерную файловую систему, по принципам работы близкую к VMFS, но основанную на открытом коде и допускающую глубокую адаптацию под особенности отечественной инфраструктуры.GFS2: преимущества архитектуры и механизма блокировокGFS2 — это POSIX-совместимая кластерная файловая система, разработанная для работы с общими блочными устройствами, подключенными по Fibre Channel или iSCSI. Ее ключевая особенность — координация доступа к метаданным и содержимому файлов при помощи распределенного менеджера блокировок DLM (Distributed Lock Manager). Это позволяет гарантировать целостность данных даже при одновременном доступе с нескольких узлов. В отличие от NFS или SMB, где блокировки файлов координируются через сеть, GFS2 использует механизм локальных блокировок. Это принципиальное отличие: при сетевых файловых системах каждая операция блокировки требует удаленного вызова — будь то RPC-запрос или SMB-пакет — и ожидания ответа от сервера. Такие сетевые транзакции неизбежно добавляют задержку, особенно при высокой нагрузке или нестабильных сетевых условиях.В GFS2 же блокировки реализованы на уровне узлов кластера с использованием локальных ресурсов ОС и специализированных сервисов. Это значит, что большинство операций синхронизации выполняется без сетевых вызовов, напрямую внутри ядра или через быстрые механизмы обмена между узлами. Благодаря этому GFS2 обеспечивает более предсказуемое время отклика и лучшую масштабируемость при росте количества клиентов, чем файловые протоколы NFS или SMB.По сравнению с прямым пробросом блочных устройств в виртуальные машины, GFS2 предлагает более безопасный и управляемый способ совместного доступа к данным, не жертвуя скоростью и устойчивостью файловой системы.GFS2 демонстрирует сбалансированный и технологически обоснованный подход к организации доступа к общим данным, выгодно отличаясь как от сетевых решений вроде NFS или SMB, так и от схем прямого подключения блочных устройств без кластеризации.В сетевых файловых системах вся работа с данными происходит через удаленный сервер — каждая операция чтения, записи или блокировки требует сетевого запроса и подтверждения. Это упрощает администрирование, но создает точку отказа и ограничивает масштабирование: при росте нагрузки сервер становится узким местом, а задержки на уровне сети напрямую влияют на производительность.В противоположность этому, при прямом пробросе блочного устройства (или подход называемый SharedLVM) в несколько узлов вся логика управления данными перекладывается на хосты. Такой подход обеспечивает высокую скорость, но лишен механизмов согласования и защиты целостности: две машины могут записать разные данные в один и тот же блок, разрушив файловую систему.GFS2 решает обе проблемы одновременно. Она обеспечивает одновременный блочный доступ к устройству, но при этом использует встроенные кластерные механизмы — журналирование, распределенный менеджер блокировок (DLM) и систему метаданных, общую для всех узлов. За счет этого данные остаются согласованными, а операции синхронизации происходят быстрее, чем в сетевых файловых системах, поскольку выполняются на уровне ядра и не требуют посредничества сервера. Таким образом, GFS2 сочетает надежность и консистентность сетевых решений с низкими задержками и эффективностью блочного доступа.Второе важное преимущество — масштабируемость. Архитектура GFS2 допускает одновременный доступ к файловой системе с большого числа узлов, не требуя специальных клиентских или серверных реализаций. Это делает ее пригодной для использования как в компактных конфигурациях с 2–3 серверами, так и в полноценных кластерах с десятками узлов. При этом поддержка iSCSI и Fibre Channel дает гибкость в выборе СХД и не требует полной перестройки инфраструктуры. Третье — отказоустойчивость. В отличие от решений, где потеря одного узла может повлечь за собой повреждение данных или зависание всей файловой системы, GFS2 умеет корректно обрабатывать сбои. Механизмы fencing и quorum позволяют изолировать некорректно работающий узел, предотвращая split-brain и обеспечивая консистентность хранилища. Это особенно важно в условиях непрерывной работы платформ виртуализации, где потеря доступности даже части пула данных может повлечь каскадный отказ сервисов.Наконец, GFS2 упрощает администрирование за счет единого пространства имен и поддержки различных форматов данных. Это позволяет централизованно управлять пулами хранения и использовать привычные средства резервного копирования, миграции и диагностики. Для конечного пользователя это выражается в предсказуемом поведении системы и возможности разворачивать кластерные решения без глубокого погружения в особенности конкретных СХД или драйверов.На фоне альтернатив — GlusterFS, CEPH, Lustre — GFS2 занимает уникальное положение: это именно кластерная файловая система, работающая поверх одного общего устройства хранения, а не распределенная система с репликацией. Что делает ее особенно актуальной для сценариев, где важна экономия места, контроль над изоляцией данных и высокая предсказуемость поведения I/O.Для чего мы используем GFS2 в SpaceVMОдной из ключевых архитектурных задач в SpaceVM стало обеспечение полноценного доступа к общему хранилищу с возможностью запуска и миграции виртуальных машин на любых узлах кластера. Без использования внешней кластерной файловой системы это невозможно реализовать корректно. В виртуализированной инфраструктуре, где диски ВМ хранятся на сетевых LUN, только кластерная файловая система обеспечивает безопасный параллельный доступ с нескольких узлов, защиту от повреждения данных и поддержание высокой доступности. Именно в этом контексте в архитектуру SpaceVM была интегрирована GFS2 как штатный компонент хранилищ виртуальных машин.Благодаря GFS2, платформа получает следующие возможности, критически важные для промышленного использования:Хранение образов ВМ в виде файлов (формата qcow2) на общем устройстве, доступном всем серверам одновременно;Реализация высокой доступности (HA) за счет возможности мгновенного запуска виртуальной машины на любом из узлов без предварительной миграции её диска;Поддержка динамического перераспределения ресурсов (DRS), включая live migration, без необходимости вручную управлять блочными устройствами;Использование моментальных снимков, клонов и тонких дисков, с одновременной защитой от потерь данных при overcommit-сценариях — благодаря механизмам блокировок GFS2;Выполнение требований безопасности, предъявляемых заказчиками: например, возможность использовать атрибуты безопасности на уровне файлового слоя, чего невозможно достичь при прямом пробросе блочных устройств в ВМ.GFS2 в SpaceVM — не просто еще один способ монтировать хранилище, а полноценный архитектурный уровень, обеспечивающий устойчивость, управляемость и безопасность всей платформы.Однако простая интеграция GFS2 в SpaceVM оказалась невозможной. Нужно было добиться не только корректной работы файловой системы, но и органичной интеграции в архитектуру SpaceVM, где свои требования к кластерному транспорту, управлению ограждением узлов и мониторингу.Архитектура и технические сложностиФайловая система GFS2 не существует в изоляции — она требует работы в составе кластера и взаимодействует с рядом системных компонентов, формирующих так называемый кластерный транспорт. В этой архитектуре важны не только сама файловая система, но и сопутствующий ИТ-ландшафт, обеспечивающий согласованность операций, отказоустойчивость и синхронизацию между узлами.В первую очередь, GFS2 включает в себя менеджер DLM, который отвечает за координацию доступа к метаданным и содержимому файлов. Чтобы избежать ситуаций типа split-brain и гарантировать консистентность, применяется механизм ограждения (fencing), реализуемый через SBD (Storage-Based Death), чаще всего с использованием аппаратного watchdog-интерфейса IPMI. Файловая система GFS2 не работает автономно — она является частью кластерной инфраструктуры и требует взаимодействия с рядом системных сервисов, которые обеспечивают согласованность доступа и защиту данных. В отличие от обычных файловых систем, где единственный узел управляет диском, в кластере одновременно несколько машин читают и записывают данные на одно и то же блочное устройство. Это создает типичные распределенные проблемы — от гонок за блокировки до рассинхронизации метаданных при сбоях узлов.Чтобы избежать подобных ситуаций, GFS2 использует DLM. Он координирует доступ к метаданным и содержимому файлов, гарантируя, что ни один узел не изменит данные, пока другой с ними работает. Механизм блокировок распределенный, но выполняется внутри кластера, без участия центрального сервера, что снижает задержки и устраняет единую точку отказа.Однако даже с DLM остаются риски, связанные с частичными сбоями — например, когда узел теряет связь с сетью, но продолжает считать себя активным. Чтобы предотвратить разрушение данных в таких сценариях, применяется механизм fencing — изоляции или «отключения» проблемного узла. В GFS2 это реализуется через SBD (Storage-Based Death), который хранит управляющие метки на общем хранилище и при необходимости инициирует аппаратный перезапуск узла через интерфейс IPMI или watchdog. Таким образом, система сохраняет консистентность даже в условиях сетевых сбоев и частичных отказов, что критически важно для кластерной файловой системы.Кластерный транспорт GFS2 опирается на стек Corosync — это системный уровень, обеспечивающий коммуникацию между узлами и согласование их состояний. Важной частью надежной работы всей системы является синхронизация времени на всех узлах, обеспечиваемая через NTP: даже небольшие расхождения могут привести к рассогласованию в блокировках или ложным срабатываниям watchdog-механизма.Кроме того, для подключения LUN в инфраструктуре используются утилиты multipath-tools (в случае Fibre Channel) и open-iscsi (для iSCSI). Они обеспечивают корректное определение, маршрутизацию и управление путями к блочным устройствам, поверх которых и разворачивается файловая система GFS2.GFS2 — целостная кластерная подсистема, в которой управление доступом, синхронизацией, диагностикой и восстановлением после сбоев вынесено на уровень кластера. К системе/кластеру предъявляется ряд требований: строгое время синхронизации между узлами, устойчивость к потере сетевого трафика и возможность автоматического fencing'а при ошибках. Любые отклонения ведут к перезагрузкам узлов или развалу кластера.Трудности при развертывании GFS2: от теории к полевым реалиямХотя GFS2 — зрелая и мощная технология, ее внедрение в реальной инфраструктуре связано с рядом инженерных сложностей. Эти проблемы не всегда очевидны на этапе проектирования, но проявляются в момент масштабирования, нагрузки или интеграции в существующую платформу. Наш опыт развертывания GFS2 в составе SpaceVM показал: надежная работа кластерной файловой системы требует учета множества низкоуровневых факторов.Один из критических аспектов — синхронизация времени между узлами. GFS2 использует DLM и SBD, которые чувствительны даже к незначительным расхождениям во времени. Практика показала: если разница между узлами превышает 60 секунд, возможны некорректные решения по кворуму или ошибочные fencing-сценарии. Поэтому требуется жесткая настройка NTP-инфраструктуры с контролем точности синхронизации вплоть до миллисекунд.Следующая проблема — сетевые коллизии. При использовании iSCSI поверх общей инфраструктуры наблюдались случаи, когда трафик от виртуальных машин перекрывал каналы кластерного транспорта. Это приводило к задержкам, потере пакетов и, как следствие, рассинхронизации между узлами, развалу кворума и перезагрузкам. Особенно остро это проявлялось при объединении сетевых интерфейсов по LACP: реализация агрегации каналов у разных производителей оборудования иногда конфликтовала с работой iSCSI, вызывая нестабильные и трудноотлавливаемые ошибки. Мы отказались от LACP в пользу Active-Backup-режима, обеспечив тем самым предсказуемую работу с минимальной зависимостью от поведения сетевого оборудования.Отдельного внимания требует механизм ограждения через аппаратный watchdog. В нашем случае это IPMI-интерфейс, который должен реагировать на команды от SBD каждые 5 секунд. Однако в ряде инсталляций IPMI не успевал обработать запросы вовремя — из-за параллельных обращений от других сервисов или под нагрузкой. Это приводило к ложным fencing-сценариям: узел перезагружался без реальной причины. Решение — настройка возможности изменения таймаута watchdog’а и тщательное тестирование поведения IPMI на каждом типе оборудования.Также важно отметить трудности, возникающие при блокировке монтирования LUN. В случае некорректного завершения работы или проблем с доступом к СХД, файловая система могла зависать в состоянии ожидания, блокируя операции на уровне ядра. Требовались либо перезагрузка узла, либо использование низкоуровневых утилит для разблокировки ресурса. Подобные случаи невозможно игнорировать в продуктивной системе, и мы встроили в SpaceVM механизмы предиктивной диагностики, позволяющие заранее идентифицировать потенциальные блокировки по косвенным признакам.Интеграция GFS2 в платформу виртуализацииИнтеграция GFS2 в платформу виртуализации SpaceVM потребовала глубокого осмысления архитектуры кластерного транспорта и адаптации логики GFS2 под централизованную модель управления. В отличие от классического использования GFS2 в кластерах с ручной настройкой, здесь требовалось встроить файловую систему в оркестратор, который сам управляет вычислительными узлами, хранилищами и службами мониторинга через API и веб-интерфейс.Одна из ключевых проблем заключалась в несовместимости базовой логики GFS2 с концепцией SpaceVM. GFS2 ставит во главу угла сохранность данных: при любых ошибках, нарушениях кворума или сбоях с доступом к хранилищу система инициирует ограждение узла, то есть его принудительную перезагрузку. Напротив, SpaceVM ориентирован на непрерывную доступность ВМ и динамическую работу с узлами — даже при частичных деградациях инфраструктуры. В результате потребовалось реализовать механизм точной настройки поведения GFS2 через интерфейс платформы, чтобы у администратора была возможность балансировать между отказоустойчивостью хранения и живучестью вычислительного пула.Также было необходимо централизовать управление кластерным транспортом. В классическом варианте администратор вручную настраивает corosync, DLM, SBD, конфигурирует кольца, задает пороги кворума и поведение watchdog. В SpaceVM же вся эта конфигурация задается через API и визуальный мастер (wizard) создания GFS2-пула. Он позволяет в одном окне:выбрать диски,указать тип монтирования,настроить кластерный транспорт и fencing,развернуть файловую систему.Эта автоматизация особенно важна, поскольку GFS2 имеет высокий порог входа в плане требований к администратору. Ручная настройка — сложна и не масштабируется. Внедрение через интерфейс SpaceVM снимает барьер для широкого использования технологии.Мы внедрили централизованную валидацию настроек, автоматическое создание кластерного транспорта, подключение или отключение ограждений узлов, а также поддержку резервных сетей на базе протокола SCTP — они выбираются из UI и позволяют без прерывания работы переключаться между каналами связи.На уровне гипервизора также произошла серьезная интеграция. Контроллер SpaceVM передает вычислительным узлам команды через GRPC, синхронизируя настройки кластера, хранилищ и дисков. Все компоненты — от планировщика задач и очередей до сервисов синхронизации — взаимодействуют через собственную распределенную архитектуру. В этом окружении GFS2 стала частью общей системы: она управляется не вручную, а автоматически, через механизмы модулей Puppet, SSH-контроллеров и специализированных микросервисов.Особое внимание мы уделили типам монтирования и поведению ограждений. В GFS2 предусмотрены два основных сценария fencing’а:При потере кворума кластерного транспорта (split-brain).При ошибках записи или потере связи с блочным хранилищем.Мы реализовали возможность включать или отключать каждый из этих типов независимо, а также контролировать режим монтирования LUN (например, errors=panic или debug). В результате администратор получает не «чёрный ящик», а управляемую систему, где последствия сбоев можно смоделировать и задать заранее.Также были предприняты меры по валидации некорректных состояний:перед созданием нового транспорта проверяются конфигурации всех оставшихся узлов;при попытке монтирования система автоматически выявляет заблокированные LUN и предотвращает зависание задачи;по умолчанию отключена устаревшая опция создания дисков с полным выделением пространства (full), вместо неё используется falloc — это снижает нагрузку на GFS2 и исключает ошибки при записи.Таким образом, мы не просто подключили GFS2 как файловую систему — мы встроили её в архитектуру платформы виртуализации, превратив в управляемый, безопасный и масштабируемый компонент. Получилась не просто интеграция, а полноценная унификация двух миров: надёжного хранения и гибкой оркестрации вычислений.Оптимизация производительностиОднако даже после корректной настройки GFS2 может демонстрировать нестабильную производительность. В процессе внедрения мы протестировали и включили ряд оптимизаций:BFQ-алгоритм планирования ввода-вывода, сглаживающий пики нагрузки между ВМ;Обязательное использование virtio и hyper-v оптимизаций для улучшения взаимодействия между хостом и гостевыми ОС;Отказ от метода выделения диска full в пользу falloc — это устраняет ошибки записи и уменьшает нагрузку на подсистему хранения.Итоги: GFS2 — зрелое решение с инженерной спецификойGFS2 — мощный инструмент, который требует глубокого понимания. Его сила — в контроле над данными, гибкости и отказоустойчивости. Чтобы получить все преимущества его использования нужно глубокое понимание принципов его работы, особенно если разворачивать его «сбоку», не связывая с платформой. Сложность GFS2 — это не недостаток, а плата за мощь. Осознавая это, разработчики SpaceVM провели кропотливую работу по ее комплексной интеграции в свою платформу. В результате, то, что в изоляции воспринималось как слабость (чувствительность к среде, сложность настройки), было устранено. Пользователь получает всю силу GFS2 — контроль, гибкость, отказоустойчивость — через доступный интерфейс, автоматизацию и диагностику SpaceVM.Мы добились того, чего ждали от VMFS в отечественном исполнении: общего хранилища, совместимого с кластерами, виртуальными машинами и требованиями безопасности. А главное — доказали, что даже сложные opensource-компоненты могут органично встраиваться в коммерческие платформы, если подойти к этому как инженеры.Теги:gfs2gfsвиртуализациявиртуализация серверовхранилищахранение данныхинфраструктураvmware vspherevmfsХабы:Блог компании SpaceВиртуализацияХранение данных",259,0,0,13 мин,https://habr.com/ru/companies/spacevm/articles/965388/,22523,2805,3
Оптимизация налогообложения в игровой индустрии: как снизить расходы и защитить ключевые активы,DaniilKadyrov,2025-11-13T05:14:56.000Z,['Финансы в IT'],"DaniilKadyrov 6 часов назадОптимизация налогообложения в игровой индустрии: как снизить расходы и защитить ключевые активыУровень сложностиПростойВремя на прочтение6 минКоличество просмотров91Финансы в ITОбзорСовременная игровая индустрия давно перестала быть исключительно сферой развлечений и превратилась в мощный сегмент мировой экономики. Сегодня цифровые активы — будь то скины, внутриигровая валюта или даже целые виртуальные миры — оцениваются в миллионы долларов. Показательные примеры: продажа планеты Calypso в Entropia Universe за 6 млн долларов или виртуального Амстердама в Second Life за 50 тыс. долларов. Эти сделки демонстрируют финансовый потенциал GameDev. Но вместе с доходами неизбежно появляются и налоговые обязательства. Как разработчикам в России и за рубежом выстроить грамотную стратегию налогообложения и обеспечить защиту активов? Рассмотрим ключевые подходы.Важно отметить, сам термин «оптимизация налогообложения» многие налоговые консультанты используют с осторожностью. В отличие от классической оптимизации расходов, здесь речь идёт не о сокращении обязательств, а о выборе наиболее подходящей модели ведения бизнеса, использовании предусмотренных законом льгот и специальных режимов. Однако, не имея более точного определения, будем использовать привычный термин «оптимизация».Налогообложение внутриигровых транзакций в РоссииВ российской юрисдикции для игровой индустрии не предусмотрено отдельного налога на виртуальные активы, поэтому компании подчиняются общим нормам Налогового кодекса РФ.Основные налоговые обязательства включают:налог на прибыль — 25%;налог на добавленную стоимость (НДС) — 20%;налог на имущество — до 2,2% (ставка зависит от региона);страховые взносы — около 30% от выплат в пользу физических лиц;НДФЛ — от 13% с доходов граждан.Любая реализация внутриигровых предметов, включая проекты по модели free-to-play, рассматривается как доход, облагаемый налогом на прибыль и, в ряде случаев, НДС. С правовой точки зрения такие транзакции квалифицируются как передача прав на использование «неактивированных данных и команд» по лицензионному соглашению.Эта позиция получила официальное подтверждение в письме ФНС России от 2017 года № СД-4-3/988@, где было закреплено, что при корректном оформлении подобные операции могут освобождаться от уплаты НДС. Для разработчиков это стало важным инструментом снижения налоговой нагрузки.Кейс Mail.ru: уроки из практикиВ 2015 году налоговые органы оспорили освобождение от НДС по операциям, связанным с продажей внутриигровых предметов в проектах Mail.ru Group (Warface, «Аллоды Онлайн»). Инспекция квалифицировала такие сделки не как передачу прав на программное обеспечение, а как оказание услуг по организации игрового процесса, что влекло обязанность уплачивать НДС.Суды поддержали позицию налоговиков, что создало неблагоприятный прецедент для разработчиков. Однако в 2017 году ситуация изменилась, в письме ФНС № СД-4-3/988@ было закреплено право рассматривать подобные операции именно как передачу прав на использование программного обеспечения. Это дало студиям возможность корректно оформлять сделки и существенно снижать налоговую нагрузку.Налоговые льготы для игровой индустрииРазработчики игр могут значительно снизить фискальную нагрузку, используя предусмотренные законом льготные режимы. Рассмотрим ключевые инструменты, доступные как в России, так и за её пределами.1. IT-аккредитация в РоссииСтатус аккредитованной IT-компании, подтверждаемый Минцифры РФ, предоставляет целый ряд преимуществ:налог на прибыль снижается до 5%;страховые взносы уменьшаются до 7,6%;передача прав на ПО из реестра российского ПО освобождается от НДС.Для получения аккредитации требуется, чтобы более 70% дохода компании приходилось на IT-деятельность.Пример: студия Astrum Entertainment, разработчик проекта BLACK RUSSIA.2. Режим «Сколково»Резиденты инновационного центра «Сколково» могут рассчитывать на ещё более заметные преимущества:налог на прибыль — 0%;НДС — освобождение;страховые взносы — 14%.Ограничения: льготы действуют в течение 10 лет и прекращают применяться при выручке свыше 1 млрд рублей или прибыли более 300 млн рублей. При грамотном комбинировании с IT-аккредитацией возможно снижение страховых взносов до 7,6% при нулевом налоге на прибыль.3. Упрощённая система налогообложения (УСН)Для небольших студий эффективным инструментом становится УСН:ставка 6% от дохода — без учёта расходов;ставка 15% от дохода за вычетом расходов — с возможностью учитывать траты на разработку и оборудование.УСН освобождает от НДС при выручке до 60 млн рублей. При превышении этого порога применяются льготные ставки:5% НДС при доходе до 250 млн рублей;7% НДС при доходе до 450 млн рублей.Ограничения:годовой доход — до 450 млн рублей;численность сотрудников — до 130 человек;остаточная стоимость основных средств — не выше 150 млн рублей.4. Международные инструменты: IP Box и налоговая отсрочкаМногие игровые компании регистрируют бизнес за рубежом, чтобы воспользоваться специализированными налоговыми режимами. Наибольшей популярностью пользуются Кипр, Эстония и ОАЭ.IP Box — режим снижения налоговой нагрузки на доходы от интеллектуальной собственности (ИС):Кипр: 80% прибыли от ИС исключается из налогооблагаемой базы;ОАЭ: ставка по прибыли от ИС — 0%;Казахстан: налоговая база уменьшается на 100%.Льгота зависит от коэффициента связи (nexus ratio), который отражает долю квалифицированных затрат (например, зарплат разработчиков) в общих расходах на создание продукта.Пример: в ОАЭ при nexus ratio 80% и прибыли $1 млн налог 0% применяется к $800 тыс., а оставшиеся $200 тыс. облагаются по ставке 9%.Отсрочка уплаты налога на прибыль действует в Эстонии и Грузии, налог платится только при распределении дивидендов. Это позволяет без потерь реинвестировать доходы в новые проекты, активы или объекты ИС, что особенно выгодно для холдингов и венчурных фондов.R&D-вычетыВ Чехии и Дании расходы на разработку ПО можно списывать по повышенной ставке (например, 200%). Это снижает налогооблагаемую базу.Пример: при выручке $1 млн и расходах на R&D $300 тыс. с вычетом 200% налогооблагаемая прибыль сокращается с $700 тыс. до $400 тыс. В Чехии подобные вычеты можно переносить на будущие периоды (до трёх лет), что особенно выгодно для стартапов без текущей прибыли.Защита и сопровождение активовДля компаний игровой индустрии нематериальные активы являются фундаментом бизнеса. Исходный код, графика, игровые механики, бренд и торговые марки — всё это требует не только технической проработки, но и надёжной правовой защиты.Ключевыми инструментами в этой сфере выступают:грамотно составленные пользовательские соглашения;прозрачные политики конфиденциальности, соответствующие требованиям международных платформ и регламентов (включая GDPR);регистрация прав на объекты интеллектуальной собственности.Без этих документов невозможно выйти на международный рынок, легально монетизировать продукт или привлечь инвестиции.Особое внимание необходимо уделять формулировкам, особенно в проектах с внутриигровыми покупками, подписками или возможностью вывода средств. Некорректные условия могут привести не только к блокировке приложения в App Store или Google Play, но и к риску признания игры азартной с последующими правовыми последствиями.На практике защита активов — это не разовое оформление документов, а непрерывный процесс, включающий аудит, регулярное обновление соглашений и мониторинг регуляторных требований в различных юрисдикциях. Только комплексный подход позволяет минимизировать риски штрафов, санкций и судебных разбирательств, сохраняя при этом контроль над ключевыми элементами продукта.Риски и практические рекомендацииНалоговые спорыОколо 20% конфликтов в IT-сфере связано с вопросами налогообложения роялти и проблемой двойного налогообложения. Снижение рисков достигается за счёт продуманного структурирования бизнеса — например, регистрации компании в юрисдикциях с режимом IP Box (Кипр, Нидерланды), что позволяет оптимизировать налогооблагаемую базу.Санкции со стороны платформНесоблюдение правил App Store или Google Play — например, публикация контента, не соответствующего возрастным ограничениям, — может привести к предупреждениям, удалению или даже полной блокировке приложения. Регулярная проверка документации, аудит пользовательских соглашений и контроль контента позволяют минимизировать эти угрозы.Внутренние конфликтыОколо 30% споров в GameDev связано с разногласиями между основателями или ключевыми сотрудниками. Чётко прописанные трудовые контракты, партнёрские соглашения и документы о распределении долей существенно снижают вероятность подобных конфликтов.Выбор юрисдикцииПравильный выбор страны регистрации играет решающую роль. Кипр, Эстония и Гонконг востребованы благодаря выгодным налоговым режимам и надёжной защите интеллектуальной собственности. При этом стоит избегать юрисдикций с высокой налоговой нагрузкой (Франция, Италия) или слабой правовой защитой прав на ИС (например, Украина).Комплексное внимание к этим аспектам позволяет не только снизить риски, но и создать устойчивую основу для долгосрочного развития игровой компании.ЗаключениеСовременная разработка игр — это не только креативные идеи и технологические решения, но и выстроенная правовая устойчивость, прозрачная налоговая структура и стратегическое планирование. Успешный проект требует не только качественного кода и проработанной механики, но и корректно оформленных прав на интеллектуальную собственность, продуманной налоговой модели и строгого соблюдения требований платформ и регуляторов.В российской практике разработчики могут использовать такие инструменты, как IT-аккредитация, статус резидента «Сколково» или УСН, чтобы сократить налоговую нагрузку. Международные механизмы — от IP Box и отсрочки налога на прибыль до R&D-вычетов — дают возможность масштабировать бизнес и защищать доходы на глобальном рынке. Однако все эти инструменты эффективны лишь при условии наличия комплексной юридической инфраструктуры.Без детально проработанных пользовательских соглашений, надёжной политики конфиденциальности и зарегистрированных прав на ключевые активы даже самая популярная игра остаётся уязвимой. Поэтому юридическое сопровождение — это не второстепенный элемент, а обязательная часть стратегии выхода на рынок и устойчивого развития любой GameDev-компании.Теги:оптимизация налогообложения gamedevналоги в игровой индустрииналоговые льготы для it-компанийналоговая оптимизация it-бизнесаХабы:Финансы в IT",91,0,0,6 мин,https://habr.com/ru/articles/965894/,10422,1287,1
Архитектурный выбор: Монолит против микросервисов без технического диплома,lukyan73,2025-11-13T09:59:44.000Z,"['Анализ и проектирование систем *', 'Микросервисы *', 'Управление продуктом *', 'Управление проектами *', 'Управление разработкой *']","lukyan73 1 час назадАрхитектурный выбор: Монолит против микросервисов без технического дипломаУровень сложностиПростойВремя на прочтение3 минКоличество просмотров167Анализ и проектирование систем * Микросервисы * Управление продуктом * Управление проектами * Управление разработкой * МнениеRecovery ModeКак нетехническому специалисту участвовать в принятии решений, от которых зависят бюджет, сроки и масштабируемость продуктаАрхитектурные решения — это фундамент цифрового продукта. Выбор между монолитной и микросервисной архитектурой определяет, насколько быстро вы сможете выпускать новые функции, как будет масштабироваться бизнес и какие команды вам потребуются. Это не чисто технический вопрос, а стратегический, напрямую влияющий на финансовые и операционные показатели.Многие нетехнические специалисты — продуктологи, менеджеры, основатели стартапов — чувствуют себя исключенными из этого разговора. Их задача — не писать код, а понимать бизнес-последствия выбора и говорить с разработчиками на понятном им языке. Вот практический фреймворк, который поможет участвовать в этих обсуждениях на равных.Базовые концепции: два подхода к архитектуреМонолит — единая, неделимая система. Все компоненты (база данных, серверная и клиентская логика) тесно связаны и развертываются как одно целоеМикросервисы — набор независимых сервисов, каждый из которых отвечает за конкретную бизнес-возможность и общается с другими через четко определенные интерфейсы (API)Три критерия для принятия решения на языке бизнесаВместо споров о технологиях сосредоточьтесь на факторах, которые понятны любому руководителю.1. Структура команды: Масштабируемость против оперативной скоростиОрганизация вашей команды напрямую определяет оптимальный архитектурный выбор.Монолит эффективен для небольших сплоченных команд. Разработчики работают с единой кодовой базой, что позволяет быстро вносить изменения и оперативно решать задачи. Отсутствие необходимости согласовывать форматы взаимодействия между независимыми сервисами ускоряет разработку на ранних этапах.Микросервисы становятся оправданы при наличии нескольких автономных команд, каждая из которых фокусируется на своей зоне ответственности. Эта модель позволяет командам разрабатывать, тестировать и развертывать свои сервисы независимо. Однако попытка разрабатывать микросервисную архитектуру силами одной небольшой команды ведет к резкому росту сложности и непропорциональным временным затратам.2. Стабильность требований: Гибкость против предсказуемостиВыбирайте монолит для продуктов с быстро меняющимися требованиями — стартапы, MVP, экспериментальные направления. Это позволяет быстро итерировать и перенаправлять ресурсы, не разрывая контракты между сервисами.Микросервисы эффективны для стабильных продуктов с четкими границами доменов. Когда функциональность модуля определена на месяцы вперед, его можно выделить в отдельный сервис. Частые кросс-сервисные изменения в микросервисной архитектуре требуют значительных координационных усилий.3. Толерантность к отказам: Простота против отказоустойчивостиВ монолите отказ одного модуля часто означает остановку всей системы. Это приемлемо на ранних стадиях, когда кратковременные простои не так критичны для бизнеса.Микросервисы обеспечивают изоляцию сбоев — если один сервис недоступен, остальные продолжают работать. За эту отказоустойчивость вы платите: мониторинг десятков сервисов и обеспечение их стабильного взаимодействия требуют значительных операционных ресурсов.Ключевой вывод для бизнесаПравило простое: начинайте с монолита, эволюционируйте к микросервисам через осознанный переход.Стартапы и новые продукты: Ваша главная валюта — скорость выхода на рынок и проверка гипотез. Монолит дает вам максимальную гибкость при минимальных операционных затратах.Растущие продукты: Когда команды начинают мешать друг другу в монолите, а границы сервисов стали четкими и стабильными — это сигнал к постепенному, обоснованному переходу на микросервисы.Самые дорогостоящие архитектурные ошибки происходят, когда компания пытается построить распределенную систему без соответствующих командных структур и процессов.P.S. Если вы хотите не просто понимать такие решения, а уверенно участвовать в архитектурных обсуждениях, задавать правильные вопросы и оценивать риски — в моей книге «Птичий язык: Как говорить на языке разработчиков, не написав ни строчки кода» я подробно разбираю логику IT-архитектуры, процессы разработки и модели сотрудничества. Это поможет вам говорить с техническими специалистами на одном языке и принимать взвешенные совместные решения.Теги:монолитархитектурамикросервисыХабы:Анализ и проектирование системМикросервисыУправление продуктомУправление проектамиУправление разработкой",167,0,0,3 мин,https://habr.com/ru/articles/966028/,4695,544,5
Как поменять улыбку без масштабного лечения?,docdeti_docmed,2025-11-13T10:33:45.000Z,"['Блог компании Сеть клиник docmed и docdent', 'Здоровье', 'Научно-популярное']","docdeti_docmed 42 минуты назадКак поменять улыбку без масштабного лечения?Время на прочтение1 минКоличество просмотров175Блог компании Сеть клиник docmed и docdentЗдоровьеНаучно-популярноеКейсБывает так, что улыбка не нравится, и хочется всё изменить, но... прямо сейчас нет возможности провести большое лечение. Но это не повод ходить грустить и не улыбаться. Как можно поменять улыбку рассказывает и показывает стоматолог-ортопед Мария СпивакДевушка обратилась с жалобами на внешний вид передних зубов: ей не нравилась и форма и их состояниеФото ""до""На осмотре мы определили ряд проблем: старые несостоятельные реставрациикариозные полостивыраженная стираемость передних зубовОбсудили с пациенткой план лечения и других зубов — с возможными удалениями, установкой имплантов и коронок.Но пока девушка не готова к масштабному лечению, решили начать с малого и заняться передними зубами, которые её беспокоят. Так бывает, и задача врача — услышать пациента и подобрать адекватные альтернативы, которые улучшат жизнь человека прямо сейчасЯ предложила промежуточный этап, который устроил пациентку, и мы воплотили план в жизньЧто сделали? удалили все старые реставрации с передних зубовпролечили кариозные полостивосстановили цвет и эстетику композитными винирамиФорму и цвет подбирали вместе с пациенткойВ будущем планируем провести большую работу со всеми зубами: с удалениями, имплантами и коронками. Композитные реставрации придётся менять, и пациентка к этому готова. Впереди тотальная реабилитация, но несколько лет красивой улыбки мы ""выиграли"", и сейчас девушка улыбается открытоПолучить красивую улыбку можно уже сейчас, не откладывая зубные проблемы ""на потом"". А над более сложными этапами планомерно работать вместе с врачом Теги:улыбказубвинирстоматологияХабы:Блог компании Сеть клиник docmed и docdentЗдоровьеНаучно-популярное",175,0,0,1 мин,https://habr.com/ru/companies/docmed_docdent/articles/965800/,1837,233,3
"Почему простые фичи — самые сложные: история о пет-проекте, Дженге и маржинальной торговле",impatient,2025-11-13T05:16:22.000Z,"['Java *', 'Программирование *', 'Финансы в IT']","impatient 6 часов назадПочему простые фичи — самые сложные: история о пет-проекте, Дженге и маржинальной торговлеУровень сложностиСреднийВремя на прочтение9 минКоличество просмотров538Java * Программирование * Финансы в ITИз песочницыПривет, Хабр! Меня зовут Иван, и сегодня я хочу поделиться историей о своём пет-проекте A-Zero. Истории про провалы традиционно интереснее историй об успехах, и моя как раз такая (почти). Довольно бодроначинавшийся проект чуть было не свёл меня с ума из‑за одной единственной фичи, «просочившейся» в MVP, и сейчас я расскажу, как я из этого выкарабкался и чему научился по дороге.Дисклеймер: в тексте присутствует некоторое количество терминов, относящихся к трейдингу. Для удобства не столь искушённого читателя большинство из них снабжены всплывающими подсказками с пояснениями.Мир, где всё простоВсё началось со слегка безумной идеи написать с нуля фреймворк для алгоритмического крипто-трейдинга в качестве сольного пет-проекта. Я понимал, что единственный шанс для начинающего Java-разработчика (меня) преуспеть в таком нелёгком деле — чётко следовать принципу итеративной сложности. Нужно было начать с чего-то совсем элементарного и очень маленькими порциями достраивать функционал — как будто играешь в Дженгу и боишься, что от каждой следующей палочки всё рухнет.Поначалу всё шло довольно бодро, и мы с проектом дожили до релиза 0.1.0 — он состоял из CLI утилиты для выгрузки исторических данных, плюс я успел прикрутить небольшой CI пайплайн. Вскоре у меня уже были готовы основные интерфейсы и базовая реализация движка для бэктестинга, который умел моделировать спотовые трейды. Казалось, что фундамент моей башни в Дженге заложен, и теперь осталось только докладывать на него палочки-фичи, а на горизонте уже замаячил следующий релиз с полноценной утилитой для бэктестинга — нужно было только реализовать логику описания стратегий и написать поверх всего этого CLI-обёртку.Стоит сказать, что слово ""базовая"" в применении к реализации движка тут не было скромностью — он действительно был безумно простым и использовал очень базовую логику:Один счёт (double balance).Только одна открытая позиция в один момент времени.Простая логика: купил — баланс уменьшился, продал — увеличился.И вот тут предыдущий успех, видимо, заставил меня расслабиться, и...Кроличья нораЯ решил, что такой релиз получится уж слишком скучным и примитивным. А вот если добавить в него всего одну «палочку» — шорт‑трейды... Тут, каюсь, сыграли роль сразу два фактора: во‑первых, отсутствие у меня чёткого плана по MVP, во‑вторых — немного замутнившиеся воспоминания из моего прошлого трейдерского опыта о том, как трейдинг, собственно, устроен. Поэтому я практически машинально добавил в модель стратегии возможность шортить активы, и только потом уже задумался, рассчитан ли на это мой движок — но обо всём по порядку.Изначально мне казалось, что добавление шорт‑трейдов — фича чисто номинальная, и на логику программы в целом не повлияет. Первым звоночком стало нарушение модели баланса (double balance). Дело в том, что изначально она была призвана играть двоякую роль: с одной стороны — отражать покупательную способность в ходе симуляции (то есть какие трейды мы можем себе позволить), с другой — показывать состояние нашего капитала, то есть сколько мы заработали/потеряли.Но при открытии шорта мы не тратим, а получаем средства — и, наоборот, теряем при закрытии. Так модель double balance моментально перестала работать. «Ничего, просто чуть‑чуть усложним модель!» — оптимистично подумал я и взял в руки следующую палочку Дженги: полноценный Map<String, Double> кошелёк, отслеживающий баланс (в т.ч. отрицательный) каждого имеющегося актива.Теперь я мог учитывать, что при открытии шорта для базовой валюты баланс уменьшается, а для валюты котировки — увеличивается. Также пришлось добавить движку два режима симуляции: маржинальная торговля и спотовая торговля, а ещё поддерживать в симуляции неограниченное количество открытых позиций.И вот тут я ощутил, что начинаю падать в кроличью нору маржинальной торговли. Теперь для реалистичной симуляции мне нужно было рассчитывать ещё и такие вещи как:Залог: Просто так взять актив в долг нельзя — биржа рассчитывает требуемый объём обеспечения заёма имеющимися у трейдера средствами. В современных крипто-биржах существует концепт ""объединённого торгового аккаунта"", для которых это обеспечение рассчитывается исходя из балансов всех активов на аккаунте. Биржи предоставляют некоторую документацию касательно алгоритма этого расчёта, которую мне пришлось изучить, а затем практически полностью переписывать логику исполнения ордеров.// Рассчитываем необходимую начальную маржу для новой позиции
BigDecimal imr = calculateInitialMarginRate();
BigDecimal newMarginRequired = positionValue.multiply(imr);
BigDecimal totalEquity = calculateTotalEquity(this.currentPrices);
BigDecimal existingMargin = calculateTotalInitialMargin();

// Проверяем, достаточно ли у трейдера общей эквити для открытия
if (totalEquity.subtract(existingMargin).compareTo(newMarginRequired) < 0) {
    log.warn(
        ""MARGIN CHECK FAILED: Cannot open {} position for {}. Required: {}, Available: {}"",
        direction, symbol, newMarginRequired, totalEquity.subtract(existingMargin));
    return;
}Поддерживающая маржа: Помимо расчёта обеспечения в момент заёма актива, постоянно проверяется, что общая стоимость активов трейдера не упала ниже уровня поддержания маржи (зависит от общего объёма заёмных активов; обычно равен определённой доле от изначального обеспечения при заёме). Нужно было добавить дополнительную логику на каждом цикле симуляции.// Обновляем актуальную информацию о ценах активов
context.updateCurrentPrices(currentPrices);

// С помощью хелперов проверяем, нужно ли запускать ликвидацию
if (config.getAccountMode() == AccountMode.MARGIN) {
    if (context.isMarginCallTriggered()) {
        context.liquidateAllPositions();
    }
}Принудительная ликвидация: Самая страшная и самая важная часть. Если баланс активов падает ниже уровня поддерживающей маржи (см. предыдущий пункт), происходит margin call — биржа начинает принудительно продавать активы трейдера, чтобы покрыть недостаток обеспечения. Это необходимо было реализовать, чтобы симуляция была ""честной"".Сложнее всего было, конечно, остановиться: можно было провести ещё неопределённое количество времени в попытках сделать симуляцию всё более и более реалистичной, но в какой-то момент я сказал себе ""стоп"". Во-первых, это нарушало бы принцип итеративной сложности. Во-вторых, очень сильно замедлило бы меня. В-третьих, не было особенно осмысленным: бэктестер призван служить первой ""дешёвой"" ступенью анализа, а следующий этап — демо-трейдинг, реализация которого у меня есть дальше в дорожной карте, — в любом случае решил бы проблему с неточной симуляцией. В итоге я уговорил себя сделать следующие упрощения:Явную формулу для расчёта объёма обеспечения заёма найти не удалось (точнее, формула из документации банально расходилась с тем, что я видел на самой бирже), поэтому в движке я использовал заведомо более «агрессивную» формулу — это значило, что движок позволит стратегии взять в долг чуть меньше, чем, возможно, позволила бы биржа (что безопасно).Вместо каскадной ликвидации (продажи активов в определённом биржей порядке) при принудительной ликвидации движок моделирует закрытие всех позиций — на мой взгляд, это простительная неточность, так как принудительная ликвидация сама по себе означает «фейл» стратегии, и моделировать реальные последствия не так критично для реализма.На руинах APIРазобравшись с тем, что фактически стало полным рефакторингом движка для бэктестинга, я обнаружил, что, помимо изменений в логике, теперь у меня полностью разрушились контракты API. Вот пара примеров:Пример 1: Эволюция TradingContext. Изначально это был крайне минималистичный интерфейс, описывающий взаимодействие стратегии с ""биржей"". В начале стало очевидно, что примитивные методы executeBuy/Sell плохо смотрятся в контексте нескольких режимов торговли (спот и маржинальная) — вместо них появился более абстрактный и семантичный submitOrder. После этого, во многом в процессе тестирования, стало понятно, что интерфейс слишком закрытый — нужен гораздо более широкой read-only доступ к состоянию аккаунта. Здесь помогла ментальная модель ""TradingContext — абстракция над веб-интерфейсом криптобиржи"". Так появились методы для получения состояния кошелька, общей стоимости активов на аккаунте и т.д.Пример 2: Эволюция Strategy и рождение MarketEvent. В сценарии, где стратегия взаимодействовала с несколькими активами одновременно, простой метод onCandle(Candle c, ...) уже не работал — тип Candle (свеча) был намеренно сделан минималистичным и не давал контекста о том, по какому активу получена информация. Из этого родился новый тип MarketEvent — ""обёртка"" над свечой и символом актива, а интерфейс Strategy теперь содержал onMarketEvent(MarketEvent event, ...). Так стратегия получала всю нужную её внутренней логике информацию — а ещё это сделало интерфейс более гибким: при необходимости я мог бы расширить тип MarketEvent, не меняя контракты API.С одной стороны, можно с уверенностью сказать, что это были правильные и необходимые изменения. С другой — они происходили абсолютно не в том порядке, в котором я хотел бы их вносить. Вместо ""отлично, работает, давайте улучшать"" получилось ""оно сейчас развалится, если я ничего не сделаю"".TDD?Неожиданно для меня там, где в борьбе с потихоньку наступавшим в проекте хаосом паттерны проектирования мне уже не помогали, на помощь пришли тесты — но не совсем в привычном их понимании.Из-за внезапного бурного разрастания бизнес-логики стало сложно держать в голове не только то, как именно она работает, но даже то, что она вообще говоря должна делать. И здесь тесты оказались прекрасным инструментом — не для того, чтобы верифицировать поведение, а для того, чтобы его прояснять. Фактически, я пытался декларативно описывать желаемое поведение через тесты (почти как в Test-Driven Development), чтобы затем на падающих тестах смотреть, в чём именно ошибка — в бизнес-логике или в моих от неё ожиданиях.Например, при тестировании движка для бэктестинга неожиданно стал падать тест spot_shortAttempt_ShouldThrowException. Вместо исключения при попытке в режиме спот-торговли продать актив, которого нет на балансе аккаунта, система... Не делала ничего. Проверив, по какой ветке идёт бизнес-логика, я выяснил, что вместо того, чтобы обрабатывать такой ордер как шорт-трейд, движок воспринимал его как попытку продать актив в большем количестве, чем есть у трейдера — и просто игнорировала его, выводя предупреждающее сообщение.Поведение системы оказалось правильнее моего собственного понимания — там, где я ожидал, что невозможная операция приведёт к ошибке, движок просто корректно её игнорировал. В этот момент я понял, что тестами можно не просто ловить баги и отслеживать, где логика работает не так, как задумано — хорошо написанные тесты помогают выявить места, где логика изначально была задумана неправильно.Отдельным вызовом при тестировании стала дилемма ""инкапсуляция vs. тестируемость"". Поскольку многие компоненты были довольно сложными stateful объектами, тестировать их без верификации внутреннего состояния было практически бессмысленно — но как сделать внутреннее состояние доступным в тестах, не засоряя публичный API? Решением стали аккуратно подобранные package-private методы, создававшие специальное тестовое API с минимальной необходимой ""площадью покрытия"":// package-private метод для мониторинга завершённых трейдов в тестах
List<Trade> getExecutedTradesForTest() {
    return List.copyOf(this.executedTrades); // Возвращяем безопасную immutable копию
}Хэппи эндВ итоге, спустя пару недель рефакторинга и доработки, релиз 0.2.0 был готов. Помимо CLI-бэктестера и YAML формата для описания трейдинговых стратегий, в нём теперь было гораздо более надёжное, гибкое и близкое к реальности ядро в виде бэктест-движка и API-контрактов. Но самым ценным для меня, пожалуй, стали не фичи, а сам опыт, который я приобрёл в процессе разработки:Беспощадное ""M"" в MVP. Невероятно трудно в процессе написания кода не хвататься за каждую возможность что-нибудь ""улучшить"" и добавить в свою башню в Дженге ещё одну палочку. Но, как показал мой кейс, очень важно иметь дисциплину этого всё-таки не делать — иначе всю башню придётся бесконечно пересобирать заново. Правильным решением в моём случае было бы остановиться в MVP на реализации логики спотовой торговли, а потом итеративно усложнять уже функционирующий движок.Упрощай, прежде чем усложнять. В процессе добавления в движок логики маржинальной торговли в какой-то момент начало казаться, что каждая новая реализованная концепция тянет за собой ещё две нереализованных. Для продуктивной разработки гораздо ценнее иметь что-то простое и работающее, поэтому было важно ""провести черту"" в том, насколько точно я хочу симулировать реальность. Логику всегда можно усложнить впоследствии — по результатам тестов, которые покажут, где именно эти усложнения действительно необходимы. А для этого нужно сначала создать что-то, что уже можно будет тестировать.Тесты как инструмент прояснения, а не только проверки. Я не могу с чистой совестью назвать свой подход реальным TDD — всё-таки тесты писались уже после того, как была написана основная масса логики. И всё-таки в критический момент именно тесты как раз оказались таким островком предсказуемости и стабильности, благодаря которому проект не развалился. Можно сказать, что в ходе разработки у меня самопроизвольно зародился некий паттерн TDC — ""Test Driven Clarification"". И я уверен, что обязательно прибегну к этой практике в дальнейших этапах работы над моим проектом.Спасибо, что дочитали мою первую публикацию до конца :-) Буду рад услышать ваши мысли и критику в комментариях. Расскажите, какие «простые» фичи стопорили ваши проекты?Весь код ядра проекта A-Zero открыт и доступен на GitHub.Теги:трейдингпет-проектХабы:JavaПрограммированиеФинансы в IT",538,0,0,9 мин,https://habr.com/ru/articles/965896/,13957,1864,3
Go-to-Community вместо Go-to-Market,badcasedaily1,2025-11-12T13:12:31.000Z,"['Блог компании OTUS', 'Developer Relations *', 'Управление продуктом *']","badcasedaily1 22 часа назадGo-to-Community вместо Go-to-MarketУровень сложностиСреднийВремя на прочтение17 минКоличество просмотров181Блог компании OTUSDeveloper Relations * Управление продуктом * ОбзорПривет, Хабр! Сегодня поговорим про стратегию Go‑to‑Community вместо Go‑to‑Market. Звучит конечно круто, но суть простая: перестать видеть разработчиков только как лидов в воронке продаж и начать работать с ними как с сообществом на равных, с созданием ценности для всех. Go-to-Market vs Go-to-Community: в чем разница?Для начала небольшой ликбез. Go‑to‑Market (GTM) это традиционный подход вывода продукта на рынок. Маркетологи гонят рекламу, собирают лиды, ведут их по воронке (от узнавания — к интересу — к триалу — к покупке). Цель GTM — захватить максимальную ценность (value capture) из аудитории: сконвертировать как можно больше людей в клиентов и продажи. Вы наверняка такое видели.Go‑to‑Community (GTC) — альтернативный (и дополняющий) путь. Проще говоря, вместо того чтобы на каждом шаге пытаться выудить из аудитории пользу для себя, мы сначала создаем ценность вместе с сообществом и для сообщества. Мы привлекаем вокруг продукта широкое техническое коммьюнити, даже тех, кто прямо сейчас ничего не купит и не принесёт денег. Пусть люди учатся, обмениваются знаниями, придумывают интеграции, помогают друг другу, а там, глядишь, со временем часть из них созреет и до продаж. Да и не только продажи важны, лояльное сообщество будет поддерживать продукт, создавать контент, рекомендовать его коллегам. Маркетинг ловит лишь тех, кто готов купить, а комьюнити охватывает всех, кому интересна тема продукта, и вовлекает их на своих условиях.GTC не противоречит GTM, а дополняет его. Никто не отменяет воронку продаж, просто параллельно выстраивается воронка сообщества, и они должны работать синхронно. Идеальный сценарий: обе стратегии согласованы, и участники сообщества постепенно переходят в разряд лидов, когда придёт время. Пока же они остаются в комьюнити, получают там пользу и сами вносят вклад. Если же стратегии разрознены, то конечно будет перекос. Либо вы бескорыстно вкладываетесь в комьюнити без всякой выгоды для компании (отличная благотворительность, но бизнес это долго не выдержит), либо зациклены на продажах и игнорируете остальных фанатов продукта (упускаете кучу возможностей и сами того не зная отталкиваете людей). Нужно равновесие.При согласованной стратегии Go‑to‑Community подпитывает Go‑to‑Market воронку. Разрозненные стратегии ведут к перекосу, либо ценность создаётся только для сообщества, либо упор только на продажи без вовлечения комьюнити.Ставку на сообщество делают многие успешные технологические компании. В 2021-м сразу несколько «единорогов» с сообществом в ДНК вышли на IPO, вспомним хотя бы GitLab, HashiCorp, Duolingo. Они смогли превратить комьюнити вокруг своих продуктов в реальный драйвер роста и выручки. Например, у HashiCorp открытое сообщество не было чем‑то второстепенным, оно с первых дней определяло архитектуру продуктов, монетизацию и всю стратегию компании.Выходит, Go‑to‑Community не благотворительность, а инвестиция. Маркетинг приносит рост до определённого предела, дальше без сообщества не выехать. Разработчики больше доверяют друг другу, чем рекламе, любят сами пробовать и учиться. Поэтому стоит задача создать среду, где люди получают ценность: знания, поддержку, возможность влиять на продукт. Тогда сообщество само станет вашим маркетингом. Удовлетворённые участники начнут советовать решение коллегам, писать статьи и туториалы, расширять продукт под свои нужды, словом, делать ту работу, за которую маркетинг платит бюджетами.Роли в сообществе и их цепочки ценностиДопустим вы решились сместить фокус с агрессивного маркетинга на комьюнити. Возникает вопрос: «А что вообще из себя представляет мое сообщество? Кто эти люди и как с ними работать?» Ошибка в том, чтобы считать комьюнити однородной массой. На самом деле внутри есть несколько ролей. Люди по‑разному взаимодействуют с вашим продуктом и вносят разный вклад. В этой статье выделим три роли: мейнтейнеры, интеграторы и эдьютейтеры (расскажу, что имеется в виду). Для каждой опишем её мотивацию и цепочку ценности, то есть, какую ценность эта роль приносит проекту и что сама получает взамен. Мейнтейнеры (Maintainers) – хранители кодаЕсли ваш продукт связан с open‑source или имеет бесплатное ядро, наверняка есть люди, отвечающие за поддержание и развитие этого проекта, помимо вашей команды. Это и есть мейнтейнеры: разработчики, которые на постоянной основе вкладываются в код, ревьюят чужие контрибьюты, следят за качеством. Чаще всего это либо сотрудники компании, либо ключевые внешние энтузиасты, заслужившие доверие. Их главная цель сделать проект лучше для всех пользователей. Мотивация обычно техническая и идеологическая: мейнтейнеры хотят, чтобы продукт решал их (и не только их) задачи эффективно, был надежным, соответствовал видению. Часто они сами начинали этот проект или присоединились, потому что горят этой технологией.Ценность для компании: мейнтейнеры фактически ваши добровольные разработчики и архитекторы. Они фиксят баги, пилят фичи, держат все в порядке. По сути, без них проект бы загнулся от перегрузки. Хороший мейнтейнер экономит компании кучу ресурсов, обеспечивая качество продукта и доверие сообщества. Кроме того, мейнтейнеры мост между компанией и широким кругом контрибьюторов, они направляют новых участников, формируют культуру проекта. В идеале мейнтейнер из комьюнити становится настоящим лидером мнений по вашему продукту. Его одобрение или критика очень влияют на репутацию проекта среди разработчиков.Ценность для мейнтейнера: а что ему с этого? Разные люди находят разную выгоду. Кто‑то просто решает свои задачи. Кто‑то прокачивает навыки, строит карьеру в опенсорсе, статус мейнтейнера известного проекта дорогого стоит на рынке труда. Кто‑то получает моральное удовлетворение и уважение коллег. Иногда бывают и прямые выгоды: компания может спонсировать ключевых мейнтейнеров, донатить им, приглашать на конференции. HashiCorp, например, на заре развития своих OSS‑продуктов активно нанимала внешних контрибьюторов на работу и спонсировала их проекты. В итоге многие мейнтейнеры стали сотрудниками HashiCorp, классический win‑win, когда энтузиаст получает стабильную работу над любимым детищем, а компания лояльного эксперта.Как работать с мейнтейнерами: прежде всего, уважать и признавать их вклад. Ваша комьюнити‑стратегия должна явно давать мейнтейнерам место и голос. Простые шаги: регулярно благодарить публично, давать статус (например, звание Core Contributor, доступ в приватный Slack с командой). Прислушиваться, звать в совет проекта, собирать фидбэк перед релизами. Предоставить ресурсы: может быть, вы поможете им с инфраструктурой для тестирования, оплатите облако, пришлёте мерч. Сюда же со‑creation активности: проводите кодовые спринты вместе с мейнтейнерами, брейнштормьте фичи. Если есть возможность, выделите бюджет на гранты или part‑time контракты для особо ценных мейнтейнеров. Словом, встроите их в свою ценностную цепочку: мейнтейнеры дают проекту свой труд, а компания возвращает им поддержку и возможности. Тогда они будут еще больше заинтересованы развивать продукт, и вокруг них подтянутся другие контрибьюторы.Интеграторы (Integrators) – двигатели экосистемыПод интеграторами я понимаю всех, кто внедряет ваш продукт в реальных решениях и интегрирует его с другими системами. Это могут быть разработчики в сторонних компания, которые внедряют вашу библиотеку/платформу у себя в продакшене. Либо авторы плагинов, расширений, SDK для вашего продукта. Либо партнеры — консалтеры, системные интеграторы, делающие комплексные проекты на базе вашего решения. По сути, это продвинутые пользователи, которые не просто «пощупали» продукт, а глубоко его используют и зачастую расширяют функциональность под свои нужды.Ценность для компании: интеграторы — те самые люди, которые находят новым технологиям реальные применения. Они показывают, как продукт вписывается в разные use‑case, часто в связке с другим софтом. Например, кто‑то сделал открытый коннектор, чтобы ваша платформа работала с Kafka, и вот у вас целый новый сценарий использования для целой группы потенциальных клиентов. Интеграторы фактически расширяют рынок вашего продукта, создают вокруг него экосистему. Многие SaaS и платформы выстрелили именно благодаря сообществу, написавшему кучу плагинов и интеграций (взять ту же VS Code, тысячи расширений написаны внешними девелоперами). Кроме того, интеграторы часто становятся адвокатами продукта: раз они встроили его в своё решение, то будут убеждать и других в его ценности. Они же первыми ловят узкие места API, дают глубочайший фидбэк, могут помочь ответами на форумах. Ценность для интегратора: эти ребята обычно решают прикладные задачи. Их главный профит — рабочее решение проблемы. Если ваш продукт помог закрыть потребность — интегратор уже выиграл. Но сверх того: интегратор вкладывает время, чтобы проектировать архитектуру, писать код интеграции, и ему важно, чтобы усилия были оценены. Например, если он сделал плагин, видеть, что сообщество им пользуется, получить звезд на GitHub, благодарности, может даже клиентов. Многие интеграторы по совместительству партнеры в бизнес‑смысле. Компания‑разработчик может направлять клиентов к сертифицированным интеграторам для внедрения. Тогда интегратор зарабатывает как эксперт. Либо вы откроете свой Marketplace расширений и разрешите авторам монетизировать плагины. Либо хотя бы упомянете их кейс в блогах/на конференциях, что приносит признание. Короче, интегратору важно, чтобы его работа была востребована и приносила ему выгоду, будь то деньги, репутация или новые возможности.Как работать с интеграторами: в первую очередь, облегчить им жизнь технически. Документация, стабильные API, SDK — это база. Сделайте отличные примеры интеграции, опишите case studies, чтобы новым людям было проще повторить успех. Создайте каналы связи: технические каналы поддержки специально для интеграторов (Slack/форум с инженерами). Хороший ход запустить программу партнеров/интеграторов. Например, HashiCorp помимо сообщества юзеров имеет сеть системных интеграторов и облачных провайдеров, которые обучают и поддерживают новых клиентов. Их вовлекают: дают материалы, возможно, сертификации. Да, кстати, сертификация важный мотиватор. Если интегратор может получить статус «Certified Expert по продукту X», он с большим энтузиазмом погрузится (HashiCorp выдала уже 20k сертификатов через свою обучающую платформу). Не забудьте про витрину успехов: рассказывайте о решениях, которые делают интеграторы. Например, раздел на сайте «Built with OurProduct», чтобы все видели, какие крутые штуки делают люди. И, конечно, обратная связь: регулярно спрашивайте интеграторов, что улучшить. Может, проведите совместный co‑creation спринт: соберите самых активных внедренцев и ваших инженеров, и за пару дней допилите вместе интеграцию с популярным инструментом — они принесут экспертизу домена, вы — ресурсы разработки. Эдьютейтеры (Edutainers) – евангелисты и учителяТретья важнейшая группа — люди, которые обучают и вдохновляют остальных пользователей. Назовем их условно эдьютейтеры (от education + entertainer): авторы статей, туториалов, докладчики на митапах, ютуберы, создатели курсов. Про них часто говорят «Developer Advocates», «Evangelists», но мы сейчас имеем в виду внешних энтузиастов, не штатных деврелов компании. В каждом активном сообществе находятся личности, которые обожают рассказывать другим, как пользоваться технологией, делиться своим опытом, упрощать сложное.Ценность для компании: эти люди — настоящие мультипликаторы знаний. Благодаря им даже небольшой проект может получать непропорционально широкое внимание. Написал кто‑то толковый гайд на медиуме и сотни новых разработчиков узнали о вашем инструменте. Записал обзор на YouTube, тысячи посмотрели и заинтересовались. Контент от сообщества бьет все рекорды доверия: он независимый, «от такого же разработчика, как я». А если у вас ещё и своя площадка для контента… DigitalOcean выезжает на том, что тысячи авторов публикуют обучающие статьи на их платформе, кучу туториалов создали эту экосистему знаний вокруг продукта. Это работает лучше любой рекламы и SEO: люди приходят за решением проблемы и попутно узнают о вашем бренде. Помимо привлечения новых пользователей, эдьютейтеры очень помогают с onboarding, ускоряют активацию новичков. Хороший видеоурок или примеры от опытного пользователя сокращают время, за которое начинающий разработчик получит первый результат. А чем быстрее он увидит пользу, тем больше шанс, что останется с продуктом. В итоге эдьютейтеры снижают нагрузку на вашу команду и масштабируют охват аудитории.Ценность для эдьютейтера: многие делают это из страсти, нравится им делиться. Но обычно есть и расчёт: создание контента добавляет личного бренда. Стать известным спикером, набрать подписчиков, получить статус эксперта — всё это ценно для карьеры. Плюс банальное человеческое спасибо: когда твоя статья собирает апвоуты и благодарности, это мотивирует. Некоторые получают и материальное вознаграждение. Кто‑то монетизирует YouTube‑канал или платные курсы. В любом случае, эдьютейтеры ищут аудиторию и признание. И им гораздо приятнее сотрудничать с компанией, которая их ценит, чем делать все втуне.Как работать с эдьютейтерами: находить и вдохновлять их. Выявляйте активных авторов в сообществе: кто пишет блоги, отвечает на Stack Overflow, делает демо‑проекты. Начните с простого: репостните их статью в своих соцсетях, похвалите в рассылке. Дайте почувствовать, что компания видит их вклад. Далее можно формализовать. Многие запускают программы амбассадоров. Например, HashiCorp запустила HashiCorp Ambassador Program, сейчас там 100+ человек со всего мира. Отбор по критерию: делится знаниями, помогает другим, проявляет экспертизу. Амбассадоры получают официальный статус, мерч, а главное эксклюзивную инфу: брифинги о новых релизах, превью фич, закрытые сессии с командой. Это отличный стимул для эдьютейтеров, они чувствуют себя инсайдерами, первыми узнают новости и могут готовить контент заранее. Плюс им просто приятно быть в клубе причастных. Ваша задача — сделать так, чтобы создавать контент по вашему продукту было легко и выгодно. Предоставьте материалы: готовые презентации, демо‑проекты, библиотеку изображений. Запустите конкурс статей или хакатон по созданию туториалов, с призами и публикацией лучших работ. Организуйте мероприятия, где эдьютейтеры смогут выступить: митапы, вебинары. Поощряйте их рост: может, кто‑то из них созреет стать официальным Developer Advocate в вашей команде — прекрасный вариант рекрутинга из сообщества.Подытожим сегментацию: в сообществе есть разные роли, и у каждой своя цепочка ценности. Мейнтейнеры улучшают продукт и получают поддержку и признание. Интеграторы расширяют сферу применения продукта и получают решения для своих задач (плюс статус экспертов и, возможно, бизнес‑возможности). Эдьютейтеры распространяют знания и получают аудиторию и благодарность. Эти цепочки переплетены: обучающие статьи привлекают новых интеграторов, хорошие интеграции разгружают мейнтейнеров от просьб о фичах, мейнтейнеры дают материал для новых статей и так далее Наша задача как DevRel‑стратегов — поддерживать баланс, чтобы каждый тип участников чувствовал: вклад окупается, ему есть смысл дальше участвовать. Для этого пригодятся специальные методики, о которых далее.Методы DevRel 2.0: value chain mapping, persona-jobs, co-creationКогда мы поняли, кто наше сообщество и что ценно для разных людей, стоит применить несколько инструментов. Расскажу о трех: community value chain mapping, persona‑jobs и co‑creation спринты. Community Value Chain Mapping – карта ценности сообществаЗвучит мудрено, но идея простая: картирование цепочки ценности означает явным образом расписать, как каждая роль в сообществе создает ценность и получает её обратно. Фактически, мы частично это сделали выше в описании ролей. Зачем нужна такая карта? Чтобы убедиться, что нигде не образуется разрыв. Если обнаружим, что какая‑то группа дает ценности больше, чем получает (или наоборот — много получает, но мало отдает), можно внести коррективы в программу работы с комьюнити.Как это сделать на практике: возьмите роли (персоны) — мейнтейнер, интегратор, эдьютейтер. Для каждой нарисуйте две колонки: «Что он дает проекту» и «Что проект (компания) дает ему». Подробно перечислите пункты. Например:Мейнтейнер дает: время на разработку, ревью кода, отвечает на issues, направляет архитектуру. Получает: влияние на развитие продукта, благодарность сообщества, поддержку ресурсами (в идеале финансами), повышение статуса.Интегратор дает: новые кейсы использования, обратную связь, готовые интеграции/плагины для других пользователей, экспертные ответы новичкам. Получает: решение своих бизнес‑задач, улучшение продукта под свои нужды, признание как эксперта, возможно клиентов/доход через партнерство.Эдьютейтер дает: контент (статьи, доклады, примеры кода), обучает новых юзеров, увеличивает охват аудитории, снижает нагрузку на техподдержку. Получает: популярность в сообществе, прямую благодарность от аудитории, эксклюзивный доступ к информации, мерч/призы, карьерные возможности.Если выяснится, что интеграторы у нас очень ценны, а мы им почти ничего не предлагаем, надо думать, как увеличить для них отдачу. Или наоборот, мы всем дарим мерч и даём статус амбассадора, а толку от человека ноль, надо условия изменить, требовать минимальный вклад. Эта же карта ценности помогает обосновать руководству бюджет на комьюнити‑программы: вы показываете, какую работу выполняет каждая группа пользователей, и что нужно вложить, чтобы это продолжалось. По сути, value chain mapping переводит мягкое «строим отношения» в понятный бизнес‑язык «мы инвестируем X и получаем Y ценности». В open‑source мире такой подход уже применяется для оценки устойчивости проектов, упоминается создание «симбиотической цепочки ценности», где участники взаимно выгодно связаны.Рекомендую пересматривать карту ценности хотя бы раз в год. По мере роста сообщества могут появляться новые роли (например, отдельным сегментом выделятся дизайнеры или студенты), добавляйте их в карту. Так вы не упустите новую аудиторию. Кроме того, на основе этой карты удобно строить метрики: если знаете, что ценность дает, скажем, количество написанных комьюнити‑статей, можете отслеживать этот показатель и целенаправленно его растить инициативами.Persona-Jobs – объединяем портреты и задачиТеперь о методе persona‑jobs. Он объединяет классический подход персонажей (persona) с фреймворком Jobs‑to‑be‑Done (JTBD, «работы, которые хотят выполнить пользователи»). Идея пришла из продуктового маркетинга: чтобы лучше понять потребности, полезно описать не только «кто наш пользователь», но и «какую работу он нанимает наш продукт выполнить». В контексте DevRel и сообществ это означает: для каждой ключевой персоны из нашего комьюнити мы прописываем её конкретные задачи/цели, с которыми она к нам приходит, и проблемы, мешающие их достичь.Мы берем нашу персону — например, «Интегратор Игорь» (можно даже придать образ: архитектор 35 лет, в большой компании, отвечает за внедрение технологий). Выписываем, какие у него Jobs‑to‑be‑Done относительно нашего продукта. Допустим интегрировать библиотеку в существующую систему без простоев; убедиться в безопасности решения для продакшена; обучить команду пользоваться новым инструментом; и тому подобное Для каждой такой задачи укажем, что ему помогает или мешает. Возможно, у Игоря боль в нехватке документации по масштабированию, или бюрократия на согласование новых технологий. Проделав это, мы увидим, как лучше помочь интеграторам: например, сделать whitepaper «Как убедить менеджмент внедрить OurProduct» или добавить раздел доки про безопасность. Точно так же делаем для мейнтейнера Марины (job: привлечь новых контрибьюторов, автоматизировать релизы, и так далее) и эдьютейтера Евгения (job: быстро разбираться в новых фичах, получать благодарную аудиторию, иметь доступ к примерам из реальной практики, и так далее).Персона сама по себе фокусируется на кто наш пользователь, каков его контекст и мотивы. А Jobs‑to‑be‑Done фокусируется на что пользователь пытается сделать и почему. Вместе они позволяют выйти за рамки стереотипов. Например, если смотреть только на персону «DevOps инженер, 5 лет опыта, такого‑то возраста», мы можем упустить, что конкретно ему нужно от нашего сообщества. А если смотреть только на абстрактный «job: получить ответ на вопрос по настройке CI/CD», упустим контекст, новичок это или эксперт, как он предпочитает учиться (читать, смотреть видео, задавать в чате?). Persona‑JTBD гибрид учитывает и то, и другое.В итоге получаем очень интересные инсайты. Например, выясняется, что молодые разработчики (персона: Студент Саша) хотят прокачать навыки (job: найти pet‑проект и ментора). Тогда вам стоит запустить для них программу стажировок в опенсорс‑проекте сообщества или выделить «good first issues». А, скажем, Solution‑архитекторы в компаниях (персона: Архитектор Антон) хотят делиться экспертизой (job: выступать на конференциях, чтобы признали). Значит, их можно вовлекать модераторами вебинаров, авторами гостевых постов, то есть давать площадку для самореализации в рамках вашего комьюнити. Применять persona‑jobs можно на этапе планирования DevRel‑инициатив. Собираетесь сделать хакатон, подумайте, для каких персон и каких «jobs» он вообще нужен. Закрывает ли он задачу мейнтейнера (например, собрать новых контрибьюторов)? Или нацелен на эдьютейтеров (дать им показать свой проект)? Если не понимаете, для кого стараетесь, возможно, мероприятие получится пустым. А когда явно видишь: для такой‑то персоны решаем такую‑то задачу, успех измеряется легко. Co-creation спринты – совместное творчество с комьюнитиПоследний метод — co‑creation спринты, то бишь совместные короткие циклы разработки/творчества с участием сообщества. Идея навеяна дизайн‑спринтами и хакатонами, но с упором на совместную работу команды продукта и внешних участников. Если перевести дословно — спринты с со‑творчеством. Это могут быть разные форматы:Community Hackathon, классика: вы объявляете тему (например, расширения для вашего API), собираете команды из внешних разработчиков и своих менторов, и за выходные они пилят готовые проекты. В отличие от обычного хакатона, здесь важно участие ваших инженеров бок о бок с комьюнити — это ломает барьеры «разработчик vs компания». Все становятся коллегами на пару дней.Documentation Sprint, узконаправленный спринт, когда собираются техписатели, эдьютейтеры, разработчики и дружно улучшают документацию или обучающие материалы. Feedback/Design Sprint — это когда сообщество участвует в проектировании новых фич. Например, у вас назрел большой релиз, проведите двухдневный спринт с наиболее вовлеченными пользователями и мейнтейнерами. В первый день соберите боль и хотелки (что нужно улучшить, какие use‑case не покрыты), во второй совместно приоритизируйте и набросайте макеты решений. Можно даже прототипировать вместе. Content Sprint, похож на докатон, но шире по форматам. Например, объявляете «Writing Sprint» на неделю: каждый день даёте тему (пн — установка продукта, вт — кейсы интеграции, ср — разбор ошибок и тому подобное), участники пишут небольшие заметки или снимают скринкасты. В конце недели готов целый пакет контента от сообщества для сообщества.Ключевое в co‑creation спринтах это не соревнование (как часто бывает в хакатонах), а именно сотрудничество. Здесь уместно убрать соревновательность и делать упор на общий результат. Как в Open Source, все коммитят в один проект. Роли можно распределять: кто‑то кодит, кто‑то тестирует, кто‑то пишет документацию. Это очень сплочает. Люди чувствуют: «Мы вместе сделали что‑то крутое, и наш вклад встроен прямо в продукт/доку/базу знаний».Эффект от таких спринтов потрясающий. Во‑первых, куча полезных артефактов, новые фичи, улучшенные доки, примеры, плагины. Во‑вторых, вы выращиваете новых лидеров: тот, кто блеснул на спринте идеей или решением, потом наверняка станет еще активнее в сообществе. В‑третьих, это привлекает внимание более широкой аудитории, результаты спринта можно анонсить, хвастаться, мол вот, сообщество вместе с нами сделало релиз. Для участников co‑creation это тоже реклама и признание.Планируя co‑creation sprint, четко обозначьте цель и формат. Люди должны понимать, что на выходе. Желательно ограничить время (не более недели, а лучше 1–3 дня). И обязательно отпразднуйте результаты, финальный демо‑день, список всех авторов, сертификаты участникам, сувениры, словом, отметьте вклад каждого. Тогда в следующий раз желающих будет ещё больше.Конечно, полностью заменить классический маркетинг на одну только работу с сообществом не получится. Да и не нужно. Правильный шаг — интегрировать GTC и GTM. Например, KPI маркетинга и DevRel должны быть взаимосвязаны. Сообща решайте, как перевести рост активности в сообществе в бизнес‑метрики: «community members → leads → клиенты». Куда приятнее иметь дело с технологией, вокруг которой есть дружное сообщество, где твой вклад ценят, где можно учиться и расти вместе. Такие продукты мы выбираем сердцем, и остаёмся с ними надолго. Так что стройте сообщества, а не только воронки продаж. Если у вас есть опыт внедрения подобных стратегий, расскажите в комментариях.Чтобы превратить GTC из концепции в работающий процесс, присмотритесь к курсу OTUS «DevRel». В программе — EVP/EJM, стратегия HR-бренда, метрики комьюнити и отчётность, контент- и event-практики, работа с амбассадорами и внешними сообществами на реальных кейсах. Если хотите понять формат обучения — записывайтесь на бесплатные демо-уроки от преподавателей курса:20 ноября: «DevRel и HR на практике: формула успешных мероприятий для разработчиков». Записаться25 ноября: «DevRel и HR-метрики: какие показатели будут важны стейкхолдерам в 2026 году». ЗаписатьсяТеги:devrelgtm.gtcGo-to-CommunityGo-to-Marketсообщество разработчиковворонка сообществаметрики комьюнитиHR-брендХабы:Блог компании OTUSDeveloper RelationsУправление продуктом",181,0,0,17 мин,https://habr.com/ru/companies/otus/articles/961436/,26000,3438,3
Без дизассемблера: как предварительный анализ документа GOFFEE раскрыл всю цепочку заражения,remadev,2025-11-13T08:18:22.000Z,['Информационная безопасность *'],"remadev 3 часа назадБез дизассемблера: как предварительный анализ документа GOFFEE раскрыл всю цепочку зараженияУровень сложностиПростойВремя на прочтение5 минКоличество просмотров238Информационная безопасность * Из песочницыКиберугрозы постоянно эволюционируют, и для эффективного противодействия важно понимать тактики и инструменты злоумышленников. Группировка GOFFEE, также известная как Paper Werewolf, представляет собой яркий пример такой угрозы:    АспектОписаниеПериод активностиС начала 2022 года по настоящее время.Основные целиОрганизации на территории Российской Федерации, в частности: медиа- и   телекоммуникации, строительные, государственные структуры, энергетические   компании.Основной вектор атакЦелевой фишинг (Spear-phishing) с   вредоносными вложениями.Ключевые инструментыPowerModul, PowerTaskel, модифицированный файл explorer.exe, Owowa (вредоносный   модуль IIS).GOFFEE демонстрирует высокую адаптивность, постоянно обновляя свои схемы заражения. Исследователи отмечают, что во второй половине 2024 года группировка активно внедряла свой имплант PowerModul, написанный на PowerShell, который способен загружать и выполнять дополнительные скрипты с командного сервера. Наряду с этим, GOFFEE использует инструменты для похищения данных с USB-накопителей, такие как FlashFileGrabber, и даже распространяет свою нагрузку через зараженные съемные носители. Несколько атак с использованием Zero-day уязвимостей было замечено летом 2025 года.    В этом материале мы разберем один из вредоносных документов этой группировки без применения глубоких знаний в реверс-инжиниринге и убедимся, что большую часть цепочки заражения можно восстановить, не прибегая к сложным инструментам вроде дизассемблера или отладчика. Меня зовут Александр, я вирусный аналитик и реверс инженер.Приступим! ДисклеймерСразу хочу отметить, что все нижеперечисленные действия необходимо выполнять в изолированной среде.    Этап 1. Анализ входных данных    Всё начинается с письма с вложением. По легенде аналитики SOC обнаружили, как сотрудникам компании был направлен следующий документ:    Прежде, чем открывать его и смотреть на поведение ОС, можно поискать информацию в открытых источниках. Например, по вычисленной хэш-сумме файла можно проверить, встречался ли он в публичных онлайн-песочницах. Для этого отлично подходит утилита HashMyFiles. После открытия файла в утилите, можно увидеть следующую картину:    Полученный хэш SHA-256: 8f7c38804d63d89a83d11c5c112850febf6d5e302f63f367860e78ac72f09e4c    Поискав этот хэш в VirusTotal, мы выяснили, что документ уже числится во многих антивирусных базах:    Подсказка «Code Insights» помогает догадаться, что внутри документа зашит какой-то макрос. Но что делать, если информации о вредоносном документе ещё не появилось? Правильно, анализировать вручную! Для анализа метаданных документа можно воспользоваться утилитой ExifTool. В выводе видим много полезной информации:Следующим этапом с помощью скрипта oleid определим структуру вредоносного образца. Как видно из вывода работы скрипта, исследуемый файл содержит макросы VBA:    Далее с помощью утилиты olevba можно получить сам макрос с целью его дальнейшего глубокого изучения:    На данном этапе мы определили, что исследуемый файл представляет собой  документ (.docx) Misrosoft Word, содержащий в себе Macros VBA. Извлекли макрос с помощью утилиты olevba. Следующим шагом при глубоком анализе нам необходимо использовать инструменты для просмотра и отладки VBS-макросов (что в рамках текущей статьи мы делать не будем).Также не стоит забывать, что файл DOCX — это не монолитный файл, а, по сути, архив в формате ZIP, содержащий набор XML-файлов и других ресурсов, которые вместе описывают содержимое, стили, настройки и медиа-файлы документа. Его можно разархивировать обычным 7-Zip. Чуть более подробно про структуру .docx файла можно почитать здесь. Нам же интересен единственный файл – document.xml - ""cердце"" документа. Здесь хранится весь текстовый контент в виде XML-тегов. Абзацы, таблицы, пробелы и сам текст. В самом конце этого файла можно обнаружить странную сигнатуру, с закодированной в Base64 текст, разделенный ключевым словом «Checksum»:Декодировав строки, мы обнаружим следующие интересные зацепки: В первом случае код представляет собой обфусцированный вредоносный скрипт, создающий файлы UserCacheHelper.lnk.js и UserCache.ini в директории %USERPROFILE%. Также для создания скрытых процессов скрипт использует WMI.    В случае с декодированной последовательностью после «Checksum», можно наблюдать ещё одну Base64-закодированную строку, и, судя по переменной $code, в ней содержится какая-то исполняемая часть, и здесь мы попали прямо в точку:Судя по всему, этот код создает постоянное соединение с командным сервером для удаленного управления зараженной системой.Этап 2. Поведенческий анализ    Пришло время запустить вредоносный образец, чтобы посмотреть в динамике на его поведение. Для этого будем использовать утилиты Process Monitor, System Informer и Fakenet-NG. Запустим их. После запуска файла видим закодированный текст:Чтобы отобразить весь текст, MS Word предлагает нажать кнопку «Enable Content». System Informer отобразил появившийся процесс Winword.exe и его PID = 2264. Поставим фильтр в Process Monitor по этому PID и начнем запись событий. Далее отобразим весь контент:  Process Monitor сразу же отобразил следующие события:Видим создание двух файлов в директории %USERPROFILE% - UserCache.ini.hta и UserCache.ini. В эти файлы записана как раз та нагрузка, которую мы обнаружили в ходе анализа document.xml. Также в каталоге %USERPROFILE% мы можем наблюдать файл UserCacheHelper.lnk.js:    Этот js файл маскируется под ярлык (на это указывает расширение .lnk). Внутри можем наблюдать ещё один обфусцированный js, который, как несложно догадаться, выполняет скрытый запуск файла UserCache.ini через WMI.    Зачем WMI?С помощью WMI вредонос запускается вне текущего дерева процесса, что значительно усложняет поиск нужного процесса. Также опция ShowWindow = 0 запускает процесс в «скрытом» окне    Теперь проверим как вредонос закрепляется в системе с помощью AutorunsFakenet-NG также зафиксировал подозрительную активность на адресе 45.84.1[.]150 – правда подключиться к этому адресу не удастся:    По исследованиям специалистов из Лаборатории Касперского ответы с этого адреса содержали скрипты для управления зараженным устройством.Этап 3. Собираем всё вместеПроведя поэтапный анализ, мы смогли восстановить полную цепочку заражения, используя исключительно базовые методики предварительного анализа — без дизассемблера и отладчика. Давайте соберём все элементы пазла в единую картину атаки:Цепочка компрометации группировки GOFFEE:Фишинговое письмо → Документ maldoc.doc с социальной инженерией (имитация официального письма) T1566, T1589;Активация макроса → Пользователь нажимает ""Enable Content"" T1059.005; Извлечение нагрузки → Макрос декодирует Base64-строки из document.xml T1027 T1059.005;Создание файлов → В %USERPROFILE% появляются:UserCache.ini (PowerShell-лоадер) T1059.001;UserCache.ini.hta (исполнитель);UserCacheHelper.lnk.js (обфусцированный JavaScript) T1059.007Закрепление → HTA-файл регистрируется в автозагрузке реестра T1547.001;Запуск импланта → Через цепочку HTA → JS → WMI выполняется PowerModul T1059.003, T1047;Установление связи → подключение к C2: http[:]//45.84.1[.]150:80 T1071.001.Теги:goffeepowermodulmalware analysisХабы:Информационная безопасность",238,0,0,5 мин,https://habr.com/ru/articles/965970/,7422,898,1
Забудьте про print(): Современное и красивое логирование в Python с помощью Loguru,enamored_poc,2025-11-13T10:19:30.000Z,['Python *'],"enamored_poc 48 минут назадЗабудьте про print(): Современное и красивое логирование в Python с помощью LoguruУровень сложностиСреднийВремя на прочтение14 минКоличество просмотров330Python * ОбзорВведение: Боль и страдания от print() и стандартного loggingЕсли вы пишете на Python, скорее всего, ваша карьера разработчика начиналась с одной простой, но незаменимой команды — print(). Нужно проверить значение переменной? print(my_variable). Хотите убедиться, что функция вообще вызвалась? print(""Я внутри функции!""). Этот метод прост, интуитивно понятен и кажется верным другом в мире отладки.Но дружба эта длится ровно до первого серьезного проекта. Внезапно оказывается, что ваш терминал завален десятками отладочных сообщений, и вы уже не понимаете, какое из них к чему относится. Вы начинаете писать print(""--- HERE ---""), чтобы хоть как-то ориентироваться в этом хаосе. А когда приходит время выкатывать код в продакшен, вы судорожно ищете и комментируете все свои print(), надеясь не пропустить ни одного.В этот момент опытные коллеги (или статьи в интернете) говорят вам: ""Для этого есть стандартный модуль logging!"". И они правы. logging — это мощный, гибкий и правильный инструмент. Но давайте будем честны, его настройка часто напоминает бюрократическую процедуру. Чтобы просто начать писать логи в файл с указанием времени и уровня, нужно написать что-то вроде этого:import logging

# Настройка... снова и снова
logging.basicConfig(
    level=logging.INFO,
    format=""%(asctime)s - %(levelname)s - %(message)s"",
    handlers=[
        logging.FileHandler(""debug.log""),
        logging.StreamHandler()
    ]
)

logging.info(""Пользователь вошел в систему."")
logging.warning(""Не удалось найти файл конфигурации."")
Это работает, но это громоздко. Пять-шесть строк кода только для того, чтобы начать. А если вам понадобится ротация файлов, кастомные форматы или что-то посложнее? Конфигурация становится еще запутаннее.Именно в этот момент на сцену выходит Loguru. Эта библиотека была создана для того, чтобы избавить нас от страданий. Она берет лучшее из двух миров: феноменальную простоту print() и всю мощь взрослого логирования, но без лишнего ""шума"" и сложной настройки.Что, если я скажу вам, что цветное логирование в консоль, запись в файл с автоматической ротацией и сжатием, а также невероятно удобная отладка исключений могут быть настроены всего одной строкой кода?В этой статье мы раз и навсегда забудем про print() для отладки и посмотрим, как Loguru может сделать ваше логирование не только мощным, но и по-настоящему красивым и удобным.Часть 1: Первое знакомство с Loguru — магия ""из коробки""��так, мы оставили позади боль от print() и сложность logging. Давайте посмотрим, что предлагает Loguru и почему его называют ""логированием для удовольствия"".Установка: один шаг до магииКак и положено современному Python-пакету, установка предельно проста и выполняется одной командой в терминале:pip install loguru
Всё, библиотека готова к работе. Никаких зависимостей, никаких сложных настроек окружения.""Hello, Loguru!"": начинаем за 5 секундПомните, сколько кода нужно было для базовой настройки logging? Забудьте. С Loguru вы просто импортируете готовый к использованию объект logger и начинаете его использовать.from loguru import logger

logger.info(""Hello, Loguru!"")
logger.warning(""Это предупреждение, будьте внимательны."")
logger.debug(""Это сообщение не будет видно по умолчанию."")
Запустите этот код, и вы увидите в консоли что-то вроде этого:2025-11-13 13:00:00.000 | INFO     | __main__:__main__:3 - Hello, Loguru!
2025-11-13 13:00:00.000 | WARNING  | __main__:__main__:4 - Это предупреждение, будьте внимательны.
Обратите внимание: debug-сообщение не появилось. По умолчанию Loguru, как и любая серьезная система логирования, показывает сообщения уровня INFO и выше. Это правильное поведение, которое избавляет от лишнего шума в консоли.Разбираем стандартный вывод: всё, что нужно, и ничего лишнегоДавайте внимательно посмотрим на первую строку вывода:2025-11-13 13:00:00.000 | INFO | __main__:__main__:3 - Hello, Loguru!Даже без единой строчки конфигурации Loguru сразу предоставляет нам богатый контекст:Дата и время: 2025-11-13 13:00:00.000 — точная временная метка, когда произошло событие. Больше не нужно гадать, к какому моменту времени относится print.Уровень лога: INFO — показывает важность сообщения. WARNING говорит о потенциальной проблеме, ERROR — о серьезной ошибке.Местоположение: __main__:__main__:3 — это самая полезная часть. Loguru автоматически указывает имя файла, функцию и номер строки, откуда был сделан вызов. Это невероятно ускоряет отладку!Сообщение: Hello, Loguru! — непосредственно то, что мы хотели записать.Магия цвета: прощай, монотонная консоль!Если вы запустите приведенный выше код в терминале, который поддерживает цвета (а сегодня это делают почти все), вы увидите еще одну приятную особенность: вывод будет раскрашен!(Примечание: в статье можно вставить реальный скриншот)INFO обычным белым.WARNING — заметным желтым.ERROR и CRITICAL — тревожно-красным.Это не просто украшение. Цвета помогают мгновенно сканировать лог глазами и находить проблемы, не вчитываясь в каждое слово. Это та самая ""красота"" и ""удобство"", которых так не хватает стандартным инструментам.Всего один import — и мы уже получили цветное, информативное и правильно структурированное логирование. Это и есть магия Loguru ""из коробки"". А ведь мы еще даже не начали знакомиться с его основными возможностямиЧасть 2: Основные возможности на практических примерахМы убедились, что Loguru прекрасен ""из коробки"". Но его настоящая сила раскрывается, когда мы начинаем использовать его основ��ые функции. И здесь Loguru остается верен своему принципу: максимум пользы при минимуме кода.Уровни логирования: управляем потоком информацииКак мы уже видели, Loguru поддерживает стандартные уровни важности сообщений:TRACE (самый подробный)DEBUGINFOSUCCESS (приятное дополнение для успешных операций)WARNINGERRORCRITICALПо умолчанию в консоль выводятся сообщения от INFO и выше. Это легко изменить. Например, чтобы включить DEBUG-сообщения, нужно перенастроить стандартный обработчик (sink), указав новый уровень:from loguru import logger

# Удаляем стандартный обработчик и добавляем новый с уровнем DEBUG
logger.remove()
logger.add(sys.stderr, level=""DEBUG"") # sys.stderr - это стандартный поток ошибок (консоль)

logger.debug(""Теперь это сообщение будет видно!"")
logger.info(""И это, конечно, тоже."")
Запись логов в файл: одна строка, чтобы править всемиЭто одна из самых мощных и востребованных функций. Забудьте про FileHandler и его сложную настройку. С Loguru все делается одной командой — logger.add().Просто записать в файл:from loguru import logger

logger.add(""my_app.log"")

logger.info(""Это сообщение попадет и в консоль, и в файл my_app.log"")
Но настоящая магия начинается, когда мы добавляем параметры в эту команду.Автоматическая ротация файлов (Rotation)Ваш лог-файл не будет расти бесконечно, занимая все место на диске. Loguru может автоматически создавать новый файл, когда старый достигает определенного размера или проходит определенное время.# Создавать новый файл, как только текущий достигнет 500 MB
logger.add(""big_file.log"", rotation=""500 MB"")

# Создавать новый файл каждую неделю
logger.add(""weekly_log.log"", rotation=""1 week"")

# Создавать новый файл каждый день в полночь
logger.add(""daily_log.log"", rotation=""00:00"")
Автоматическая очистка старых логов (Retention)Чтобы старые логи не копились вечно, можно указать, как долго их хранить.# Хранить логи за последние 10 дней
logger.add(""cleaned_log.log"", retention=""10 days"")
Автоматическое сжатие (Compression)Loguru может даже самостоятельно архивировать старые лог-файлы, чтобы они занимали меньше места.# При ротации сжимать старые файлы в zip-архив
logger.add(""compressed_log.log"", rotation=""10 MB"", compression=""zip"")
Давайте соберем все вместе в один мощный пример для реального проекта:from loguru import logger

# Настраиваем логирование для продакшена:
# - Пишем в файл `prod.log`
# - Уровень - INFO и выше
# - Ротация при достижении 10 MB
# - Храним файлы 1 месяц
# - Сжимаем старые логи в .gz
logger.add(""prod.log"", level=""INFO"", rotation=""10 MB"", retention=""1 month"", compression=""gz"")

logger.info(""Система запущена и готова к работе!"")
Всего одна строка кода заменяет десятки строк конфигурации стандартного logging. Это невероятно удобно.Форматирование в стиле f-stringЕще одна мелочь, которая делает жизнь разработчика проще. Loguru использует для форматирования строк фигурные скобки, как в f-strings, что гораздо читабельнее и привычнее, чем старый %-стиль.user_id = 123
status = ""success""

# Привычный и читаемый синтаксис
logger.info(""Аутентификация для пользователя {id} прошла со статусом: {status}"", id=user_id, status=status)
Вывод будет таким: ... | INFO | ... - Аутентификация для пользователя 123 прошла со статусом: successМы рассмотрели базовые, но самые часто используемые возможности Loguru. Уже на этом этапе видно, насколько он упрощает и ускоряет разработку. Но это еще не все — в следующей части мы погрузимся в продвинутые техники, которые сделают вашу отладку по-настоящему волшебной.Часть 3: Продвинутые техники для настоящих профиМы освоили основы, которые уже делают работу с логами приятнее. Но Loguru был создан не только для удобства — он предоставляет мощнейшие инструменты для отладки, которые могут сэкономить вам часы поиска ошибок. Давайте рассмотрим функции, которые отличают новичка от профессионала.1. Идеальная отладка исключенийЭто, пожалуй, главная ""киллер-фича"" Loguru. Стандартный трейсбэк в Python информативен, но часто хочется видеть значения переменных, которые привели к ошибке.Логирование внутри try...exceptВо-первых, вы можете использовать logger.exception(), который автоматически захватит и красиво отформатирует информацию о последнем исключении.from loguru import logger

try:
    result = 10 / 0
except ZeroDivisionError:
    logger.exception(""Произошла ошибка при вычислении."")
Вывод будет содержать не только ваше сообщение, но и полный, аккуратно отформатированный трейсбэк ошибки.Декоратор @logger.catch — ваша секретная кнопка ""Найти ошибку""Но настоящий прорыв — это декоратор @logger.catch. Вы можете просто ""обернуть"" в него любую функцию, и Loguru автоматически поймает любое исключение, которое в ней произойдет, и выведет самый подробный отчет из возможных.Посмотрите на этот пример:from loguru import logger

@logger.catch
def calculate(a, b, c):
    return a / b + c

calculate(10, 0, 5) # Эта строка вызовет ошибку
Без @logger.catch вы бы получили стандартный трейсбэк. Но с декоратором Loguru выведет в консоль нечто гораздо более ценное:> 2025-11-13 13:00:00.000 | ERROR    | __main__:calculate:5 - An error has been caught in function 'calculate', process 'MainProcess' (1234), thread 'MainThread' (5678):
Traceback (most recent call last):
...
  File ""my_script.py"", line 6, in <module>
    calculate(10, 0, 5)
    │         │  │  └ c = 5
    │         │  └ b = 0
    │         └ a = 10
  File ""my_script.py"", line 5, in calculate
    return a / b + c
           ──┘ └───
            │   └ 0
            └ 10

ZeroDivisionError: division by zero
Посмотрите внимательно: Loguru не просто показал, где произошла ошибка, он показал значения всех аргументов (a = 10, b = 0, c = 5) в момент падения! Это бесценная информация для отладки, которую вы получаете, добавив всего одну строку кода (@logger.catch).2. Кастомизация формата логовСтандартный формат хорош, но иногда его нужно адаптировать под требования проекта. Это легко сделать с помощью параметра format в logger.add().Вы можете собрать свой формат из готовых ""кирпичиков"":from loguru import logger
import sys

# Удаляем стандартный обработчик, чтобы не было дублирования
logger.remove()

# Создаем свой, очень подробный формат
custom_format = (
    ""<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | ""
    ""<level>{level: <8}</level> | ""
    ""<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>""
)

logger.add(sys.stderr, format=custom_format)

logger.info(""Лог в кастомном формате."")
Здесь мы добавили теги для цвета (<green>, <level>) и указали новые поля, например, {function} и {name}. Полный список доступных полей есть в официальной документации Loguru.3. Структурированное логирование (JSON)В современных системах логи часто собираются и анализируются машинами (например, в ELK Stack, Graylog или Datadog). Для этого они должны быть в структурированном формате, чаще всего — JSON.С Loguru это делается элементарно — добавлением одного параметра: serialize=True.from loguru import logger

logger.add(""structured_data.log"", serialize=True)

user_data = {""id"": 123, ""name"": ""John Doe""}
logger.info(""Пользователь {user} обновил профиль"", user=user_data)
В файле structured_data.log появится запись в формате JSON, идеально подходящая для парсинга:{
    ""text"": ""Пользователь {'id': 123, 'name': 'John Doe'} обновил профиль\n"",
    ""record"": {
        ""elapsed"": {""repr"": ""0:00:00.001000"", ""seconds"": 0.001},
        ""exception"": null,
        ""extra"": {},
        ""file"": {""name"": ""my_script.py"", ""path"": ""/path/to/my_script.py""},
        ""function"": ""<module>"",
        ""level"": {""icon"": ""ℹ️"", ""name"": ""INFO"", ""no"": 20},
        ""line"": 6,
        ""message"": ""Пользователь {'id': 123, 'name': 'John Doe'} обновил профиль"",
        ""name"": ""__main__"",
        ""process"": {""id"": 1234, ""name"": ""MainProcess""},
        ""thread"": {""id"": 5678, ""name"": ""MainThread""},
        ""time"": {""repr"": ""2025-11-13T13:00:00.000000+03:00"", ""timestamp"": 1762989600.0}
    }
}
4. Добавление контекста с помощью bind()В сложных приложениях, например, в веб-сервисах, важно отслеживать цепочку событий, относящихся к одному запросу. Loguru позволяет ""привязать"" контекст к логгеру, и этот контекст будет добавляться во все последующие сообщения.from loguru import logger
import uuid

# Создаем логгер с привязанным ID запроса
request_id = str(uuid.uuid4())
context_logger = logger.bind(request_id=request_id)

context_logger.info(""Получен новый запрос."")
# ... какой-то код
context_logger.info(""Данные из базы успешно получены."")
# ... еще код
context_logger.warning(""Внешний API ответил с задержкой."")
В каждом из этих сообщений будет автоматически добавлено поле request_id, что позволит вам в системе сбора логов легко отфильтровать все события, связанные с одним конкретным запросом.Эти продвинутые техники превращают Loguru из простого инструмента для записи сообщений в мощный фреймворк для отладки и мониторинга, который остается таким же простым в использовании.Часть 4: Loguru в реальном проекте: Советы и лучшие практикиМы изучили мощные функции Loguru, но как грамотно внедрить его в полноценное приложение? Просто импортировать logger в каждом файле — это начало, но для создания надежной и масштабируемой системы стоит учесть несколько моментов.1. Создайте централизованную конфигурациюЧтобы не настраивать логгер в разных частях вашего проекта, лучше всего создать один модуль, отвечающий за всю конфигурацию логирования. Это сделает ваши настройки последовательными и легко изменяемыми.Создайте файл, например, app/logging_config.py:# app/logging_config.py
import sys
from loguru import logger

def setup_logging():
    """"""
    Настраивает логгер для всего приложения.
    """"""
    # Удаляем стандартный обработчик, чтобы избежать дублирования
    logger.remove()

    # Добавляем обработчик для вывода в консоль (для разработки)
    # Уровень DEBUG, цветной вывод
    logger.add(
        sys.stderr,
        level=""DEBUG"",
        format=""<white>{time:HH:mm:ss}</white> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>"",
        colorize=True
    )

    # Добавляем обработчик для записи в файл (для продакшена)
    # Уровень INFO, ротация, сжатие
    logger.add(
        ""logs/app.log"",
        level=""INFO"",
        rotation=""10 MB"",
        retention=""1 month"",
        compression=""zip"",
        serialize=False, # В данном примере используем текстовый формат
        format=""{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}""
    )

    logger.info(""Конфигурация логирования завершена."")
Теперь в главном файле вашего приложения (например, main.py) вам нужно просто импортировать и вызвать эту функцию один раз при старте:# main.py
from app.logging_config import setup_logging
from loguru import logger

# Вызываем настройку в самом начале работы приложения
setup_logging()

@logger.catch
def main_logic():
    logger.info(""Приложение начинает работу."")
    # ... ваш основной код ...
    a = 10
    b = 0
    result = a / b # Это вызовет ошибку, которую поймает @logger.catch

if __name__ == ""__main__"":
    main_logic()
Такой подход гарантирует, что логирование будет работать одинаково во всех частях вашего проекта.2. Интеграция с библиотеками, использующими стандартный loggingЧто делать, если вы используете библиотеки (например, requests, SQLAlchemy, uvicorn), которые пишут свои логи через стандартный модуль logging? Loguru может элегантно ""перехватить"" эти сообщения и направить их в свои обработчики.Для этого нужно создать специальный класс-обработчик и настроить logging на его использование. Этот код может показаться сложным, но вы можете просто скопировать его в свой конфигурационный файл — он работает как готовый рецепт.# Добавьте это в ваш app/logging_config.py

import logging

class InterceptHandler(logging.Handler):
    def emit(self, record):
        # Получаем соответствующий уровень loguru
        try:
            level = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno

        # Находим вызывающий код
        frame, depth = logging.currentframe(), 2
        while frame.f_code.co_filename == logging.__file__:
            frame = frame.f_back
            depth += 1

        logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())

# ... внутри функции setup_logging() добавьте эту строку:
def setup_logging():
    # ... предыдущие настройки ...

    # Настраиваем перехват логов из стандартного logging
    logging.basicConfig(handlers=[InterceptHandler()], level=0)
    logger.info(""Стандартный logging перехвачен."")
Теперь все сообщения от сторонних библиотек будут проходить через Loguru и записываться в ваши файлы с нужным форматом и правилами ротации.3. Безопасность прежде всего: осторожно с данными в продакшене!Функция @logger.catch и logger.exception() с параметром diagnose=True (который включен по умолчанию) — это мощнейший инструмент отладки. Но в продакшене он может стать источником утечки конфиденциальных данных! Он выводит значения всех переменных, а среди них могут оказаться пароли, ключи API, персональные данные пользователей.Правило для продакшена: всегда отключайте диагностику.# НЕ ДЕЛАЙТЕ ТАК В ПРОДАКШЕНЕ
@logger.catch
def process_user_data(user, password):
    # ...

# ДЕЛАЙТЕ ТАК В ПРОДАКШЕНЕ
@logger.catch(diagnose=False)
def process_user_data(user, password):
    # ...
То же самое касается и обработки исключений: logger.opt(exception=True, diagnose=False).error(""Произошла ошибка"").4. Потокобезопасность и асинхронностьХорошая новость: Loguru потокобезопасен ""из коробки"". Вам не нужно беспокоиться о блокировках при использовании логгера в многопоточных приложениях.Для высоконагруженных или асинхронных приложений, где операции ввода-вывода (запись на диск) могут блокировать основной поток или event loop, Loguru предлагает параметр enqueue=True.logger.add(""high_load_app.log"", enqueue=True)
С этой опцией сообщения сначала помещаются в очередь, а запись на диск происходит в отдельном процессе, не замедляя работу вашего основного приложения.Следуя этим простым правилам, вы сможете построить надежную, информативную и безопасную систему логирования для проектов любого масштаба.Домашнее задание: Закрепляем Loguru на практикеПрочитав статью, вы узнали, как сделать логирование в Python удобным и мощным. Теперь давайте применим эти знания на практике! Нажмите на каждую задачу, чтобы раскрыть её условие.Задача 1: Первая настройка и знакомствоЦель: Убедиться, что Loguru установлен, и научиться выводить базовые сообщения.Создайте новый Python-файл (например, task_1.py).Импортируйте logger из библиотеки loguru.Напишите код, который последовательно выводит в консоль сообщения следующих уровней: DEBUG: ""Это сообщение для отладки.""INFO: ""Приложение успешно запущено.""SUCCESS: ""Операция выполнена успешно!""WARNING: ""Внимание: используется устаревшая версия API.""ERROR: ""Не удалось подключиться к базе данных.""Запустите скрипт и посмотрите на вывод в консоли. Обратите внимание, какие сообщения были выведены, а какие нет, и почему (вспомните про уровень по умолчанию).Задача 2: Запись логов в файл с ротациейЦель: Научиться настраивать запись логов в файл, используя logger.add(), и настроить автоматическую ротацию.Создайте файл task_2.py.Настройте логгер так, чтобы он записывал сообщения в файл app_actions.log.Добавьте в настройку logger.add() следующие параметры: Уровень логирования для файла должен быть INFO и выше.Ротация: новый файл должен создаваться, как только текущий достигнет размера 1 KB (килобайт).Сжатие: старые файлы логов должны сжиматься в формат zip.Напишите цикл, который 100 раз выводит информационное сообщение (например, logger.info(f""Запись номер {i}"")).Запустите скрипт и проверьте папку с вашим проектом. Вы должны увидеть несколько файлов: текущий app_actions.log и несколько архивов .zip со старыми логами.Задача 3: Магия отладки с @logger.catchЦель: Практически применить самую мощную отладочную функцию Loguru для перехвата исключений.Создайте файл task_3.py.Напишите функцию divide_numbers(a, b), которая принимает два числа и возвращает результат их деления.Оберните эту функцию декоратором @logger.catch.Вызовите вашу функцию с параметрами, которые приведут к ошибке ZeroDivisionError (например, divide_numbers(10, 0)).Запустите скрипт и изучи��е вывод в консоли. Обратите внимание, как Loguru показал не только ошибку, но и значения переменных a и b в момент её возникновения.Задача 4: Создание собственного формата логовЦель: Научиться кастомизировать формат вывода сообщений для лучшей читаемости.Создайте файл task_4.py.Удалите стандартный обработчик с помощью logger.remove().Добавьте новый обработчик для вывода в консоль (sys.stderr), который будет использовать следующий кастомный формат: {time:HH:mm:ss} | {level.icon} | {message} {time:HH:mm:ss} — время в формате ""часы:минуты:секунды"".{level.icon} — иконка, соответствующая уровню лога (например, ℹ️ для INFO).{message} — само сообщение.Выведите несколько сообщений разных уровней (INFO, WARNING, ERROR) и убедитесь, что они отображаются в консоли в вашем новом, лаконичном формате.Задача 5: Сборка всего вместе — конфигурация для проектаЦель: Симулировать настройку логирования для реального проекта, вынеся конфигурацию в отдельную функцию.Создайте файл task_5_config.py. В нём определите функцию configure_logger().Внутри этой функции настройте два обработчика (sinks): Первый (для консоли): должен выводить сообщения уровня DEBUG и выше, быть цветным и использовать простой формат (например, {level} | {message}).Второй (для файла): должен записывать сообщения уровня WARNING и выше в файл project_warnings.log в формате JSON (serialize=True).Создайте главный файл task_5_main.py.В task_5_main.py импортируйте функцию configure_logger() и вызовите её в самом начале.После вызова конфигурации, напишите код, который генерирует несколько сообщений разных уровней (DEBUG, INFO, WARNING, ERROR).Запустите task_5_main.py. Убедитесь, что: В консоли отображаются все сообщения, начиная с DEBUG.В файле project_warnings.log появились только сообщения WARNING и ERROR, записанные в формате JSON.Анонс новых статей, полезные материалы, а так же если в процессе решения возникнут сложности, обсудить их или задать вопрос по статье можно в моём Telegram-сообществе.Уверен, у вас все получится. Вперед, к практике!Теги:logurupythonpython3python для начинающихлогированиеХабы:Python",330,0,0,14 мин,https://habr.com/ru/articles/966048/,23954,2913,1
CyBОК. Глава 3. Законы и регуляторные нормы. Часть 4,RRakhmetov,2025-11-13T10:14:00.000Z,"['Блог компании Security Vision', 'Информационная безопасность *', 'Учебный процесс в IT', 'Профессиональная литература *']","RRakhmetov 1 час назадCyBОК. Глава 3. Законы и регуляторные нормы. Часть 4Уровень сложностиПростойВремя на прочтение7 минКоличество просмотров60Блог компании Security VisionИнформационная безопасность * Учебный процесс в ITПрофессиональная литература * ОбзорМы продолжаем серию публикаций, посвященную своду знаний по кибербезопасности - Cybersecurity Body of Knowledge (CyBOK). В Главе 3 данного свода знаний описываются основные регуляторные нормы и принципы международного права, которые имеют отношение к кибербезопасности и могут применяться при оценке киберрисков, управлении ИБ, расследовании киберинцидентов. Сегодня – четвертая часть обзора Главы 3 CyBOK, в которой описываются различные типы киберпреступлений и особенности применения норм права в отношении кибератак.Руслан Рахметов, Security VisionПредыдущие главы и части:Глава 1Глава 2 часть 1, часть 2Глава 3 часть 1, часть 2, часть 33.5. Компьютерные преступления.Термин «киберпреступность» часто используется для трёх различных категорий криминальной активности:- «классические» преступления, в которых киберпространство используется как инструмент (например, финансовое мошенничество, кибермошенничество);- распространение противоправной информации в киберпространстве;- преступления, нацеленные непосредственно на инфраструктуру киберпространства (например, кибератаки, взломы).Данный раздел посвящен как раз третьей категории – компьютерным преступлениям, т.е. преступлениям, направленным против информационных систем, что представляет интерес для специалистов по ИБ. 3.5.1. Преступления в отношении информационных систем.По мере развития информационных технологий всё больше вредоносных действий стало совершаться в киберпространстве, однако законодательные изменения не всегда успевают за прогрессом. Ключевой проблемой остаётся международное применение согласованных мер по борьбе с трансграничной киберпреступностью. Одной из важных мер стала разработка Будапештской конвенции («Конвенция о преступности в сфере компьютерной информации»), которая была утверждена Советом Европы в 2001 году и к настоящему времени подписана и ратифицирована 66 странами. В 2013 году в ЕС была принята Директива 2013/40, которая обязала всех стран-членов актуализировать их уголовное законодательство для включения киберпреступлений в состав уголовно наказуемых деяний. При этом в конце 2024 года Генеральная Ассамблея ООН одобрила новую «Конвенцию против киберпреступности», принятую в целях укрепления международного сотрудничества для борьбы с компьютерными преступлениями и обмена доказательствами по таким преступлениям. Работа над данным документом была инициирована Россией и велась в течение 5 лет.Далее авторы документа приводят классификацию киберпреступлений в соответствии с положениями Будапештской конвенции. 3.5.1.1. Несанкционированный доступ к информационной системе.Несанкционированный (неавторизованный) доступ к информационной системе означает доступ к системе без разрешения её владельца, в нарушение правил и порядка доступа и обычно обозначается общим термином «взлом» (англ. «hacking»). При этом в разных странах действия, составляющие неавторизованный доступ, отличаются: в Великобритании попыткой взлома считается ввод пароля без согласования владельца системы, а в США киберпреступлением считается попытка установить несанкционированное сетевое соединение с системой. Отметим, что термин «неавторизованный доступ» до сих пор законодательно не определен с необходимой точностью во всех странах и зависит от решения должностных лиц в каждом конкретном случае. 3.5.1.2. Несанкционированное взаимодействие с данными.Несанкционированным взаимодействием с данными в соответствии с Будапештской конвенцией считается несогласованное удаление, повреждение, порча, изменение данных или нарушение доступности данных. Данные положения могут использоваться в отношении тех, кто разрабатывает или распространяет вирусы-шифровальщики. 3.5.1.3. Несанкционированное взаимодействие с системами.На заре киберпреступности атакующие взламывали системы и меняли данные в них, но с ростом других типов атак, прежде всего DoS/DDoS, в законодательство были внесены изменения, которые отражают новые типы вредоносной активности – теперь нарушением считаются в том числе действия, которые привели к снижению производительности систем. 3.5.1.4. Несанкционированный перехват коммуникаций.Следствием принятия различных законодательных норм по защите приватности стала криминализация действий по несанкционированному перехвату сетевого трафика, особенно в публичных сетях. 3.5.1.5. Несанкционированная разработка хакерских инструментов.Во многих странах правонарушением является разработка или распространение хакерских инструментов с целью их дальнейшего использования для взлома информационных систем. Подобные нормы могут создавать сложности для тех, кто создает решения для проведения тестирований на проникновение и выполнения иных легитимных ИБ-задач. 3.5.2. Исключения в силу незначительности нарушения.В некоторых случаях применение законодательных норм ограничено только действиями, которые можно признать значительными. Например, в Директиве ЕС 2013/40 говорится, что киберпреступлением может считаться только действие против информационных систем, являющихся значимыми, а уровень значимости зависит от относительной опасности создаваемого риска или ущерба, нанесенного несанкционированными действиями. Подобные исключения создают неопределенности в части расчета опасности последствий: в некоторых случаях ущерб очевиден, но в каких-то инцидентах оценить весь масштаб и последствия атаки будет сложно. 3.5.3. Меры принуждения и ответственность за киберпреступления.Каждая страна сама принимает решения относительно расследования киберпреступлений и возбуждения уголовных дел. Ответственность виновных суды также определяют самостоятельно, руководствуясь границами, предусмотренными уголовным законодательством. Например, в Великобритании типовые сроки заключения за киберпреступления составляют от 2 до 5 лет, но даже такие приговоры выносятся редко. Для сравнения, в США расследование дел о киберпреступлениях часто при��одит к срокам заключения в 20 и более лет.Вопрос адекватного наказания за киберпреступления остается открытым, особенно с учетом развития технологий – например, взлом широко применяемых IoT-устройств может привести к ущербу для жизни граждан или их частной собственности. В Директиве ЕС 2013/40 сказано, что сроки лишения свободы должны быть больше в случае кибератак на критическую национальную инфраструктуру или в случае причинения существенного ущерба. В США с 2015 года закон «Computer Misuse Act» предусматривает тюремный срок до 14 лет в случае причинения существенного ущерба, а в случае серьезного ущерба (или риска) здоровью и благополучию граждан или национальной безопасности может быть назначен пожизненный срок заключения. 3.5.4. Санкционированные государственные действия.В случае совершения действий, связанных с расследованием преступления или защитой национальной безопасности, выдаётся специальное разрешение – ордер на выполнение определенных действий. Лицо, выполняющее санкционированные в соответствии с данным ордером операции, не несет ответственности за выполненные действия, включая взлом систем. 3.5.5. Действия по исследованию и разработке, выполняемые негосударственными организациями.Негосударственные организации, которые исследуют вопросы кибербезопасности или разрабатывают ИБ-решения, могут столкнуться со сложностями, поскольку некоторые из их действий могут подпадать под определение киберпреступлений, например:- Несогласованный анализ мер защиты, реализованных на серверах третьих лиц;- Несогласованный удаленный анализ Wi-Fi оборудования третьих лиц;- Несогласованный анализ сетевой инфраструктуры третьих лиц;- Проведение согласованного нагрузочного тестирования (стресс-тестирования) оборудования, при котором деградирует производительность инфраструктуры третьих лиц, не осведомленных и не согласовывавших тестирование;- Анализ ВПО и тестирование методов защиты от ВПО;- Анализ компонентов и функционала ботнетов;- Создание и распространение инструментов для тестирования киберзащищенности;- Использование различных техник сбора разведданных.При рассмотрении случаев применения инструментов для проведения пентестов оценивается обычно не их технический функционал, а цели и намерения организации или лица, которое их создаёт или распространяет, а ответственность наступает в случае, если эти инструменты предполагалось использовать для нарушения закона. Исследователи ИБ, вендоры и профильные ИБ-компании могут столкнуться со сложностями при оценке рисков ведения тех или иных исследований или разработок – в некоторых обстоятельствах и условиях они могут столкнуться с обвинениями в нарушении законодательства; кроме того, важно оценивать все применимые законодательные нормы во всех юрисдикциях, где ведется потенциально рискованная деятельность. 3.5.6. Самозащита: блокировки ПО и ответный взлом.Под терминами «самопомощь», «самозащита» (англ. self-help) понимаются действия, которые лицо предпринимает для защиты своих прав без привлечения представителей государственной власти. В общем случае подобные действия, как правило, не приветствуются во многих странах, поскольку негосударственное лицо пытается выполнить функцию обеспечения законности, которая должна исполняться властью. Если же в некоторых странах разрешено выполнять определенные действия по самозащите, то существует множество сопутствующих ограничений и условий. Выполнение действий по самозащите может привести к обвинениям в нарушении законодательства и судебным искам. 3.5.6.1. Недекларированные блокировки ПО.Существует практика наложения разработчиками ограничений на использование ПО или сервисов: например, некоторое ПО не будет работать после истечения срока лицензии, а облачный провайдер вправе отключить доступ к сервисам при просрочке платежа. Однако проблемы у вендора или провайдера могут возникнуть в случае, когда подобный функционал блокировки не описан в договоре, лицензионном соглашении или инструкции к ПО. Например, с точки зрения законодательства будет нарушением блокировать работу ПО с применением недекларированного функционала даже в случае, если покупатель не оплатил продление лицензии или нарушил условия лицензионного договора. 3.5.6.2. Ответный взлом.Термин «ответный взлом», «ответный удар» (англ. hack-back) используется для описания действий по проведению ответной кибератаки против ИТ-инфраструктуры, из которой была проведена кибератака. Подобные действия обычно оцениваются в контексте кибератаки, которая была проведена с территории иностранного государства, при этом взаимодействие с ним по вопросу расследования данной кибератаки, скорее всего, не принесет результата. Подобный ответный взлом может включать в себя DDoS атакующей инфраструктуры, взлом или вывод из строя атакующей инфраструктуры и т.д. В подобном случае такой ответный взлом будет расценен как компьютерное преступление в стране, с территории которой он производится, и в целевой стране, а также в странах, чья инфраструктура будет задействована при данных действиях. Кроме того, страна, ставшая целью такого ответного взлома, может задействовать принципы международного права и механизмы защиты собственного суверенитета в отношении лиц, которые проводят такой hack-back, и в отношении инфраструктуры, используемой для ответного взлома.Теги:рекомендациистандартыучебникилучшие практикиХабы:Блог компании Security VisionИнформационная безопасностьУчебный процесс в ITПрофессиональная литература",60,0,0,7 мин,https://habr.com/ru/companies/securityvison/articles/966040/,11498,1344,4
Как нас четыре раза пытались купить,ntsaplin,2025-11-13T07:01:54.000Z,"['Блог компании RUVDS.com', 'Хостинг', 'IT-инфраструктура *']","ntsaplin 4 часа назадКак нас четыре раза пытались купитьУровень сложностиПростойВремя на прочтение10 минКоличество просмотров1.9KБлог компании RUVDS.comХостингIT-инфраструктура * ОбзорНашему VDS-хостингу — 10 лет. За это время нас серьёзно пытались купить четыре раза, ещё пару раз — несерьёзно.И каждый раз это была совершенно другая история, как будто из разных учебников по бизнесу. У нас было всё — от пособия для начинающих рейдеров до конкурентной разведки. Ну, знаете, как Яндекс любит приезжать, задавать вопросы про финансы, сообщать про то, что возможна сделка, а потом говорить спасибо за ценные данные.Мы не хотели продаваться, но пару раз были близки к этому, потому что наступали моменты, когда не хватало денег и мы их искали. Одним словом, бывали ситуации, когда мы действительно рассматривали интересные предложения.Начну с самой отбитой категории покупателей. Я даже не могу назвать их инвесторами. Схема у них примитивная и рассчитана на молодых и голодных, какими мы были в первые годы. Они никогда не покупают компанию за живые деньги. Они приходят и говорят:— Чуваки, вы хотите больше? Мы, естественно, такие: — Да, мы хотим! — Отлично! — говорят они. — Мы сейчас дадим вам денег. Однако для этого вы отдадите нам половину компании. Но на самом деле денег мы вам не дадим!«Благодетель»Процесс вы уже, возможно, поняли.Инвестор оценивает вас, скажем, в пять выручек последнего года (тогда был такой стандарт рынка для хостингов) и говорит, что даст их деньгами за 50% компании. Получается, что компания теперь стоит 10 выручек, у инвестора остаётся 50%, у исходного владельца — тоже 50%. Но реальных денег владелец не видит, потому что инвестор вкладывает их в компанию. Вы, основатель, остаётесь с нулём в кармане, но с партнёром, перед которым теперь должны отчитываться. По факту вы рискуете стать просто наёмным сотрудником в своей же фирме, отдав половину бизнеса даром.Эта схема имеет право на жизнь, когда суммы и условия немного другие и когда инвестор профильный или стратегический. В смысле он знает, что делать с вашим бизнесом, и его ему не хватает для какого-то комплекса, который через год он рассчитывает продать, например. То есть он разбирается в вопросе и предлагает реальную ценность. Если же он самоустраняется от управления, то ничем, кроме геморроя, это не закончится. И по принятию решений, и много по чему ещё.Почему я считаю, что сделка была плохой:Инвестор не разбирался в ИТ, а занимался стройкой.Кредитное предложение денег (тогда) выглядело примерно так же привлекательно, но не требовалось отдавать кому-то 50% бизнеса. По крайней мере, пока шли выплаты.Первый раз такие ребята нарисовались в самый паршивый для нас момент. Наш банк, где мы кредитовались, внезапно рухнул. Мы оказались в подвешенном состоянии, постоянно нужны были деньги, чтобы просто держаться на плаву, и мы искали хоть какие-то точки опоры.И тут появились они — представители одного московского инвестхолдинга. Ситуацию весело дополняло то, что я поспрашивал про них у коллег из финансов (а мы пришли в хостинг из финансов) и узнал, что они с рейдерской репутацией, мягко говоря.Они говорили правильные слова, называли нас «ценным активом», но суть предложения была та самая: мы отдаём им долю, они вливают деньги в развитие, и всем становится хорошо. Мы сели считать. Вот мы с их деньгами через год вырастаем в два раза. А наша доля прибыли в абсолютных цифрах остаётся ровно такой же, как сейчас.Однако есть одно огромное «но»: это ещё не факт. И главное — у нас появляется партнёр, который в любой момент может сказать: «Чуваки, а зачем вам дивиденды или зарплаты? Давайте лучше ещё вложим, ещё вырастем!»Если что, у нас с такими людьми фундаментально разные цели. Наша цель — заработать и на что-то жить всё время. Их цель — раздуть компанию, потом отжать менеджмент и продать её по-настоящему. Есть риски потерять контроль, не получив ни копейки.Жизнь становится более нервной, вы постоянно испуганы и параноидальны, потому что не знаете, чего ждать от человека из не самого прозрачного бизнеса.А если проект провалится? Ты, как соучредитель, отвечаешь по всем долгам.Именно поэтому я не доверяю таким инвесторам. Меня в этом сильно укрепила история моего друга и однокурсника. Он работал техническим директором в американском стартапе Trucker Path. Получил опцион, вкалывал. Компания выстрелила, стала «единорогом» с оценкой больше миллиарда долларов. А потом их всех, держателей опционов, кинули. Классическая практика: как только стартап становится успешным, инвесторы — а там был какой-то крупный китаец — проводят фиктивную сделку. Продают компанию за копейки подставной фирме, «высаживают» всех миноритариев, а потом уже перепродают по-настоящему. Мой друг остался ни с чем. Он звонил даже мне, искал юристов: хотел судиться. Эта история навсегда отпечаталась в моей памяти как пример того, почему нельзя продавать долю в компании без получения реальных денег на руки.Возможно, конечно, я динозавр и чего-то не понимаю, но лучше пускай я останусь динозавром с бизнесом, чем лохом с отличной историей для кухонных рассказов.«Вы, Никита, ценный актив»Второй заход был от зарубежного олигарха, которого на нас вывели знакомые. С нами общался его человек из Нью-Йорка. План был такой: купить нашу платформу и построить на ней международный хостинг, благо у нас уже было много зарубежных дата-центров. Но разговор очень быстро скатился к обсуждению затрат.Они относились к нашей команде, как к гастарбайтерам, которым можно платить три копейки. Когда они услышали, что наши инженеры хотят зарплату больше 100 тысяч рублей, у них глаза на лоб полезли. Началась эта песня: «Ох, так вы хотите зарабатывать и жить в комфорте прямо сейчас или вам нужно построить что-то крупное?»То есть, по их логике, строить великое можно только натощак.В целом это правда: в культурном коде стартаперов из Долины и Европы учредители действительно получают символические зарплаты. Основные деньги приходят с продажи компании или после объединороживания, когда можно позволить платить и себе, и Совету директоров. До этого надо жить на накопления.Они даже сделали оценку: около пяти годовых выручек за всю компанию. Но схема была мутная. Опять та же история: мы теряем контроль и не получаем денег.Мы подумали и отказались.«Вы неправильно работаете»Был и ещё один, уже третий подход от владельца крупного сервиса, который хорошо сочетался с хостингом. Профильный игрок, казалось бы. Но разговор — тот же: «Вот, давайте мы так хотим…» Я сидел, слушал и ощущал дежавю.Ещё до подписания чего-либо они уже начали учить нас жизни. Оказалось, что у себя они используют б/у железо. И нам говорят: «Нет, вот вы дорого покупаете, надо брать бэушное». Мы им: «Ребят, мы на таком работать не будем». Это был конец разговора.Если ещё до сделки начинается такое продавливание, то что будет после этого?Цивилизованный флиртМы надолго забили на все эти переговоры, решив для себя, что это не очень осмысленная трата времени. Но потом на рынке случилась движуха: государство ввело реестр хостинг-провайдеров и ужесточило требования по СОРМ. Один из крупных игроков рынка тут же подсуетился и начал рассылать письма всем, кто поменьше. Посыл был такой: «Вам сейчас будет пипец, вы разоритесь, давайте мы вас купим». По сути, они хотели скупить клиентские базы. Всем нужны клиентские базы. Бренд не нужен никому. Даже если мы сейчас будем брать региональный хостинг, то бренд не будет нас интересовать — только база.Так вот, нам тоже пришло такое письмо. Мы об этом написали на VC и Хабре. Покупатели после такой огласки сначала отморозились, но через пару недель мне позвонили: «Слушайте, то было экстренное реагирование, но всё равно мы хотели бы с вами поговорить». И вот тут начался совсем другой разговор. Они назвали ожидаемые нами цифры: оценка была в районе шести годовых выручек. Мы впервые подписали соглашение о неразглашении и вступили в процесс due diligence. Это когда покупатель проводит полную проверку вашего бизнеса, чтобы убедиться, что вы не продаёте ему кота в мешке.Они вели себя очень цивилизованно, задавали логичные вопросы без залезания в трусы.Но вскрылись две проблемы. Во-первых, технологический стек: у нас всё на одном гипервизоре, а у них — на другом. Они поняли, что перенос клиентов будет сложным и дорогим. Во-вторых, им не понравилось, что у нас есть зарубежные локации. В итоге предложения не последовало.Но мы не расстроились: это был первый опыт, когда с нами говорили как с равными, а не как с просителями.Высшая лигаИ вот совсем недавно на нас вышел игрок совсем другого масштаба. Назовём его N3. Они пришли не сами, а через профессиональных посредников — M&A-компанию. И вот тут мы увидели, как на самом деле выглядят серьёзные сделки.Там всё начинается не со звонка основателя, а с общения с этими M&A-специалистами, по сути — инвестбанкирами. Они задают очень правильные вопросы про качество актива: срок жизни клиента, динамику роста и так далее. Они не лезут в ноу-хау: все понимают границы. Если их всё устраивает, то следует звонок от самих покупателей. Они обсуждают сценарий сделки. И если стороны сходятся, то присылают Letter of Intent — официальное предложение с главными условиями.Только после его подписания начинается самый ад — due diligence, который может длиться месяцами. На этом уровне никто уже не считает по выручке. Всё считается по мультипликатору к EBITDA: это, грубо говоря, ваша прибыль до налогов, процентов и амортизации. Самый честный показатель. Нормальным для текущей рыночной ситуации считается мультипликатор от 5. Хочешь больше? Докажи, что у тебя есть супертехнология или ты растёшь как на дрожжах.И самое интересное — деньги. 100% суммы тебе никогда не заплатят сразу. Аудиторы посчитают все риски, которые сочтут вероятными для покупателя, и порекомендуют заморозить эту сумму на эскроу-счёте на три года. Вы получите её, только если за это время не будет претензий.Процесс этот дико дорогой. Покупатель тратит на аудиторов миллионов 30–50. Но и вы, продавец, если хотите, чтобы ваши интересы нормально представляли, должны нанять своих консультантов. А это ещё минимум 20–40 миллионов. Получается, что сама сделка обходится обеим сторонам под 80–100 миллионов. Поэтому покупать маленькую конторку в таком формате просто бессмысленно. Но у M&A-отделов этих гигантов есть своя пропускная способность: они не могут покупать по 10 компаний в год, т. к. ведут одну сделку по полгода. Поэтому все эти байки про «M&A-бюджет, который надо освоить до Нового года» — чушь: считают каждый рубль.По итогам due diligence определяются финальная сумма и структура сделки. И вот тут начинается самое интересное — игра в деньги и власть.Для вас, как для продавца, идеальный сценарий — это получить все деньги сразу. Чемодан с немечеными купюрами, как в кино. Ну или просто всю сумму на счёт за вычетом того самого эскроу на риски. Получил, попрощался — ушёл в закат.Но покупатель, особенно в текущей экономической ситуации, смотрит на это совсем иначе. Сейчас высокие ставки, дешёвых денег нет ни у кого, даже у гигантов. Поэтому платить всю сумму сразу им дико невыгодно — им гораздо интереснее растянуть сделку во времени. И они предлагают схему, которая на бумаге выглядит логично, а на деле это адская ловушка для продавца.Схема называется earn-out, или выплата по результатам. Выглядит она так: «Мы платим тебе часть суммы, скажем, 50%, прямо сейчас. А за это получаем контрольный пакет — 51% акций. Оставшиеся 50% ты получишь частями в течение следующих двух-трёх лет, если выполнишь определённые KPI, например, по росту той же EBITDA».Для покупателей это соломоново решение. Во-первых, им не нужно вываливать всю сумму сразу: по факту для них это бесплатная рассрочка. Во-вторых, они могут внедрить своих людей, всё проконтролировать и убедиться, что вы не увели клиентскую базу или не открыли такой же бизнес с другой буквой в названии. Им так комфортнее.А вот для вас, продавца, в этот момент всё меняется. В тот самый миг, когда вы подписываете договор и получаете первый транш, вы теряете контроль над своей компанией. Вы больше не хозяин.И тут же возникает очевидный и неразрешимый конфликт интересов. Покупателю теперь НЕВЫГОДНО, чтобы вы выполнили свои KPI. Ведь если вы их выполните, то ему придётся платить вам следующий большой транш. А зачем, если контроль над активом он уже получил? И он будет делать всё что угодно, чтобы вы не достигли этих показателей. Вплоть до умышленного вредительства.Как это выглядит на практике? Очень просто. Вы приходите и говорите: «Нам нужно поднять цены для клиентов, чтобы увеличить прибыль и выполнить KPI». А новый мажоритарный акционер отвечает: «Нет, мы не можем: это отпугнёт клиентов». Вы говорите: «Нужно вложиться в маркетинг, чтобы привлечь новых пользователей». Они: «Бюджета нет». Нужно нанять ключевого разработчика? «Давайте подождём». Любая инициатива, направленная на рост, будет тормозиться. Вам просто не дадут работать.Для продавца есть одно золотое правило, которое нужно высечь в граните: рассматривайте первый транш как финальную и единственную сумму сделки. Всё остальное — это бонус, которого вы, скорее всего, никогда не увидите. Нужно быть готовыми к тому, что покупатель всеми силами, опираясь на своих сильных юристов и новый статус хозяина, не захочет вам платить. И это его право: он играет по правилам, которые сам же и установил в договоре. Вы утратили контроль и теперь находитесь в его власти.С N3 мы в итоге не сошлись в оценках. Мы хотели мультипликатор 8 к EBITDA, а они — немного меньше (сколько — не позволяет сказать NDA) .Мы такие: «ОК, давайте поговорим через год. Мы не торопимся».ИтогоПервое. Если вы хотите идти на продажу, то готовиться к ней надо с первого дня. Если у вас бардак в бухгалтерии и какие-то мутные схемы, то потом будет мучительно больно: либо сделка сорвётся, либо вам насчитают такой эскроу, что от суммы продажи ничего не останется. Финансовая гигиена — это основа.Бизнес «для себя» оптимизирован для вашего личного комфорта и эффективности. Там могут найтись какие-то схемы, которые тебе посоветовал бухгалтер на аутсорсе, чтобы платить поменьше налогов. Юридическая структура может быть запутанной. Идеальная документация — в голове. И если на такое приходят аудиторы из «Большой четвёрки», то начинается ад. Всё это риски и красные флаги, нет документа — это юридическая проблема. Они пересчитают все эти риски в конкретные суммы и либо вычтут их из цены, либо заморозят на эскроу-счёте на три года, либо вообще уйдут из сделки. Нельзя десять лет строить сарай для себя, а потом за один день пытаться продать его как дворец.Второе. Ваш бренд, скорее всего, не стоит ничего. Покупателю нужны ваша клиентская база, а также EBITDA. После сделки ваш бренд убьют. Никто уже не помнит, что был Foodfox, а не «Яндекс Еда». Исключение — если какой-нибудь Wildberries решит выйти на рынок ЦОДов и купит компанию с именем, чтобы не стартовать с нуля.Третье. Не ждите щедрости. Покупатели — не дураки. Они найдут все ваши косяки, даже те, о которых вы не подозревали. Они будут торговаться за каждый рубль.Четвёртое. Все вокруг говорят про «экзит» как главную цель любого стартапа. Продать компанию гиганту, получить мешок денег и уехать на Бали. Звучит красиво, как в кино. Но в реальной жизни, если твой бизнес работает и приносит стабильный доход, продавать его — это экономически нелогично. Зачем резать курицу, которая несёт золотые яйца?На мой взгляд, есть только две по-настоящему веские причины для продажи. Первая — у вас на горизонте появилось что-то более выгодное, более маржинальное, и вам нужны деньги и время, чтобы туда вложиться. Это холодный рациональный расчёт. А вот вторая причина — вы просто устали. Это выгорание другого уровня. Это когда вы десять лет тащите на себе ответственность за всё: за людей и их зарплаты, за серверы, которые могут упасть ночью, за клиентов, которые могут уйти. Проблемы, которые раньше были интересными вызовами, превращаются в тупое нудное раздражение. И в этот момент продажа — это не про деньги. Это про то, чтобы вернуть себе свою жизнь.Если вам нужны деньги — рассмотрите кредит или другие варианты. Там можно продать только будущую выручку (или прибыль) без других частей компании.Мы не продались не потому, что мы такие гордые, а потому, что ни одно из предложений не было для нас по-настоящему выгодным. Но общаться с потенциальными покупателями полезно. Вы смотрите на свой бизнес, на своё детище не только своими любящими глазами, но и холодным взглядом рынка.И это очень отрезвляет и помогает понять, что на самом деле вы построили.© 2025 ООО «МТ ФИНАНС»Теги:хостингбизнесрыноксделкадью-диллиженсслияние и поглощениеотжимоценкамифический экзитХабы:Блог компании RUVDS.comХостингIT-инфраструктура",1900,0,0,10 мин,https://habr.com/ru/companies/ruvds/articles/965860/,16616,2520,3
"LLM vs. почерк: практическое сравнение GPT-5, Gemini и Claude в задачах OCR",Tehnologika,2025-11-13T09:18:26.000Z,"['Natural Language Processing *', 'Искусственный интеллект', 'Машинное обучение *', 'Обработка изображений *', 'Исследования и прогнозы в IT *']","Tehnologika 2 часа назадLLM vs. почерк: практическое сравнение GPT-5, Gemini и Claude в задачах OCRУровень сложностиПростойВремя на прочтение7 минКоличество просмотров436Natural Language Processing * Искусственный интеллектМашинное обучение * Обработка изображений * Исследования и прогнозы в IT * АналитикаРаспознавание рукописного текста — задача, которая остаётся болезненной даже в 2025 году. Именно это не позволяет оцифровать многие архивы и документы, а также является камнем преткновения в разной бизнес деятельности. OCR-движки вроде Azure Document Intelligence, Google Vision или ABBYY уже давно научились безошибочно читать печатные формы, но всё рушится, когда на сцену выходит человек с ручкой.Почерк — это хаос, в котором буквы скачут, строки уползают, а “5M” внезапно превращается в “5 PM”, если повезёт. И если обычный OCR видит только буквы и пиксели, LLM видит смысл.Производители заявляют, что модели вроде GPT-5, Gemini 2.5 Pro и Claude Sonnet 4.5 способны не просто распознать текст, а догадаться, что автор имел в виду: исправить пунктуацию, восстановить сокращения, даже понять, что стоит за пометками на полях.Звучит красиво. Но работает ли это на реальных документах, а не в демо-видео с идеально отсканированными формами?  Чтобы ответить, мы провели исследование и сравнили, как три топ-LLM обрабатывают рукописные и смешанные документы — с точки зрения точности, структурной консистентности и понимания контекста.СодержаниеМетодологияЧто мы тестировалиКак мы тестировалиПочему три модели?Как справились моделиФормуляр в Музей трамваев (Streetcar Museum Event Form)Анкета для фотооархива (“Johnny – King of Sausages” Photo Form)Форма для смены в больнице (Change of Shift Huddle)Кто победил?Себестоимость и скоростьА можно ли лучше?Пример на практике: Azure Document Intelligence + Gemini 2.5 ProЧто делать, когда облако – не вариант?Заключение и выводыМетодологияЧто мы тестировалиДокументы, которые мы отдавали в LLMЧтобы не скатиться в синтетические бенчмарки, мы взяли три реальных документа, типичных для корпоративных сценариев, где OCR-ошибки могут стоить времени и денег:Формуляр мероприятия в Музей трамваев (Event Form to Streetcar Museum)Анкета для фотооархива (Photo Submission Form “Johnny – King of Sausages”)Форма для смены в медицинском учреждении (Medical Change of Shift Huddle Form)Каждый документ имеет свои особенности и сложности для OCR и LLM:Формуляр в Музей трамваев – смешанные шрифты, наложения текста, пересекающиеся строки и неоднозначные цифрыАнкета для фотооархива – курcив, капс, подписи, апострофы и артефакты от скана старой бумагиФорма для смены в больнице – несколько почерков, медицинские сокращения, пересекающиеся сетки таблицКак мы тестировалиДля чистоты эксперимента:Все три документа поданы в неизменном виде, без предварительной очистки изображений.Каждая модель получала один и тот же ввод (скан) и задачу: извлечь данные в структурированном JSON.Оценка велась вручную и по метрикам:CA (Character Accuracy) – сколько символов распознано корректноFA (Field Accuracy) – сколько полей правильно извлеченоSA (Semantic Accuracy) – насколько правильно понят смысл и контекстC (Completeness) – доля извлечённых полей от ожидаемогоS (Schema Consistency) – единообразие JSON-структуры между документамиПочему три модели?Особенности каждой LLMДля эксперимента были выбраны три модели разных архитектурных подходов:Claude Sonnet 4.5 — фокус на интерпретации и языковом рассуждении;Gemini 2.5 Pro — структурная точность и стабильность вывода;GPT-5 — сильная контекстная и семантическая обработка.Это позволило оценить не только базовую точность OCR, но и то, как модели восстанавливают смысл, формат и структуру данных после распознавания.Как справились модели1. Формуляр в Музей трамваев (Streetcar Museum Event Form)Эта форма — старомодный бланк с полями вроде Sponsor, Date of Event, Category, Mailing Address. Часть напечатана, часть вписана вручную — и именно это делает задачу интересной.Основные сложности:строки налезают на границы таблиц;одни слова написаны капсом, другие — курсивом;рукописные цифры 2 и 5 путаются с буквами S и Z;некоторые поля («Tour» с обведённым кружком) требуют не просто OCR, а понимания логики формы.Как LLM справились с музейным формуляромClaude 4.5 справился посредственно: понял суть, но несколько полей спутал, а слова вроде “Cultural” превратил в мифических существ (“Scolyunile”). Он явно склонен к «творческой интерпретации».Gemini 2.5 Pro показал почти безупречный результат. Он не только правильно распознал все поля, но и нормализовал их формат: аккуратные даты, выровненные адреса, единый стиль JSON. Даже догадался, что “5M” — это “5 PM”, и восстановил недостающие запятые.GPT-5 занял второе место. Он хорошо понял смысл, корректно обработал сложные фразы вроде “Share the fun of a streetcar ride…”, но слегка потерял структурность — часть данных «сплющил» в одну строку.Итог: Gemini — образцовый инженер, GPT-5 — умный редактор, Claude — вдохновлённый поэт.2. Анкета для фотооархива (“Johnny – King of Sausages” Photo Form)Форма из городского архива, заполненная синей ручкой поверх выцветших полей. Здесь OCR сталкивается с типичными проблемами старых документов: кривые линии, неровный почерк, пересечение букв с рамками и случайные артефакты сканера.Как LLM справились с анкетойClaude 4.5 всё понял правильно, но внёс хаос в ключи — где-то snake_case, где-то CAPS, а где-то просто пропустил пустые поля.Gemini 2.5 Pro снова показал себя с лучшей стороны: идеально воспроизвёл текст, сохранил пунктуацию (даже кавычки в “King of Sausages”), выстроил вложенные секции “WHEN / WHERE / WHO” и сдал JSON, который можно прямо загружать в базу.GPT-5 чуть проиграл по структурной чистоте, но оказался лучшим по естественности текста. Он сгладил орфографические неровности и исправил регистр, делая результат ближе к «читаемому человеку описанию».Итог: Gemini — идеален для продакшена, GPT-5 — ближе к “human-readable”, Claude — опять решил, что JSON — это рекомендация, а не стандарт. 3. Форма для смены в больнице (Change of Shift Huddle)Форма с дежурства — таблица, где вписаны имена, числа, состояния пациентов и короткие заметки вроде “MRSA developing in a patient” или “Wiping down the surfaces”.Такие документы — кошмар для любого OCR: сетка таблицы ломает структуру, буквы сливаются, а почерки отличаются настолько, что кажется, будто форму заполняли трое разных людей.Как LLM справились с медицинской формойClaude 4.5 сработал лучше, чем ожидалось: правильно распознал числовые поля и основные секции (Team, Advocate, Motivate), но перепутал имена и местами дублировал значения.Gemini 2.5 Pro снова лидер: он сохранил структуру таблицы, корректно распределил имена по колонкам, а странное «Einnish» (OCR-ошибка) — единственный заметный промах. При этом общая точность полей — 99 %.GPT-5 показал сильное понимание контекста: фраза “Extremely busy but managed well” была не просто прочитана, а осмыслена как unit status. Но часть имён он перепутал (“Mac” вместо “Max”) — классическая жертва OCR.Итог: Gemini вновь впереди, GPT-5 почти догнал, Claude — стабильный, но не продакшен-уровень.Кто победил?После десятков прогонов трёх LLM по реальным рукописным формам результат оказался довольно однозначным — Gemini 2.5 Pro уверенно вырвался вперёд по всем формальным метрикам.МетрикаClaude Sonnet 4.5Gemini 2.5 ProGPT-5КомментарийCharacter Accuracy(сколько символов распознано корректно)93–97 %98–99 %97–98 %Gemini чище по символам, Claude чаще теряет пробелы и пунктуациюField Accuracy (сколько полей правильно извлечено)90–96 %99–100 %96–98 %Gemini безошибочно выстраивает ключи и значенияSemantic Accuracy (насколько правильно понят смысл и контекст)95–98 %99–100 %98–99 %GPT-5 чуть сильнее в логике, но слабее в формеCompleteness (доля извлечённых полей от ожидаемого)96–100 %100 %96–100 %Все трое неплохо, но Gemini стабильно полонSchema Consistency (единообразие JSON-структуры между документами)СредняяОчень высокаяВысокаяJSON у Gemini можно подавать в прод без пост-обработкиЕсли коротко:Gemini 2.5 Pro показал лучшую комбинацию точности (до 99 %) и структурной стабильности.GPT-5 чуть уступает в формализме, но сильнее в семантическом «понимании» текста.Claude 4.5 часто ошибается в полях, но умеет красиво пересказывать — что полезно для описательных задач.Себестоимость и скоростьМы замеряли не только качество, но и цену и время обработки, потому что в продакшене важно не просто «чтобы красиво», а чтобы дёшево и предсказуемо.МодельСтоимость на документ (USD)Среднее время обработкиПримечаниеClaude Sonnet 4.50.0165~ 75 сБыстрее, но дороже и менее точенGemini 2.5 Pro0.0080~ 90 сОптимальное соотношение цена / качествоGPT-50.0094~ 120 сМедленнее, но стабильно выдаёт контекстно-богатый результатЧто это значит:Обработка одного рукописного документа в Gemini стоит достаточно дёшево.Claude — самое «дорогое вдохновение», но не лучший вариант, если счёт идёт на тысячи страниц.GPT-5 — универсал: если нужно чуть больше глубины в понимании текста, разница в 30 секунд может быть оправдана.А можно ли лучше? Когда речь идёт о распознавании реальных документов, ни один инструмент в одиночку не идеален. OCR прекрасно видит буквы, но не понимает смысла. LLM — наоборот: понимает смысл, но не знает, где на картинке что написано.Поэтому оптимальная архитектура гибридная:OCR → LLM → Валидация.Оптимальная гибридная архитектураПример на практике: Azure Document Intelligence + Gemini 2.5 ProAzure DI делает то, что умеет лучше всего — распознаёт текст, возвращает bounding-box координаты и confidence-оценку для каждого слова.Gemini 2.5 Pro принимает этот результат и:исправляет очевидные OCR-ошибки («5M» → «5 PM», «SBO» → «SBO Card»);нормализует формат (даты, капитализацию, пунктуацию);собирает данные в чистый JSON;при необходимости восстанавливает контекст («Tour» отмечено кружком — значит, выбрано).Система валидации проверяет поля с низкой уверенностью Azure и подставляет исправления Gemini.После того как мы применили этот подход, мы получили лучшие результаты. Вместе эти инструменты формируют систему, где человек нужен только в спорных случаях, а всё остальное идёт автоматически. Вот результаты:точность полей выросла с ~84 % (Azure DI в одиночку) до ~99 %;ручная проверка сократилась на 90–95 %;JSON стал полностью пригоден для загрузки в базу без ручного редактирования.Что делать, когда облако – не вариант? Облачные OCR и LLM-платформы вроде Gemini, GPT-5 или Azure Document Intelligence уже обеспечивают почти идеальную точность и масштабируемость. Но не все компании могут позволить себе отправить документ в облако и дождаться JSON.  Иногда это просто запрещено политиками безопасности. Если данные нельзя выносить в облако, ту же логику можно воспроизвести локально:Внутренний OCR → приватная LLM.Локальная реализация гибридаЛокальный гибрид может быть идеален для некоторых компаний компаний, поскольку:Никакие данные не покидают инфраструктуру организации.Модели можно обучать и настраивать на образцах почерка, характерных для конкретной бизнес-темы.Работает в отключенных сетях или в средах с высоким уровнем безопасности.Единовременная стоимость установки и неограниченное использование.Возможность прямой подачи данных во внутренние озера данных или ERP-системы без внешних зависимостей.Заключение и выводыLLM действительно умеют читать почерк — не идеально, но с пониманием.И если раньше «распознать рукописную форму» означало час ручной чистки Excel, то теперь это вопрос одной модели и пары API-вызовов.После трёх десятков тестов, сотен строк JSON и пары нервных шуток про курсив можно сделать несколько уверенных выводов.LLM-OCR перестал быть экспериментом. Гибриды вроде Azure DI + Gemini 2.5 Pro уже сегодня обеспечивают до 99 % точности и сокращают ручную проверку на 90–95 %.Gemini 2.5 Pro — оптимальный выбор для продакшена. Высокая структурная точность, стабильный JSON и лучшая цена за страницу делают его рабочей лошадкой enterprise-уровня.GPT-5 — лидер в понимании контекста. Он «думает» о смысле текста, но требует строгих схем и валидации, если данные идут в базу.Claude 4.5 Sonnet — отличный интерпретатор и storyteller, но не всегда детерминирован. Хорош для описательных задач, не для бухгалтерии.On-prem OCR — не про экономию, а про суверенность данных. Его выбирают не те, кто хочет сэкономить, а те, кто не может позволить себе облако.Теги:llm-моделиchatgpt-5claude sonnetgemini proязыковые моделиобработка документовпочеркпочерк врачейgpt-5обработка изображенийХабы:Natural Language ProcessingИскусственный интеллектМашинное обучениеОбработка изображенийИсследования и прогнозы в IT",436,0,0,7 мин,https://habr.com/ru/articles/966002/,12541,1665,5
"Нефункциональные требования. Список, который вспоминают в последний день перед релизом. Часть 1",SiYa_renko,2025-11-12T17:33:30.000Z,"['Блог компании OTUS', 'Управление разработкой *', 'Анализ и проектирование систем *']","SiYa_renko 17 часов назадНефункциональные требования. Список, который вспоминают в последний день перед релизом. Часть 1Уровень сложностиПростойВремя на прочтение9 минКоличество просмотров911Блог компании OTUSУправление разработкой * Анализ и проектирование систем * ОбзорПредставьте, что вы покупаете мотоцикл. Чего вы от него ожидаете? Чтобы он мог разгоняться до 180км/час и при этом не разваливался? Чтобы к нему можно было прикрепить коляску? И не забудем про систему безопасности.Эти требования не описывают основную функцию мотоцикла — перемещать человека из точки А в точку Б — но они важны для удовлетворения ваших потребностей, как водителя.Точно так же, как у мотоциклов, да и любой другой техники, у программных продуктов есть свои нефункциональные требования. То есть атрибуты качества, которые будут удовлетворять потребности конечного пользователя.Сегодня я предлагаю рассмотреть те нефункциональные требования, которые влияют на деньги, но для начала...Что это вообще такое?Обратившись к терминологии мы поймем, что нефункциональные требования — это набор спецификаций, описывающих эксплуатационные характеристики системы и её ограничения. По сути, это требования, определяющие как хорошо система должна работать (скорость отклика, безопасность, надёжность, целостность данных и так далее).Однако это лишь один тип требований в разработке ПО, поэтому прежде чем углубляться в них, стоит сказать о другой группе — функциональных требованиях.И функциональные, и нефункциональные требования описывают характеристики, которые продукт должен иметь, чтобы удовлетворить потребности стейкхолдеров и бизнеса. Однако, как видно из названий, они фокусируются на разных вещах.Функциональные требования определяют, что система должна делать, какие функции и возможности иметь. Это может быть как возможность отправлять сообщения и редактировать их, так и оплачивать подписку.Нефункциональные требования определяют как система должна работать. Например, говорить о том, что измененное сообщение у участников чата должно обновляться не позднее, чем через 0.1 секунды, при условии, что все пользователи онлайн и имеют подключение уровня LTE или выше.Все требования к системе обычно фиксируются в спецификации требований к программному обеспечению и документе требований к продукту. Эти документы содержат описание функций и возможностей, которые продукт должен предоставлять, а также список ограничений и допущений. Погуглите материалы про SRS и PRD, если вам интересно.Теперь, когда общая картина требований понятна, рассмотрим нефункциональные требования подробнее.Типы нефункциональных требованийНаиболее распространенные группы нефункциональных требований включают:Производительность (Performance) — насколько быстро система возвращает результат.Масштабируемость (Scalability) — как изменяется производительность при росте нагрузки.Переносимость (Portability) — на каком оборудовании, операционных системах, браузерах и их версиях может работать система.Совместимость (Compatibility) — не конфликтует ли система с другими приложениями или процессами. Надёжность (Reliability) — как часто происходят критические сбои. Сопровождаемость (Maintainability) — сколько времени требуется для устранения проблемы при её возникновении. Доступность (Availability) — каков средний простой системы. Безопасность (Security) — насколько хорошо защищены система и данные от атак. Удобство использования (Usability) — насколько легко пользователю взаимодействовать с системой.Помимо мною перечисленных, некоторые выявляют и другие атрибуты качества, которые могут быть включены в список, это уже зависит от специфики проекта. Мы же рассмотрим подробнее именно эти группы.Требования к производительностиПроизводительность определяет, насколько быстро программная система (или её компонент) реагирует на действия пользователя при определённой нагрузке. Это один из ключевых типов нефункциональных требований — без него не обходится ни одна система.В большинстве случаев метрика производительности описывает, как долго пользователь должен ждать до выполнения целевой операции (отображение страницы, обработка транзакции и так далее) при текущем количестве активных пользователей.Однако это не единственный вариант. Требования к производительности могут также описывать фоновые процессы, которые пользователь не видит — например, резервное копирование или обработку данных.Но в рамках этого раздела мы сосредоточимся на показателях производительности, влияющих на взаимодействие пользователя с системой.Примеры требований к производительностиПосадочная страница, поддерживающая 5 000 пользователей в час, должна обеспечивать время отклика не более 6 секунд в браузере Chrome на десктопе, включая загрузку текста и изображений при подключении уровня LTE.Результаты поиска по товарам должны отображаться не более чем за 3 секунды в 90% запросов при нормальной нагрузке.Система должна быть способна принимать потоки данных со скоростью не менее 1 000 000 записей в минуту из различных источников без потери данных.Для обновления данных на дашборде в реальном времени, система должна обновлять и отображать новые аналитические данные в течение 10 секунд после получения новых данных.Как работать с требованиями к производительностиНачните с рекомендаций Google для обычных веб‑страниц. Google уделяет особое внимание скорости загрузки на десктопах и мобильных устройствах. Поэтому, если вам нужны базовые ориентиры производительности для публичных веб‑страниц, используйте сервис Google PageSpeed Insights. Он поможет оценить время полной загрузки, время до полного рендера, отклик интерфейса на действия пользователя.На основе совокупности факторов сервис формирует оценку производительности, по которой можно судить о скорости вашего сайта. Это особенно важно для посадочных страниц, поскольку низкая скорость загрузки может привести к снижению ранга страницы в поисковой выдаче.Базовые рекомендации по времени отклика появились еще в 1993 году, их выделил Якоб Нильсен и они представляют собой три ключевых порога времени реакции системы. Несмотря на то, что эти значения появились давно, они по‑прежнему актуальны, поскольку основаны на особенностях восприятия и внимания человека:Время реакцииКак воспринимается0.1 секундыРеакция системы ощущается мгновенной1 секундаЗадержка заметна, но мыслительный поток не прерывается10 секундВнимание пользователя полностью теряетсяКак правило, до 10 секунд доходить нельзя, потому что около 55% пользователей покидают сайт, если он грузится дольше 3 секунд.А согласно исследованию Portent:«Сайт, загружающийся за 1 секунду, имеет конверсию в 3 раза выше, чем сайт, загружающийся за 5 секунд».То есть время в прямом смысле деньги.Что нужно учестьУточняйте сценарий измерения производительности. При формулировании требований важно указать что именно измеряется и включает ли метрика только время доставки данных до браузера (время ответа сервера) или полное время рендера страницы?Если разные типы контента загружаются с разной скоростью, то для каждого из них нужны отдельные ограничения.Также уточняйте нагрузочный сценарий. Если, к примеру, днем на сайте 5000 активных пользователей, а ночью 1000, необходимо указать, к какому сценарию относится метрика (средняя нагрузка, пиковая или стресс‑тест). Иногда фиксируют оба варианта.Исключайте задержки, зависящие от сторонних систем. Если система зависит от внешнего API, не следует включать в требование время, которое уходит на ответ третьей стороны. Команда разработки не отвечает за производительность внешних сервисов.Требования к масштабируемостиМасштабируемость определяет, при какой максимальной нагрузке система всё ещё способна соответствовать требованиям к производительности и удобству использования. Важно понимать, что масштабируемость относится к способности системы справляться с ростом как объёма данных, так и нагрузки со стороны пользователей.Существует два основных способа масштабирования системы по мере увеличения нагрузки:Горизонтальное масштабирование — добавление новых серверов в пул ресурсов.Вертикальное масштабирование — увеличение мощности существующих серверов (добавление CPU, RAM и так далее).Таким образом, ключевая цель требований к масштабируемости — обеспечить, чтобы система оставалась стабильной и сохраняла требуемый уровень производительности при увеличении числа пользователей, объема данных, количества бизнес‑процессов, количества модулей и интеграций.Примеры требований к масштабируемостиВеб‑сайт должен масштабироваться до 1 000 000 одновременных посещений, сохраняя оптимальную производительность.ERP‑система должна быть способна увеличивать объёмы хранения и обработки данных по мере роста компании, включая потенциальное увеличение числа транзакций в 10 раз за пять лет.Система мониторинга IoT‑устройств должна поддерживать масштабирование с 10 000 до 500 000 устройств, без потери точности данных или снижения качества наблюдения.eCommerce‑платформа должна быть способна выдерживать рост трафика на 300% в периоды распродаж или сезонных пиков без ухудшения времени отклика и доступности сервиса.Как формулировать и прорабатывать требования к масштабируемостиОпределяйте ожидания с точки зрения бизнеса. Сначала оцените текущую нагрузку: количество пользователей, транзакций, объём данных. Затем определите прогноз роста. Сколько пользователей планируется поддерживать в будущем? Какие рынки предполагается расширять? Какие новые модули или функции могут быть добавлены позже?Учитывайте особенности отрасли. Например для eCommerce, игр, видеостриминга критичен рост числа пользователей. Если мы говорим про финтех или банки, ключевыми параметрами будут являться скорость и объем транзакций.Делайте требования измеримыми. Используйте конкретные параметры, такие как количество одновременных пользователей, объем данных, скорость обработки транзакций, время отклика при максимальной нагрузке.Самое главное, нужно ставить реалистичные цели. Масштабируемость должна опираться на реальные сценарии использования и прогнозируемые пики нагрузки, а не на абстрактные «пусть выдерживает любое количество пользователей».Требования к доступностиДоступность определяет вероятность того, что система будет доступна пользователю в определенный момент времени. Её можно выражать как долю успешных запросов или процент времени, когда система находится в работоспособном состоянии за заданный период.Доступность один из наиболее критичных бизнес‑показателей, но чтобы корректно определить ее, необходимо так же учитывать надежность (то есть как часто случаются сбои) и сопровождаемость (как быстро система восстанавливается.Примеры требований к доступности Дашборд должен быть доступен пользователям из РФ 99.98% времени в месяц в рабочие часы по МСК.Приложение должно обеспечивать стабильную работу на разных устройствах с уровнем надёжности 99%.Видеостриминг должен быть непрерывным 99,8% времени при нормальных сетевых условиях.Как формулировать требования к доступностиНачните с бизнес‑ограничений. Поймите, допустимо ли, чтобы приложение было недоступно 5% времени? Какой финансовый или KPI‑ущерб это создаст? Полностью безотказных систем не существует в природе, нужно определить критический порог.Уточните, для какого компонента определяется доступность. Часто разные части системы имеют разные SLA, например платежные модули, лендинги, админ‑панель.Учитывайте разные сценарии нагрузки. Доступность может снижаться при пиковых нагрузках, при стресс‑нагрузке и при деградации внешних сервисов. Поэтому необходимо зафиксировать требуемые показатели для нормального режима, режима с повышенной нагрузкой и аварийных условий.Немного подытожим. Сегодня мы рассмотрели нефункциональные требования, которые наиболее важны для бизнеса и менеджеров, поскольку влияют на деньги. Это производительность, доступность и масштабируемость.Для того чтобы было проще воспринимать информацию, я решила разбить требования на три логических блока, так что в следующей части мы рассмотрим нефункциональные требования, которые влияют на архитектуру и код — сопровождаемость, надежность и безопасность.Продолжить осваивать базу системного анализа — от фиксации требований до проектирования интерфейсов и API — можно на курсе «Системный аналитик. Basic». Это практический курс: научитесь формулировать NFR, декомпозировать и приоритизировать требования, оформлять документацию и проводить приёмку. Чтобы оставаться в курсе актуальных технологий и трендов, подписывайтесь на Telegram-канал OTUS.Теги:нефункциональные требованиядоступностьпроизводительностьмасштабируемостьХабы:Блог компании OTUSУправление разработкойАнализ и проектирование систем",911,0,0,9 мин,https://habr.com/ru/companies/otus/articles/963896/,12535,1516,3
Аудит доступности веб-приложения Приорбанка,archik,2025-11-12T18:55:34.000Z,"['Accessibility *', 'HTML *', 'CSS *', 'JavaScript *']","archik 16 часов назадАудит доступности веб-приложения ПриорбанкаУровень сложностиПростойВремя на прочтение8 минКоличество просмотров627Accessibility * HTML * CSS * JavaScript * Артур БасакWeb UI/UX EngineerЭта статья выросла из ростка моего цифрового сада.Я долго думал, аудит какого веб-приложения провести первым для своей небольшой заметки, чтобы показать наглядно подход из 5 шагов. С одной стороны, это должно быть что-то массовое, чем могут пользоваться большое количество людей с ограничениями. С другой стороны, владелец портала должен иметь достаточный бюджет для того, чтобы иметь возможность нанять высококвалифицированных веб-разработчиков, которые могут реализовать доступность.Наивно ожидать доступности от госучреждений, сайтов госполиклиник или порталов чиновников — там нет таких зарплат, как в частном секторе коммерческого ИТ. Также приложение должно быть хорошо известно обывателю и быть на слуху, даже если он им не пользуется.Кто же будет первым? Более 12 лет я являюсь клиентом Приорбанка (Беларусь, РБ). Банки — это важные сервисы, они определенно должны быть доступны людям с ограничениями. Я решил начать именно с него, это сервис который важен и для меня, поэтому в двойне интересно это сделать. Да простят меня сотрудники банка!)Методология аудитаЯ провел быстрый аудит из 5 шагов веб-приложения Приорбанка. Приложение большое, поэтому я выбрал проверить достаточно простой и популярный кейс — авторизироваться (или зарегистрироваться) и проверить свой счет на карте. Итак, пройдем быстро по всем пунктам и узнаем что получилось.1. Заявление о доступности и Skip LinksСразу отмечу отсутствие двух фундаментальных элементов:Заявление о доступности (Accessibility Statement)Ссылки для пропуска навигации (Skip Links)Подобные элементы — базовый шаг, и их отсутствие сигнализирует о поверхностном подходе. Отсутствие этих шаблонов — явные маркеры того, что подход к доступности у разработчиков и менеджеров продукта не очень серьезный, а значит можно ожидать явных проблем в этой области дальше.Подвал сайта Приорбанк2. Навигация с клавиатурыПытаюсь навигировать клавиатурой на главной странице и немного удивляюсь, что после верхнего меню попадаю на контролы управления динамической каруселью, которая рекламирует кредиты, реферальную программу и мобильное приложение. Только вот обо всем этом я не узнаю с плохим зрением, так как контент динамический и не размечен aria-live, а значит скринридер эти изменения озвучивать не будет.Кажется, что форма логина — это главная фича этого экрана, и я должен иметь возможность попасть на нее как можно раньше, но нет. Между шапкой и формой мне надо пройти рекламные сообщения.Карусель с рекламой акций банкаЕще грустнее, что фокус на многих элементах выключен. На самой главной кнопке формы «Войти» нет стилизации фокуса (:focus-visible). Очевидно, это очень плохо, так как я не вижу, где нахожусь. Проблема стара как мир: дизайнеры и разработчики не уделили время дизайну фокуса, и он работает так, как получилось.Фокус на кнопке Войти формы логинаА вот у соседней кнопки «Регистрация» фокус есть, но очень кривенький. А знаете почему? Потому что это не кнопка, а ссылка. На ссылках стилизация фокуса на сайте есть. Но странно, почему это ссылка, а не кнопка? Тут уже вопросы к проработке дизайн-системы. Кажется, что рядом с primary-кнопкой должна быть secondary. Причем ссылка обычно используется, когда ведет на физически другую страницу или маршрут, здесь же при регистрации открывается модальное окно.Фокус на кнопке Регистрация формы логинаМежду тем, закрыть модальное окно для русскоязычного пользователя непросто. Кнопка закрыть размечена английским словом «Close», хотя сайт на русском. Но в любом случае, наличие такого aria-label лучше, чем его полное отсутствие. Скорее всего, это что-то стандартное у UI Kit или просто разработчик на автомате вставил, чтобы кнопка с иконкой имела хоть какую-то подпись.Модальное окно регистрации и кнопка закрыть с подписью Close3. Контрастность цветовЧерный, желтый, белый — неплохое сочетание, достаточно контрастная комбинация.Узким местом стали несколько вторичных подписей и ссылок, а также плейсхолдеры инпутов. Тут текст серого цвета на белом, и он имеет плохой контраст. Соотношение, как видим на картинке, 2.4:1, что не попадает в WCAG Level AA.Вообще, даже без WCAG, можно просто прищуриться и попытаться разобрать текст, попробуйте, это будет сложновато.Проверка контраста цвета через CCA4. Работа со скринридером и клавиатуройВключаю плагин размытия, чтобы сэмулировать свое плохое зрение, и начинаю работать с сайтом через скринридер VoiceOver и клавиатуру. Вот тут самое интересное, потому что это реальная работа человека с ограничениями.Натыкаюсь опять на контролы управления каруселью вместо формы логина. Они никак не подписаны, сделаны ссылками, все это вводит в большое заблуждение конечного пользователя. Кажется, карусель сделана каким-то древним jQuery-плагином (вроде Owl Carousel) или руками с нуля, современные штуки, вроде Swiper, даже старых версий без настройки очень неплохо поддерживают доступность.Фокус на контролах управления каруселью в включенным VoiceOverНажимаю на кнопку «Регистрация» — открывается модальное окно, и я не попадаю в него. Фокус остается под модальным окном, и чтобы переместиться в него, надо пройти весь контент подложки. Из позитивного: когда я все же попал внутрь окна, то там фокус замкнулся (focus trap) и из окна уже не выходил. Но то, что при открытии я никак не был уведомлен об окне и тем более тут же не попал в него, — это большая проблема. Она полностью ломает навигацию.Фокус остался на кнопке Регистрация после открытия модального окнаВсе это говорит о незрелой дизайн-системе и не очень качественной UI-библиотеке. Как правило, современные фреймворки и даже нативный HTML dialog умеют захватывать фокус при открытии окна и автоматически направлять его на первый элемент внутри окна. Тут модалка сделана обычными div-контейнерами.Фокус остался на кнопке Регистрация после открытия модального окнаОшибки внутри формы не размечены динамическим контентом (aria-live) и никак не связаны с полями ввода (aria-errormessage), поэтому при попытке нажать «Продолжить» и не заполнив обязательные поля, скринридер ничего мне не сообщает об ошибках. То же самое и на форме логина.Нет уведомления от скринридера о появившихся ошибках на форме регистрацииКстати, бывших тестировщиков не бывает, нашел баг, где плейсхолдер поля ввода наложился на маску:Плейсхолдер поля ввода номера телефона наложился на маску вводаЕсть и другие проблемы для незрячих. Обнаружил несколько ссылок, которые были очень неинформативны. К примеру, номера для связи с диспетчером или обратной связью никак не связаны с подписями и озвучиваются просто как наборы цифр без контекста. Кнопка «Вверх» не имеет подписи и вводит в замешательство, так как непонятно, что она делает.Номер телефона диспетчера диктуется скринридером без контекстаКнопка подкрутки вверх не имеет подписиОк, идем наконец внутрь. Попадаю в личный кабинет и... Ну, тут тоже так быстро шапку и меню не пропустить. Мне всегда было очень интересно, как сделать макеты дебетовых карт доступными. И каково мое разочарование, когда я узнаю, что не могу попасть на них клавиатурой, потому что это не контролы. Хотя курсором мыши они кликабельны, чтобы провалиться внутрь. Ну и контент в них — это просто несвязные символы и цифры, обернутые в div и span с очень странными аттрибутами title. Для скринридера это фиаско.Компоненты дебетовых картЛадно. Идем в таблицу моих продуктов, может, тут я смогу понять, сколько денег на моей карте. Сразу же проблема с озвучиванием колонки с карточками: мне говорит, что ячейка пустая, хотя в ней отображается иконка типа карты (Visa, MasterCard, МИР и т.д.). Было бы хорошо это озвучить.Скринридер озвучивает таблицу моих продуктов - столбец тип картыОк, в последнем столбце мне удается все-таки услышать, сколько денег у меня на счету.Скринридер озвучивает сумму на карте5. Сканирование инструментамиПоследний шаг — это сканирование инструментами. Axe DevTools показал 12 ошибок на странице логина и 14 ошибок на главном дашборде личного кабинета. В основном это проблемы контраста серого на белом, неверные роли ARIA и даже картинки без alt-текста (среди них есть и не декоративные).Axe DevTools показывает 12 ошибок на странице логинаAxe DevTools показывает 14 ошибок на странице личного кабинетаКнопка профиля в шапке сделана картинкой без каких-либо подписей. Из-за того, что это не контрол, на нее нельзя даже попасть клавиатурой. Тут явно надо button, причем с aria-haspopup, так как она открывает диалоговое окно с настройками и информацией о профиле.Верстка кнопки профиля в шапке сайтаWAVE насчитал 16 ошибок на главной и 49 на дашборде (почему так много? 47 из них связаны с отсутствием href у ссылок, т.е. это просто так сверстаны кнопки).WAVE показывает 16 ошибок на странице логинаWAVE показывает 49 ошибок на странице личного кабинетаИнтересно глянуть на структуру заголовков: H1, а потом сразу H4 и H5. Причем именно второй уровень — это H5, а H4 — это заголовок третьего уровня в одном из виджетов. Такая ситуация это классика — встречается очень часто, так как на таких больших проектах над разными виджетами работают разные команды, и в семантике полный хаос и никто не думает про информационную архитектуру (IA).WAVE подсвечивает заголовки в личном кабинете пользователяLighthouse показал 81% доступности. Из интересного в дополнение к другим инструментам он подсветил, что контролы карусели на главной должны быть большего размера, чтобы на них было легче попасть курсором.Lighthouse подсвечивает проблемы с размерами контролов управления карусельюВыводыРезультаты аудита разочаровывают. Для такого крупного и финансово успешного банка, каким является «Приорбанк», уровень доступности его ключевого цифрового продукта недопустимо низок. Обнаруженные проблемы — от отсутствия базовой навигации до полной недоступности критического функционала для скринридеров — создают непреодолимые барьеры для людей с ограниченными возможностями.У банка, несомненно, есть ресурсы для формирования сильной фронтенд-команды и привлечения экспертов по доступности. Однако текущее состояние приложения говорит о том, что этот аспект пользовательского опыта либо не приоритизирован, либо реализуется бессистемно. Пользователям с инвалидностью пользоваться этим сервисом крайне неудобно, а в случае с банковскими операциями — практически невозможно.Скорее всего, проблема в том, что этот фронтенд жутко старое легаси, которое было немного разукрашено и подштопано под современный визуал. Мой анализатор показывает следующий стэк: jQuery UI, jQuery Mobile, Kendo UI, Bootstrap, Font Awesome, RequireJS, Animate.css.Стэк явно очень старый и отчасти это основная проблема. Взять тот же Font Awesome, он уже давно критикуется за свою любовь вставлять иконки тэгом <i>, что нарушает семантику и может плохо сказываться на доступности. У Приорбанка именно такие иконки, хотя Font Awesome можно настроить и на более нейтральный тэг вроде <span>.В таком легаси будет титанически сложно навести порядок. Уровень доступности jQuery UI по умолчанию очень низкий и безнадёжно устарел. Создание доступных интерфейсов с этой библиотекой требует огромных дополнительных усилий. А ведь я посмотрел очень маленький кусок приложения. С другой стороны, выглядит как отличный кейс для ИИ.Основные проблемы:Отсутствие базовых элементов доступности: нет заявления о доступности и skip linksПроблемы с навигацией клавиатурой: неправильный порядок табуляции, отсутствие видимого фокуса на ключевых элементахНизкий контраст: вторичный текст и плейсхолдеры не соответствуют WCAG Level AAПроблемы со скринридерами: модальные окна не захватывают фокус, отсутствие aria-live для динамического контента, неинформативные ссылки, плохая поддержка валидации форм скринридеромСемантические ошибки: неправильная структура заголовков, использование изображений вместо кнопок, отсутствие альтернативного текстаРекомендации:Внедрить skip links и заявление о доступности, что само собой будет служить уже ориентиром и дорожной картой по улучшения этого функционалаПереработать дизайн фокуса для всех интерактивных элементовУлучшить контрастность вторичного текста и плейсхолдеровИсправить работу модальных окон (использовать нативный dialog или правильно управлять фокусом)Добавить aria-live регионы для динамического контента и другую ARIA-разметку подкрепляя это тестированием на основных популярных скринридерах VoiceOver, JAWS, NVDAИсправить семантику: правильная структура заголовков, использование кнопок вместо изображений для интерактивных элементовДобавить альтернативный текст для всех значимых изображений и скрыть декоративные через роль role=""presentation""P.S.Не хочется только ругать свой банк, я уверен, что внутри есть множество продуктов, где доступность это одно из требований.К примеру, посмотрите на банкомат Приора, где есть панели с рельефно-точечным тактильным шрифтом для незрячих пользователей.Фото панели банкомата со шрифтом БрайляМеня всегда радуют такие штуки. Хотя, искренне не знаю, каков процент тех, кто знает шрифт Брайля из тех кто плохо видит или не видит совсем. На мой взгляд аудио-дикторы и Voice UI будут более востребованы в будущем.P.P.S.Подумываю сделать серию таких вот легких аудитов, без какого-то rocket science с очень простыми и очевидными советами как что-то улучшить. Если тема интересна, то обязательно пишите кто мог бы быть следующим кандидатом на аудит.Теги:accessibilityauditfrontendfrontend-разработкафронтендфронтенд-разработкадоступность сайтадоступностьwcagХабы:AccessibilityHTMLCSSJavaScript",627,0,0,8 мин,https://habr.com/ru/articles/965834/,13562,1838,4
"Практичные Python-привычки, которые реально повышают качество кода",robomania,2025-11-13T06:15:35.000Z,['Python *'],"robomania 5 часов назадПрактичные Python-привычки, которые реально повышают качество кодаВремя на прочтение2 минКоличество просмотров1.3KPython * Из песочницыНедавно я начал систематизировать практики которые обычно используются и помогают экономить время. Я хочу поделиться некоторыми из них. 1. Явное состояние и мемоизация   Скрытые состояния в замыканиях и декораторах часто приводят к трудноуловимым багам.  from functools import wraps

def memoize(func):
    cache = {}
    @wraps(func)
    def wrapper(*args):
        if args not in cache:
            cache[args] = func(*args)
        return cache[args]
    return wrapper Использование @wraps сохраняет имя функции, docstring и метаданные — критично для дебага и интеграции с Flask.   2. Асинхронность для продакшн   Асинхронность часто ухудшает код, если использовать её неправильно.  import asyncio, aiohttp

TOTAL_REQUESTS = 1_000_000
sem = asyncio.Semaphore(1000)

async def fetch(session, url):
    async with sem:
        try:
            async with session.get(url) as resp:
                return await resp.text()
        except aiohttp.ClientError:
            return None
          
async def main():
    urls = [f""https://api.site/{i}"" for i in range(TOTAL_REQUESTS)]
    async with aiohttp.ClientSession() as session:
        for i in range(0, len(urls), 10_000):
            chunk = urls[i:i+10_000]
            tasks = [fetch(session, url) for url in chunk]
            await asyncio.gather(*tasks) Контроль через Semaphore + чанки предотвращает OOM и блокировки API. Используйте create_task() для управления жизненным циклом корутин.   3. Ошибки и raise   Не ловите всё подряд и используйте новые возможности языка.  # Python 3.11+
user.is_admin or raise PermissionError(""Not allowed!"") raise как выражение и except* (Python 3.11) делают обработку ошибок лаконичной и безопасной:  # Python 3.11+
try:
    await asyncio.gather(fail1(), fail2())
except* ValueError as ve:  # Только ValueError
    print(f""ValueErrors: {ve.exceptions}"")
except* TypeError as te:   # Только TypeError
    print(f""TypeErrors: {te.exceptions}"")4. Типизация и валидация  from pydantic import validate_call
from typing import Annotated
from pydantic.types import Gt, Ge, Le 

@validate_call
def calculate_discount(
        price: Annotated[float, Gt(0)],
        discount: Annotated[float, Ge(0), Le(100)]
) -> float:
    return price * (1 - discount / 100) Constraints прямо в аннотациях делают сигнатуры самодокументируемыми и безопасными без дублирования проверок.  5. Ленивая загрузка и кеширование  @cached_property и functools.cache экономят время при дорогих вычислениях.  from functools import cached_property
import time

class UserReport:
    def __init__(self, user_id):
        self.user_id = user_id        
    @cached_property
    def total_spent(self):
        print(""Querying database..."")
        time.sleep(2)  # expensive call
        return 199.99
      
r = UserReport(123)
print(r.total_spent)     # computed once
print(r.total_spent)     # cached instantly
del r.__dict__[""total_spent""]
print(r.total_spent)     # recomputed after cache resetРезультат хранится в dict объекта, можно сбросить при необходимости. Отлично подходит для API-запросов и конфигураций.  6. Python 3.14+Новые возможности языка ускоряют работу и упрощают код:uuid7() — уникальные и сортируемые по времени ключиContextVar как контекстный менеджерt-strings (t""..."") для отложенных шаблоновsubTests для granular тестированияfrom string.templatelib import Template

def render(template: Template):
    parts = []
    for item in template:
        if isinstance(item, str):
            parts.append(item)
    return """".join(parts)

user = ""Alice""
role = ""admin""
t = t""user: {user} — role: {role}""
s = render(t)Жалко что не добавили .format() для t -строкЗаключениеДаже небольшие изменения в подходе к разработке дают ощутимый эффект на качество кода и скорость разработки.   Теги:pythonпривычки программистаХабы:Python",1300,0,0,2 мин,https://habr.com/ru/articles/965908/,3960,448,1
Распознаём позу человека во Flutter Web с MediaPipe,alexeyinkin,2025-11-12T19:46:55.000Z,"['Dart *', 'Flutter *']","alexeyinkin 15 часов назадРаспознаём позу человека во Flutter Web с MediaPipeУровень сложностиСреднийВремя на прочтение20 минКоличество просмотров268Dart * Flutter * ТуториалПереводАвтор оригинала: Alexey InkinДавайте распознаем позу по видео с вебкамеры вот так:Финальное приложение, которое мы сделаем в этой статье.Для этого есть библиотека MediaPipe, которая может распознавать много всего в картинках, текстах и звуках. Среди прочего там есть модель для распознания положения тела на изображениях.Здесь можно попробовать официальное демо.Ещё есть CodePen, чтобы быстро попробовать код на JavaScript:https://codepen.io/mediapipe‑preview/pen/abRLMxNНо эта модель работает только в Android, iOS, на Python и JavaScript, но не во Flutter напрямую.Кто‑то сделал пакет flutter_mediapipe, но он заброшен уже 4 года и не поддерживает веб.Поэтому давайте подключим официальную реализацию на JavaScript в качестве собственного веб‑плагина для Flutter.Готовое демо моего приложения — здесь (только Chrome)Скачайте исходники здесь (потому что я буду пропускать некоторые вещи):https://github.com/alexeyinkin/flutter‑mediapipeСоздаём плагинПлагин — это специальный вид пакета Dart, который подключает разные имплементации в зависимости от того, под какую платформу собираем приложение.Вот отличный официальный учебник, как писать плагины:https://docs.flutter.dev/packages‑and‑plugins/developing‑packagesЕсть ещё прекрасное введение в написание именно веб‑плагинов, от автора официального пакета url_launcher. Там рассказано, как они добавили поддержку веба в этот пакет, когда Flutter только начал поддерживать веб:Часть 1 объясняет базовый подход — такой же, как был в плагинах для Android и iOS: так называемый method channel, чтобы делегировать что‑то нативному коду на этих платформах.Часть 2 упрощает это и убирает method channel, потому что веб‑плагины и так написаны на Dart, а значит, можно вызывать все методы имплементации напрямую.Обе статьи обращаются только к стандартному API браузера и не обращаются к произвольному JavaScript, взятому где‑то ещё. Поэтому в этой статье я расскажу, как обращаться к произвольному JavaScript, основываясь на всём, что вы узнали в тех статьях.Используя архитектуру из последней статьи по url_launcher, я сделал три пакета Dart:flutter_mediapipe_vision — главный пакет. Все приложения, которые хотят распознавать позы на изображениях, подключают его как зависимость. И только его. Он уже подтягивает в проект другие пакеты как зависимости и вызывает методы конкретной имплементации. Ненужные имплементации Flutter сам стрясёт с помощью tree‑shaking.flutter_mediapipe_vision_platform_interface описывает интерфейс, которому каждая имплементация должна соответствовать. Этот пакет не делает ничего полезного, только подменяет имплементацию, чтобы первый пакет об этом ничего не знал.flutter_mediapipe_vision_web — реализация для веба, главный фокус этой статьи. Он зависит от второго пакета, потому что содержит имплементацию интерфейса, описанного там, и ничего не знает о первом пакете. Первый же пакет зависит от него и подтягивает его рекурсивно во все приложения, где используется.flutter_mediapipe_visionКаким должен быть интерфейс конечного пользователя? Статические функции хорошо работают с подменяемыми имплементациями:class FlutterMediapipeVision {
  static Future<void> ensureInitialized() async {
    await FlutterMediapipeVisionPlatform.instance.ensureInitialized();
  }

  static Future<PoseLandmarkerResult> detect(Uint8List bytes) async {
    return await FlutterMediapipeVisionPlatform.instance.detect(bytes);
  }
}Этот класс преобразует вызовы статических функций в вызовы методов конкретной имплементации.Первая функция инициализирует модель. Её можно назвать как угодно, но ensureInitialized() — это удобно. Вы же помните WidgetsFlutterBinding.ensureInitialized()Вторая функция получает байты изображения (каждого кадра) и вызывает detect() на модели. Эта функция называется именно так во всех имплементациях MediaPipe.Обратите внимание на возвращаемый тип. Скоро мы его опишем.flutter_mediapipe_vision_platform_interfaceТипы данныхНачнём с типов. В библиотеке JavaScript, которую мы подключим, есть типы для распознанных точек и общего результата. Однако, наш плагин должен возвращать что‑то независимое от платформы, поэтому нужно описать собственные типы.Это точка, распознанная в позе:class NormalizedLandmark {
  final double x;
  final double y;

  const NormalizedLandmark({required this.x, required this.y});

  Offset get offset => Offset(x, y);
}Она называется normalized, потому что x и y будут от 0 до 1, если они помещаются в кадре. Они также могут быть меньше нуля или больше единицы, если изображение обрезано и модель думает, что эта конкретная точка находится за кадром — как мой локоть здесь:Некоторые точки находятся вне кадра, и модель пытается отгадать, где мой локоть.А почему бы нам не использовать Offset из dart:ui? Библиотека ещё возвращает z — расстояние от камеры, и ещё несколько интересных вещей, которые нам пока не нужны, но хорошо иметь возможность их потом добавить. Поэтому просто Offset нам не хватит.Кроме того, этот тип NormalizedLandmark есть во всех имплементациях: TypeScript, Java, и др. Поэтому лучше, когда всё согласуется.Дальше — результат распознания всего изображения:class PoseLandmarkerResult {
  final List<List<NormalizedLandmark>> landmarks;

  const PoseLandmarkerResult.empty() : landmarks = const [];
  const PoseLandmarkerResult({required this.landmarks});
}Библиотека возвращает список распознанных поз (первое измерение списка). Каждая поза — список точек с фиксированными индексами (второе измерение списка):Индексы точек в PoseLandmarkerResult.Базовый класс имплементацийС этими типами мы теперь можем описать класс, который каждая имплементация будет наследовать:abstract class FlutterMediapipeVisionPlatform extends PlatformInterface {
  FlutterMediapipeVisionPlatform() : super(token: _token);

  static final Object _token = Object();

  static FlutterMediapipeVisionPlatform _instance =
    FlutterMediapipeVisionMethodChannel();

  static FlutterMediapipeVisionPlatform get instance => _instance;

  static set instance(FlutterMediapipeVisionPlatform instance) {
    PlatformInterface.verify(instance, _token);
    _instance = instance;
  }

  Future<void> ensureInitialized() {
    throw UnimplementedError();
  }

  Future<PoseLandmarkerResult> detect(Uint8List bytes) {
    throw UnimplementedError();
  }
}Тут много всего.Самое главное — мы определяем функции бизнес‑логики ensureInitialized и detect.Дальше — нужен какой‑то _instance по умолчанию, который мы создаём. Подробнее о нём — чуть позже.И наконец, есть объект _token. Вот зачем он нужен. Разработчики Flutter могут что‑то поменять в базовом классе PlatformInterface, и это не должно ломать наш код. Поэтому они сказали, что этот класс можно только наследовать, но не реализовывать. Мы здесь используем extends, поэтому нам проблемы не грозят. Но вообще мы не можем контролировать, кто ещё сделает имплементацию нашего плагина под ещё какую‑нибудь платформу (или даже подменит нашу имплементацию для веба), и мы не влияем на то, будет ли у них extends или implements. Если они сделают implements, их код может какое‑то время работать, а потом внезапно сломается с каким‑нибудь обновлением для какой‑нибудь одной платформы. Поэтому мы делаем такую штуку, которая сломает их код раньше. Мы используем объект _token, у которого единственная работа — быть всегда одним и тем же (как у Зюганова). Если чья‑то реализация сделает implements, она не сможет предъявить тот самый _token, поэтому set instance выдаст ошибку.Так, а что там за instance по умолчанию?const MethodChannel _channel
  = MethodChannel('ainkin.com/flutter_mediapipe_vision');

class FlutterMediapipeVisionMethodChannel
  extends FlutterMediapipeVisionPlatform {
  @override  Future<void> ensureInitialized() async {
    await _channel.invokeMethod<void>('ensureInitialized');
  }

  @override  Future<PoseLandmarkerResult> detect(Uint8List bytes) async {
    final native = await _channel.invokeMethod<void>('detect');
    throw UnimplementedError('TODO: Convert.');
  }
}В стародавние времена, когда Flutter поддерживал только Android и iOS, единственным способом вызвать что‑то нативное было создать объект MethodChannel и вызывать на нём «методы» с помощью invokeMethod(name). Flutter смотрел на название канала и название метода и дёргал нужный метод в нативном коде. Не было никаких подменяемых instance, потому что вся подмена работала при сборке приложения под конкретную платформу.Для обратной совместимости, если Flutter не проист наш плагин сделать ничего необычного, мы по умолчанию должны продолжать делать то же самое. Поэтому мы закладываем это в instance по умолчанию.Однако, мы не будем сейчас поддерживать другие платформы, кроме веба. Поэтому нам не нужна такая имплементация. Нам несложно вызвать потенциальный нативный ensureInitializee() и подождать, пока он вернёт управление. Но мы пока не можем сделать ничего осмысленного в detect(), потому что это потребует какой‑то контракт на обмен данными с нативным кодом, а мы пока не хотим с этим разбираться. Поэтому выбросим ошибку.flutter_mediapipe_vision_webНачнём плагин с вот этого:class FlutterMediapipeVisionWeb extends FlutterMediapipeVisionPlatform {
  static void registerWith(Registrar registrar) {
    FlutterMediapipeVisionPlatform.instance = FlutterMediapipeVisionWeb();
  }

  Future<void>? _initFuture;

  @override
  Future<void> ensureInitialized() =>
    _initFuture ?? (_initFuture = _initOnce());

  Future<void> _initOnce() async {
    // ...
  }

  @override
  Future<PoseLandmarkerResult> detect(Uint8List bytes) async {
    // ...
  }
}registerWith() — это волшебная функция, которую Flutter вызовет где‑то на ранней стадии после запуска, если приложение собрано для веба. Создаём наш instance и устанавливаем его для использования во всех вызовах, зависящих от платформы.Добро пожаловать в веб!Код Dart транспилируется в JavaScript или WASM. В любом случае у него есть прямой доступ ко всему в браузере, как у любого кода на JavaScript — через глобальную переменную globalContext, определённую в dart:js_interop. Поэтому нет почти никакой разницы между объектами Dart и JavaScript, они все однородные для браузера, в котором приложение работает.Загружаем код MediaPipeЯ позаимствовал этот кусок из Firebase и немного упростил. Обидно, что нужно самим писать код подгрузки JavaScript. Лучше бы во Flutter сделали для нас что‑то, что можно вызвать одной строкой.Этот код загружает скрипт из src и помещает объект его модуля в глобальную переменную, определяемую windowVar:  Future<void> _injectSrcScript(String src, String windowVar) async {
    final web.HTMLScriptElement script =
      web.document.createElement('script') as web.HTMLScriptElement;
  
    script.type = 'text/javascript';
    script.crossOrigin = 'anonymous';
  
    final stringUrl = src;
    script.text =
      '''
      window.my_trigger_$windowVar = async (callback) => {
        console.debug(""Initializing MediaPipe $windowVar"");
        callback(await import(""$stringUrl""));
      };
      ''';
  
    web.console.log('Appending a script'.toJS);
    web.document.head!.appendChild(script);
  
    Completer completer = Completer();
    globalContext.callMethod(
      'my_trigger_$windowVar'.toJS,
      (JSAny module) {
        globalContext[windowVar] = module;
        globalContext.delete('my_trigger_$windowVar'.toJS);
        completer.complete();
      }.toJS,
    );
    await completer.future;
  }Значение windowVar может быть любым, если оно не конфликтует с другими глобальными переменными. Начнём initOnce() с загрузки кода MediaPipe:const _windowVar = 'flutter_mediapipe_vision';
// ...

  Future<void> _initOnce() async {
    await _injectSrcScript(
      'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js',
      _windowVar,
    );
    // ...Это загрузит последнюю версию. Ещё можно было скачать её один раз и положить в assets нашего пакета, но пока хватит и этого.Когда этот код завершится, модуль MediaPipe будет в глобальной переменной, и к нему можно будет обращаться через globalContext[_windowVar]. Можно сразу же вызывать нужные нам функции:globalContext[_windowVar]['PoseLandmarker'].callMethod(
  'createFromOptions',
  ...
);Но лучше обеспечить безопасность типов.Накладывание интерфейсов Dart на объекты JavaScriptПомните наш класс NormalizedLandmark? На стороне JavaScript ему соответствует обычный объект со свойствами x и y, к которым можно обратиться из Dart через landmark['x'] и landmark['y'], потому что у JSObject есть оператор []. Но так можно легко допустить ошибки. К счастью, мы можем наложить на этот объект интерфейс Dart:extension type NormalizedLandmark._(JSObject _) implements JSObject {
  external num get x;
  external num get y;
}И теперь, если мы приведём этот объект точки, полученный из MediaPipe, к этому классу, то компилятор гарантирует, что обращения к свойствам будут правильными:final landmark = unsafeLandmark as NormalizedLandmark;
print(landmark.x);Extension typesНо что это за интерфейс такой? Это конструкция extension type, которая буквально и означает наложение на любой объект интерфейса, который в ней определён — без оборачивания в объект‑адаптер. Это абстракция времени компиляции, которая не существует во время выполнения. Можете прочитать про неё в документации Dart здесь.Давайте чуть отвлечёмся от главной задачи и разберём extension types подробнее, чтобы потом вернуться к JavaScript с этим новым знанием.Документация Dart по extension types содержит такой пример, который сужает интерфейс int до одного оператора:extension type IdNumber(int id) {
  // Wraps the 'int' type's '<' operator:
  operator <(IdNumber other) => id < other.id;
  // Doesn't declare the '+' operator, for example,
  // because addition does not make sense for ID numbers.
}
// ...
final safeId = IdNumber(42);Этот код говорит, что:Мы используем какой‑то IdNumber, чтобы работать с какими‑то ID.Это не класс, существующий во время выполнения, потому что это слишком дорого, поэтому extension type.Вместо класса мы используем int для хранения этих ID, потому что int — это самый дешёвый способ хранения чисел. Поэтому после названия типа идёт (int id) — это показывает, что именно мы оборачиваем.Этот интерфейс лишает наш int всех методов, операторов и свойств, которые мы не объявим далее.Мы описываем operator <, и это всё, что можно делать с нашим ID.Конструктор этого типа не описан как функция‑член, потому что конструктор как функция‑член нужен только настоящим типам, которым потенциально может понадобиться несколько конструкторов, потому что в них может проделываться какая‑то работа и мы можем захотеть проделывать её по‑разному. Но у extension type конструктор — это просто оборачивание во время компиляции, которое не превращается ни в какой код, поэтому у него всегда ровно один конструктор, и не было смысла делать для него синтаксис функции‑члена. Поэтому для него сделали такой синтаксис: (int id) сразу после названия типа.Ладно, а как это всё применимо к нашему примеру? Он, вроде бы, совсем другой:extension type NormalizedLandmark._(JSObject _) implements JSObject {
  external num get x;
  external num get y;
}В нашем примере:Мы оборачиваем JSObject и сразу же реализуем тот же интерфейс JSObject. Это значит, что мы не лишаем объект никаких членов этого интерфейса и будем только добавлять. Это нужно, потому что вскоре у нас будет JSArray<NormalizedLandmark>, а JSArray может принимать только JSObject и его подклассы, поэтому это наследование нужно сохранить.Параметр конструктора называется _, потому что в отличие от примера с ID мы не перенаправляем никакие вызовы на этот объект, а значит, ему не нужно никакое имя.Конструктор мы сделали приватным с помощью ._ Поэтому мы можем только приводить объекты к этому типу с помощью as, но не создавать их напрямую.Помечаем геттеры как external. Так мы говорим компилятору, что эти свойства уже есть в объекте JavaScript, и они просто будут работать.Когда мы оборачиваем объекты, исходящие от JavaScript или WASM, в extension type, это называется «interop type„.“»Создаём все interop typesНам нужно гораздо больше таких типов, чтобы создать объект, обнаруживающий позы на картинках, вызывать его методы и возвращать результат.Можно написать эти типы вручную, глядя на исходники MediaPipe на TypeScript:fileset_resolver.ts.templatelandmark.d.tspose_landmarker.tspose_landmarker_options.d.tspose_landmarker_result.tstask_runner_options.d.tsvision_task_options.d.tsВ теории все interop types в Dart можно сгенерировать из исходников TypeScript, но я пока с этим не разбирался. Написать их вручную — хорошая практика для начала.Вот, что я выцепил из TypeScript — только те методы и свойства, которые нам понадобятся.Результат функции detect:extension type PoseLandmarkerResult._(JSObject _) implements JSObject {
  external JSArray<JSArray<NormalizedLandmark>> get landmarks;
}Объект, распознающий позы:extension type PoseLandmarker._(JSObject _) implements JSObject {
  external JSPromise<PoseLandmarker> createFromOptions(
    WasmFileset fileset,
    PoseLandmarkerOptions options,
  );

  external void detect(HTMLImageElement img, JSFunction callback);
}Объект параметров для него:extension type PoseLandmarkerOptions._(JSObject _) implements JSObject {
  external PoseLandmarkerOptions({
    BaseOptions baseOptions,
    int numPoses,
    String runningMode,
  });

  external BaseOptions get baseOptions;
  external int get numPoses;
  external String get runningMode;
}Вложенный объект в нём:extension type BaseOptions._(JSObject _) implements JSObject {
  external BaseOptions({String modelAssetPath});
  external String get modelAssetPath;
}WasmFileset, что бы это ни значило:extension type WasmFileset._(JSObject _) implements JSObject {}Fileset resolver:extension type FilesetResolver._(JSObject _) implements JSObject {
  external JSPromise<WasmFileset> forVisionTasks(String basePath);
}И наконец, главный объект модуля MediaPipe:import 'fileset_resolver.dart' as fsr;
import 'pose_landmarker.dart' as plm;

extension type MediaPipe._(JSObject _) implements JSObject {
  external fsr.FilesetResolver get FilesetResolver;
  external plm.PoseLandmarker get PoseLandmarker;
}Инициализируем модельПродолжим писать функцию инициализации плагина:MediaPipe get mp => globalContext[_windowVar] as MediaPipe;

PoseLandmarker? _landmarker;Future<void> _initOnce() async {
  await _injectSrcScript(
    'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js',
    _windowVar,
  );

  final fs = await mp.FilesetResolver.forVisionTasks(
    'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm',
  ).toDart;

  final options = PoseLandmarkerOptions(
    baseOptions: BaseOptions(
      modelAssetPath:
        ""packages/flutter_mediapipe_vision_platform_interface/assets/""
        ""assets/models/pose_landmarker_lite.task"",
    ),
    numPoses: 5,
    runningMode: ""IMAGE"",
  );
  
  _landmarker = await mp.PoseLandmarker.createFromOptions(fs, options).toDart;
}Файл модели ещё нужно скачать отдельно (я выбрал версию lite) вот здесь:https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarkerПоскольку модель одна и та же для MediaPipe на всех платформах, лучше всего положить её в общий пакет, а не в веб‑пакет. Лучше всего подойдёт flutter_mediapipe_vision_platform_interface, потому что все имплементации от него зависят, хотя формально модель не относится к интерфейсу.В общем, когда функция завершится, мы получим объект, обнаруживающий позы, в поле _landmarker.Распознаём позыВсю работу делает этот метод:  @override  Future<PoseLandmarkerResult> detect(Uint8List bytes) async {
    final el = await _createImageFromBytes(bytes);
    // ...
  }Начинаем с создания HTMLImageElement из переданных байт, потому что функция detect в MediaPipe принимает именно его. Вот так:Future<web.HTMLImageElement> _createImageFromBytes(Uint8List bytes) async {
  final completer = Completer();

  final blob = web.Blob(
    [bytes.toJS].toJS,
    web.BlobPropertyBag(type: _detectImageFormat(bytes)),
  );
  final imageUrl = web.URL.createObjectURL(blob);
  final el = web.document.createElement('img') as web.HTMLImageElement;

  el.onload = () {
    web.URL.revokeObjectURL(imageUrl);
    completer.complete();
  }.toJS;
  el. {
    web.URL.revokeObjectURL(imageUrl);
    completer.completeError('Cannot load the image.');
  }.toJS;

  el.src = imageUrl;
  await completer.future;
  return el;
}В JavaScript конструктор объекта Blob (binary long object) принимает двумерный массив байт. Поэтому сначала превращаем на Uint8List в массив JavaScript, вызывая его геттер .toJS. Он есть у многих типов Dart, чтобы превратить их во что‑то, подходящее для передачи в функции JavaScript. Потому что код Dart хоть и транспилируется в JavaScript, но всё‑таки иногда он транспилируется в свои особые объекты с несовместимым интерфейсом, а иногда компилятору просто нужно такое приведение для формальной правильности типов. После этого оборачиваем этот массив в ещё один List и преобразуем его тоже в массив JavaScript, чтобы получить нужный двумерный массив.Формат картинки определяем по первым байтам, я здесь пропущу функцию _detectImageFormat.Дальше нам нужен какой‑то URL, который мы установим в наш тег img, потому что только так картинки можно поместить в элемент HTML.Для этого используется так называемый blob URL. Так мы говорим браузеру: «Эй, нам надо показать эти байты в объекте img. Дай нам, пожалуйста, какой‑нибудь виртуальный URL, чтобы он на них ссылался.»Браузер сохраняет эти байты куда‑то к себе во внутреннюю таблицу и даёт нам «талончик», по которому страница может обращаться к этой картинке. Он выглядит примерно так:blob:http://localhost:40000/fd108f07-5e55-43d1-b5cd-691b973c03d6Это внутренняя штука для даннй сессии браузера. Интересно, что по такому адресу можно даже открыть картинку в новой вкладке:Картинку можно открыть в новой вкладке по blob URL.Короче, мы создаём объект img и устанавливаем наш URL в его src. Теперь нужно дождаться, когда картинка загрузится. Для этого нужно указать два обработчика:  el.onload = () {
    web.URL.revokeObjectURL(imageUrl);
    completer.complete();
  }.toJS;
  el.onerror = () {
    web.URL.revokeObjectURL(imageUrl);
    completer.completeError('Cannot load the image.');
  }.toJS;Они оба завершают completer, так что функция может вернуть готовый к работе img или выбросить ошибку. Ещё они освобождают URL, чтобы не тратить память браузера. В конце концов, мы будем делать это для каждого кадра.Ещё обратите внимание, что когда мы передаём функцию Dart как колбэк JavaScript, нужно преобразовать её в функцию JavaScript с помощью геттера toJS.Когда мы получили элемент img, можно передавать его в функцию detect:import 'src/interop/pose_landmarker_result.dart' as js_plr;
// ...
  @override
  Future<PoseLandmarkerResult> detect(Uint8List bytes) async {
    PoseLandmarkerResult r = PoseLandmarkerResult.empty();
    final el = await _createImageFromBytes(bytes);
  
    _landmarker!.detect(
      el,
      (js_plr.PoseLandmarkerResult? result) {
        r = result?.toDart ?? PoseLandmarkerResult.empty();
      }.toJS,
    );
  
    return r;
  }Обратите внимание, что функция detect в MediaPipe не возвращает результат, а передаёт его в колбэк. Это позволяет ей освободить память, когда вызов завершится. На практике объект переживает этот колбэк, но на это нельзя полагаться. Нужно вытащить из этого объекта всё, что нам нужно, пока работает колбэк, и положить это в наш кросс‑платформенный объект результата, который мы определили во втором пакете.Вот и весь код наших пакетов!Связываем пакеты вместеПакет с веб‑имплементацией должен объявить в своём pubspec.yaml, что он содержит имплементацию плагина, чтобы Flutter знал, какой метод вызвать при запуске приложения, чтобы подменить имплементацию на эту:flutter:
  plugin:
    platforms:
      web:
        pluginClass: FlutterMediapipeVisionWeb
        fileName: flutter_mediapipe_vision_web.dartПакет с интерфейсом должен объявить asset с моделью, чтобы она попадала в сборки приожений, которые используют пакет:flutter:
  assets:
    - assets/models/pose_landmarker_lite.taskА публичный пакет для пользователей должен заэндорсить пакет с веб‑имплементацией:flutter:
  plugin:
    platforms:
      web:
        default_package: flutter_mediapipe_vision_webПриложениеПоказываем видео с камерыПервым делом нужно просто показать видео с камеры на экране. Для этого создадим контроллер камеры и покажем видео в виджете CameraPreview:import 'package:camera/camera.dart';
import 'package:flutter/material.dart';

late CameraController cameraController;

Future<void> main() async {
  WidgetsFlutterBinding.ensureInitialized();
  await FlutterMediapipeVision.ensureInitialized();

  cameraController = CameraController(
    (await availableCameras()).first,
    ResolutionPreset.low,
    enableAudio: false,
  );
  await cameraController.initialize();

  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      home: Scaffold(
        appBar: AppBar(title: Text('MediaPipe demo')),
        body: Center(
          child: CameraPreview(cameraController),
        ),
      ),
    );
  }
}Это минимальное приложение, чтобы показывать видео с камеры на экране. Тут есть недостатки. Например, приложение запрашивает разрешение на видео в самом начале, ещё ничего не показывая и не объясняя пользователю, и на это время всё блокируется. И никак не обрабатывает отказ. Но оно делает самое главное:Минимальное приложение для показа видео с камеры.Получаем и анализируем кадрыДавайте напишем контроллер для распознания:class InferenceController extends ChangeNotifier {
  final CameraController cameraController;

  PoseLandmarkerResult get lastResult => _lastResult;
  PoseLandmarkerResult _lastResult = PoseLandmarkerResult.empty();

  InferenceController({required this.cameraController});

  Future<void> start() async {
    while (true) {
      await _tick();
    }
  }

  Future<void> _tick() async {
    final file = await cameraController.takePicture();
    final bytes = await file.readAsBytes();
  
    _lastResult = await FlutterMediapipeVision.detect(bytes);
    notifyListeners();
  }
}Когда вызывается start(), он работает бесконечно. С этим будут проблемы на мобильных устройствах, когда приложение может быть вытеснено из памяти, но этого достаточно для минимальной веб‑версии.В цикле мы получаем кадры с помощью cameraController.takePicture(), потом передаём их байты в плагин и сохраняем результат анализа.Давайте создадим этот контроллер в main():late InferenceController inferenceController; // ИЗМЕНЕНО

Future<void> main() async {
  WidgetsFlutterBinding.ensureInitialized();
  await FlutterMediapipeVision.ensureInitialized();

  final cameraController = CameraController(
    (await availableCameras()).first,
    ResolutionPreset.low,
    enableAudio: false,
  );

  await cameraController.initialize();

  // ДОБАВЛЕНО:
  inferenceController = InferenceController(cameraController: cameraController);

  unawaited(inferenceController.start());
  runApp(const MyApp());
}Показываем скелет поверх видеоДля этого сделаем виджет CameraOverlayWidget:class CameraOverlayWidget extends StatelessWidget {
  final InferenceController inferenceController;

  const CameraOverlayWidget({required this.inferenceController});

  @override
  Widget build(BuildContext context) {
    return ListenableBuilder(
      listenable: inferenceController,
      child: CameraPreview(inferenceController.cameraController),
      builder: (context, child) {
        return CustomPaint(
          foregroundPainter: CameraOverlayPainter(
            inferenceController: inferenceController,
          ),
          willChange: true,
          child: child,
        );
      }
    );
  }
}Этот виджет слушает контроллер распознания и перестраивается при каждом уведомлении о его обновлении. Обратите внимание, что мы создаём виджет CameraPreview вне функции builder и передаём его как child в ListenableBuilder. Это исключает CameraPreview из перестройки на каждом кадре и экономит ресурсы.Виджет CustomPaint применяет foregroundPainter, чтобы рисовать поверх child.Давайте сделаем этот CameraOverlayPainter:class CameraOverlayPainter extends CustomPainter {
  final InferenceController inferenceController;

  static final _paint = Paint()
    ..color = Colors.white
    ..isAntiAlias = true
    ..style = PaintingStyle.fill
    ..strokeWidth = 5;

  static const _pointRadius = 5.0;

  CameraOverlayPainter({required this.inferenceController});

  @override  void paint(Canvas canvas, Size size) {
    _paintPose(canvas, size);
  }

  void _paintPose(Canvas canvas, Size size) {
    final pose = inferenceController.lastResult.landmarks.firstOrNull;
    if (pose == null) {
      return;
    }

    final leftShoulder = pose[Points.leftShoulder].offset.timesSize(size);
    final rightShoulder = pose[Points.rightShoulder].offset.timesSize(size);
    // Same for every point.

    _paintLine(canvas, leftShoulder, rightShoulder);
    // Same for every line.

    _paintPoint(canvas, leftShoulder);
    _paintPoint(canvas, rightShoulder);
    // Same for every point.
  }

  void _paintPoint(Canvas canvas, Offset offset) {
    canvas.drawCircle(offset, _pointRadius, _paint);
  }

  void _paintLine(Canvas canvas, Offset pt1, Offset pt2) {
    canvas.drawLine(pt1, pt2, _paint);
  }

  @override
  bool shouldRepaint(covariant CustomPainter oldDelegate) {
    return true;
  }
}

extension on Offset {
  Offset timesSize(Size size) => Offset(dx * size.width, dy * size.height);
}

abstract final class Points {
  static const leftShoulder = 11;
  static const rightShoulder = 12;
  // Same for every point.
}Этот класс выбирает из распознанного результата главные точки, которые нас интересуют, и соединяет их линиями. Все координаты от 0 до 1, поэтому умножаем их на size — текущий размер виджета. Поскольку виджет поверх видео и имеет такой же размер, все координаты правильны.Наш финальный результат:Скелет, нарисованный поверх видео с камеры. Всё готово.Вот задеплоенное демо ещё раз:https://alexeyinkin.github.io/flutter‑mediapipe/Совместимость с браузерамиВ Chrome всё работает.В Firefox 144 не работает, потому что в пакете camera есть баг, который я скоро опишу и отправлю.В Safari не работает просто так, без всяких симптомов. Если знаете, почему — скажите.ДальшеВ следующей статье доработаем архитектуру и используем распознанные точки, чтобы распознать движения более высокого уровня.Подпишитесь в Telegram, чтобы не пропустить: ainkin_comРусские переводы реже и с задержкой здесь: ainkin_com_ruТеги:mediapipecomputer visionХабы:DartFlutter",268,0,0,20 мин,https://habr.com/ru/articles/965852/,30767,3498,2
Проблемы контурных карт: анализ графики Europa Universalis 5,5a5ha,2025-11-12T17:04:17.000Z,"['Игры и игровые консоли', 'CGI (графика) *', 'Разработка игр *', '3D-графика *']","5a5ha 18 часов назадПроблемы контурных карт: анализ графики Europa Universalis 5Уровень сложностиПростойВремя на прочтение8 минКоличество просмотров4KИгры и игровые консолиCGI (графика) * Разработка игр * 3D-графика * ОбзорОткрывая глобальную стратегию, обычно вы ожидаете лёгкую нагрузку для видеокарты, т.к. такие игры никогда не славились выдающейся графикой. Однако, из-за ряда решений, которые скорее всего были приняты для упрощения разработки, мы получаем довольно плохую производительность. Видя низкий фпс, мне стало интересно, а собственно, что здесь занимает столько ресурсов?Для анализа взята релизная версия (1.0.0) с рендером Vulkan (который разработчики позиционируют как основной), GPU Capture сделана при помощи Nvidia Nsight, разрешение экрана 2560×1440.ВступлениеЧтобы поместить в перспективу то, о чём я буду говорить дальше, вкратце опишу почему слишком много треугольников это плохо. Если вы в целом знакомы с конвейером рендера, этат раздел можно пропустить.У нас есть 5 основных этапа вызова отрисовки для opaque (непрозрачных) объектов: привязка меша, вершинный шейдер, сборка примтивов + растеризатор, тест глубины и стенсила, пиксельный шейдер. Без лодов (т.е. без привязки другого меша на первом этапе), вершинный шейдер и сборка примитивов будут одинаково работать, занимай ваш объект хоть весь экран, хоть один пиксель. Растеризатор превращает информацию треугольников в пиксели, и здесь я остановлюсь поподробнее. Когда растеризуется меш, растеризатор гарантирует, что один конечный пиксель будет соответствовать одному семплу одного треугольника. Для этого производится тест покрытия: берётся точка (семпл) (в данной ситуации центре пикселя, но в зависимости от MSAA точек может быть несколько в разных местах), и если она находится внутри треугольника, то треугольник проходит тест, и его данные берутся для пиксельного шейдера. Если несколько треугольников проходят тест, то берётся тот, который ближе к камере (имеет меньшую глубину). Если семпл приходится на грань между треугольниками, то выбирается левый или верхний треугольник относительно точки (top-left fill rule). При этом, растеризатор не знает ничего про другие меши в других вызоывах отрисовки, так что вся эта работа может быть впустую, если в дальнейшем это место перекроет другой меш.После растеризатора происходит тест глубины, который сравнивает, ближе ли текущий пиксель к камере, чем значения из буфера (возможен более ранний тест глубины, но в EU5 его нет). Если ближе, значит этот пиксель виден, запускается пиксельный шейдер, который перезаписывает буфер цвета. При этом, пиксельный шейдер запускается не просто ради одного пикселя, а для блока 2х2. Это связанно с тем, как шейдер определяет, какой лод текстуры использовать: при помощи операций ddx() и ddy(), которые возвращают частную производную (дельту) переданной переменной по отношению к соседнему пикселю. Т.е., если вызвать ddx(x), когда у текущего пикселя x = 0.1, а у соеднего 0.4, ddx вернёт разницу 0.3. Понимая, как быстро меняются uv координаты треугольника, можно вычислить, текстура какого размера будет совпадать по своей плотности пикселей с необходимым колличеством пикселей на экране. При этом есть несколько нюансов: мы не можем выбрать с каким конкретно соседом сравнивать значение. Если ddx вызывает верхний правый шейдер в блоке 2x2, то вернётся дельта из верхнего левого (в некоторых реализациях при горизонтальной проверке сравнивается верхний и нижний ряд, а их результат усредняется). в ситуации с одним пикселем на треугольник, результат операции ddx может давать  некорректные значения. Поэтому, из-за увелечения треугольников на экране, качество изображения может ухудшаться, а не улучшаться.если растеризированный треугольник состоит только из одного пикселя, то мы всё равно вызовем пиксельный шейдер 4 раза, и отбросим лишние результаты. Поэтому стоимость отрисовки треугольника из одного пикселя может быть равна стоимости отрисовки треугольника из 3 пикселей. Детальная 3D-картаАнализируемый кадрСначала идёт процесс отрисовки теней. В нём нет ничего особенно интересного, за исключением того, что даже такие маленькие объекты, вроде забора огорода или бобров, являются шадоукастерами даже на большом расстоянии. Возможно, на более близком зуме их можно разглядеть, однако тут это будет невозможно. Также, можно было бы плоские участки земли исключать из шадоупаса (т. к. тень от неё в принципе никуда не может упасть). Земля занимает значительное колличесто пикселей шадоумапы, которые по сути рассчитались просто так. Можно сделать предположение, что в шадоупас просто забита вся возможная геометрия в видимой части экрана. Тень от забора огородаОтрисовка террейнаПервый вызов отрисовки, после теней, это вызов отрисовки террейна. Учитывая, что у террейна большой шейдер, имело бы смысл отсортировать непрозрачную геометрию по близости к камере, а затем вызвать отрисовку террейна. Учитывая, что подавляющее число объектов на карте являются непрозрачными, это могло бы быть довольно хорошей оптимизацией. Однако, террейн рисуется всегда и везде, а у вызовов отрисовки объектов не прослеживается чёткого порядка. Вместо разделения террейна на биомы или регионы, парадоксы пошли по пути убершейдера, который применяется везде. Последствия этого очевидны: вместо специализированного набора данных для каждого региона, мы всегда держим в памяти данные всего мира. Данный шейдер использует 145 (!) текстур слоёв террейна. Они имеют формат RGBA_BC3_UNORM_SRGB или RGBA_BC3_UNORM (для нормалей) с разрешением 512х512. В основном это диффузные текстуры + нормали. При этом, прямо скажем, не все эти текстуры блистают детализацией. К примеру:Едва различимые градиенты земли.Если взглянуть на самый большой буфер pdx_hlsl_cbPdxTerrain2MaterialConstants 55.3 KB, то можно увидеть индексы для 69 материалов, после которых идут какие-то константы материалов для 199 биомов, в некоторых биомах могут быть индексы до 15 материалов. После текстур материалов идёт ряд глобальных текстур, которые должны быть общими вне зависимости от биома. Самая первая из них, ProvinceColorIndirectionTexture_Texture, имеет формат R8G8_UNORM с разрешением 16384×8192. Одна только эта текстура занимает 256mb VRAM.ProvinceColorIndirectionTexture_TextureЕщё один пример нерационально��о использования VRAM: текстура VirtualHeightmapPhysicalTexture, является атлассом текстур высот. Однако, даже половина его не заполнена. R16_UNORM 8192×8192 занимает 128mb.VirtualHeightmapPhysicalTextureСуммарно все «глобальные» текстуры занимают  838.336mb. Шейдер занимает 4253 SPIR-V кода или 1282 GLSL.Меши рек имеют ~2 до ~4 тысяч вершин, среди которой есть много лишней геометрии, особенно на прямых.Отрисовка рекПример излишней геометрииЕсли смотреть на вызовы отрисовки остальной геометрии, там тоже всё довольно печально. Система лодов выглядит сломанной. К примеру, рядом друг с другом идут команды отрисовки детализированных домов и чуть менее детализированных. Но даже менее детализированные имеют намного больше вершин, чем колличество занимаемых ими пикселей.Lod0Lod1Занимаемые пиксели домами. Излишняя геометрия добавляет шум в конечное изображение. Если дом по центру ещё можно как-то различить, то городская площадь левее превращается в набор цветных клякс.Самые крупные вызовы отрисовки достались деревьям. Во первых, они имеют большой фрагментный шейдер (SPIRV 2578 строк или GLSL 779), в который, например, входит логика для наложения тумана войны. То есть, вместо специального шейдера для тумана, каждый объект на карте индивидуально для себя рассчитывает наложение тумана.Меш деревьевПервый вызов отрисовки деревьев: vkCmdDrawIndexed() с instanceCount 1979. Однако, этот вызов рисует только часть деревьев.Суммарно, на отрисовку всех деревьев тратится 11 вызовов отрисовки (это ещё ~половина экрана без леса), в каждом из которых от ~2 до ~4 тысяч мешей, каждый меш это 5-7 квадов. Вы уже можете догадаться что это значит.Деревья стоят очень плотно друг к другу, и нет никакого препасса глубины. Это выливается просто в адский Overdraw, видеокарте приходится раз за разом рассчитывать тонны пикселей, которые даже не попадут в финальное изображение. При этом, стоимость расчёта каждого пикселя высокая, из-за большого пиксельного шейдера. В итоге экран заполненный деревьями, по сравнению с пустыней, роняет фпс примерно в два раза.Также, тут стоит упомянуть, что в качестве дизера используется Bayer4x4, который к тому-же статичен от кадра к кадру, что не позволяет сгладить ошибку за счёт накопления информации между кадрами. Он обладает очень низким качеством, и паттерн очень чётко виден, если вы поставите DLSS в режим производительности. Bayer4x4Сравнение дизера Bayer4x4 vs Rdither vs IGN. Код лежит на shadertoy, там же можно взглянуть на динамический вариант. Подробнее про дизер можно почитать тут.После отрисовки основного массива деревьев рисуются различные животные. Олени, лоси, коровы, овцы, волки и... бобры?Прекрасный в своей детализации бобрБобра видите? Нет? А он есть.Напоминаю, что даже если пиксельный шейдер полностью скипнут тестом глубины, перед этим вершинный шейдер всё равно должен отработать для всех вершин меша. Кроме стандартных матриц трансформации, данный вертексный шейдер также читает две текстуры высоты, текстуру шума и тумана войны. И да, бобр к тому же ещё и анимирован. Это даёт около 256 строк GLSL кода или 805 SPIR-V.СпойлерВот он, бобр, занимает неполных 11х4 пикселей. Для этого используется 485 вершин, то есть более 10 вершин на пиксель. Если честно, это уже начинает напонимать другой проект Paradox Interactive — Cities Skylines 2, где у людей были детализированные зубы во рту.Глобальная картаВ целом, глобальная карта работает довольно быстро, так что претензии в этом разделе будут менее значительными с точки зрения производительности.Анализируемый кадрНачало сразу многообещающее, относительно небольшой шейдер (447 GLSL, 1625 SPIR-V), большая часть визуала  отрисована  одним квадом. Это очень хороший результат, учитывая какой процент пикселей сразу принимают конечный вид.Дальше идут сотни вызовов отрисовки текста, по одному на название страны. После них начинают рисоваться границы, по мешу на страну.До отрисовки меша границ Валахии. Обратите внимание, что шейдер карты уже нарисовал границы слегка более тёмным цветом.После отрисовки меша границ Валахии.Вершины мешей границ лежат в нескольких больших буферах, поэтому буду приводить цифры треугольников: 4 621 треугольник у Валахии. В режиме wireframe в чёрных областях заметна излишняя геометрия. Можно сказать, что этот меш сделан с приличным запасом для приближения.При этом, при резких поворотах границы куча треугольников начинают пересекать друг друга, создавая много лишней геометрии. Эта проблема хроническая, и присутствует также у рек.Даже европейский малыш, который занимает 5х7 пикселей может похвастаться 353 треугольниками.При этом, граница это место между территориями двух (или более) государств. Так что пиксели границ в среднем рисуются два и более раз.Два вызова vkCmdDrawIndexed - две отрисовки меша границ.Вернёмся к шейдеру карты. Как он нарисовал границы? При помощи текстуры BorderDistanceFieldTexture_Texture  R8_UNORM  4096x2048 (8mb VRAM). SDF текстуры (поля дистанций) являются хорошим выбором для отрисовки различных чётких линий, т.к. дистанция правильно интерпалируется между пикселями.BorderDistanceFieldTexture_TextureРазрешения этой текстуры, конечно, начинает не хватать в области священной римской империи. Поэтому меня возникает вопрос, почему бы не разделить мир на 8 регионов, каждому из них не назначить по одной такой SDF текстуре? Вершинные буферы в данной ситуации занимают больше VRAM, мне удалось насчитать ~76.84 MB  + ~4.8mb буфера индексов. При этом, дело не только в памяти, но и в колличестве вызовов отрисовки и использованных треугольниках. Чисто механически, видеокарта может совершать в вершинных шейдерах столько же работы, что и в пиксельных. Самое узкое бутылочное горлышко в растеризаторе и сборке примитивов, видеокарты просто не рассчитаны на слишком большое колличество треугольников. Так что, вариант с несколькими SDF текстурами займёт меньше памяти и ресурсов.ВыводСтудия последовательно шла по пути упрошения логики рендера ради ускорения разработки. Это видно по одному большому убершейдеру со всеми данными мира, вместо набора специализированных шейдеров и данных, по отсутствию правильного порядка вызовов отрисовки, по неконтролируемому овердроу, по примитивному алгоритму извлечения мешей из линий (границы и реки), по частично сломанной системе LOD и по отсутсвию ограничений шадоукастеров.Последствия очевидны и измеримы. Хронически низкая эффективность пиксельных шейдеров из-за излишней геометрии, большие требования к видео памяти, тяжёлые пиксельные шейдеры без защиты от overdraw, вершинные шейдеры выполняются даже для крошечных или невидимых на экране объектов. Платить за это приходится производительностью в реальном времени.Есть и плюсы. Режим политической карты ощутимо легче географического по затратам. Переходы между режимами глобальной карты выглядят хорошо. Технически наибольший эффект для производительности даст возврат к более специализированным шейдерам, сортировка непрозрачных вызовов front-to-back, исправление LOD и агрессивная фильтрация/ограничение мелких шадоукастеров. Если у вас слабое железо, советую избегать детальной карты, особенно леса.Теги:eu5рендеркадр3d-графикакомпьютерная графикаанализ кадраeuropa universalis 5Хабы:Игры и игровые консолиCGI (графика)Разработка игр3D-графика",4000,0,0,8 мин,https://habr.com/ru/articles/963470/,13471,1789,4
Автоматизация печати в конструкторском бюро (в двух частях),mmatroskin,2025-11-13T11:15:52.000Z,"['Lisp *', 'C++ *', 'Программирование *', 'Windows *', 'CAD/CAM *']","mmatroskin 3 минуты назадАвтоматизация печати в конструкторском бюро (в двух частях)Уровень сложностиСреднийВремя на прочтение20 минКоличество просмотров14Lisp * C++ * Программирование * Windows * CAD/CAM * Из песочницыЧасть 1. DWG. Как побочный кейс стал основнымПутешествие в кроличью нору автоматизации процессов может привести к занятным результатам. Казалось бы, тривиальная задача часто оказывается не таким уж и простым в реализации кейсом. Особенно без поддержки со стороны вендора.Содержание:НачалоПрыжок в кроличью нору или ПроблемаМоре слёз или Обзор рынкаСкачки наперегонки или ПроектБезумное чаепитие или Реализация Среда AutoCADИнтерфейсный модульКто украл пирожные или Проблемы и костылиПоказания Алисы или РезультатЧто дальше?НачалоМеня зовут Макс и некоторое время назад я занимался автоматизацией процессов в конструкторском бюро. Однажды передо мной встала задача автоматизации печати чертежей из AutoCAD. Казалось бы, ничего необычного, задача решалась не раз в разных проектных институтах, и решение должно быть типовое, но... В моем случае это было именно КБ с partial remote командой из 16 инженеров. И их нужно было освободить от бумажного апокалипсиса.AbstractПубликация описывает собственную разработку - десктопное приложение для пакетной печати и конвертации DWG-чертежей, которое используется в КБ с 2020 года.Решение оказалось надежным, легко масштабируемым, простым в использовании и не нуждающимся в постоянной поддержке. Оно позволило сократить время выпуска чертежей на 50%. А еще в процессе разработки было обнаружено несколько сюрпризов, и не все из них удалось обойти без костылейА самое главное: решение стало инновацией. Ранее операции печати и конвертации выполнялись вручную, занимая до нескольких дней на проект и требуя участия инженеров. Разработанное мной приложение полностью исключило ручной труд при печати, обеспечив автоматическую пакетную обработку чертежей DWG. Одним из ключевых изобретений стал алгоритм автоматического определения области печати и выбора принтера для каждого чертежа.Follow the white rabbit (c)Прыжок в кроличью нору или ПроблемаТак исторически сложилось, что в КБ используется именно AutoCAD. Альтернативы от ODA по объективным причинам не подошли. сейчас не буду погружаться в технические детали. Достаточно знать, что работать будем с файлами DWG. Каждый DWG-файл состоит из пространств модели (предназначенной для 2d или 3d модели оборудования или площадки) и пространств листов в которые проецируют виды модели и добавляют необходимые форматные рамки и прочие элементы для получения чертежей по ГОСТ. Файлы чертежей всегда находятся на выделенном файловом сервере. Это позволяет в рантайме видеть изменения всем участвующим в разработке. При выпуске чертежи печатают в 2-4 экземплярах (если заказчик желает странного предусмотрено контрактом) и конвертируют в формат PDF для отправки в электронном виде (основной способ передачи документов с 2020 года). Как показала практика - от бумажных книг в регионах еще не ушли и вряд ли уйдут в ближайшее времяПроцесс AS IS описывался достаточно простым алгоритмом:Для каждого файла DWG в проекте:Для каждого листа:Определить принтер и параметры печатиОтправить на печатьВернуться к 2Вернуться к 1Для конвертации в PDF процесс повторить (конвертация - это та же самая печать в файл на программном принтере, в нашем случае это оказалось важным)Специально обученного человека (или двух), который будет заниматься печатью и сборкой комплектов чертежей как это принято у взрослых в традиционных проектных институтах нет и не будет.Решение должно позволять пакетно обрабатывать файлы DWG, выводить на печать все чертежи из любых пространств файла в автоматическом режиме с заданными пользователем параметрами. Решение должно устанавливаться на пользовательские ПК, иметь GUI, быть производительным, поддерживаемым, масштабируемым, иметь возможность настройки под стандарты предприятия, а главное - быть настолько простым в использовании чтобы им мог пользоваться пользователь и без технического бэкграунда.Здесь важно, что в КБ уже проведена работа, позволяющая автоматизацию подобных процессов - стандартизация чертежей в электронном виде обеспечивается программно на этапе их выполнения в AutoCAD. Каким образом - тема отдельного поста.Море слёз или Обзор рынкаСразу оговоримся: я человек ленивый и если есть существующие решения- стоит посмотреть в сторону их использования (ИМХО).AutoITПервое, что выдаст Google. Позволяет автоматизировать вообще все что угодно и не требует особых знаний в программировании. Почему мы не будем использовать AutoIT?AutoIT работает через эмуляцию кликов и клавиш и привязку к элементам UI, окнам, координатам. Любое изменение версии AutoCAD или UI - и сценарий ломаетсяСложно сопровождать (UI-зависимый код)Сложно передавать между пользователями (напомню - команда partial remote)PrintConductor/2Printer/PaperCut(любое приложение использующее библиотеки Open Design Alliance)Позволяет пакетно печатать файлы DWG/DXF на принтер или конвертировать их без AutoCAD. В нашем случае есть риски некорректной обработки составной и нестандартной графики и часто достаточно сложный интерфейс. Угадайте кого будет звать с матами пользователь, чтобы найти нужную настройку или когда у него не выйдет на печать треть чертежаСкачки наперегонки или ПроектДелать нечего, расчехляем лаптоп и пишем сами. По традиции у нас два стула варианта:Приложение на базе Open Design Alliance SDKВыглядит наиболее перспективно с точки зрения разработки. Заходим на сайт ODA и видим предложение купить лицензию минимум на 100 мест. О как... На момент написания статьи есть триальный период, но SDK для использования в России недоступен. Все риски некорректной обработки составной и нестандартной графики - на разработчике. Бизнес не пойметАвтоматизация на уровне CAD-ядра с программой-интерфейсомТрадиционный (почти) подход, подразумевает использование:AutoCADстандартных ключей запускавстроенного скриптового механизма (.scr, LISP, Automation API)Решение детерминированно (AutoCAD выполняет программный код независимо от программы-интерфейса), практически отсутствуют риски связанные с прокси-графикой, достаточно простое в поддержке и масштабировании. Плюс наличие у КБ лицензий AutoCAD. На этом варианте и остановимсяАрхитектурно решение выглядит так:Крокет у королевы или РеализацияЧто и для чего будем использовать:инсталлятор - Inno Setupпрограмма-интерфейс - C++ и QTпрограммы выполняемые в AutoCAD + VisualLispПочему VisualLisp? Поддерржка этого языка Autodesk не прекращалась (и, ИМХО, вряд ли прекратится в будущем). А еще код на VisualLisp выполнится в любой версии CAD-программы. Плюс нет необходимости ежегодно компилировать исполняемые файлы под очередной релиз AutoCAD.Что будем печатать? Предполагаем что уровень обеспечения стандартизации достаточен для использования всеми сотрудниками КБ определенных блоков рамок чертежа. Мы будем искать в каждом пространстве один или более блок и отправлять на печать занимаемую им область.Как определим на каком из принтеров печатать? Для этого соберем информацию о доступных в AutoCAD принтерах и их свойствах, и дадим пользователю выбрать в настройках программы какой принтер для какого размера использовать. Настройки будем хранить в конфигах.Для этого воспользуемся объектной Моделью AutoCAD (потом это еще сыграет с нами злую шутку) - иерархической структуре, где чертеж и его элементы представлены в виде программных объектов с определенными свойствами и методами. Что-то мне это напоминает, да?Основная иерархия объектов:Application (Приложение): Корневой объект, представляющий весь сеанс AutoCAD.Document (Документ/Чертеж): Доступ к текущему открытому чертежу.Layout (Лист): Каждый документ содержит коллекцию Layouts (Листов). Объект Layout хранит все настройки, необходимые для печати конкретного вида (пространства модели или листа компоновки), включая формат бумаги, ориентацию, используемый плоттер (принтер) и таблицу стилей печати.Для нас также будут представлять интерес специфические объекты для печати:Plot (Печать): Объект, который содержит методы для запуска процесса печати, Нас будут интересовать методы PlotToDevice и PlotToFile.PlotSettings (Настройки печати): Объект, содержащий идентичную информацию о настройках печати, что и Layout, но может существовать отдельно и использоваться как пресет.Закончим с теорией и начнем кодить. Код разделим на модули:Скрипты выполняющиеся в среде AutoCADКод программы-интерфейса1. Среда AutoCADЗдесь мы будем получать параметры печати для принтеров и собственно, печатать. Для начала используя Модель мы извлечем информацию о доступных принтерах и и сохраним ее на диск:(defun MT-exportPlottersPrefs (path / doc activeLayout oldPaperSize plotDevicePropertyList plotDevicesList iniFile item)
	(setq iniFile
		(strcat path ""\\"" ""MT_batchplot.ini"")
	)
	(vl-load-com)
	(setq doc (vla-get-ActiveDocument (vlax-get-acad-object)))
	(setq activeLayout (vla-get-ActiveLayout doc))
	(setq plotDeviceList (MT-getPlotDeviceList activeLayout))
	(setq plotDevicePropertyList (MT-getDevicePropertyList iniFile plotDeviceList activeLayout doc))
	(foreach item plotDevicePropertyList
		(progn
			(MT-savePlotterDesc 
				item 
				(strcat path ""\\Printers"")
			)
		)
	)
)

В вызываемую функцию мы передаем путь к папке приложения, файлы описания принтеров будем хранить во вложеной папке. В функции MT-getPlotDeviceList мы получаем активный lauout и для него получаем список устройств печати через метод vla-GetPlotDeviceNames. Полученный результат используется для получения списка свойств устройств в коде ниже:(defun MT-getDevicePropertyList (iniFile plotDeviceList activeLayout doc / plotDevicePropertyList paperSizes)

	(setq plotDevicePropertyList '())
	
	(setq oldConfig 
		(vla-get-configname activeLayout)
	)
	
	(foreach name plotDeviceList
		(progn
			(vla-put-configname activeLayout name)
			(vla-RefreshPlotDeviceInfo activeLayout)
			(setq first
				(cons ""device"" name)
			)
			(setq paperSizes (MT-getDevicePaperSizes iniFile activeLayout doc))
			(setq second
				(list ""paperSizes"" paperSizes)
			)
			(setq item
				(list first second)
			)
			(setq plotDevicePropertyList
				(cons item  plotDevicePropertyList)
			)
		)
	) 
	
	(vla-put-configname activeLayout oldConfig)

	plotDevicePropertyList
)
Здесь мы применяем каждое устройство печати к текущему layout делаем обновление, и формируем список из элементов вида ((""device"" . deviceName) data) где объект data вида ((""UserXXXX"" . ""A2"") (420 510) T) получаем выполняя код:(defun MT-getDevicePaperSizes (iniFile activeLayout doc / mediaNames mediaNamesLocal sizeList Width Height name 
								sectName keyName maxMargin left right top bottom oversize X effectiveWidth 
								effectiveHeight MarginLowerLeft MarginUpperRight )
	(setq sectName ""plot"")
	(setq keyName ""_marginMax"")
	(setq maxMargin (atof (MT-getValue iniFile sectName keyName)))
	(setq mediaNames (MT-getMediaNames doc))
	(foreach name mediaNames
		(vla-put-CanonicalMediaName activeLayout (car name))
		(vla-GetPaperSize activeLayout 'Width 'Height)
		(vla-GetPaperMargins activeLayout 'MarginLowerLeft 'MarginUpperRight)
		(setq 
			left (vlax-safearray-get-element MarginLowerLeft 0)
			right (vlax-safearray-get-element MarginUpperRight 0)
			top (vlax-safearray-get-element MarginUpperRight 1)
			bottom (vlax-safearray-get-element MarginLowerLeft 1)
		)
		(setq effectiveWidth
			(- Width (+ left right))
		)
		(setq effectiveheight
			(- Height (+ top bottom))
		)
		(if (and 
				(< (- Width effectiveWidth ) (* 2 maxMargin))
				(< (- height effectiveHeight) (* 2 maxMargin))
			)
			(setq oversize T)
			(setq oversize nil)
		)
		(setq sizeList(append sizeList(list(list name (list Width Height) (list effectiveWidth effectiveHeight) oversize))))
    )
)
Здесь функция MT-getMediaNames используется для получения списка пар имен размеров бумаги (системного и видимого пользователю) используя методы vla-GetLocaleMediaName и vla-GetCanonicalMediaNames. Полученный список мы сохраняем на диск используя вызов функции MT-savePlotterDesc как список INI-файлов с секциями вида:[""ISO_full_bleed_A4_(210.00_x_297.00_MM)""]
localName=""ISO full bleed A4 (210.00 x 297.00 MM"")
paperW=210
paperH=297
effectW=210
effectH=297
isOversize=1
А сейчас отправим что-либо на печать. Для этого получим указатель на текущий документ, установим для него текущий Layout  с именем  tab:(setq activeTab 
	(vla-put-ActiveLayout 
		(vla-get-ActiveDocument 
			(vlax-get-acad-object)
		)
		(vla-item layouts tab)
	)
)
Установим область печати со сторонами areaW и areaH:(setq pointFrame0 (vlax-make-safearray vlax-vbDouble '(0 . 1)))
(setq pointFrame1 (vlax-make-safearray vlax-vbDouble '(0 . 1)))
(vlax-safearray-fill pointFrame0 insertPoint)
(vlax-safearray-fill
	pointFrame1
	(list
		(+ (car insertPoint) areaW)
		(+ (cadr insertPoint) areaH)
	)
)
								
(vla-SetWindowToPlot activeTab pointFrame0 pointFrame1) 
Установим параметры печати:(vla-put-UseStandardScale activeTab :vlax-true)
(vla-put-StandardScale activeTab acScaleToFit) ; устанавливаем масштаб

(setq origin (vlax-make-safearray vlax-vbDouble '(0 . 1)))
(vlax-safearray-fill origin (list 0.0 0.0))

(vla-put-PlotOrigin activeTab origin) ; устанавливаем смещение
(vla-put-CenterPlot activeTab :vlax-true) ; устанавливаем центрирование
(vla-put-PlotRotation activeTab ac0degrees); устанавливаем поворот
(vla-put-PlotHidden activeTab :vlax-false) ; устанавливаем сокрытие элементов листа
(vla-put-PlotViewportBorders activeTab :vlax-false) ; устанавливаем сокрытие контуров видовых экранов
(vla-put-PlotViewportsFirst activeTab :vlax-true)
(vla-put-PlotWithLineweights activeTab :vlax-true) ; устанавливаем печатать толщин линий
(vla-put-ScaleLineweights activeTab :vlax-false) ; устанавливаем масштабирование типов линий
(vla-put-PlotWithPlotStyles activeTab :vlax-true) ; устанавливаем использование стилей печати
И в отправим на принтер выполнив метод:(vla-PlotToDevice 
	(vla-get-Plot 
		(vla-get-ActiveDocument 
			(vlax-get-acad-object)
		)
	)
)
PROFIT!Если, несмотря на обилие скобок, еще интересен полный код(defun MT-printFrames (path plotTable copies / config doc item tab height width framesLayer activeTab printerName isOversize name
						framesList valuesList toleranceMax layouts x paperName, papersizesConfig canonicalPaperName valuesListItem
						paperSizes insertPoint scale isPlotSuccess denom areaW areaH paper printerDesc areaWReal areaHReal isFit)
	(vl-load-com)
	(setq doc (vla-get-ActiveDocument (vlax-get-acad-object)))
	(setq layouts (vla-get-Layouts doc))

;  из конфига приложения получим имя слоя, где будем искать блоки рамок и допуски для размеров бумаги
	(setq config (MT-readConfig (strcat path ""\\"" ""MT_batchplot.ini"")))
	(setq framesLayer (MT-getConfigValue config ""main"" ""_commonLay""))
	(setq toleranceMax (atof (MT-getConfigValue config ""plot"" ""_toleranceMax"")))

; получим список параметров для каждого размера бумаги из конфига
	(setq papersizesConfig (MT-readConfig (strcat path ""\\"" ""MT_papersizes.ini"")))
	(setq framesList (MT-getConfigSectList papersizesConfig))
	(foreach x framesList
		(setq item 
			(list
				x
				(MT-getConfigSectValuesList papersizesConfig x)
			)
		)
		(setq valuesList 
			(append valuesList 
				(list item)
			)
		)
	)

; получим список областей печати c найдеными рамками
	(setq plotAreaList (MT-getPlotAreaList valuesList framesLayer))

; и начнем печатать все найденные в файле области печати
	(foreach x plotAreaList
		(setq tab (cdr (assoc ""TAB"" x)))
		(setq name (cdr (assoc ""NAM"" x)))
		(setq insertPoint
			(list
				(car (cdr (assoc ""INS"" x)))
				(cadr (cdr (assoc ""INS"" x)))
			)
		)
		(setq scale (cdr (assoc ""SCL"" x)))

;
		(setq valuesListItem (cadr (assoc name valueslist)))
		(setq areaW (atof (cdr (assoc ""paperW"" valuesListItem))))
		(setq areaH (atof (cdr (assoc ""paperH"" valuesListItem))))

; получим имя принтера
		(setq printerName (cdr (assoc ""printerName"" valuesListItem)))

; получим поддерживаемые принтером размеры бумаги
	  	(setq printerDesc (MT-getPlotterDesc (strcat path ""\\Printers"") printerName))
		(setq paperSizes (cadr (assoc ""paperSizes"" printerDesc)))

; получим бумагу, соответствующую нашей области печати с учетом допусков
		(setq paper (MT-getPaperForPrint paperSizes areaW areaH toleranceMax))	

 ; получим параметры листа и реальные размеры области печати	
		(setq isFit (car (cddddr paper)))
		(setq isOversize (cadddr paper))
		(setq width (caadr paper))
		(setq height (cadadr paper))
		(setq effectWidth (caaddr paper))
		(setq effectHeight (car (cdaddr paper)))
		(setq paperName (cdar paper))
		(setq canonicalPaperName (caar paper))
		(setq areaWReal (* areaW scale))
		(setq areaHReal (* areaH scale))

; устанавливаем Layout
		(vla-put-ActiveLayout doc (vla-item layouts tab))
		(setq activeTab (vla-get-ActiveLayout doc))

; устанавливаем значения для текущего Layout 
		(vla-put-ConfigName activeTab printerName)
		(vla-put-CanonicalMediaName activeTab canonicalPaperName)
		(vla-put-PaperUnits activeTab acMillimeters)

; устанавливаем минимальную и максимальную точку области печати
		(setq pointFrame0 (vlax-make-safearray vlax-vbDouble '(0 . 1)))
		(setq pointFrame1 (vlax-make-safearray vlax-vbDouble '(0 . 1)))
		(vlax-safearray-fill pointFrame0 insertPoint)
		(vlax-safearray-fill
			pointFrame1
			(list
				(+ (car insertPoint) areaWReal)
				(+ (cadr insertPoint) areaHReal)
			)
		)

; применяем область печати к активному Layout
		(vla-SetWindowToPlot activeTab pointFrame0 pointFrame1)
		(vla-put-PlotType activeTab acWindow)

; применяем настройки масштаба и печати к Layout
		(if(/= scale 1.0)
			(progn
				(vla-put-UseStandardScale activeTab :vlax-false)
				(vla-put-StandardScale activeTab acVpCustomScale)
				(setq num 1.0)
				(if (= isFit nil)
					(setq denom scale)
					(if (= rotate nil)
						(setq denom
							(max
								(/ areaWReal effectWidth)
								(/ areaHReal effectHeight)
							)
						)
						(setq denom
							(max
								(/ areaHReal effectWidth)
								(/ areaWReal effectHeight)
							)
						)
					)
				)
				(vla-SetCustomScale activeTab num denom)
			)
			(progn
				(vla-put-UseStandardScale activeTab :vlax-true)
				(if (= isFit nil)
					(vla-put-StandardScale activeTab ac1_1)
					(vla-put-StandardScale activeTab acScaleToFit)
				)
			)
			
		)
		(setq origin (vlax-make-safearray vlax-vbDouble '(0 . 1)))
		(vlax-safearray-fill origin (list 0.0 0.0))
		(vla-put-PlotOrigin activeTab origin)
		(vla-put-CenterPlot activeTab :vlax-true)
		(if (= rotate nil)
			(vla-put-PlotRotation activeTab ac0degrees)
			(vla-put-PlotRotation activeTab ac90degrees)
		)
		(vla-put-PlotHidden activeTab :vlax-false)
		(vla-put-PlotViewportBorders activeTab :vlax-false)
		(vla-put-PlotViewportsFirst activeTab :vlax-true)
		(vla-put-PlotWithLineweights activeTab :vlax-true)
		(vla-put-ScaleLineweights activeTab :vlax-false)
		(vla-put-PlotWithPlotStyles activeTab :vlax-true)
		(vla-put-ShowPlotStyles activeTab :vlax-true)
		(vla-put-StyleSheet activeTab plotTable)

; отправляем Layout на печать в нужном количестве копий
		(while (> copies 0)
			(progn
				(setq isPlotSuccess (vla-PlotToDevice (vla-get-Plot doc)))
				(setq copies (1- copies))
			)
		)
	)
)
Здесь пропущено чтение конфигов, логгирование и установка системных переменных чтобы не отвлекаться но мы помним что мы это тоже делаем.Итак мы получили LISP-файл (IRL три), который выполняет то что нам требуется. Его мы будем выполнять в AutoCAD для чего нам понадобится скрипт выполняющийся при запуске, который будет загружать наш файл и выполнять методы из него. выгдядеть он будет примерно так:(setq fileName ""myFile.LSP"")
(setq path ""C:\\MyApp\\"")
(load (findfile (strcat path fileName)))
(myFoo path plotStyle 1)
_quit
_y

Теперь перейдем к интерфейсному модулю.2. Интерфейсный модульЗдесь мы будем искать установленные на ПК экземпляры AutoCAD, профили AutoCAD, изменять настройки и, конечно, печатать файлы. Для последнего действия мы будем запускать в фоновом режиме выбранный экземпляр AutoCAD и отображать на UI прогресс и результат.Я сознательно пропущу работу с конфигами, логгированием, реестром Windows и прочие стандартные движения.Для поиска установленных AutoCAD мы заглянем в реестр. Нас интересует раздел  HKCU\Software\Autodesk\AutoCAD. Раздел содержит подразделы для каждого установленного экземпляра AutoCAD с ключем CurVer. Напишем метод возращающий список из объектов со всей необходимой нам информацией:    vector<Acad::AcadApp> AcadSrv::GetInstalledAcad(){

        vector<Acad::AcadApp> result;
        wstring keyAcad = L""Software\\Autodesk\\AutoCAD"";

        vector<wstring> subKeys = Reg::RegFunc::GetSubKeys(HKEY_CURRENT_USER, keyAcad);
        size_t subKeysSize = subKeys.size();
        for (size_t i = 0; i < subKeysSize; i++){
            wstring keyPath = keyAcad + L""\\"" + subKeys[i];
            wstring keyValue = Reg::RegFunc::GetParametr(HKEY_CURRENT_USER, keyPath, L""CurVer"");
            Acad::AcadApp *acd = new Acad::AcadApp(keyPath + L""\\"" + keyValue);
            if(acd->GetProductName() != L""""){
                result.push_back(*acd);
            }
            delete acd;
        }
        std::reverse(std::begin(result), std::end(result))
        return result;
    }
В конце результат переворачиваем для отображения более поздней версии ACAD первой. Получим список объектов вида:	class AcadApp
	{
	protected:
        	wstring productName;
        	wstring release;
        	wstring acadLocation;
        	wstring productId;
        	wstring localeId;
        	vector<wstring> acadProfiles;
        	wstring regKeyPath;

	public:
		AcadApp();
        	AcadApp(wstring acadKeyPath)
		~AcadApp();

        	wstring GetProductName();
        	wstring GetRelease();
        	wstring GetAcadLocation();
        	wstring GetProductId();
        	wstring GetLocaleId();
        	wstring GetRegKeyPath();

        	vector<wstring> GetAcadProfiles();
        	vector<wstring> GetPlotters(wstring profile);
        	vector<pair<wstring, wstring>> GetProfileProperties(
			wstring profile,
			vector<wstring> keyList);
		vector<pair<wstring, unsigned long>> GetProfileSysvars(
			wstring profile,
			vector<pair<wstring, unsigned long>> keyList);
		bool SetProfileSysvars(
            		wstring profile,
            		vector<pair<wstring, unsigned long>> keyList);
        	bool SetProfileSysvars(
            		wstring profile,
            		vector<pair<wstring, wstring>> keyList);
        	bool IsSecureModeOn(wstring profile);
        	bool SecureModeChange(wstring profile);

	private:
        	void SetProductName();
		void SetRelease();
		void SetAcadLocation();
		void SetProductId();
		void SetLocaleId();
        	void SetRegKeyPath(wstring path);
		void SetAcadProfiles();
	};
Под капотом Конструктор вызывает приватные сеттеры, в которых все те же обращения к интересным нам ключам реестра в ветках HKLM и HKCU:    AcadApp::AcadApp(wstring acadKeyPath){
		SetRegKeyPath(acadKeyPath);
		SetProductName();
		SetRelease();
		SetAcadLocation();
		SetProductId();
		SetLocaleId();
		SetAcadProfiles();
	}

    void AcadApp::SetProductName(){

        wstring currentPath = GetRegKeyPath();
        productName = Reg::RegFunc::GetParametr(HKEY_LOCAL_MACHINE, currentPath, L""ProductName"");
    }

    void AcadApp::SetRelease(){

        wstring currentPath = GetRegKeyPath();
        release = Reg::RegFunc::GetParametr(HKEY_LOCAL_MACHINE, currentPath, L""Release"");
    }

    void AcadApp::SetAcadLocation(){

        wstring currentPath = GetRegKeyPath();
        acadLocation = Reg::RegFunc::GetParametr(HKEY_LOCAL_MACHINE, currentPath, L""AcadLocation"");
    }

    void AcadApp::SetProductId(){
        wstring currentPath = GetRegKeyPath();
        productId = Reg::RegFunc::GetParametr(HKEY_LOCAL_MACHINE, currentPath, L""ProductId"");
    }

    void AcadApp::SetLocaleId(){

        wstring currentPath = GetRegKeyPath();
        localeId = Reg::RegFunc::GetParametr(HKEY_LOCAL_MACHINE, currentPath, L""localeId"");
    }

    void AcadApp::SetRegKeyPath(wstring str){

        regKeyPath = str;
    }

    void AcadApp::SetAcadProfiles(){

        wstring profilesPath = GetRegKeyPath() + L""\\Profiles"";
        acadProfiles = Reg::RegFunc::GetSubKeys(HKEY_CURRENT_USER, profilesPath);
    }
C полученной коллекцией объектов и будем работать на UI далее: предложим пользователю выбрать экземпляр AutoCad с именем productName и профиль из списка acadProfiles, в которым будем работать далее. Сконструирeм строку параметров запуска:wstring params = fileName +
    "" /product ACAD /nologo /nossm /p \"""" +
    profile +
    ""\"" /s  "" +
    scriptFileName;
где fileName - путь к файлу DWG (если мы печатаем) или к файлу шаблона DWT (если собираем информацию о принтерах), profile - выбранный профиль, а scriptFileName - путь к скрипту, который будет выполнен при старте. Скрипт мы будем создавать ""на лету"" во временном каталоге пользователя и удалять после завершения работы.Начиная с AutoCAD2013 Autodesk любезно предоставляет приложение AcCoreConsole, которое предоставляет консольный сеанс AutoCAD без пачки сторонних процессов. Идеально для для наших автоматизаторских целей. Запустим его, выполнив методы:    wstring AcadSrv::GetAcadFullName(Acad::AcadApp *acd){
        wstring result = acd->GetAcadLocation() + L""\\accoreconsole.exe"";
        return result;
    }

    HANDLE AcadSrv::StartApplication(wstring name, wstring params, bool hidden){

        HANDLE handle = NULL;
        unsigned short show = hidden ? SW_HIDE : SW_SHOWNORMAL;
        wstring strTmp = name + L"" "" + params;
        LPWSTR cmdStr = new TCHAR[strTmp.size() + 1];
        std::copy(strTmp.begin(), strTmp.end(), cmdStr);
        cmdStr[strTmp.size()] = 0;
        STARTUPINFO sti;
        ZeroMemory(&sti, sizeof(STARTUPINFO));
        PROCESS_INFORMATION pi;
        sti.dwFlags = STARTF_USESHOWWINDOW;
        sti.wShowWindow = show;

        bool res = CreateProcess(
            NULL, 
            cmdStr,
            NULL,
            NULL,
            FALSE,
            0,
            NULL,
            NULL,
            &sti,
            &pi);

        if(res){
            handle = pi.hProcess;
        }

        delete[] cmdStr;
        return handle;
    }
Для скрытого запуска мы передадим false в параметре hidden. Запускать будем в новом потоке, для чего введем класс Thread наследуясь от QThreadnamespace Ui {

    class Thread : public QThread
    {
    Q_OBJECT
    public:
        Thread();
        ~Thread();
        void SetArg(QString str,
                    QStringList fileNameListIn,
                    QString appNameIn,
                    QString profileIn,
                    QString supportPath,
                    int timeOut,
                    bool isSilent);

    protected:
        void run();

    signals:
        void progress(int);
        void finish(int);
        void reportResult(int);
        void reportItemResult(int);

    public slots:
        void Canceled();

    private:
        bool running;
        int count;
        QString scriptFileName;
        QStringList fileNameList;
        QString appName;
        QString profile;
        QString supportPath;
        int timeOut;
        bool isSilent;

    };

}
где в методе run() мы и будем вызывать StartApplication
    void Thread::run(){

        int appResult = 0;
        running = true;
        count = 0;
        scriptFileName = Files::FilesQt::ChangeSlash(scriptFileName);
        scriptFileName = Files::FilesQt::AddQuotes(scriptFileName);

        while (!fileNameList.empty() && (running == true)){
            int result = 1;
            QString fileName = fileNameList.front();
            fileNameList.pop_front();
            fileName = Files::FilesQt::ChangeSlash(fileName);
            fileName = Files::FilesQt::AddQuotes(fileName);
            QString params = ""/i "" + Files::FilesQt::AddSlash(fileName) +
                    "" /product ACAD /p \"""" +
                    profile +
                    ""\"" /s  "" +
                    Files::FilesQt::AddSlash(scriptFileName);
            if (supportPath != """"){
                params = params +
                        "" /supportfilesmap \"""" +
                        supportPath +
                        ""\""""; 
            }

            wstring wsCmdLine = params.toStdWString();
            wstring wsAppName = appName.toStdWString();

            HANDLE handle = Acad::AcadSrv::StartApplication(
                        wsAppName,
                        wsCmdLine,
                        this->isSilent
                        );
            if(handle != NULL){
                result = Acad::AcadSrv::WaitForAppFinished(handle, timeOut);
            }
            if(result == 1){
                appResult = 1;
            }

            count++;
            emit progress(count);
            emit reportItemResult(result);
        }
        emit finish(count); 
        emit reportResult(appResult);
    }
Здесь мы уже сталкиваемся с различиями между QString и std::wstring и необходимостью явного приведения типов. Таков костыль путь, если мы стараемся избегать использования QT для уровня ниже интерфейсного.Теперь мы можем воспользоваться нашим классом Thread,  для информирования пользователя о прогрессе будем использовать модальное окно QProgressDialog()Ввиду большого объема спрячем под катvoid MainWindow::StartACDInThread(QStringList fileNameList,
                                  QString scriptFileName,
                                  QString profile,
                                  QString path,
                                  QString text,
                                  int timeOut,
                                  bool silent){

    wstring wsAppName = Acad::AcadSrv::GetAcadFullName(acd);
    QString appName = QString::fromStdWString(wsAppName);

    QString supportPath;
    for(size_t i = 0; i < currentProfilePropertyesList.size(); i++){
        if(currentProfilePropertyesList[i].first == L""ACAD""){
            supportPath = QString::fromStdWString(currentProfilePropertyesList[i].second);
        }
    };
    if(!path.isEmpty()){
        supportPath = path + "";"" + supportPath;
    }

    QPalette plt = this->palette();
    plt.setColor(QPalette::Highlight, QColor(77, 166, 255, 255));

    count = 0;

    threadObject = new Ui::Thread;
    threadObject->SetArg(scriptFileName,
                         fileNameList,
                         appName,
                         profile,
                         supportPath,
                         timeOut,
                         silent);
    connect(this, &MainWindow::workCanceled, threadObject, &Ui::Thread::Canceled);
    connect(threadObject, &Ui::Thread::progress, this, &MainWindow::SetValue);
    connect(threadObject, &Ui::Thread::finish, this, &MainWindow::AppStop);
    connect(threadObject, &Ui::Thread::reportItemResult, this, &MainWindow::ItemReport);
    connect(threadObject, &Ui::Thread::reportResult, this, &MainWindow::AppReport);

    threadObject->start();
    appRuning = true;

    QProgressDialog *pprd = new QProgressDialog();
    pprd->setModal(true);
    pprd->setWindowFlags(Qt::WindowTitleHint | Qt::CustomizeWindowHint);
    pprd->setWindowTitle(""Обработка задания"");
    pprd->setLabelText(text);
    pprd->setMinimum(0);
    pprd->setMinimumDuration(0);
    pprd->setAutoReset(false);
    pprd->setAutoClose(true);
    pprd->setPalette(plt);

    int countMax;
    if (!printEnable){
        pprd->setCancelButton(0);
        QTime time;
        time.start();
        int i;
        pprd->setMaximum(timeOut);
        while (appRuning){
            i = time.elapsed();
            pprd->setValue(i);
            qApp->processEvents();
            if(pprd->wasCanceled()){
                emit workCanceled();
                break;
            }
        }
    }
    else{
        countMax = fileNameList.size();
        pprd->setMaximum(countMax);
        while(count < countMax){
            QString textTmp = "" "" + QString::number(count + 1) + "" из "" + QString::number(countMax) + "":\n"";
            QString fileName = fileNameList[count]; // current drawing file
            this->currentFile = fileName.toStdWString();
            pprd->setLabelText(text + textTmp + fileName);
            pprd->setValue(count);
            qApp->processEvents();
            if(pprd->wasCanceled()){
                pprd->setWindowTitle(""Отмена задания"");
                emit workCanceled();
                break;
            }
        }
    }

    pprd->setValue(pprd->maximum());
    threadObject->wait();

    delete pprd;
    delete threadObject;

}
Компилируем и наслаждаемся появлением процесса accoreconsole.exe в диспетчере задач и шуршанием бумаги за стеной (нет). Ну, окей... открываем командную строку, в ней accoreconsole.exe, запускаем наш скрипт вручную...Курение гугла и англоязычных форумов дает ответ:ActiveX isn't supported in accoreconsoleШтош, будем запускать acad.exe со всем зоопарком который он за компанию потянет. для этого изменим ключи запуска в методе Thread::run():            QString params = Files::FilesQt::AddSlash(fileName) +
                    "" /product ACAD /nologo /nossm /p \"""" +
                    profile +
                    ""\"" /b  "" +
                    Files::FilesQt::AddSlash(scriptFileName);
            if (supportPath != """"){
                params = params +
                        "" /s \"""" +
                        supportPath +
                        ""\""""; 
И исправим имя запускаемого приложения    wstring AcadSrv::GetAcadFullName(Acad::AcadApp *acd){
        wstring result = acd->GetAcadLocation() + L""\\acad.exe"";
        return result;
    }
Компилируем заново и теперь действительно слушаем звуки из-за стены. PROFIT!Кто украл пирожные или Проблемы и костылиПри скрытом запуске внезапно нет способа в рантайме узнать об ошибках и вообще о том что происходит при выполнении скрипта в сеансе AutoCAD. Максимум что мы можем -  увидеть не упал ли процесс с ошибкой. Чтобы обойти это мы должны логгировать  результаты и ошибки в среде AutoCAD. По этой же причине введен таймаут, после которого мы будем убивать дочерний процесс случае  ""зависания"".Нельзя просто так взять и прочитать PC3 файлы виртуальных принтеров. Поэтому - танцы с запуском AutoCAD при настройке приложения. Обмен данными между приложениями?  Через файловую систему.Разные типы данных в стандартной библиотеке и QT требуют прямого и обратного приведения. Таков путь.И на закуску - тот самый волшебный консольный AcCoreConsole, который Autodesk прикрутил в 2012 году специально для задач автоматизации НЕ РАБОТАЕТ с ActiveX и объектной моделью AutoCAD в lisp. Даже сейчас. Скажем НЕТ быстродействию и ДА - всей пачке фоновых процессов, что тянет за собой acad.exe. Слава AutodeskПоказания Алисы или РезультатВ процессе решения задачи я был и архитектором, и единственным разработчиком решения - от проектирования до внедрения и поддержки. В результате изысканий и экспериментов было создано десктопное приложение с интуитивно понятным для целевой аудитории (инженер-проектировщик с уровнем владения ПК выше минимального) интерфейсом. Решение не оптимально, но стало инновацией, позволившей пакетно печатать большое количество файлов чертежей. В том числе с внешними ссылками и с прокси-графикой. И снизить количество ошибок при печати.Как дополнительный кейс Приложение получило возможность сохранения чертежей в PDF формат в автоматическом режиме в указанный каталог.Приложение позволяло работать с ним по принципу ""выстрелил и забыл"": при первичной настройке пользователь указывал принтеры которые следовало использовать для чертежей разных размеров, принтер для создания PDF и при последующих запусках все его действия сводились к  добавлению файлов, выбору стиля печати и указанию количества экземпляров. Или папки куда сохранять файлы PDF.Что дальше?Дальше были презентация, внедрение, вопросы пользователей (решаемые чтением мануала об одной странице). Пользователи оценили как выросла производительность и надёжность печати. Затем пришел ковид и... Ничего не изменилось. Единственное что потребовалось от разработчика, когда все ушли на удаленку - выпустить патч увеличивающий таймауты в конфигах.2020-2021 годы стали периодом тотальной цифровизации и все выглядело так, что необходимость в печати чертежей уже неактуальна. Уже в 2021 году основным сценарием использования стала добавленная ""в довесок"" конвертации чертежей в PDF.Со временем приложение показало себя как простое и удобное для пользователей, гибкое в настройке и надежное даже при ограниченной поддержке и нестабильной сети. И как показатель его жизнеспособности - спустя пять лет после внедрения пользователи продолжают обращаться с вопросами. Значит, решение по-прежнему актуально и работает на практике.Как то так...Теги:автоматизацияautocadautomationautodeskprintinglispc++qt5печатькейсХабы:LispC++ПрограммированиеWindowsCAD/CAM",14,0,0,20 мин,https://habr.com/ru/articles/966074/,36826,3860,5
Почему мы отказались от AD и FreeIPA и написали свой каталог на Python,MULTIFACTOR_company,2025-11-13T11:17:04.000Z,"['Блог компании МУЛЬТИФАКТОР', 'Системное администрирование *', 'Информационная безопасность *', 'IT-инфраструктура *', 'Облачные сервисы *']","MULTIFACTOR_company 3 минуты назадПочему мы отказались от AD и FreeIPA и написали свой каталог на PythonВремя на прочтение6 минКоличество просмотров19Блог компании МУЛЬТИФАКТОРСистемное администрирование * Информационная безопасность * IT-инфраструктура * Облачные сервисы * Привет, Хабр!Меня зовут Дмитрий Макаров, я руководитель продукта MULTIDIRECTORY — российской службы каталогов с ядром собственной разработки.После ухода Microsoft с российского рынка многие инженеры и администраторы столкнулись с вопросом: чем заменить Active Directory? При этом ещё нужно сохранить совместимость и привычные инструменты, обеспечить надежность и безопасность. Samba и FreeIPA казались логичным выбором, но на практике часто оказывались либо сильно устаревшими, либо слишком сложными для использования.Мы решили начать с нуля и создать службу каталогов, которая сочетала бы знакомый функционал AD с современным стеком, гибкой архитектурой и возможностью интеграции в гибридные инфраструктуры. Так началась история MULTIDIRECTORY (MD).Почему начали с чистого листаНа старте мы рассмотрели все популярные решения — OpenLDAP, Samba AD-DC, FreeIPA, 389 Directory Server. Но у каждого из них оказались ограничения, не позволяющие построить действительно масштабируемую и отказоустойчивую систему.Почему не Samba?Samba — проект, глубоко осевший в эпохе Windows NT, чьи корни уходят во времена ранних версий Windows Server. Несмотря на свою популярность среди Linux-пользователей, этот инструмент имеет ряд существенных недостатков, которые делают его малопригодным для современных высоконагруженных инфраструктур:Доменные контроллеры Контроллеры доменов Samba спроектированы таким образом, что плохо справляются с высокими нагрузками. Это особенно заметно при увеличении числа пользователей и сервисов внутри инфраструктуры.Масштабируемость Расширение сети на большее количество серверов требует значительных усилий и затрат ресурсов. Добавление новых узлов — трудоёмкий процесс со сложной конфигурацией, необходимо проводить долгие настройки каждого нового элемента системы.Устаревший стек Проект исторически развивается на языке C и в 90-е годы это было естественным возможным решением. Сегодня такой технологический стек усложняет развитие продукта, повышает стоимость поддержки и делает внедрение современных практик безопасности и масштабирования трудоёмким. Работа с низкоуровневым кодом затрудняет быстрые изменения и интеграцию новых возможностей, тогда как современные решения требуют гибкости, модульности и расширяемости. Почему не FreeIPA?FreeIPA — это open-source-проект для управления пользователями, политиками и аутентификацией, ориентированный прежде всего на Linux-инфраструктуру. Он действительно закрывает многие базовые сценарии, но при этом имеет ряд архитектурных особенностей и ограничений, которые становятся серьёзным препятствием при использовании в корпоративных средах.Плоская структура организацииВ FreeIPA отсутствует привычная для многих иерархическая структура подразделений (OU), как в Active Directory. Все объекты — пользователи, группы, компьютеры — размещаются в общей плоской структуре. Это может быть удобно для простых сценариев, но становится существенным ограничением при масштабировании или необходимости разграничения прав доступа между подразделениями, филиалами или бизнес-единицами.Производительность Из-за сложной архитектуры и большого количества промежуточных слоёв FreeIPA работает медленнее, чем специализированные решения. Каждая операция проходит через многочисленные компоненты, что замедляет работу всей системы в целом.Обслуживание Обновления и поддержка становятся настоящим испытанием, поскольку изменения в одном компоненте могут привести к непредвиденным последствиям в других частях проекта. Здесь нужно поддерживать высокий уровень экспертизы инженеров.Сложности с интеграциейРешение FreeIPA слабо совместимо с экосистемой Windows и Active Directory. Двусторонние доверительные отношения (trust) реализованы ограниченно и нестабильно, что мешает полноценной интеграции в гибридные инфраструктуры. Настройка таких связей требует тонкой ручной работы, часто сопровождается багами и вызывает трудности при эксплуатации.Асинхронное ядро службы каталогов MULTIDIRECTORYМы отказались от идеи наследовать чужой код и решили писать своё ядро на Python. У этого языка зрелая экосистема, высокая читаемость и нет санкционных рисков, как у .NET или Go-приложений. MULTIDIRECTORY строится как модульная система с веб-интерфейсом на FastAPI и асинхронной моделью обработки запросов через asyncio. Это даёт стабильную производительность даже при высокой нагрузке.Сервис разделён на несколько независимых компонентов — LDAP-каталог, MIT Kerberos, DNS (Bind9), Kea DHCP, SaltStack и административный API. Такое разделение упрощает масштабирование и обновление.Схема LDAP адаптирована под Microsoft Active Directory, поэтому большинство приложений и сервисов видят MD как знакомое AD и работают без изменений. Для интеграции с внешними системами реализован REST API, который позволяет управлять пользователями, группами и политиками через стандартные HTTP-запросы, что удобно для автоматизации и внедрения в существующую инфраструктуру.Хранение данных: PostgreSQL вместо LDAP-БДВместо специализированных LDAP-хранилищ мы выбрали PostgreSQL — стабильную, гибкую и хорошо поддерживаемую систему. Это позволило использовать инструменты, знакомые большинству инженеров: например, мониторинг, резервное копирование и репликацию.Для надёжности применяется архитектура Master-Replica на базе Patroni: при сбое основного узла система автоматически переключается на резервный. Синхронная репликация исключает потерю данных, а совместимость с российскими форками (Postgres Pro, Jatoba и др.) обеспечивает локальную поддержку.Контейнеризация и деплойЧтобы упростить жизнь администраторам, мы упаковали MD в Docker-контейнеры. Это позволило значительно сократить время запуска инфраструктуры — теперь буквально за считанные минуты можно развернуть рабочую версию приложения на любом сервере или рабочей станции.Преимущества подхода:Универсальность: приложения работают на стандартной операционной системе Linux Debian, которая известна своей надёжностью и проверена временемПростота настройки: чтобы запустить систему, нужно поменять всего один специальный файл настроек (docker-compose.yml) или создать конфигурационные файлы для управления контейнерами, например, в KubernetesСовместимость: MD запускается одинаково хорошо на любых компьютерах, где установлен Docker — неважно, используете вы Windows, macOS или разные версии LinuxМасштабирование: удобное масштабирование и управление с помощью KubernetesЕдиная точка истиныВся информация в службе каталогов MULTIDIRECTORY хранится централизованно в единой базе данных, что кардинально отличается от Active Directory. В AD каждый развёрнутый контроллер домена обладает собственной базой данных, и для поддержания согласованности между ними осуществляется постоянный процесс обмена информацией (процесс репликации). В MD всё логичнее: здесь используется единая база данных. Это упрощает администрирование системы, поскольку отсутствуют проблемы с конфликтующими версиями данных и ошибками, возникающими вследствие некорректной репликации.Модель данных MD позволяет администраторам гибко добавлять собственные атрибуты, не ломая при этом само ядро. Такой подход повышает удобство управления системой и снижает риск возникновения ошибок при внесении изменений в схему данных.Что MD дает инженеруСлужба каталогов MULTIDIRECTORY была создана как ответ на ограничения существующих решений вроде Samba и FreeIPA. В ней реализовано всё, что действительно нужно администраторам, без избыточной сложности.В MD сохранён базовый функционал Active Directory, но на современном технологическом стеке с удобным интерфейсом и поддержкой двухфакторной аутентификации через решение MULTIFACTOR — оно работает для всех протоколов без внешних модулей и адаптеров.Система устойчива к сбоям, быстро разворачивается и проста в обслуживании. Доступны две версии — Community (бесплатная версия с базовым функционалом) и Enterprise (с максимальным набором функций для организаций с повышенными требованиями к безопасности). Связаться с разработчиками и получить консультацию напрямую можно в Telegram-чате.Что в итогеMULTIDIRECTORY — это результат нашего решения пересобрать службу каталогов, опираясь на реальный опыт эксплуатации и современные принципы проектирования, а не просто российская замена Active Directory.Если вы хотите мигрировать с AD, работаете с FreeIPA или просто интересуетесь архитектурой каталогов — попробуйте MD, поэкспериментируйте и расскажите в Telegram-чате, что вам показалось удачным, а что требует доработки. Обратная связь от инженеров для нас очень важна.А если вы хотите узнать больше о продукте и увидеть его в действии, приглашаем вас 4 декабря в 11:00 (МСК) на вебинар «Твоя безопасность. Твои правила. Твой контроль. Всё о службе каталогов MULTIDIRECTORY».В прямом эфире расскажем: — почему сегодня важно переходить на российские решения; — как устроена архитектура и функционал MULTIDIRECTORY в версиях Community и Enterprise; — как работают групповые политики; — каким образом реализована безопасность через ролевую модель; — как управлять парольными политиками; — и как работает 2FA на отечественных ОС.Регистрируйтесь и подключайтесь!Теги:multidirectoryинформационная безопасностьсистемное администрированиеit-инфраструктураоблачные сервисыХабы:Блог компании МУЛЬТИФАКТОРСистемное администрированиеИнформационная безопасностьIT-инфраструктураОблачные сервисы",19,0,0,6 мин,https://habr.com/ru/companies/multifactor/articles/966076/,9545,1137,5
Две истории о преобразовании бизнеса: от хаоса к порядку,Novus_Dess,2025-11-13T09:36:33.000Z,"['1С *', 'Управление проектами *', 'Управление разработкой *']","Novus_Dess 1 час назадДве истории о преобразовании бизнеса: от хаоса к порядкуУровень сложностиПростойВремя на прочтение6 минКоличество просмотров1391С * Управление проектами * Управление разработкой * РетроспективаRecovery ModeЗа годы работы в управлении проектами я убедился, что каждое внедрение ИТ-решения — это не просто установка программы. Это история о людях, о проблемах, которые они решают день за днём, и о преобразованиях, которые происходят, когда технология встречается с реальностью. Я хочу рассказать о двух проектах, которые научили меня больше, чем любые учебники.КДПВИстория первая: Когда мебель находит своё местоОгромный склад и одна простая проблемаОгромный двухэтажный склад. Сотни позиций товаров. Владелец сети мебельных магазинов решил открыть новый складской центр. Современное помещение, новое оборудование — наконец-то порядок в бизнесе. Но появился один вопрос: где хранить всё это?Когда я встретился с командой, чтобы обсудить решение, стало ясно — это будет сложно. У заказчика была точная фраза: «Искать товар по памяти будет трудно». Действительно. На складе с тысячами предметов мебели без адресного хранения это становится хаосом.Простая идея разрастается в большой проектЕкатерина предложила использовать товарные места и адресное хранение. На словах просто: сконфигурируем ордерные склады, установим топологию, добавим упаковки. На практике проект разросся в полноценное внедрение всей системы.Вместо пары недель началась многомесячная работа. Автоматизация загрузки штрихкодов, работа с нестандартными форматами от разных поставщиков, интеграция с оборудованием, настройка прав доступа — всё это пришлось решать заново.Вика взялась за самый сложный вызов — механизм работы со штрихкодами. Реальность оказалась намного более запутанной, чем теория. Разные поставщики использовали несовместимые форматы. Некоторые товары вообще не имели кодов. Другие были размечены совершенно нелогично. Когда Павел вернулся из отпуска и увидел объём работы, его реакция была предсказуема — разочарование от масштаба вызова. Но команда не отступила. Они переосмыслили подход и начали переделывать решение с нуля.Данил занимался печатными формами, разбираясь с кладовщиками, как им удобнее видеть информацию. Сначала решили группировать товар по алфавиту. Казалось идеально. Но потом потребовалось использовать типовой механизм печати, и пришлось переделывать всё. Данил спокойно воспринял эту переработку — для него главное было то, чтобы процесс в конечном итоге работал правильно.Почему один человек важнее целой командыЗдесь нужно сказать о Марии. Она работала в команде заказчика и была тем человеком, который действительно понимал бизнес изнутри. Мотивированная, ответственная, наделённая полномочиями — такие люди находятся на вес золота.Когда Мария ушла в отпуск в критический момент (прямо когда завершалась обработка штрихкодов), проект замедлил ход. Это наглядно показало: один правильный человек может быть важнее, чем целая команда специалистов.Я понял, что технология — это лишь половина. Вторая половина — люди, их вовлечённость и взаимопонимание.Запуск: когда система начинает работатьСклад переехал. Система заработала. Не идеально, но живо. Кладовщики начали работать с адресной системой. Менеджеры теперь могли найти нужный товар быстро вместо часов поиска. Вопросы возникали постоянно, и мы помогали оперативно.Из этого этапа я вынес несколько важных выводов.Во-первых, всегда уточняй мельчайшие детали. Даже если вопрос кажется очевидным, спроси его. Пусть пользователь покажет, что ему нужно. Максимум информации на начальном этапе — это инвестиция в успех.Во-вторых, тестируй под профилем конечного пользователя. Программист видит логику. Кладовщик видит препятствия. Это разные миры.В-третьих, границы проекта должны быть ясными с начала. Когда задача растёт без контроля, никто не выигрывает. Нужно остановиться и пересмотреть план.Что получилось в итогеМы имели:Полноценное адресное хранение на складеАвтоматизированную загрузку упаковок и штрихкодовМобильные рабочие места для кладовщиковИнструкции, которые помогают людям работатьНе идеально, но жизнеспособно. Склад работает. Люди адаптировались.История вторая: Когда украшение встречается с CRMДва мира, которые никак не могут встретитьсяДругой проект, другой заказчик. Владелица интернет-магазина украшений хотела привести в порядок свой бизнес. Когда я посмотрел на структуру, я увидел IT-лоскут — несвязанные между собой системы.Отдел продаж работал в одной CRM. Отделы логистики и сопровождения работали в другой системе. Заказы переносились вручную. Никакой синхронизации. Два параллельных мира, которые пересекались через электронные таблицы.Было обидно, потому что компания уже оплатила лицензию интеграции, но попытка внедрить её провалилась раньше. История повторялась: новый подрядчик, новые надежды.Когда даже заказчик не знает, чего хочетКогда мы проводили интервью с заказчиком, она попросила «не беспокоить её по вопросам проекта». Этот сигнал означал одно: даже сама заказчица не была уверена, чего она хочет.Параллельно с нами работала другая компания, которая занималась внедрением CRM. Мы начали координировать усилия, потому что знали: если две команды не будут работать слажено, проект провалится.Координатором проекта была назначена Лариса, бухгалтер. Но она была внешним аутсорсером, занималась только денежными средствами и налогами. Она не вникала в процессы других отделов, не понимала логистику. Она компетентна в своей области, но координировать проект не входило в её интересы. От реальной координации она открещивалась.Интеграция наконец работаетНесмотря на организационные сложности, мы начали. Первая задача была ясной: интегрировать CRM и систему управления так, чтобы менеджеры не переносили заказы вручную.Это сработало. Менеджеры теперь видели остатки товара прямо в CRM при оформлении заказа. Заказы автоматически синхронизировались между системами. Информация о доставке мгновенно появлялась, и менеджеры видели, что заказ уходит клиенту.Мы обновили версию системы, поддержали налоговые изменения, создали удобные рабочие места. Нашли и исправили ошибки предыдущей команды — неправильные остатки, некорректные налоговые настройки.Мы также помогли заказчику сэкономить значительную сумму, убедив его, что полная переустановка интеграции — совершенно лишнее. Иногда экономия денег клиента — это тоже часть успеха.Где кончается технология, начинается организацияБыл и ещё один уровень задач — управленческий учет. Звучит просто, пока не начнёшь делать. Нужна единая система, где руководитель видит всё: доходы, расходы, остатки, прибыль.Мы начали планировать. Но выяснилось: данные о деньгах в одной системе, данные о товарах в другой, данных о себестоимости товара нет нигде — это конфиденциальная информация.И заняться этим было некому. Внутреннего бухгалтера, который вёл бы управленческий учет, нет. Аутсорсер этим не будет заниматься — у неё другие цели.Мы столкнулись с реальностью: технология не может заменить людей и их решения. Нельзя просто настроить систему и ждать, что всё заработает волшебством. Нужен кто-то внутри, кто будет этим управлять и развивать.Компромиссы, которые работаютМы пришли к компромиссам. От синхронизации между бухгалтерской программой и оперативным учётом отказались. От полного расчета финансовых результатов в одной системе тоже. Бухгалтерия решила вопросы самостоятельно.Но главные задачи выполнены. CRM и система управления работают вместе. Менеджеры работают удобно. Логистика скоординирована. Остатки видны в реальном времени.Люди из отделов логистики вначале сопротивлялись изменениям (все люди так делают), но потом освоились. Их работа стала проще.То, что я теперь знаю навернякаЭто был сложный проект, потому что мы учились общаться с заказчиком по ходу работы. Вот что я теперь знаю наверняка:Если видишь запрос на управленческий учет при отсутствии своего бухгалтера — объясни, что так не будет работать. Это не пессимизм. Это реальность.Если нет ответственного за проект со стороны заказчика — успех сомнителен. Ответственный человек должен понимать цели, иметь полномочия, быть заинтересованным. Такой человек — половина успеха.Между «всё», «идеально» и «вовремя» выбирай «вовремя». Нефиг страдать в поисках совершенства. Лучше сделать главное вовремя, чем переделывать потом.Задачи и потребности клиента меняются во время проекта. Это нормально. Нужно обсуждать это честно, пересмотреть приоритеты, скорректировать план. Но это обсуждение должно быть открытым.Две истории, одна истинаРазные проекты, разные люди, разные проблемы. Но оба научили одному: успех внедрения ИТ зависит на 50% от технологии и на 50% от людей, их организации и коммуникации.В первом проекте у нас был хороший координатор на стороне заказчика, и это помогло. Во втором проекте координатор не был полностью готов, и это замедлило ход. Но в обоих случаях мы двигались вперед, учились и адаптировались.Когда я вижу, что система, которую мы внедрили, помогает сотням людей работать удобнее — что кладовщик находит товар за секунды вместо часов, что менеджер видит остатки и статус в своём мобильнике — я понимаю, что это было стоящего.Каждый проект — это история о людях. И каждая такая история важна.Теги:1суправление проектамиуправление командой итвнедрение 1сХабы:1СУправление проектамиУправление разработкой",139,0,0,6 мин,https://habr.com/ru/articles/966014/,9268,1212,3
Странности в исключениях JVM с точки зрения декомпилятора,Sivchenko_translate,2025-11-13T06:47:53.000Z,"['Программирование *', 'Java *', 'Компиляторы *', 'Занимательные задачки']","Sivchenko_translate 4 часа назадСтранности в исключениях JVM с точки зрения декомпилятораВремя на прочтение11 минКоличество просмотров231Программирование * Java * Компиляторы * Занимательные задачкиПереводАвтор оригинала: Alisa SirenevaНекоторое время назад я немного поэкспериментировала, пытаясь научиться декомпилировать файлы классов Java более эффективно, чем позволяют традиционные инструменты, предназначенные для этого — например, Vineflower. В конце концов, я написала статью, в которой изложила мой подход к декомпиляции потока управления. Мои находки позволили значительно ускорить работу получившегося у меня прототипа.  На тот момент я полагала, что этот метод не составит труда расширить и на декомпиляцию потока управления, возникающего при обработке исключений, то есть что ему будут поддаваться блоки try…catch. В ретроспективе признаю: следовало ожидать, что это будет не так просто. Оказывается, здесь возникает множество пограничных случаев, варьирующихся от странного поведения javac до последствий, отражающихся на самой структуре JVM и формате файлов классов. Всё это – серьёзные осложнения. В данном посте я разберу все эти детали, расскажу, почему простые решения не работают, и на каком подходе я в итоге остановилась.Основы JVMJVM — это стековый язык. Большинство инструкций взаимодействуют исключительно со стеком. Например, iadd выталкивает два целых числа с вершины стека, а их сумму записывает обратно в стек. Есть несколько таких инструкций как if_icmpeq, которые передают управление по заданному адресу, в зависимости от того, выполняется ли примитивное условие (напр., в данном случае, value1 == value2). Этого достаточно, чтобы реализовать if, while и многие другие конструкции, описывающие явный поток управления.Но поток управления, применяемый при исключениях, является неявным, поэтому с ним нельзя обращаться таким образом. Отдельно взятый блок try должен отлавливать все исключения, возникающие в зоне его ответственности, будь эти исключения пересланы от вызова метода вinvokevirtual, спровоцированы делением на ноль вidiv или разыменованием нулевого указателя вgetfield. Такие отношения невозможно эффективно выразить в самом байт-коде, поэтому они хранятся отдельно в таблице исключений.В каждой записи из этой таблицы указывается, какая именно область инструкций ассоциирована с каким обработчиком исключений. Если исключение возникает в пределах такой области, то стек очищается, объект исключения записывается в стек, а управление передаётся первой инструкции обработчика.Например, вот как выглядят байт-код и таблица исключений для простого метода, в котором содержится один блокtry…catch (получено при помощиjavap -c):static void method() {
    try {
        System.out.println(""Hello, world!"");
    } catch (Exception ex) {
        System.out.println(""Oops, an error happened"");
    }
}Code:
   // В следующих 3 строках на экран выводится ""Hello, world!"".
   0: getstatic     #7                  //Поле java/lang/System.out:Ljava/io/PrintStream;
   3: ldc           #13                 // Строка Hello, world!
   5: invokevirtual #15                 // Метод java/io/PrintStream.println:(Ljava/lang/String;)V
   // После успешного вывода ""Hello, world!"" на экран переходим к концу функции 
   8: goto          20
   // Здесь начинает действовать обработчик исключений. `ex` первым делом записывается в стек, и 
   // эта инструкция сохраняет её в локальную переменную.
  11: astore_0
   // В следующих 3 строках выводится ""Oops, an error happened"".
  12: getstatic     #7                  // Поле java/lang/System.out:Ljava/io/PrintStream;
  15: ldc           #23                 // Строка Oops, an error happened
  17: invokevirtual #15                 // Метод java/io/PrintStream.println:(Ljava/lang/String;)V
   // Возврат из функции
  20: return
Exception table:
   from    to  target type
   // Исключения, возникающие при выполнении инструкций, начиная с адреса 0 (включительно) до 8 (невключительно) обработчик
   // разбирает по адресу 11.
       0     8    11   Class java/lang/ExceptionЕсли в таблице исключений много строк, то используется первая подходящая. Например, при наличии вложенных блоковtry, то первым в списке пойдёт внутренний блокtry, а за ним последует объемлющий его. Вложение Обратите внимание: таблица исключений — это просто список областей. В JVM не предъявляется никаких требований к тому, как эти области могут вкладываться друг в друга. Например, два диапазона могут пересекаться, притом, что ни один из них не будет вложен в другой, а target может располагаться до from или даже внутри диапазона from…to.Как мы вскоре увидим, это не гипотетический сценарий, и реальные классы файлов часто нарушают эти «очевидные» допущения. Из-за этого возникает проблема, решить которую просто необходимо, если вам нужно написать не просто безусловно корректный декомпилятор, какой пытаюсь сделать я, а просто любой декомпилятор.FinallyПрежде, чем обсудить эту проблему, давайте разберёмся с тем, как javac обрабатывает блоки try…finally. Тело finally должно быть выполнено независимо от того, было ли выброшено исключение. Но тот момент, куда именно передаётся управление по завершенииfinally, зависит от наличия исключения:Непонятно, откуда блоку finally знать, куда именно нужно далее передать управление. Один из вариантов — хранить возможное исключение для rethrow в скрытой переменной и расценивать null как сигнал, что блок try закончил работу без исключения — но этого недостаточно. В блоке try могут быть такие точки выхода, через которые его можно покинуть ещё до того, как успеешь пройти на всю глубину. Точками выхода могут быть continue, break и даже return. Для каждой из них потребуется своя цель, идущая уже после finally. Для обработки этого свойства потребовалась бы таблица переходов, которая, скорее всего, без оптимизации получится медленной. Я уже не говорю о том, как она спутает JIT-компиляторы и статические анализаторы, в том числе таковой в JVM, в обязанности которого входит проверка, в самом ли деле код не обращается к неинициализированным переменным.Вместо этого javac вытворяет нечто одновременно порочное и гениальное; добавляет к телу finally его дубль на каждом пути к выходу. Рассмотрим следующий фрагмент кода:static void method() {
    try {
        try_body();
    } catch (Exception ex) {
        throw ex;
    } finally {
        finally_body();
    }
}Code:
   0: invokestatic  #7                  // Метод try_body:()V
   3: invokestatic  #12                 // Метод finally_body:()V
   6: goto          18
   9: astore_0
  10: aload_0
  11: athrow
  12: astore_1
  13: invokestatic  #12                 // Метод finally_body:()V
  16: aload_1
  17: athrow
  18: return
Exception table:
   from    to  target type
       0     3     9   Class java/lang/Exception
       0     3    12   any
       9    13    12   anyСначала javac убеждается, что тело try можно пройти насквозь, поэтому добавляет вызов finally_body сразу после тела try, после чего следует переход к return. Тело catch насквозь пройти нельзя, поэтому finally_body не вставляется после 11: athrow.Затем javac выясняет, что тело try может выбрасывать исключение, поэтому обёртывает его во всеулавливающий обработчик (в таблице это 0 3 12 any), который сохраняет выброшенное исключение, вызывает finally_body, а затем повторно выбрасывает сохранённое исключение. Аналогично, блок catch может выбрасывать исключение, поэтому он тоже обёртывается во всеулавливающий обработчик (в таблице это 9 13 12 any).По какой-то причине область этого последнего всеулавливающего обработчика также распространяется на первую инструкцию самого обработчика. Я локализовала эту проблему вплоть до этой небесспорной строки  в коде javac, но она там находится так давно, что сомневаюсь, что кто-нибудь возьмётся её трогать. Даже если рано или поздно этот недочёт исправят, старые файлы классов по-прежнему будут страдать от этой проблемы. Поэтому маловероятно, что можно надеяться когда-нибудь об этом забыть.Инструкции, выбрасывающие исключенияТеперь вам, возможно, кажется, что инструкция astore_1 не может выбрасывать исключения, поэтому проблему должно быть легко исправить на этапе парсинга. Просто понижаем to до target, если в диапазоне target…to нет никаких инструкций, которые могут выбрасывать исключения. Но подоплёка этих решений гораздо шире, чем может показаться.Во-первых, любая инструкция JVM может выбросить исключение. В спецификации JVM об этом сказано очень чётко: «VirtualMachineError “[…] может быть выброшено в любой момент эксплуатации Виртуальной машины Java». VirtualMachineError — это надкласс таких проблемных штук как OutOfMemoryError и StackOverflowError — думаю, вам не составит труда вообразить такой интерпретатор JVM, который выбрасывает StackOverflowError, когда внутренняя функция JVM выходит за пределы стека, либо представить OutOfMemoryError, когда не удаётся выделить память прямо на месте выполнения. Реалистичен даже такой сценарий, при котором исключение выбросит astore_1 — в случае, когда массив локальных переменных выделяется по запросу. Хорошо уже, что не приходится иметь дело с Thread.stop, который, начиная с Java 20, может где угодно выбрасывать произвольные исключения.Но ложноположительные срабатывания (например, отлавливание исключения там, где оно не должно отлавливаться) — это лишь часть проблемы. Бывает и так, что мы можем получить ложноотрицательный результат. Рассмотрим следующий код:static int method(boolean condition) {
    try {
        if (condition) {
            return 1;
        }
    } finally {
        finally_body();
    }
    return 2;
}Наша основная цель здесь — создать оператор return внутри блока try…finally. Тогда как часть if (condition) и инициализация 1 попадают в пределы области try, самому return не обязательно должен предшествовать вызов finally_body, который должен находиться за пределами try. Так куда же пойдёт сама инструкция return? Оказывается, что javac генерирует её за пределами блока try:Code:
   // материал `if` .
   0: iload_0
   1: ifeq          11
   // сохраняем `1` для `return` на будущее.
   4: iconst_1
   5: istore_1
   // тело `finally` 
   6: invokestatic  #7                  // Метод finally_body:()V
   // Загружаем возвращаемое значение и возвращаемся.
   9: iload_1
  10: ireturn
   // Проваливаемся до цели, т.e., до конца тела `try`.
   // `finally` body.
  11: invokestatic  #7                  // Метод finally_body:()V
   // Переход к 2`.
  14: goto          23
   // Обработчик исключения. Сохраняем исключение, выполняем тело `finally`,  повторно выбрасываем исключение.
  17: astore_2
  18: invokestatic  #7                  // Метод finally_body:()V
  21: aload_2
  22: athrow
   // `возвращаем 2`.
  23: iconst_2
  24: ireturn
Exception table:
   from    to  target type
       0     6    17   anyСудя по исходному коду, следовало бы ожидать возникновения исключений в ходе выполнения return, так, чтобы они отлавливались в блоке try — но этого не происходит. Однако определённо return может выбрасывать только такие VirtualMachineError, на которые можно закрыть глаза, верно? Не вполне: согласно спецификации JVM, return также может выбрасывать IllegalMonitorStateException. Например, в том случае, если несколько мониторов, которыми функция завладела в процессе выполнения, не были высвобождены к моменту возврата функции. Компилятор javac генерирует код, в котором такое поведение никогда не проявляется, и, поскольку мониторы несовместимы с корутинами, маловероятно, что в какой-то другой клиентской части эта фича будет активно использоваться. Но если байт-код на Java написан вручную, то не гарантировано, что он будет полностью корректен в этом отношении. Поэтому декомпилятору всё равно приходится учитывать при работе эту причуду проектирования.Вас не впечатлит, как я решила эту проблему. Если мониторы поддаются статической проверке на корректность, то return не может выбрасывать исключения, и худшее, что может произойти — это нехватка памяти (OOM) или переполнение стека в то время, как astore ошибочно отловлено/не отловлено. Такое не может произойти ни на HotSpot, ни на какой-либо другой достаточно эффективной реализации JVM. Таким образом, можем исходить из того, что во всех отношениях и с любой точки зрения большинство инструкций не могут выбрасывать исключения. С другой стороны, если невозможно проверить, правильно ли оформлены мониторы, то декомпилятор всё равно не сможет выдавать код Java, поэтому и неважно, как именно javac интерпретирует полученный в результате псевдокод.ДостижимостьПрежде чем перейдём к обсуждению других тонких материй, хочу рассмотреть более простую тему.Одна из странностей JVM заключается в том, что в ней есть два механизма проверки типов. Если компилятор байт-кода предоставляет таблицу (именуемую StackMapTable) с информацией о том, каков тип каждого элемента стека в любой момент, то JVM остаётся лишь удостовериться, что все операции правильно типизированы. Если такая таблица не предоставляется, то JVM приходится логически выводить типы. Поскольку на вывод типов требуется немало времени, StackMapTable обязательно должна присутствовать во всех файлах классов, начиная с Java 6. Правда, современные JVM по-прежнему способны загружать старые файлы классов, поэтому некоторое время придётся мириться с сосуществованием двух систем проверки типов.Между двумя этими системами есть серьёзное отличие. Тогда как при проверке типов путём верификации (т.e., с использованием StackMapTable) проверяется каждая инструкция в байт-коде, при проверке типов путём логического вывода приходится довольствоваться проверкой лишь всех достижимых инструкций, поскольку программа не может знать, как скомпонован стек недостижимых инструкций. Таким образом, в старых файлах классов могут присутствовать недопустимые сочетания инструкций байт-кода, например, iconst_1; ladd, а в новых их не будет.Насколько это важно при обработке исключений? Поскольку у строк в таблице исключений по два параметра to и target, которые на уровне кода Java обычно идут рядом (try заканчивается на }, за которым сразу следует catch (...) {), но на уровне байт-кода часто сильно разнесены (например, из-за того, что между ними встанет goto), можно по глупости попытаться расширить диапазон try прямо до target, если в диапазоне to…target не найдётся никаких инструкций, способных выбрасывать исключения. У такого расширения есть странный побочный эффект: если ранее ни одна инструкция из диапазона from…to не была достижима, но диапазон to…target достижим, то вы только что сделали обработчик исключений достижимым в коде, а в байт-коде он остался недостижимым. Из-за этого в старых файлах классов валидный код мог показаться некорректно типизированным. Это плохо! Разумеется, вам может быть совсем не интересна обработка старых файлов классов, но время поговорить о том, почему такой «пластырь» всё равно работать не будет, сколько шансов ему ни давай.Диапазоны Вероятно, интуиция подсказывает, что один блок try…catch должен компилироваться в одну строку таблицы исключений, но на самом деле это не так. Поскольку блоки finally необходимо дублировать в каждой точке выхода, а мы, конечно же, не хотим отлавливать исключения внутри finally, на некоторые подобласти обработка ошибок распространяться не должна. Например:try {
    if (condition) {
        return 1;
    } else {
        return 2;
    }
} finally {
    finally_body();
}Code:
   0: iload_0
   1: ifeq          11
   4: iconst_1
   5: istore_1
   // Точка выхода `return 1` 
   6: invokestatic  #7                  // Метод finally_body:()V
   9: iload_1
  10: ireturn
  11: iconst_2
  12: istore_1
   // Точка выхода `return 2` 
  13: invokestatic  #7                  // Метод finally_body:()V
  16: iload_1
  17: ireturn
   // Точка выхода при исключении.
  18: astore_2
  19: invokestatic  #7                  // Метод finally_body:()V
  22: aload_2
  23: athrow
Exception table:
   from    to  target type
       0     6    18   any
      11    13    18   anyДаже при отсутствии блока finally, javac просто считает его пустым и всё равно исключает из диапазонов try   операторы return и goto:try {
    if (condition) {
        return 1;
    } else {
        return 2;
    }
} catch (Exception ex) {
    return 3;
}Code:
   0: iload_0
   1: ifeq          6
   4: iconst_1
   5: ireturn // Исключения, возникающие в ходе этого `return`, не обрабатываются.
   6: iconst_2
   7: ireturn
   8: astore_1
   9: iconst_3
  10: ireturn
Exception table:
   from    to  target type
       0     5     8   Class java/lang/Exception
       6     7     8   Class java/lang/Exception(Это также означает, что код между to и target не всегда ограничивается goto или return — он также может включать содержимое блока finally, для которого невозможно гарантировать, что исключения в нём выбрасываться не будут.)Пожалуй, наиболее смущающее следствие из этого таково: притом, что диапазоны обработки ошибок могут пересекать границы конструкций в потоке управления (например, from может находиться вне if, а to — внутри if), диапазоны изъятия из зоны действия обработчика исключений соответствуют конкретным позициям в исходном коде, поэтому описанного выше пересечения с потоком управления не будет. Так что, с точки зрения декомпилятора, предыдущий код нужно распарсить вот так:try #1 {
    if (condition) {
        int tmp = 1;
        exempt #1 {
            return tmp;
        }
    } else {
        int tmp = 2;
        exempt #1 {
            return tmp;
        }
    }
} catch (Exception ex) {
}
return 3;…а не создавать по блоку try для каждой строки. Тогда декомпилятор не сможет проверить, в самом ли деле блоки exempt присутствуют на каждом пути выхода из блока try и имеют подходящее содержимое — и упростить код до …finally. Детали этого процесса мутные, и я сама в них пока не до конца разобралась, но считаю, что это в принципе реализуемо за один шаг.ЗаключениеОдна маленькая проблема, которую осталось затронуть — как должны выглядеть обработчики исключений в IR (промежуточном представлении). При входе в обработчик стек очищается, а объект исключения заносится в стек. Занимаясь декомпиляцией, я рассчитываю на некий линейный порядок, в котором идут отдельные инструкции из байт-кода — так куда же в данном случае пойдёт сохранение в стек? Эту операцию нельзя просто вставить на входе в обработчик, поскольку первая инструкция обработчика исключений также может быть достижима goto или другим явным механизмом потока управления, а не только try…catch. Нет иного выхода, кроме как расценивать такое сохранение в стек как особенную операцию и ввести её в IR несколько позднее, например, при создании синтаксического блока для try…catch.Я хотела, чтобы этот пост рассказывал в большей мере о причудах Java, чем о моём собственном декомпиляторе, так что пока на этом всё.Теги:JavaJVMбайт-кодисключенияХабы:ПрограммированиеJavaКомпиляторыЗанимательные задачки",231,0,0,11 мин,https://habr.com/ru/articles/965922/,18830,2500,4
Назад к on-premise. Почему это снова тренд и чем будет полезен Selectel Server,Dingzhibo,2025-11-12T13:57:08.000Z,"['Блог компании Selectel', 'IT-компании', 'IT-инфраструктура *', 'Серверная оптимизация *']","Dingzhibo 21 час назадНазад к on-premise. Почему это снова тренд и чем будет полезен Selectel ServerВремя на прочтение4 минКоличество просмотров2.2KБлог компании SelectelIT-компанииIT-инфраструктура * Серверная оптимизация * МнениеРазбираемся, как контроль над инфраструктурой превращается в бизнес-преимущество в новой экономической и регуляторной реальности, а также делимся, как в этом поможет серверное решение Selectel.  Еще пять лет назад российский корпоративный сектор массово переходил в облака, надеясь на скорость цифровизации и гибкость масштабирования. Но в 2025 году тренд изменился: компании все активнее возвращаются к собственным серверным решениям (on-premise).Что стало причиной смены парадигмы и почему возвращение к on-premise — это не шаг назад, а стратегическое преимущество?Облако перестало быть универсальным решениемПопулярность облачных сервисов объяснялась простотой и отсутствием капитальных затрат. Однако практика последних лет показала, что для задач с длительной стабильной нагрузкой облачная инфраструктура может выйти дороже. Когда нагрузка непредсказуемая или нужны ресурсы под переменную нагрузку, облако — хорошее решение, так как позволяет перевести капитальные затраты в операционные, что в моменте позволяет сэкономить. Это особенно актуально для стартапов и небольших компаний. Но если ситуация обратная и/или инфраструктура нужна для крупного бизнеса, выделенное «железо» выгоднее.Например, при эксплуатации ML-кластера с GPU стоимость on-premise решения может быть в 8–10 раз ниже, чем публичное облако (IaaS). А пятилетний совокупный показатель затрат (TCO) облачных платформ превышает аналогичные показатели собственной инфраструктуры на 20–30% при стабильной нагрузке. Исследования подтверждают, что по итогам пяти лет владения собственный сервер экономически более выгоден при любом сценарии, кроме краткосрочных или резко изменчивых нагрузок.Российские гиганты рынка e-commerce, такие как Wildberries и СберМегаМаркет, уже активно строят собственные дата-центры. Подобную стратегию выбрала и западная компания 37signals (разработчик Basecamp), которая после миграции с AWS на собственную инфраструктуру окупила затраты всего за 12 месяцев, снизив ежегодные издержки на 2 млн долларов.Selectel Server органично вписывается в эту модель.Конкурентная стоимость владения благодаря наличию готовых конфигураций на складе.Отсутствие валютных рисков благодаря прозрачным рублевым ценам.Минимальный срок окупаемости за счет снижения операционных издержек.Сервер Selectel.Риски санкций требуют автономностиСентябрь 2024 года стал точкой отсчета для массового отказа от западных облачных сервисов в России, после того как санкции США напрямую запретили оказывать российским компаниям услуги облачного хранения и IT-консалтинга. Платформы вроде Notion, Miro и HubSpot полностью заблокировали доступ российским пользователям, оставив их без данных и технической поддержки.На фоне этих событий российские предприятия вынуждены были экстренно переносить инфраструктуру внутрь периметра страны. Ужесточился 152-ФЗ, который требует строгой локализации персональных данных, — это стало еще одной причиной возврата к on-premise.Увеличение инвестиций в IT-инфраструктуру подтверждает и наше исследование: за последний год 41% российских компаний увеличили инвестиции в IT-инфраструктуру в пределах 50%. Еще в 16% организаций рост составил от 50% до 100%, а 9% респондентов заявили о двукратном или более увеличении вложений.Тренды в развитии IT-инфраструктуры.Решение от Selectel обеспечивает компаниям:полный физический контроль над оборудованием и сетью;соответствие строгим требованиям законодательства по обработке персональных данных (152-ФЗ, 187-ФЗ);автономность инфраструктуры с минимальным риском санкционных ограничений.Собственный сервер SelectelВ основе — самые современные процессоры Intel® Xeon® 6, до 8 ТБ DDR5 и специально разработанная материнская плата. Арендуйте сервер у нас или закажите в свой дата-центр.Узнать подробности →Высокие нагрузки требуют высокопроизводительных решенийСовременный бизнес управляет огромными массивами данных и обрабатывает сложные задачи, связанные с аналитикой, машинным обучением и искусственным интеллектом. Здесь крайне важна не только мощность оборудования, но и его стабильность, масштабируемость и энергоэффективность.Серверы Selectel, построенные на базе процессоров Intel® Xeon® 6 поколения с DDR5-памятью и дисками NVMe PCIe Gen5, идеально отвечают этим требованиям:Высокая производительность: обработка задач происходит быстрее на 30–50% по сравнению с предыдущими поколениями оборудования.Энергоэффективность: новая архитектура серверов позволяет снизить энергопотребление до 20%, не снижая производительности.Минимальное время поставки: серверы доступны к отгрузке прямо со склада Selectel в России, что избавляет от ожидания импорта.Инфраструктура — это фактор устойчивостиКомпании, управляющие критической информационной инфраструктурой (КИИ), финансовые организации и крупные промышленные предприятия особенно требовательны к уровню безопасности.Преимущества серверного решения Selectel:Возможность создавать air-gap инфраструктуру для обеспечения полной изоляции от внешних угроз.Физический контроль доступа к серверам, системам хранения данных и сетевым устройствам.Собственный гипервизор и операционная система (SelectOS), сертифицированные и прошедшие интеграционные тесты.Selectel Server: сбалансированное решение для новой реальностиВыбор Selectel Server — это не просто инвестиция в оборудование, это инвестиция в долгосрочную устойчивость бизнеса.Что получает клиент Selectel:Готовые решения и кастомизированные конфигурации на базе проверенных платформ SSE-I112-G6 и SSE-I224-G6.Расширенная гарантия и круглосуточная техническая поддержка с минимальным временем реакции.Регулярные обновления и тестирование совместимости оборудования, что снижает эксплуатационные риски и обеспечивает уверенность в надежности инфраструктуры.Сервер Selectel в панели управления.Что в итогеВозвращение к on-premise в России — это  рациональное решение в условиях новой регуляторной и экономической реальности. Компании переходят от модели «cloud first» к модели «cloud smart», где инфраструктура строится исходя из бизнес-ценности и устойчивости решений.Теги:серверрешениеselectelit-компанииit-инфраструктураХабы:Блог компании SelectelIT-компанииIT-инфраструктураСерверная оптимизация",2200,0,0,4 мин,https://habr.com/ru/companies/selectel/articles/965678/,6369,734,4
ФСТЭК России в облаках: как жить по стандартам в программно-определяемом ЦОД,DGogolev,2025-11-12T12:07:03.000Z,"['Блог компании Orion soft', 'Информационная безопасность *', 'IT-стандарты *', 'Виртуализация *']","DGogolev 23 часа назадФСТЭК России в облаках: как жить по стандартам в программно-определяемом ЦОДУровень сложностиСреднийВремя на прочтение16 минКоличество просмотров886Блог компании Orion softИнформационная безопасность * IT-стандарты * Виртуализация * FAQСегодня российский рынок ИТ-инфраструктуры находится на стыке двух мощных трендов: мы наблюдаем все более широкое внедрение облачных и программно-определяемых решений, вместе с тем видим усиливающееся нормативное регулирование со стороны государства. Компании, которые планируют или уже эксплуатируют программно-определяемые центры обработки данных, неизбежно сталкиваются с вопросом аттестации таких решений.С одной стороны, программные ЦОДы значительно ускоряют бизнес-процессы и упрощают управление инфраструктурой. С другой стороны, российские регуляторы (ФСТЭК России, ФСБ и Росстандарт) предъявляют жесткие требования к обеспечению безопасности, надежности и соответствия национальным стандартам, особенно в сферах, затрагивающих критическую информационную инфраструктуру и обработку персональных данных.В этой статье я детально рассмотрю, как именно государство видит программно-определяемый ЦОД с точки зрения нормативных требований и сертификации. Вы узнаете, какие компании и в каких случаях обязаны получать сертификат соответствия, а кто может спокойно обойтись без него, а также получите обзор нормативных актов и стандартов, регулирующих эту сферу.Меня зовут Дмитрий Гоголев, я — директор по развитию облачной платформы Cloudlink в Orion soft, и моя цель — помочь читателям разобраться, как грамотно и без лишних затрат обеспечить соответствие российскому законодательству, не жертвуя при этом преимуществами современных технологий.1. Программно-определяемый ЦОД и российское законодательствоЧто такое программно-определяемый ЦОД: краткий экскурс в технологиюПрограммно-определяемый ЦОД (Software-Defined Data Center, SDDC) — это инфраструктурный подход, при котором все элементы центра обработки данных, включая вычислительные ресурсы, сети, хранилища данных и безопасность, управляются программными средствами. Такой подход позволяет добиться гибкости, масштабируемости и высокого уровня автоматизации, существенно упрощая управление сложными инфраструктурами.Ключевым отличием программного ЦОДа от традиционного ЦОДа является виртуализация ресурсов и централизованное управление через единый программный интерфейс, что резко повышает эффективность их использования.Место программно-определяемых ЦОДов в нормативном регулированииОднако внедрение программных ЦОДов в России не ограничивается исключительно технологическими аспектами. Регуляторы предъявляют строгие требования к обеспечению безопасности информации и устойчивости инфраструктуры, особенно в ситуациях, когда речь идет об обработке критически важных данных, включая персональные данные граждан, финансовую и медицинскую информацию, или когда инфраструктура относится к критической информационной инфраструктуре (КИИ).Регуляторная среда постоянно меняется, поэтому важно иметь четкое понимание того, какие именно требования применимы к конкретной организации.Ключевые отличия программных ЦОДов от традиционных ЦОДов с точки зрения регулятораДля регулятора программно-определяемый ЦОД — это не просто очередное технологическое решение, а новая, более сложная задача с точки зрения контроля и сертификации. В частности, внимание уделяется следующим аспектам:Уровень изоляции и защиты виртуализированных ресурсов (сетевая сегментация, разделение полномочий и функций безопасности).Применение сертифицированных программных компонентов (гипервизоры, системы виртуализации сети, СХД и средства защиты информации).Прозрачность управления и мониторинга инфраструктуры (фиксация всех событий безопасности, контроль доступа, журналирование событий и процессов управления инфраструктурой).Таким образом, для регулятора программно-определяемый ЦОД — это не только объект проверки, но и технологическое решение, требующее разработки новых подходов к сертификации, аттестации и регулированию.Скрытый текстВ российской практике защиты информации часто путают два схожих понятия — сертификацию и аттестацию ФСТЭК России. Однако по сути это разные процедуры, направленные на разные объекты.Сертификация проводится в отношении конкретных средств защиты информации. Она подтверждает, что данное средство отвечает установленным требованиям безопасности и может применяться в системах, где необходимо обеспечить защиту данных в соответствии с российским законодательством. Например, операционная система, антивирус или межсетевой экран после сертификации ФСТЭК России официально включаются в реестр и допускаются к использованию в критически важных информационных системах.Аттестация же относится не к продуктам, а к самим информационным системам или к объектам информатизации. Это процедура оценки соответствия конкретной системы требованиям по защите информации. Иными словами, аттестация подтверждает, что инфраструктура организации, построенная с применением сертифицированных средств защиты, эксплуатируется правильно и обеспечивает необходимый уровень безопасности. В результате организация получает аттестат соответствия, действующий весь срок эксплуатации объекта информатизации.Таким образом, сертификация отвечает на вопрос, насколько надежен инструмент, а аттестация — насколько правильно и безопасно построена и функционирует сама система. На практике эти процессы взаимосвязаны: без сертифицированных средств сложнее успешно пройти аттестацию, а сама аттестация становится необходимым условием для законной эксплуатации многих информационных систем в России.2. Нормативная базаЧтобы понять, каким образом государство регулирует эксплуатацию программно-определяемых ЦОДов, необходимо обратиться к ключевым документам ФСТЭК России. Именно они определяют, какие системы необходимо аттестовывать и  какие требования применяются к средствам защиты информации в системахПриказ 55 ФСТЭК России задает правовую основу и правила функционирования системы сертификации средств защиты информации. В нем описаны состав участников системы (ФСТЭК как федеральный орган, органы по сертификации, испытательные лаборатории, заявители-производители), порядок подачи заявок, проведения испытаний, выдачи сертификатов соответствия, категории СЗИ подлежащих сертификации. Важно понимать, что этот приказ не регулирует аттестацию информационных систем — он про сертификацию средств защиты. То есть важен с точки зрения выбора продуктов, которые потом будут использоваться в системе защиты.Приказ 77 устанавливает порядок проведения аттестации объектов информатизации на соответствие требованиям защиты информации ограниченного доступа. В этом приказе описаны этапы работ: от подачи документов, до проведения аттестационных испытаний, выдачи аттестата соответствия и внесения объекта в реестр аттестованных.239 приказ ФСТЭК]] России устанавливает требования безопасности объектов критической информационной инфраструктурой (КИИ): банки, энергетические компании, операторы связи, транспортная инфраструктура и другие организации, чья деятельность связана с высокой степенью риска для государственной безопасности или общественной стабильности.21 приказ ФСТЭК России устанавливает требования безопасности к информационных системам обработки персональных данных ( в том числе большого объема или особых категорий (например, медицинская информация или биометрические данные граждан).17 приказ ФСТЭК России (117 приказ ФСТЭК России с 1 марта 2026 года), устанавливает требования к информационным системам государственных структур, ведомств и предприятий с государственным участием.Модели угрозМодель угроз — один из ключевых документов в области кибербезопасности, без которого сегодня невозможно ни построение защищенной информационной системы, ни ее аттестация. В российской практике под этим термином понимается формализованное описание потенциальных угроз безопасности информации, которые могут быть актуальны для конкретной системы, с указанием источников угроз, возможных способов их реализации и последствий для владельца.Во-первых, она является основой для проектирования и обоснования мер защиты. Невозможно поставить корректные средства защиты, если заранее не определено, от чего именно необходимо защищать систему.Во-вторых, наличие модели угроз закреплено в нормативных требованиях ФСТЭК России: для государственных информационных систем, информационных систем персональных данных и объектов критической информационной инфраструктуры ее разработка — обязательное условие.Сама процедура построения модели угроз предполагает анализ бизнес-процессов и архитектуры системы, классификацию обрабатываемой информации, выделение возможных нарушителей и определение их возможностей. На практике это означает, что в документе фиксируются потенциальные источники угроз (например, внешние злоумышленники, инсайдеры, технические сбои), сценарии реализации атак, каналы несанкционированного доступа и последствия их успешной реализации. После этого угрозы ранжируются по степени актуальности, и именно на этой базе выбираются конкретные организационные и технические меры защиты.Таким образом, модель угроз можно сравнить с картой рисков для информационной системы. Она помогает владельцу понять, какие угрозы действительно критичны для его бизнеса, а какие остаются маловероятными. Без такого документа внедрение средств защиты превращается в бессистемный процесс, где меры безопасности либо избыточны и мешают работе, либо, наоборот, недостаточны и оставляют «дыры» в обороне. Поэтому грамотная модель угроз — это не просто формальное требование регулятора, а реальный инструмент управления рисками в сфере кибербезопасности.Типовая и индивидуальная модель угроз: в чем разницаТиповая модель угроз ФСТЭК России — это официальный документ, который описывает базовый набор возможных угроз для информационных систем различных классов. Ее основное преимущество — простота применения и формальное соответствие нормативным требованиям. Однако такая модель носит универсальный характер и далеко не всегда отражает реальные риски конкретной организации.Индивидуальная модель угроз, напротив, разрабатывается под систему и учитывает ее архитектуру, бизнес-процессы, технологии и особенности эксплуатации. Она дает более точное понимание актуальных угроз и позволяет выстроить систему защиты с учетом реальных рисков, а не только по букве закона. На мой взгляд, именно поэтому индивидуальные модели считаются более зрелым инструментом и чаще применяются при аттестации сложных или критически важных систем.ПараметрТиповая модель угроз ФСТЭК РоссииИндивидуальная модель угрозОсноваУниверсальный документ ФСТЭК РоссииАнализ конкретной системыУдобствоБыстрое понимание, минимальные трудозатратыТребует глубокой проработки архитектуры и процессовТочностьОбщее описание угроз, не учитывает специфику системыОтражает реальные риски и сценарии атакПрименимостьФормальное соответствие требованиямЭффективное управление безопасностьюВлияние на защитуМожет привести к избыточным или недостаточным мерамПозволяет оптимально выбирать средства защитыИспользованиеПодходит для типовых ИС и начального этапаПрименяется для критически важных и сложных систем3. Когда аттестация не нужна?Когда аттестация не нужна и как это определитьАттестация — это не универсальное требование для любой информационной системы, а строго регламентированная процедура, применяемая лишь в определенных случаях. Она обязательна для государственных информационных систем.Если система не подпадает под эти категории, прохождение аттестации не требуется. Например, корпоративные сервисы, которые используются исключительно для внутренних задач и не содержат критичных данных, могут работать без аттестата. То же самое касается сайтов, где не ведется обработка персональных данных, или тестовых контуров, предназначенных только для отработки функций без использования реальной информации. В случае, когда обрабатываются данные минимального уровня защищенности (УЗ-4), закон допускает ограничиться организационными и техническими мерами защиты без прохождения аттестации.Чтобы быстро определить необходимость аттестации, достаточно ответить на три вопроса. Обрабатываются ли в системе персональные данные — и если да, то к какому уровню они относятся? Является ли система государственной или объектом критической инфраструктуры? Содержит ли она сведения, отнесенные к государственной, служебной или иной охраняемой законом тайне? Если хотя бы на один вопрос ответ положительный, аттестация потребуется. Если же все ответы отрицательные, то формальной обязанности проходить ее нет.Типовые сценарии, при которых аттестация не обязательнаРассмотрим конкретные примеры, когда можно спокойно обойтись без прохождения аттестации:Компания разрабатывает программное обеспечение и использует программный ЦОД только для внутренних нужд и тестовых сред, при этом не собирая и не обрабатывая персональные данные.Малое или среднее предприятие использует собственный программно-определяемый ЦОД для работы с общими бизнес-данными, не попадающими под требования по защите информации от регуляторов (например, учетные и складские системы, внутренние CRM-системы).Облачный провайдер, предлагающий публичные услуги на коммерческой основе (например, виртуальные машины или веб-хостинг), не ориентированный на предоставление услуг организациям из категорий с обязательной сертификацией.Ограничения и риски отсутствия аттестатаТем не менее, даже в ситуациях, когда аттестация не обязательна, компании стоит использовать сертифицированные средства защиты и выстраивать базовые процессы кибербезопасности. Это не только снижает реальные риски инцидентов, но и повышает доверие клиентов и партнеров, что в современных условиях становится серьезным конкурентным преимуществом.Есть ряд рисков и ограничений, о которых следует помнить:Компания, выбравшая путь без сертификации, ограничивает себя в возможности работы с крупными корпоративными клиентами или госструктурами.В случае изменения бизнес-модели, необходимость аттестации неизбежно возникнет только в случае работы с ГИС.Отсутствие аттестата может осложнить взаимодействие с некоторыми крупными заказчиками или партнерами, которые по собственной инициативе или внутренним регламентам требуют наличие сертифицированных компонентов инфраструктуры даже там, где это не предписано законом.4. Модель угроз для программно-определяемого ЦОДаПрограммно-определяемый дата-центр (SDDC) принципиально меняет картину рисков по сравнению с классическим ЦОДом. Здесь все управление — от вычислений и сетей до систем хранения — вынесено в программный слой и централизовано через контроллеры и оркестраторы. Это удобно для автоматизации, но одновременно создает новые точки уязвимости.Пример схемы технологических (архитектурных) уровней SDDC Главная особенность угроз в SDDC заключается в том, что компрометация управляющей плоскости фактически равна захвату всего центра обработки данных. В отличие от традиционных угроз, связанных с физическим доступом или отказом оборудования, ключевыми становятся риски на уровне гипервизоров, SDN-контроллеров и API-интерфейсов.К числу актуальных угроз относятся: атаки на гипервизоры и контроллеры виртуальных сетей, использование ошибок конфигурации при массовом управлении, несанкционированный доступ через уязвимые API, а также распространение атак внутри виртуальной сети — там, где традиционные межсетевые экраны зачастую бессильны. Отдельно стоит выделить угрозы, связанные с шаблонами виртуальных машин и контейнеров: внедрение вредоносного кода в образы способно незаметно распространить атаку на всю инфраструктуру.Таким образом, модель угроз для SDDC должна смещать акцент с физических рисков на виртуализацию и программные средства управления. Управляющая плоскость в такой модели рассматривается как критический актив, требующий усиленной защиты, многоуровневого контроля доступа и постоянного мониторинга. Только при таком подходе можно выстроить адекватную систему безопасности, соответствующую реальным угрозам современного дата-центра.Защита программно-определяемого ЦОДаЭффективная защита программно-определяемого ЦОДа должна строиться вокруг ключевого принципа: управляющая плоскость — это критический актив. Если злоумышленник получает доступ к оркестратору или SDN-контроллеру, он фактически контролирует весь ЦОД. Поэтому меры безопасности должны быть сосредоточены на защите именно этого уровня.Во-первых, требуется строгая аутентификация и разграничение прав администраторов, вплоть до применения многофакторной аутентификации и принципа минимально необходимых привилегий. Во-вторых, важно защищать все API и интерфейсы управления: использовать шифрование, ограничение доступа по сетевым сегментам и полноценный аудит всех операций.Не менее значима сегментация виртуальной среды. Межсетевой трафик должен контролироваться не только традиционными средствами, но и встроенными механизмами микросегментации, которые позволяют отслеживать взаимодействие виртуальных машин внутри одного кластера. Дополнительно необходим постоянный мониторинг активности гипервизоров, контейнерных платформ и шаблонов виртуальных образов, чтобы исключить использование вредоносных или уязвимых конфигураций.Безусловно необходимо регулярное обновление гипервизоров, SDN- и SDS-компонентов. Уязвимости в этих продуктах часто становятся целью атак, и задержка с патчами напрямую увеличивает риск компрометации. В совокупности такие меры позволяют не только выполнить формальные требования регуляторов, но и реально снизить вероятность критических инцидентов в программно-определенном дата-центре.5. Процедура получения аттестата для программно-определяемого ЦОДаПроцедура аттестации программно-определяемого ЦОДа в целом на практике проходит в несколько этапов.Сначала организация определяет, что именно нужно аттестовать: границы информационной системы, состав компонентов, каналы взаимодействия. Для SDDC этот шаг особенно важен, потому что инфраструктура может быть распределенной и динамичной. Например, в границы объекта включаются гипервизоры, управляющие контроллеры (SDN, SDS, оркестраторы), подсистемы хранения и виртуальные сети.Далее разрабатывается модель угроз и нарушителя с учетом особенностей виртуализированной среды. Здесь фиксируются возможные атаки на гипервизор, API-интерфейсы управления, шаблоны виртуальных машин и межсетевой трафик внутри кластера. На основе модели угроз формируется техническое задание на создание системы защиты информации: какие средства СЗИ будут использоваться, как обеспечивается разграничение доступа, каким образом организуется мониторинг и журналирование действий администраторов.Следующий этап — внедрение мер защиты. В SDDC это обычно включает установку сертифицированных средств защиты (межсетевые экраны, СЗИ НСД, криптосредства), настройку микросегментации внутри виртуальной сети, внедрение систем контроля целостности гипервизоров и администрируемых образов. После этого проводится предварительное тестирование: проверяется соответствие реализованных мер документации и требованиям ФСТЭК России.Когда система готова, аккредитованная организация из списка по аттестации проводит испытания. Они включают тестирование средств защиты, проверку изоляции виртуальных сегментов, анализ журналов и протоколов, моделирование типовых атак. По итогам испытаний оформляется заключение об аттестации, и если система соответствует требованиям, выдается аттестат соответствия. Обычно он действует три года, при условии, что система эксплуатируется без существенных изменений.Таким образом, процедура аттестации для программно-определяемого ЦОДа мало отличается от классической, но ключевая особенность заключается в глубокой проработке виртуализационного слоя и управляющей плоскости. Именно они становятся предметом повышенного внимания регулятора и аккредитованной лаборатории, так как компрометация этого уровня эквивалентна взлому всего ЦОДа.Этапы аттестации программно-определяемого ЦОДаПроцесс получения аттестата можно разделить на несколько ключевых этапов:Этап 1. Определение границ системы Формулируется объект аттестации: какие элементы входят в SDDC (гипервизоры, SDN-контроллеры, подсистемы хранения, виртуальные сети, управляющие сервисы).Этап 2. Разработка модели угроз и нарушителя Фиксируются актуальные риски именно для виртуализированной среды: атаки на гипервизор, компрометация API, обход микросегментации, внедрение вредоносных шаблонов ВМ и контейнеров.Этап 3. Подготовка технического задания на систему защиты Определяются меры защиты, перечень сертифицированных средств ФСТЭК России, правила разграничения доступа, порядок журналирования и мониторинга.Этап 4. Реализация и настройка СЗИ Внедряются сертифицированные средства защиты, настраивается микросегментация трафика, вводится контроль целостности гипервизоров и шаблонов образов, обеспечивается защищенный доступ к API.Этап 5. Предварительное тестирование Проверяется корректность внедренных мер и их соответствие документации: изоляция сегментов, шифрование каналов, контроль прав и журналирование действий администраторов.Этап 6. Аттестационные испытания Аккредитованная организация проводит испытания: моделирование атак, проверку устойчивости гипервизоров, аудит журналов и протоколов. По итогам формируется заключение, на основе которого выдается аттестат соответствия.Этап 7. Получение аттестата При положительном результате заказчик получает аттестат ФСТЭК России, действующий три года (при условии отсутствия серьезных изменений в архитектуре).Дополнительно может потребоваться документация на используемые средства защиты информации и программные средства, подтверждающая их сертификацию по российским стандартам.Кто и как проверяет соответствие нормативам?Проверки проводятся аккредитованными организациями, имеющими действующие лицензии от ФСТЭК России и, при необходимости, ФСБ. В ходе аттестации привлекаются эксперты, которые проводят испытания, оценивают соответствие инфраструктуры нормативным требованиям и готовят официальные заключения.Проверка включает в себя:Анализ технической документации;Инструментальные испытания (тестирование функционала и безопасности);Аудит инфраструктуры (включая визиты экспертов и обследование объекта).Таким образом, понимание структуры процесса аттестации позволяет компаниям заранее подготовиться и минимизировать риски отказа.6. То есть все компоненты программно-определяемого ЦОД должны быть сертифицированы?Не обязательно все компоненты для аттестации программно-определяемого ЦОДа должны быть сертифицированы. Более того, даже не все программно-определяемые ЦОДы должны быть аттестованы. Здесь важно напомнить про два разных процесса: сертификация и аттестация.Сертификация касается только средств защиты информации (СЗИ) и некоторых видов программного обеспечения, которые прямо регулируются ФСТЭК России (например, ОС, средства виртуализации, межсетевые экраны, криптосредства). Сертификация проводится в аккредитованной лаборатории, и продукт после этого заносится в официальный реестр ФСТЭК России. Сертификат подтверждает, что средство может использоваться для защиты информации определенного уровня. Но при каждом изменении следует оповестить ФСТЭК России. Для серийного производства, срок сертификата – 5 лет.Аттестация, в отличие от этого, охватывает весь SDDC как систему, а также проходит проверка системы обеспечения ИБ. При аттестации проверяют, что в системе реализованы меры защиты, способные противостоять угрозам, которые были определены при проектировании системы.. То есть регулятор требует не сертифицировать каждый сервер и каждый софт, а использовать в системе только уже сертифицированные средства защиты и корректно их настраивать.Таким образом, сертифицировать все подряд в SDDC не нужно. Сертификаты требуются только для средств защиты информации, входящих в систему. А вся остальная инфраструктура — серверы, хранилища, СDN-компоненты — должна быть правильно описана в границах аттестуемого объекта и защищена этими средствами.Регулятор ориентируется в первую очередь на защищенность объекта в целом. При этом аттестация может проводиться в нескольких режимах:Аттестация всего комплекса целиком. Полная аттестация самого ЦОДа как комплексного решения — редкий и сложный случай, чаще применяемый в государственных структурах и организациях КИИ высокого уровня значимости. В этом случае проверке подлежит абсолютно вся инфраструктура, включая оборудование, гипервизоры, платформу управления (CMP), средства виртуализации и сетевые компоненты.Аттестация отдельных элементов инфраструктуры. Чаще всего компании выбирают конкретные критически важные компоненты инфраструктуры (например, виртуализацию), которые имеют сертификат ФСТЭК России.В этом подходе сертификация конкретных элементов позволяет подтвердить соответствие нормативам без необходимости сертифицировать весь комплекс инфраструктуры целиком.Использование наложенных сертифицированных средств защиты. В практике наиболее распространен и удобен именно такой подход. В этом случае сертификации по линии ФСТЭК подвергаются только наложенные средства защиты, которые обеспечивают выполнение требований регулятора по безопасности информацииЕсли инфраструктура ЦОДа не подвергается отдельной сертификации, то применение наложенных сертифицированных средств защиты позволяет обеспечить соответствие требованиям регулятора без сертифицированной платформы виртуализации и CMP непосредственно.Что это значит на практике?Если ваша компания разрабатывает собственную платформу виртуализации или CMP и планирует активно продвигать ее на рынок, ориентируясь на государственные компании и КИИ, то ее сертификация по линии ФСТЭК становится значимым конкурентным преимуществом.Если же вы используете сторонние решения, то, как правило, гораздо проще и быстрее закрыть требования регуляторов путем использования уже сертифицированных наложенных средств защиты.Важно понимать, что ФСТЭК России требует четкой логики и прозрачности защищаемых контуров. Даже если CMP и платформа виртуализации не сертифицированы напрямую, должно быть ясно, как именно реализована безопасность инфраструктуры и как исключаются риски компрометации данных.ЗаключениеПрограммно-определяемый ЦОД сам по себе не является проблемой для регулятора — ФСТЭК России оценивает не технологию как таковую, а то, насколько корректно она встроена в контур защиты. Аттестуется всегда система целиком, и ключевое требование здесь одно: использовать сертифицированные средства защиты и выстроить прозрачную модель безопасности.На практике это означает, что при выборе платформы виртуализации и систем управления облачной инфраструктурой важно опираться на решения, которые уже учитывают специфику российского регулирования. Такой подход позволяет ускорить подготовку к аттестации, снизить затраты и при этом сохранить все преимущества программно-определяемого ЦОДа.В этом смысле готовые пакеты решений, где виртуализация, CMP и средства защиты интегрированы между собой, дают компаниям серьезное конкурентное преимущество. Они не только закрывают технологические задачи, но и помогают пройти аттестацию без лишних рисков и бюрократических издержек.Теги:облакооблачные сервисыфстэк россиисертификацияcloudlinkcmpХабы:Блог компании Orion softИнформационная безопасностьIT-стандартыВиртуализация",886,0,0,16 мин,https://habr.com/ru/companies/orion_soft/articles/965688/,27106,3098,4
Как я отупел от нейросетей,onlyahead,2025-11-13T09:15:29.000Z,"['Здоровье', 'Искусственный интеллект', 'Мозг']","onlyahead 2 часа назадКак я отупел от нейросетейУровень сложностиПростойВремя на прочтение11 минКоличество просмотров1KЗдоровьеИскусственный интеллектМозгМнениеИ начал снова учиться жить своим умом. О моём личном осознании через четыре полки из Леруа и ситуации с деградацией в мире. Нейросети стали для нас прекрасным инструментом наравне со смартфоном и интернетом. Причём инструмент этот можно использовать на своё усмотрение и в меру собственных желаний, и когнитивных способностей. Хочешь развлекайся и залипай в XXX, хочешь делись собственным опытом и знаниями и развивай этот мир, хочешь обслуживай систему через создание и поддержку софта/ресурсов, короче выбор действий не особо ограничен.Нейронки тоже используют по-разному: кому-то ChatGPT заменяет понимающего партнёра, кто-то говнокодит, иной делегирует рутину, а кто-то за секунду делает подбор данных для серьезного научного исследования.Но есть и обратная сторона, о которой задумываешься, когда уже подсел на LLM-иглу.Моя история об осознании глобальной проблемы работы с AI.Четыре полкиЯ заметил, что полностью положился на нейронки не в процессе работы или в изучении своих областей. На откровение меня наткнули полки из Леруа, которые нужно было просто присобачить к стене в квартире у знакомой.Пару лет назад мой мозг решал такую задачу предварительным размышлением, и в крайнем случае листом А4 с тупо заточеным карандашом. Перед началом работы нужно понять в какой последовательности сверлить, как вбивать дюбеля чтобы не допустить сдвигов кронштейнов = уклона полки, и чтоб вообще всё было удобно и с минимумом пыли.И первым делом я полез в нейронку, чтобы она объяснила мне как оптимальнее поступить. Я залез в сеть, и случилось то, что случается постоянно – интернет не работал (по очередной, привычной и всем понятной нам причине).Здесь стоит отметить, что покупку самих полок и расчет габаритов/нужных саморезов я тоже проводил через ChatGPT за пару дней до мероприятия.“И тут, без интернета, я осознал, что каждое своё действие я уже давно обсуждаю с AI. Не только сбор информации и решение рутины, а буквально всё.Инструкции по уходу за домашними животными, обслуживанием машины или поход в магазин. Не осталось такой новой задачи (даже бытовой), на которую я бы не получал оптимальный путь решения с пошаговым её выполнением.Оставалось делать всё самому.И прежде чем начать, сперва пришлось преодолеть ступор отсутствия инструкции, а затем немного больно поскрипеть мозгами, чтобы найти собственное решение. Алгоритм верного кронштейновкручивания был найден примерно за 10-15 минут, и дело пошло.Первая полка вкрутилась не без проблем, а я всё больше и больше понимал, что окружил себя нейронками на все случаи жизни. Изначально я ходил в них для быстрого и структурированного сбора информации. Как выяснило опытное исследование, с этим прекрасно справляется Gemini, ChatGPT или Claude проводит великолепную выжимку и простые расчёты, а Grok можно использовать для нетипичных и простых задачек. Ну и Алиса отлично справляется как кухонный таймер (если не отвалится сеть). Но после изучения самих возможностей ИИ, я пошёл вглубь и судя по всему переусердствовал.Вторая полка привела меня к выводу, что на AI возлагается не сколько выдача релевантного и структурированного ответа, сколько ответственность за действия, причём во всех смыслах. Задача с полками – посильная для самостоятельного решения, и в целом несложная. Но даже в таких вещах я шел за решением к ИИ.Если говорить утрированно, работу нейронов мозга человека можно сравнить с путём электронов по проводнику – ток двигается по пути наименьшего сопротивления. Человеческий вид назвал этот принцип с одной стороны ленью, а с другой оптимизацией, на которой построена вся мировая инфраструктура от логистики до горячей воды в вашем кране или 100-мегабитного интернета.Нейросеть позволяет сразу находить путь наименьшего сопротивления, минуя мыслительные процессы.Vы делегируем сам процесс размышлений и получаем готовую инструкцию – просто добавь воды, где думать вообще не нужно.Если же у нас инструкция составленная кем-то, мы не можем говорить, что вся ответственность лежит на нас, поскольку мы доверяемся кому-то более опытному (в нашем случае получается более умному). А избавиться даже от части ответственности – вещь для человека более чем желанная, но в то же время довольно рискованная.И там, где нет процесса “подумать” мозг наш начинает работать не совсем правильно.Но об этом чуть подробнее поговорим ниже.На третьей полке новых откровений не поступило, но уже был понятный отлаженный полочный процесс, плюс знание того, что бур всё-таки нужно немного остужать водой.И под конец четвертой полки стало ясно, что нужно провести исследование и разобраться в теме куда глобальнее, чем мой личный опыт сверления и закручивания. И в этот раз я решил полностью обойтись без помощи ИИ: по классике, с гуглом, листом бумаги и сёрчем, чтобы мозг вспомнил что такое работать по старинке. Что в миреМне стала интересна ситуация в целом: кто ещё задумался об отупении от нейронок: тогда я полез в самую масштабную яму с чужими кейсами: на Reddit.В Reddit можно найти всё, кроме того, что всплывает на форчане, но на такие откровения я пока не готов с аудиторией Хабра. Так вот, на Reddit по первому же запросу нашлось колоссальное количество похожих на мою историй: нащупалась тенденция.На сабредите r/Productivity зацепил вопрос, опубликованный пол года назад:Не слишком ли мы зависимы от AI в решении простейших задач?Пост выглядел как отчаянное признание человека который понял, что способность начать думать самостоятельно выключаться. Его обеспокоенность поддержали: многие пользователи считают, что злоупотребление ИИ может погубить критическое и творческое мышление.Здесь скриншот в оригинале текста Возьмем эшелон поинтереснее: сабреддит r/Professors. Там откопался совершенно свежий и длинный тред, где сообщество преподавателей обсуждали, как AI меняет академический подход обучения. И мне приметился один комментарий:“It leads to a sort of intellectual dissociation… you stop using your brain because AI offers to do everything for you.”Автор – профессор гуманитарных наук.Он описывает, как студенты перестают пытаться читать тексты: они прогоняют их через ChatGPT, получают резюме и считают, что поняли материал.Скриншот того самого комментарияДословный перевод:“Я специалист по написанию текстов, который провел лето, тщательно тестируя GPT-5. В нашем учебном заведении все в порядке, а в отделе - нет.  Потратив много часов на эксперименты, я был в основном впечатлен тем, что он может делать, но осознавал его идиотские циклы, когда он не может следовать базовым инструкциям, и пришел к выводу, что потенциальный вред (для написания) перевешивает пользу от глубокого обучения тому, как им пользоваться. Я занимаюсь обучением грамоте в области искусственного интеллекта, но не собираюсь полностью раскрывать их, если только на меня не окажут давление со стороны отдела (этого не произойдет) или администратора (это может произойти).  Меня больше всего беспокоит когнитивная разгрузка и избегающее поведение при чтении и письме — то, что приводит к тому, что они используют это для обобщения чего-то, что не нужно обобщать, или для создания чего-то, что можно так же легко записать.  Я думаю об этом как о зависимости, которая приводит к своего рода интеллектуальной диссоциации от того, что они читают и пишут. Это усугубляет сомнительные результаты. Самая большая проблема в том, что это предлагает делать все за вас, заставляя вас переступать границы академической честности, пока вы даже не перестанете использовать свой мозг. А студенты, даже если они и умеют подсказывать, не обладают навыками оценки результатов.""Последняя строчка не переведена, так как к нашей теме не относится 🙂И историй подобных сотни, люди (вне зависимости от сферы деятельности) замечают как всё чаще делегируют мыслительную функцию на AI.Но делать вывод только из личного серча пары историй будет неправильным, поэтому я полез дальше:Даже к текущему времени существуют исследования и эксперименты с обозначенной проблемой. Например научная статья исследователя из Швейцарии под названием “AI Tools in Society: Impacts on Cognitive Offloading and the Future of Critical Thinking”, или эксперимент 666.Он взял выборку из 666 человек, деленную на три группы людей с разным уровнем образования и разнообразием профессий:17–25 (молодые);26–45 (средний возрас);46+ (старшие).И дали им большой опросник, цель которого была выяснить, как различаются показатели критического мышления при разном уровне использования AI. Ну и выяснили, что молодая группа (17–25) чаще всех пользуется AI, а их показатели критического мышления оказались самыми низкими, из-за так называемого оффладинга.Оффлоадинг – осознанный или автоматический преренос части своих мыслительных функций во внешнюю среду: устройство, инструмент, другого человека или ИИ.Как и писал выше в примере с электронами, мозг всегда стремится экономить энергию.Если появляется возможность не держать в голове, не просчитывать, не запоминать, и не анализировать он с радостью принимает эту оптимизацию.Низкое критическое мышление у молодого поколения, как вы понимаете, не самый хороший знак. И если раньше на человека и его решения ощутимо влияла пропаганда, то сейчас на её место легко поставить AI-инструменты.Но я нашел ещё более тревожное исследование: Understanding Teen Overreliance on AI Companion Chatbots Through Self-Reported Reddit Narratives.Исследование не строилось на опросах или искусственно созданных ситуациях. Учёные решили посмотреть на естественное поведение подростков – как они сами рассказывают о своих отношениях с AI-компаньонами в публичных пространствах. Для этого они собрали 318 постов и веток обсуждений подростков 13–17 лет на разных платформах: Reddit, Discord-сообществах и специализированных форумах, посвящённых чат-ботам-компаньонам. Все тексты были предварительно очищены от персональных данных и полностью анонимизированы (по сути то же, что делал я, но более глобально и системно).Дальше исследователи применили тематический анализ: вручную раскодировали каждый пост, выделяя эмоциональные и когнитивные маркеры. Их интересовали такие признаки, как тревога, зависимость, делегирование принятия решений, поиск эмоциональной поддержки, попытка заменить живое общение общением с ИИ. После кодирования все метки объединили в крупные смысловые категории. Среди них оказались темы «ИИ как эмоциональный регулятор», «делегирование решений», «страх остаться без ответа», «замещение реальных отношений» и «утрата автономности».Затем учёные посмотрели, как эти темы связаны между собой. Например, если подросток описывал что часто просит ИИ решить за него, что делать в сложной ситуации, почти всегда рядом в тексте появлялись упоминания тревожности или страха ошибиться без внешнего «умного» мнения. Это помогло увидеть не просто набор историй, а повторяющуюся структуру поведения.Общий вывод оказался жёстким: подростки используют ИИ не как инструмент, а как внешний источник мышления и эмоций, и без него испытывают полноценный и часто затяжной стресс.Ещё прошло слишком мало времени с начала массовой популяризации ИИ для серьезных научых исследований и экспериментов, но предварительный вывод подтверждается: ИИ не только удобный инструмент, но и способ долгосрочного отупения. Если, конечно, не подставить одно но.Как это работает и в чём наша проблема.Благодаря цифровизации и генерации бесконечного контента мы уже получили побочки в виде клипового мышления и дешевой дофаминовой иглы: это проблемы глобальные, влияющие на ход мышления и работы психики текущих и последующих поколений.К текущим проблемам добавляется ещё одна, возможно ещё более глобальная – нейросетевая, и она комплексная.Во-первых мы перестаём думать, а равно деградируем – полагаю аргументов в этот счёт уже достаточно.Во-вторых полагаясь на ИИ мы передаем ему ответственность за собственные решения, хотя по факту это иллюзия. Критическое мышление отвечает за способность самостоятельно сопоставлять и перерабатывать информацию, а нейросеть делает это за нас. Правильным будет как минимум не принимать на веру информацию от первого промпта, а вести дискуссию и даже процесс вопрос-ответ вести как умственную деятельность.В-третьих накладывается эффект цифровой амнезии – феномен появился задолго до бума нейросетей, но тот провоцирует её развитие еще сильнее. Если не знаете, что это, суть очень простая: когда мозг понимает что нужная информация в безлимитном доступе, он не старается её запомнить. А если же инфа доступна ограниченное время, наша память подключает ресурсы для запоминания (описание реального эксперимента).Доступность и скорость получения данных – преимущество. Однако недостаток – обесценивание и неиспользование рабочих функций мозга.То же самое случилось при массовом внедрении поисковиков, благодаря которым миллионы люди смогли полноценно самообразовываться. Но там же многие пользователи принимали любую информацию по первой ссылке за истинно-верную, даже если речь шла о пользе тушеных поганок.Происходит это и по сей день, и может происходить в эпохе ИИ.Проблема усугубляется ещё и тем, что мозг не замечает, как постепенно меняется архитектура самого мышления. Раньше между вопросом и ответом лежал путь: поиск источников, фильтрация, сомнение, сравнение фактов, построение собственной позиции. Сегодня этот путь сжат до пары секунд, а иногда до одного клика, и когнитивная цепочка превращается в короткое замыкание. Нам кажется, что мы способны «знать ещё больше», хотя на деле мы просто быстрее получаем готовый результат. Ослабляется та самая внутренняя мускулатура, которая и делает мышление мышлением, а не потреблением данных. Парадокс в том, что мы находимся на переломном этапе: ещё помним эпоху, когда информация требовала усилий, но уже живём в эпохе, когда усилия больше не требуются. Психика при этом не умеет переключаться мгновенно. Она адаптируется через откат: сначала мы теряем навыки, которые больше не тренируются, потом — даже способность понимать, что именно утрачиваем. В этом и состоит главная угроза: не в ИИ как инструменте, а в незаметной трансформации человека, который этим инструментом пользуется. При этом важно понимать: интеллект машин не равен человеческому. Он может воспроизводить структуру рассуждений, но не проживать сам процесс. Он не знает, что такое сомнение, контекст, внутренняя логика убеждений. Он лишь отражает паттерны, которые поймал в данных. И если мы передаём ему ответственность, мы одновременно отдаём и право на ошибку — то самое право, из которого и рождается способность мыслить. Чтобы нейросеть не стала основным фильтром реальности, человек должен оставаться в цикле принятия решений, иначе весь процесс превращается в автоматическую ленту рекомендаций.Есть ещё один аспект: чрезмерная лёгкость получения информации обесценивает не только знания, но и опыт. Раньше любая сложная задача – будь то поиск данных, спор, практическое действие  проходила через собственные усилия, и эти усилия создавали глубину понимания. Сейчас многие начинают воспринимать знания как что-то вроде атмосферного явления: просто нажал кнопку и дождь информации пошёл. Но если знание ничего не стоило, оно ничего и не стоит для психики. И отсюда растёт новая форма поверхностности: мы будто всё знаем, но ничего не умеем воспроизвести самостоятельно.Сами создатели LLM видят тенденцию и ищут как разобраться с проблемой.OpenAI пробует решать эту проблему в ChatGPT через режим обучения, где он не дает готовых решений, но подталкивает к нему пользователя через подсказки и наводящие вопросы/намёки + необходимый теоретический базис.Режим учеба и обучение в ChatGPT. Пример утрированный, но демонстрирует суть подхода.Эта попытка вернуть человека в процесс шаг правильный. Но она не отменяет главного: ответственность остаётся на нас. Мы сами должны удерживать границу между удобством и деградацией, между делегированием и отказом от самостоятельности. ИИ может быть гениальным помощником, но плохим хозяином. И если мы не устанавливаем рамки, рамки устанавливает сама технология – по логике максимальной эффективности, а не человеческого развития. Именно поэтому вопрос «как использовать ИИ» важнее, чем вопрос «что ИИ умеет».А рамки его разумного использования устанавливать только нам самим.Что делать, если уже в теме?Отказываться от ИИ будет также глупо, как отказаться от иных благ цивилизации, вроде обуви или 4G в смартфоне. Безусловно существуют отшельники и люди не хотящие сталкиваться с общим прогрессом, и вполне выживающие там, где бы уже пропали мы с вами, но такой вариант не подойдет 99.99% читателям данной статьи.Но использовать ИИ без самоконтроля и понимания сути инструмента тоже весьма разрушительно. И я пораскинул собственными мозгами:Важно понимать, что нельзя перекладывать всю работу на AI. Это инструмент, и продукт нашей деятельности его гибрид. Говоря проще нельзя воспринимать его как всемогущего Джина, даже если в вашем случае это так, иначе сперва ваш мозг, а потом и вы сами превратитесь в NPC из Валли.Я бы назвал этот процесс контролируемым даунгрейдом: как если мы едем из комфортной городской квартиры на не особо обустроенную дачу в деревню. Воду нужно натаскать, или накачать, печку растопить, а туалет даже не в помещении, а отдельным алтарём в дальнем углу участка.Для печки нужно наколоть дрова, для бани тоже натаскать воды – в квартире это делается в разы быстрее и удобнее. Если убрать дачный вайб, останется та самая оптимизация и улучшение уже существующих процессов, или как мы привыкли это называть – прогресс.Но отдохнуть от прогресса в ручном режиме идея неплохая.Вам важно пробовать жить без интернета и решать отдельные задачи без его помощи. Не обязательно сутки подряд или целую неделю, как любят экспериментировать отдельные блогеры – вы не делаете контент, ваша жизнь уже завязана на инфраструктуре, в том числе сетевой. Если вы любите свою работу, ищите задачи которые можно сделать самому, пускай потратив немного больше времени.Если работу проклинаете (что встречается объективно чаще), но делать её всё равно нужно: применяйте нейроаскетизм вне вашей работы.Безусловно, мы адаптируемся и к миру, где вообще не нужно ничего думать, а нейроинтерфейс справится с этим в 100 раз лучше нашего, и примет решение ещё до формулировки запроса.Но захотим ли мы в таком мире жить – вопрос совсем другой. И ответ на него, также как и методы противодействия, задавать только лично нам самим.И к слову: я веду блог о технологичных компаниях, которые привносят в мир инновации, и успешно реализуют себя на бирже, на pre-IPO и IPO-стадиях и рассказываю где их можно купить.  С вами был Александр Столыпин.Старайтесь думать своей головой и пейте витамины группы B. Увидимся в будущем! :)Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста.Вы чувствуете, что тупеете от использования AI?31.25%Да550%Нет818.75%Сомневаюсь3 Проголосовали 16 пользователей.   Воздержались 5 пользователей. Теги:ИИискуственный интеллектaiдеградацияцифровая амнезиякогнитивная психологиялеруа мерленалександр столыпинiposharksОффлоадингХабы:ЗдоровьеИскусственный интеллектМозг",1000,0,0,11 мин,https://habr.com/ru/articles/965946/,19087,2667,3
Интенсивный курс «AI-агенты» от Google День 3,noobaitranslator,2025-11-13T05:55:02.000Z,"['DevOps *', 'Машинное обучение *', 'Python *', 'SQL *']","noobaitranslator 5 часов назадИнтенсивный курс «AI-агенты» от Google День 3Уровень сложностиПростойВремя на прочтение58 минКоличество просмотров568DevOps * Машинное обучение * Python * SQL * ОбзорПереводАвтор оригинала: Kimberly Milam and Antonio GulliНа данный момент я прохожу 5-дневный интенсив по AI-агентам от Google и параллельно веду собственный конспект. Эта статья представляет собой перевод оригинального материала, выполненный с помощью Gemini и мной. В некоторых местах я немного упростила формулировки или обобщила идеи. Сегодня мы узнаем, как сделать так, чтобы ИИ не просто ""отвечал"", а ""понимал"" и ""запоминал""? И если вы когда-либо задумывались о том, как научить LLM-агентов вести осмысленные, долгосрочные беседы, эта статья станет вашим проводником в мир сессий и памяти, которые формируют ИИ агентов.  Оригинал статьи тут Context Engineering: Sessions, MemoryВведениеВ этом материале мы рассмотрим ключевую роль Сессий (Sessions) и Памяти (Memory) в создании интеллектуальных stateful-агентов на базе LLM. Это поможет разработчикам создавать более мощные, персонализированные и целостные (persistent) AI-системы.Чтобы Large Language Models (LLM) могли запоминать, обучаться и персонализировать взаимодействие, разработчики должны динамически собирать и управлять информацией в их контекстном окне. Этот процесс известен как Context Engineering (инженерия контекста).Ключевые концепции, рассматриваемые в этом материале:Context Engineering (инженерия контекста): Процесс динамической сборки и управления информацией в контекстном окне LLM для создания stateful-агентов, способных поддерживать контекст диалога.Сессии (Sessions): «Оболочка» для всего диалога с агентом, которая хранит хронологическую историю общения и оперативную (working) память агента.Память (Memory): Механизм для долговременного хранения данных, который собирает и обобщает ключевую информацию из нескольких сессий. Это обеспечивает непрерывность и персонализацию взаимодействия с LLM-агентом.Инженерия контекста (Context Engineering)По своей природе LLM не имеют состояния (stateless). Вне своих обучающих данных их логика и осведомлённость ограничены информацией, предоставленной в «контекстном окне» одного API-вызова. Это создаёт фундаментальную проблему, поскольку AI-агенты должны быть снабжены инструкциями о том, какие действия они могут выполнять, фактическими данными для анализа и информацией из текущего диалога, которая определяет задачу.Чтобы создавать stateful-агентов (агентов с состоянием), которые могут запоминать, обучаться и персонализировать взаимодействие, разработчики должны формировать этот контекст на каждом шаге диалога. Этот процесс динамической сборки и управления информацией для LLM и называется Context Engineering (инженерия контекста).Инженерия контекста — это эволюция традиционной инженерии промптов (Prompt Engineering). Инженерия промптов фокусируется на создании оптимальных, зачастую статичных, системных инструкций. В отличие от этого, инженерия контекста работает со всей полезной нагрузкой (payload), динамически создавая промпт, учитывающий состояние (state-aware), на основе данных о пользователе, истории диалога и внешних источниках. Она включает в себя стратегический отбор, обобщение и добавление различных типов информации, чтобы максимизировать релевантность и минимизировать «шум».Большую часть этого контекста хранят и обрабатывают внешние системы, такие как RAG-базы данных, хранилища сессий и менеджеры памяти. Фреймворк агента должен оркестрировать работу этих систем, чтобы извлекать и собирать контекст в итоговый промпт.Представьте инженерию контекста как подготовку рабочего места шеф-повара (mise en place) — тот самый важный этап, когда повар собирает и подготавливает все ингредиенты перед началом готовки. Если дать повару только рецепт (промпт), он, возможно, приготовит сносное блюдо из тех случайных ингредиентов, что есть под рукой. Но если вы заранее убедитесь, что у него есть все нужные, качественные ингредиенты, специальные инструменты и чёткое понимание стиля подачи, он сможет стабильно готовить превосходные, индивидуальные блюда.Цель инженерии контекста — убедиться, что у модели есть ровно столько релевантной информации, сколько нужно для выполнения задачи, — ни больше, ни меньше.Инженерия контекста управляет сборкой сложной полезной нагрузки (payload), которая может включать в себя разнообразные компоненты:1. Контекст, направляющий рассужденияЭтот тип контекста определяет фундаментальные шаблоны мышления агента и доступные ему действия, диктуя его поведение.Системные инструкции (System Instructions): Высокоуровневые директивы, определяющие роль (persona), возможности и ограничения агента.Описания инструментов (Tool Definitions): Схемы API или функций, которые агент может использовать для взаимодействия с внешним миром.Примеры для обучения (Few-Shot Examples): Тщательно подобранные примеры, которые направляют логику модели через обучение в контексте (in-context learning).2. Фактические и доказательные данныеЭто основные данные, на основе которых агент строит свои рассуждения. Они включают как ранее известные знания, так и информацию, динамически получаемую для конкретной задачи, и служат «доказательной базой» для ответа агента.Долгосрочная память (Long-Term Memory): Сохранённые знания о пользователе или теме, собранные за несколько сессий.Внешние знания (External Knowledge): Информация, полученная из баз данных или документов, часто с использованием Retrieval-Augmented Generation (RAG).Результаты работы инструментов (Tool Outputs): Данные, возвращённые инструментом.Результаты работы подагентов (Sub-Agent Outputs): Выводы или результаты, возвращённые специализированными агентами, которым была делегирована конкретная подзадача.Артефакты (Artifacts): Нетекстовые данные (например, файлы, изображения), связанные с пользователем или сессией.3. Непосредственная информация из диалогаЭтот тип контекста «заземляет» агента в текущем взаимодействии, определяя его непосредственную задачу.История диалога (Conversation History): Пошаговая запись текущего взаимодействия.Состояние / «Черновик» (State / Scratchpad): Временная, промежуточная информация или расчёты, которые агент использует в процессе рассуждений.Запрос пользователя (User's Prompt): Непосредственный запрос, на который нужно ответить.Динамическое формирование контекста критически важно. Память, например, не статична; её нужно избирательно извлекать и обновлять по мере взаимодействия пользователя с агентом или поступления новых данных. Кроме того, эффективные рассуждения часто опираются на обучение в контексте (in-context learning) — процесс, при котором LLM учится выполнять задачи на основе демонстраций прямо в промпте. Обучение в контексте может быть более эффективным, когда агент использует примеры (few-shot examples), релевантные текущей задаче, а не полагается на жёстко прописанные (hardcoded) шаблоны. Аналогично, внешние знания извлекаются RAG-инструментами на основе непосредственного запроса пользователя.Одна из самых серьёзных проблем при создании агента, осведомлённого о контексте, — это управление постоянно растущей историей диалога. Теоретически, модели с большими контекстными окнами могут обрабатывать обширные переписки. На практике же, по мере роста контекста, увеличиваются стоимость и задержки (latency). Кроме того, модели могут страдать от «затухания контекста» (context rot) — явления, при котором их способность уделять внимание критически важной информации снижается по мере увеличения объёма контекста.Context Engineering напрямую решает эту проблему, применяя стратегии для динамического изменения истории — такие как суммаризация, выборочное удаление (pruning) или другие методы сжатия. Это позволяет сохранить жизненно важную информацию, контролируя при этом общее количество токенов, и в конечном итоге приводит к созданию более надёжных и персонализированных AI-систем.На практике это реализуется как непрерывный цикл в рамках операционной петли агента на каждом шаге (turn) диалога:  Извлечение контекста (Fetch Context): Агент начинает с получения контекста — например, сохранённых данных о пользователе, документов из RAG и истории недавних сообщений. Для динамического извлечения агент использует запрос пользователя и другие метаданные, чтобы определить, какую информацию нужно получить.Подготовка контекста (Prepare Context): Фреймворк агента динамически формирует полный промпт для вызова LLM. Хотя отдельные API-вызовы могут быть асинхронными, подготовка контекста — это блокирующий процесс, находящийся на критическом пути выполнения (hot-path). Агент не может продолжить работу, пока контекст не будет готов.Вызов LLM и Инструментов (Invoke LLM and Tools): Агент итеративно вызывает LLM и необходимые инструменты, пока не будет сгенерирован финальный ответ для пользователя. Результаты работы инструментов и модели добавляются в контекст.Сохранение контекста (Upload Context): Новая информация, собранная за текущий шаг, загружается в постоянное хранилище. Часто это «фоновый» процесс, который позволяет агенту завершить свою работу, пока консолидация памяти или другая постобработка происходят асинхронно.В основе этого жизненного цикла лежат два фундаментальных компонента: сессии и память. Сессия управляет пошаговым состоянием одного диалога. Память, в свою очередь, обеспечивает механизм долговременного хранения, собирая и обобщая ключевую информацию из нескольких сессий.Представьте сессию как рабочий стол, который вы используете для конкретного проекта. Пока вы работаете, он завален всеми необходимыми инструментами, заметками и справочными материалами. Всё находится под рукой, но всё это временно и относится только к текущей задаче.Когда проект закончен, вы не просто запихиваете весь этот беспорядок в шкаф. Вместо этого вы начинаете процесс создания памяти, которая похожа на организованную картотеку. Вы просматриваете материалы на столе, выбрасываете черновики и лишние записи и подшиваете в папки с пометками только самые важные, итоговые документы. Это гарантирует, что картотека останется чистым, надёжным и эффективным источником информации для всех будущих проектов, не захламлённым временным хаосом рабочего стола.Эта аналогия напрямую отражает то, как работает эффективный агент: сессия служит временным рабочим столом для одного диалога, а память агента — это тщательно организованная картотека, позволяющая ему вспоминать ключевую информацию в ходе будущих взаимодействий.Опираясь на этот общий обзор инженерии контекста, мы можем теперь перейти к рассмотрению двух ключевых компонентов: сессий и памяти. Начнём с сессий.СессииФундаментальным элементом инженерии контекста является сессия, которая инкапсулирует непосредственную историю диалога и рабочую память для одного непрерывного разговора. Каждая сессия — это самодостаточная запись, привязанная к конкретному пользователю. Сессия позволяет агенту поддерживать контекст и давать связные ответы в рамках одного диалога. У пользователя может быть несколько сессий, но каждая из них функционирует как отдельный, не связанный с другими журнал конкретного взаимодействия.Каждая сессия содержит два ключевых компонента: хронологическую историю (события) и рабочую память агента (состояние).События (Events) — это «строительные блоки» диалога. К распространённым типам событий относятся:Ввод пользователя (user input): Сообщение от пользователя (текст, аудио, изображение и т.д.).Ответ агента (agent response): Ответ агента пользователю.Вызов инструмента (tool call): Решение агента использовать внешний инструмент или API.Результат работы инструмента (tool output): Данные, возвращённые после вызова инструмента, которые агент использует для продолжения своих рассуждений.Помимо истории чата, сессия часто включает в себя состояние (state) — структурированную «рабочую память» или «черновик» (scratchpad). Здесь хранятся временные, структурированные данные, относящиеся к текущему диалогу, например, какие товары находятся в корзине.По мере развития диалога агент будет добавлять в сессию новые события. Кроме того, он может изменять состояние (state) на основе своей внутренней логики.Структура событий аналогична списку объектов Content, передаваемому в Gemini API, где каждый элемент с role (ролью) и parts (частями) представляет собой один шаг (turn) — или одно Событие — в диалоге.Рабочая среда выполнения (execution environment) агента в продакшене, как правило, является stateless (не сохраняющей состояние). Это означает, что она не удерживает никакой информации после завершения запроса. Следовательно, история диалога должна сохраняться в постоянное хранилище (persistent storage), чтобы обеспечить непрерывность пользовательского опыта.Хотя хранение в оперативной памяти (in-memory) подходит для разработки, в рабочих приложениях (production applications) для надёжного хранения и управления сессиями следует использовать надёжные базы данных. Например, историю диалога можно хранить в управляемых решениях, таких как Agent Engine Sessions³.Различия между фреймворками и моделямиХотя основные идеи схожи, разные фреймворки для создания агентов реализуют сессии, события и состояние по-своему.Фреймворки отвечают за поддержание истории диалога и состояния для LLM, формирование запросов к LLM с использованием этого контекста, а также за обработку (parsing) и сохранение ответа от LLM. Фреймворк агента действует как универсальный переводчик между вашим кодом и LLM.Пока вы, как разработчик, работаете с едиными внутренними структурами данных фреймворка на каждом шаге диалога, сам фреймворк выполняет критически важную задачу: преобразует эти структуры в тот конкретный формат, который требует LLM.Эта абстракция очень важна, поскольку она отделяет логику вашего агента от конкретной LLM, которую вы используете, предотвращая зависимость от одного поставщика (vendor lock-in).В конечном счёте, цель — сформировать «запрос» (request), который LLM сможет понять. Для моделей Gemini от Google это List[Content] (список объектов Content). Каждый объект Content — это простая, словарно-подобная структура с двумя ключами: role, который определяет, кто говорит (""user"" или ""model""), и parts, который содержит непосредственно само сообщение (текст, изображения, вызовы инструментов и т.д.).Фреймворк автоматически выполняет сопоставление (mapping) данных из своего внутреннего объекта (например, Event в ADK) с соответствующими полями role и parts в объекте Content перед тем, как сделать API-вызов.По сути, фреймворк предоставляет разработчику стабильный внутренний API, в то же время «под капотом» управляя сложными и разнообразными внешними API различных LLMADK использует явный объект Session, который содержит список объектов Event (событий) и отдельный объект state (состояние). Session здесь похож на картотечный шкаф: одна папка для истории диалога (события), другая — для рабочей памяти (состояние).В LangGraph нет формального объекта «сессия». Вместо этого, состояние (state) и есть сессия. Этот всеобъемлющий объект state хранит и историю диалога (в виде списка объектов Message), и все остальные рабочие данные. В отличие от традиционной сессии, которая работает как лог, куда данные только добавляются (append-only), состояние в LangGraph является изменяемым (mutable). Его можно трансформировать, а такие стратегии, как сжатие истории (history compaction), могут изменять саму запись. Это полезно для управления длинными диалогами и лимитами токенов.Сессии для мультиагентных системВ мультиагентной системе несколько агентов работают совместно. Каждый агент фокусируется на своей небольшой, узкоспециализированной задаче. Чтобы эти агенты могли эффективно взаимодействовать, они должны обмениваться информацией.Как показано на диаграмме ниже, архитектура системы определяет паттерны коммуникации, которые они используют для обмена информацией. Центральным компонентом этой архитектуры является то, как система обрабатывает историю сессии — постоянный (persistent) лог всех взаимодействий.Прежде чем рассматривать архитектурные паттерны для управления этой историей, крайне важно провести различие между ней и контекстом, отправляемым в LLM.Представьте историю сессии как полную, неотредактированную стенограмму всего диалога.Контекст же, с другой стороны, — это тщательно подготовленная порция информации (payload), отправляемая в LLM для одного-единственного шага (turn) в диалоге. Агент может формировать этот контекст, выбирая из истории лишь релевантный фрагмент или добавляя специальное форматирование, например, направляющую преамбулу, чтобы скорректировать ответ модели.Этот раздел посвящён тому, какая информация передаётся между агентами, а не тому, какой контекст обязательно отправляется в LLM.Фреймворки для агентов обрабатывают историю сессий в мультиагентных системах, используя один из двух основных подходов: общая, единая история, в которую все агенты вносят свой вклад, или отдельные, индивидуальные истории, где каждый агент ведёт свой собственный лог⁴. Выбор между этими двумя паттернами зависит от характера задачи и желаемого стиля взаимодействия между агентами.Общая, единая история (Shared, Unified History)В модели с общей, единой историей все агенты в системе читают и записывают все события в один и тот же лог диалога. Сообщение каждого агента, вызов инструмента и результат наблюдения добавляются в один центральный лог в хронологическом порядке.Этот подход лучше всего подходит для тесно связанных, совместных задач, требующих единого источника истины (single source of truth), например, для многоэтапного процесса решения проблемы, где результат работы одного агента является прямыми входными данными для следующего.Даже при наличии общей истории подагент может предварительно обработать лог, прежде чем передать его в LLM. Например, он может отфильтровать его, оставив только подмножество релевантных событий, или добавить метки, чтобы указать, какой агент сгенерировал каждое событие.Если вы используете в ADK делегирование на основе LLM для передачи задач подагентам, все промежуточные события подагента будут записаны в ту же сессию, что и у корневого агента⁵.Раздельные, индивидуальные истории (Separate, Individual Histories)В модели с раздельными, индивидуальными историями каждый агент ведёт свой собственный, приватный лог диалога и функционирует как «чёрный ящик» для других агентов. Все внутренние процессы — такие как промежуточные рассуждения, использование инструментов и шаги логических выводов — остаются в приватном логе агента и не видны другим.Коммуникация происходит только через явные сообщения, в которых агент делится своим конечным результатом, а не процессом его получения.Такое взаимодействие обычно реализуется либо через паттерн «Агент как инструмент» (Agent-as-a-tool), либо с использованием протокола Agent-to-Agent (A2A).В подходе «Агент как инструмент» один агент вызывает другого так, как если бы это был обычный инструмент, передавая ему входные данные и получая итоговый, самодостаточный результат⁶.С помощью протокола A2A агенты используют структурированный протокол для прямого обмена сообщениями⁷.Мы подробнее рассмотрим протокол A2A на следующем занятии.Взаимодействие(Interoperability) между различными фреймворками для агентов Использование фреймворком внутреннего представления данных создаёт критический архитектурный компромисс для мультиагентных систем: та самая абстракция, которая отделяет агента от LLM, также изолирует его от агентов, использующих другие фреймворки.Эта изоляция закрепляется на уровне хранения данных (persistence layer). Модель хранения Сессии обычно напрямую связывает схему базы данных с внутренними объектами фреймворка, создавая жёсткую, плохо переносимую запись диалога. В результате агент, созданный с помощью LangGraph, не может напрямую интерпретировать отдельные объекты Session и Event, сохранённые агентом на базе ADK, что делает бесшовную передачу задач невозможной.Одним из новых архитектурных паттернов для координации взаимодействия между такими изолированными агентами является протокол Agent-to-Agent (A2A)⁸. Хотя этот паттерн позволяет агентам обмениваться сообщениями, он не решает основную проблему — обмен полным/богатым, контекстным состоянием. История диалога каждого агента закодирована во внутренней схеме его фреймворка. Вследствие этого, любое A2A-сообщение, содержащее события сессии, требует наличия слоя трансляции, чтобы быть полезным.Более надёжный архитектурный паттерн для обеспечения совместимости (interoperability) предполагает вынесение общих знаний в независимый от фреймворка слой данных, такой как Память (Memory).В отличие от хранилища Сессий, которое сохраняет «сырые», специфичные для фреймворка объекты вроде Events и Messages, слой памяти предназначен для хранения обработанной, канонической информации. Ключевые сведения — такие как краткие выжимки (summaries), извлечённые сущности и факты — извлекаются из диалога и обычно хранятся в виде строк или словарей. Структуры данных слоя памяти не привязаны к внутреннему представлению данных какого-либо одного фреймворка, что позволяет ему служить универсальным, общим слоем данных.Этот паттерн позволяет гетерогенным (разнородным) агентам достичь подлинного совместного интеллекта, разделяя общий когнитивный ресурс без необходимости в кастомных трансляторах.Аспекты работы с сессиями в продакшенеПри переносе агента в рабочую среду (production environment), его система управления сессиями должна эволюционировать от простого лога до надёжного сервиса корпоративного уровня. Ключевые аспекты здесь делятся на три критически важные области: безопасность и конфиденциальность, целостность данных и производительность. Управляемые хранилища сессий, такие как Agent Engine Sessions, специально разработаны для удовлетворения этих производственных требований.Безопасность и конфиденциальностьЗащита конфиденциальной информации, содержащейся в сессии, — это необсуждаемое требование.Строгая изоляция — самый важный принцип безопасности. Сессия принадлежит одному пользователю, и система должна обеспечивать строгую изоляцию, чтобы один пользователь никогда не смог получить доступ к данным сессии другого (например, с помощью ACL). Каждый запрос к хранилищу сессий должен быть аутентифицирован и авторизован в соответствии с владельцем сессии.Лучшая практика для обработки персональных данных (PII) — удалять (redact) их до того, как данные сессии будут записаны в хранилище. Это фундаментальная мера безопасности, которая значительно снижает риск и «радиус поражения» (blast radius) в случае потенциальной утечки данных. Обеспечивая, чтобы конфиденциальные данные никогда не сохранялись (например, с помощью таких инструментов, как Model Armor⁹), вы упрощаете соблюдение нормативных требований по защите данных, таких как GDPR и CCPA, и укрепляете доверие пользователей.Целостность данных и управление жизненным цикломРабочая система требует чётких правил хранения и поддержки данных сессий с течением времени. Сессии не должны храниться вечно. Вы можете реализовать политику Time-to-Live (TTL) для автоматического удаления неактивных сессий, чтобы управлять затратами на хранение и снижать накладные расходы на управление данными. Это требует чёткой политики хранения данных, которая определяет, как долго сессии должны храниться перед архивацией или окончательным удалением.Кроме того, система должна гарантировать, что операции добавляются в историю сессии в детерминированном (строго определённом) порядке. Поддержание правильной хронологической последовательности событий является основой целостности лога диалога.Производительность и масштабируемостьДанные сессии находятся на «критическом пути» (hot path) каждого взаимодействия с пользователем, что делает их производительность первостепенной задачей. Чтение и запись истории сессии должны быть чрезвычайно быстрыми, чтобы обеспечить отзывчивость пользовательского интерфейса. Среды выполнения (runtimes) агентов, как правило, являются stateless (не сохраняют состояние), поэтому вся история сессии извлекается из центральной базы данных в начале каждого шага (turn) диалога, что влечёт за собой задержки, связанные с передачей данных по сети.Чтобы уменьшить задержки (latency), крайне важно сократить объём передаваемых данных. Ключевая оптимизация — это фильтрация или сжатие истории сессии перед её отправкой агенту. Например, можно удалить старые, нерелевантные результаты вызовов функций, которые больше не нужны для текущего состояния диалога.В следующем разделе подробно рассматриваются несколько стратегий сжатия истории для эффективного управления диалогами с длинным контекстом.Управление диалогами с длинным контекстом: компромиссы и оптимизацииВ упрощённой архитектуре сессия представляет собой неизменяемый (immutable) лог диалога между пользователем и агентом. Однако по мере его роста увеличивается и количество используемых токенов. Современные LLM могут обрабатывать длинные контексты, но существуют ограничения, особенно для приложений, чувствительных к задержкам (latency)¹⁰:Лимиты контекстного окна: У каждой LLM есть максимальный объём текста (контекстное окно), который она может обработать за один раз. Если история диалога превысит этот лимит, API-вызов завершится ошибкой.Стоимость API ($): Большинство LLM-провайдеров взимают плату в зависимости от количества отправленных и полученных токенов. Более короткая история означает меньше токенов и ниже стоимость каждого шага (turn) диалога.Задержка (скорость): Отправка большего объёма текста в модель требует больше времени на обработку, что приводит к увеличению времени ответа для пользователя. Сжатие (compaction) помогает агенту оставаться быстрым и отзывчивым.Качество: С увеличением количества токенов производительность может ухудшаться из-за дополнительного «шума» в контексте и авторегрессионных ошибок.Управление длинным диалогом с агентом можно сравнить с тем, как опытный путешественник собирает чемодан для долгой поездки. Чемодан — это ограниченное контекстное окно агента, а одежда и вещи — это фрагменты информации из диалога.Если вы просто попытаетесь впихнуть в чемодан всё подряд, он станет слишком тяжёлым и неупорядоченным, и в нём будет трудно быстро найти то, что вам нужно — точно так же, как перегруженное контекстное окно увеличивает затраты на обработку и замедляет время ответа. С другой стороны, если вы возьмёте слишком мало, вы рискуете оставить дома важные вещи, такие как паспорт или тёплую куртку, поставив под угрозу всю поездку — так же, как агент может потерять критически важный контекст, что приведёт к нерелевантным или неверным ответам.И путешественник, и агент действуют в рамках схожего ограничения: успех зависит не от того, сколько вы можете унести, а от того, чтобы взять с собой только то, что действительно нужно.Стратегии сжатия (Compaction strategies) сокращают длинные истории диалогов, уплотняя их, чтобы они помещались в контекстное окно модели, снижая затраты на API и задержки. По мере удлинения диалога история, отправляемая модели на каждом шаге, может стать слишком большой. Стратегии сжатия решают эту проблему, интеллектуально «подрезая» историю и стараясь при этом сохранить наиболее важный контекст.Итак, как понять, какую информацию можно выбросить из Сессии, не потеряв ничего ценного? Стратегии варьируются от простого усечения до сложных методов сжатия.Сохранять последние N шагов (Keep the last N turns): Это самая простая стратегия. Агент хранит только N самых последних шагов (turns) диалога (так называемое «скользящее окно»(a “sliding window”)) и отбрасывает всё, что было раньше.Усечение по токенам (Token-Based Truncation): Перед отправкой истории в модель агент подсчитывает количество токенов в сообщениях, начиная с самого последнего и двигаясь в обратном порядке. Он включает в контекст столько сообщений, сколько возможно, не превышая заранее определённый лимит токенов (например, 4000). Всё, что было раньше, просто отсекается.Рекурсивная суммаризация (Recursive Summarization): Более старые части диалога заменяются на краткую выжимку, сгенерированную AI. По мере роста диалога агент периодически использует ещё один вызов LLM, чтобы суммировать самые старые сообщения. Эта краткая выжимка затем используется как сжатая форма истории, часто добавляемая в начало перед более свежими, дословными сообщениями.Например, в ADK вы можете сохранять последние N шагов, используя встроенный плагин для вашего ADK-приложения, чтобы ограничить контекст, отправляемый в модель. Важно отметить, что это не изменяет исторические события, хранящиеся в вашем хранилище сессий:Учитывая, что сложные стратегии сжатия направлены на снижение затрат и задержек, критически важно выполнять ресурсоёмкие операции (такие как рекурсивная суммаризация) асинхронно в фоновом режиме и сохранять (asynchronously in the background and persist the results) результаты. «Фоновый режим» гарантирует, что клиент не будет ждать, а «сохранение» — что дорогостоящие вычисления не будут повторяться без необходимости.Часто именно менеджер памяти агента отвечает и за генерацию, и за сохранение этих рекурсивных сводок. Агент также должен вести учёт того, какие именно события были включены в сжатую сводку; это предотвращает ненужную отправку в LLM оригинальных, более подробных событий.Кроме того, агент должен решать, когда необходимо сжатие. Механизм запуска (trigger) обычно относится к одной из нескольких категорий:Триггеры на основе количества (например, порог по размеру токенов или числу шагов): Сжатие запускается, как только диалог превышает определённый порог. Этот подход часто является «достаточно хорошим» для управления длиной контекста.Триггеры на основе времени: Сжатие запускается не из-за размера диалога, а из-за отсутствия активности. Если пользователь прекращает взаимодействовать на определённый период (например, 15 или 30 минут), система может запустить задачу сжатия в фоновом режиме.Триггеры на основе событий (например, семантическое завершение/завершение задачи): Агент решает запустить сжатие, когда обнаруживает, что определённая задача, подцель или тема разговора завершена.Например, вы можете использовать EventsCompactionConfig в ADK, чтобы запустить суммаризацию на основе LLM после заданного количества шагов (turns):Генерация памяти — это широкое понятие, означающее извлечение постоянных (persistent) знаний из подробного и «зашумлённого» источника данных. В этом разделе мы рассмотрели основной пример извлечения информации из истории диалога — сжатие сессии (session compaction). Сжатие, по сути, «дистиллирует» дословную стенограмму всего разговора, извлекая ключевые факты и краткие выжимки и отбрасывая при этом разговорный «наполнитель».Опираясь на концепцию сжатия, в следующем разделе мы рассмотрим генерацию и управление памятью в более широком смысле. Мы обсудим различные способы, которыми «воспоминания» (memories) могут создаваться, храниться и извлекаться для формирования долгосрочных знаний агента.Память (Memory)Память и Сессии тесно взаимосвязаны: сессии служат основным источником данных для формирования записей в памяти, а память, в свою очередь, является ключевой стратегией для управления объёмом сессии.Память — это, по сути, концентрированная выжимка ключевой информации, извлечённой из диалога или источника данных. Это сжатое представление, которое сохраняет важный контекст, делая его полезным для будущих взаимодействий. Как правило, данные в памяти сохраняются между сессиями, чтобы обеспечить непрерывный и персонализированный опыт.«Менеджер памяти», выступающий как специализированный, независимый (decoupled) сервис, закладывает основу для совместимости (interoperability) мультиагентных систем. Менеджеры памяти часто используют независимые от фреймворка структуры данных, такие как простые строки и словари. Это позволяет агентам, созданным на разных фреймворках, подключаться к единому хранилищу памяти, создавая общую базу знаний, которую может использовать любой подключённый агент.Примечание: некоторые фреймворки могут называть Сессии или дословную историю диалога «краткосрочной памятью». В этом материале под памятью (memories) понимается именно извлечённая информация, а не «сырой» пошаговый диалог.Хранение и извлечение данных из памяти критически важно для создания продвинутых и интеллектуальных агентов. Надёжная система памяти превращает простого чат-бота в по-настоящему интеллектуального агента, наделяя его несколькими ключевыми возможностями:Персонализация: Самый частый сценарий использования — запоминание предпочтений пользователя, фактов и прошлых взаимодействий для адаптации будущих ответов. Например, если агент помнит любимую спортивную команду пользователя или предпочитаемое им место в самолёте, это делает взаимодействие более полезным и личным.Управление контекстным окном: По мере удлинения диалогов их полная история может превысить лимит контекстного окна LLM. Системы памяти могут сжимать эту историю, создавая краткие выжимки или извлекая ключевые факты. Это позволяет сохранить контекст, не отправляя тысячи токенов на каждом шаге, что снижает и стоимость, и задержки (latency).Анализ данных и получение инсайтов: Анализируя сохранённые данные по многим пользователям (в агрегированном, анонимном виде), можно извлекать ценные сведения из информационного шума. Например, чат-бот в ритейле может обнаружить, что многие пользователи спрашивают о правилах возврата конкретного товара, сигнализируя о потенциальной проблеме.Самосовершенствование и адаптация агента: Агент учится на предыдущих запусках, создавая процедурные знания (procedural memories) о собственной производительности — он записывает, какие стратегии, инструменты или логические цепочки привели к успешным результатам. Это позволяет агенту со временем формировать базу эффективных решений, адаптируясь и совершенствуя свои навыки решения задач.Создание, хранение и использование памяти в AI-системе — это совместный процесс. У каждого компонента в общей архитектуре (stack) — от конечного пользователя до кода разработчика — своя, чётко определённая роль.Пользователь: Предоставляет исходные «сырые» данные для памяти. В некоторых системах пользователи могут предоставлять данные напрямую (например, через форму).Агент (Логика разработчика): Определяет, что и когда запоминать, управляя вызовами к менеджеру памяти.В простых архитектурах разработчик может реализовать логику так, что память всегда извлекается и её создание всегда инициируется.В более сложных архитектурах разработчик может реализовать память как инструмент (memory-as-a-tool), где агент (с помощью LLM) сам решает, когда следует извлекать или генерировать данные для памяти.Фреймворк агента (например, ADK, LangGraph): Предоставляет структуру и инструменты для взаимодействия с памятью. Фреймворк выступает в роли связующей инфраструктуры (plumbing). Он определяет, как логика разработчика может получить доступ к истории диалога и взаимодействовать с менеджером памяти, но сам не управляет долговременным хранилищем. (defines how the developer's logic can access conversation history and interact with the memory manager, but it doesn't manage the long-term storage itself.) Он также определяет, как извлечённые из памяти данные добавляются в контекстное окно.Хранилище сессий (например, Agent Engine Sessions, Spanner, Redis): Хранит пошаговый диалог Сессии. Необработанный диалог используется менеджером памяти для генерации записей в памяти.Менеджер памяти (например, Agent Engine Memory Bank, Mem0, Zep): Отвечает за хранение, извлечение и сжатие данных памяти. Механизмы хранения и извлечения зависят от используемого провайдера. Это специализированный сервис, который берёт потенциальные «воспоминания», определённые агентом, и управляет всем их жизненным циклом:Извлечение (Extraction): Извлекает ключевую информацию из исходных данных.Консолидация (Consolidation): Устраняет дубликаты и объединяет связанные данные.Хранение (Storage): Обеспечивает сохранение данных в постоянных базах данных.Получение (Retrieval): Извлекает релевантные данные для предоставления контекста в новых взаимодействиях.Такое разделение обязанностей позволяет разработчику сосредоточиться на уникальной логике агента, не создавая с нуля сложную базовую инфраструктуру для хранения и управления памятью.Важно понимать, что менеджер памяти — это активная система, а не просто пассивная векторная база данных. Хотя он использует поиск по сходству (similarity search) для извлечения данных, его основная ценность заключается в способности интеллектуально извлекать, консолидировать и курировать записи в памяти с течением времени. Управляемые сервисы памяти, такие как Agent Engine Memory Bank, берут на себя весь жизненный цикл генерации и хранения данных, освобождая вас, чтобы вы могли сосредоточиться на основной логике вашего агента.Именно благодаря функции извлечения данных память часто сравнивают с другим ключевым архитектурным паттерном: Retrieval-Augmented Generation (RAG). Однако они построены на разных архитектурных принципах: RAG работает со статичными, внешними данными, в то время как Memory курирует динамический, специфичный для пользователя контекст.Они выполняют две разные, но взаимодополняющие роли: RAG делает агента экспертом по фактам, а Memory — экспертом по пользователю.В следующей таблице представлены их основные различия:Чтобы лучше понять разницу, представьте, что RAG — это научный библиотекарь агента, а менеджер памяти — его личный помощник.Научный библиотекарь (RAG) работает в огромной публичной библиотеке, полной энциклопедий, учебников и официальных документов. Когда агенту нужен проверенный факт — например, технические характеристики продукта или историческая дата — он обращается к библиотекарю. Библиотекарь извлекает информацию из этой статичной, общей и авторитетной базы знаний, чтобы предоставить достоверные, основанные на фактах ответы. Библиотекарь — эксперт по фактам о мире, но он ничего не знает лично о пользователе, который задаёт вопрос.Напротив, личный помощник (память) всегда сопровождает агента и ведёт личный блокнот, записывая в него детали каждого взаимодействия с конкретным пользователем. Этот блокнот постоянно обновляется, строго конфиденциален и содержит личные предпочтения, историю прошлых диалогов и меняющиеся цели. Когда агенту нужно вспомнить любимую спортивную команду пользователя или контекст обсуждения проекта на прошлой неделе, он обращается к помощнику. Сильная сторона помощника — не знание фактов о мире, а глубокое понимание самого пользователя.В конечном итоге, по-настоястоящему интеллектуальному агенту нужно и то, и другое. RAG даёт ему экспертные знания о мире, а память — экспертное понимание пользователя, которому он помогает.В следующем разделе мы детально разберём концепцию памяти, рассмотрев её ключевые компоненты: типы хранимой информации, способы её организации, механизмы хранения и создания, стратегическое определение её области применения и обработку мультимодальных и текстовых данных.Типы памятиПамять агента можно классифицировать по тому, как информация хранится и как она была получена. Эти различные типы памяти работают вместе, чтобы создать богатое, контекстное понимание пользователя и его потребностей. Для всех типов памяти действует правило: память является описательной, а не предсказательной.«Единица памяти» (memory) — это атомарный/базовый/неделимый/самодостаточный   фрагмент контекста, который возвращается менеджером памяти и используется агентом. Хотя точная схема может варьироваться, одна единица памяти обычно состоит из двух основных компонентов: содержимого (content) и метаданных (metadata).Содержимое (Content) — это суть «воспоминания», извлечённая из исходных данных (например, из необработанного диалога сессии). Крайне важно, что содержимое проектируется так, чтобы быть независимым от фреймворка, используя простые структуры данных, которые любой агент может легко обработать. Содержимое может быть как структурированным, так и неструктурированным.Структурированная память включает информацию, обычно хранящуюся в универсальных форматах, таких как словарь или JSON. Её схема, как правило, определяется разработчиком, а не конкретным фреймворком. Например: {""seat_preference"": ""Window""}.Неструктурированная память — это описания на естественном языке, которые отражают суть более длительного взаимодействия, события или темы. Например: «Пользователь предпочитает место у окна».Метаданные (Metadata) предоставляют контекст о самой записи в памяти и обычно хранятся в виде простых строк. Они могут включать уникальный идентификатор записи, идентификаторы «владельца» записи, а также метки, описывающие содержимое или источник данных.Типы информацииПомимо базовой структуры, записи в памяти можно классифицировать по фундаментальному типу знаний, которые они представляют. Это различие, заимствованное из когнитивной науки¹¹, имеет решающее значение для понимания того, как агент использует память, и делит её на две основные функциональные категории: декларативная память («знать что») и процедурная память («знать как»). (derived from cognitive science: declarative memories (“knowing what”) and procedural memories (“knowing how”).)Декларативная память (Declarative memory)Это знания агента о фактах, цифрах и событиях. Это вся информация, которую агент может явно сформулировать или «декларировать». Если запись в памяти является ответом на вопрос «что?», — это декларативная память. Эта категория охватывает как общие знания о мире (семантические), так и конкретные факты о пользователе (сущностные/эпизодические).Процедурная память (Procedural memory)Это знания агента о навыках и последовательностях действий (workflows). Она направляет действия агента, неявно демонстрируя, как правильно выполнить задачу. Если запись в памяти помогает ответить на вопрос «как?» — например, какая правильная последовательность вызовов инструментов для бронирования поездки, — это процедурная память.Паттерны организации памятиПосле того как запись в памяти создана, следующий вопрос — как её организовать. Менеджеры памяти обычно используют один или несколько из следующих паттернов: Коллекции (Collections)¹², Структурированный профиль пользователя (Structured User Profile) или «Скользящая сводка» (Rolling Summary). Эти паттерны определяют, как отдельные записи в памяти соотносятся друг с другом и с пользователем.Коллекции (Collections)¹³: Этот паттерн организует контент в виде множества самодостаточных записей на естественном языке для одного пользователя. Каждая запись — это отдельное событие, краткая выжимка или наблюдение, хотя в коллекции может быть несколько записей по одной общей теме. Коллекции позволяют хранить и искать информацию в большом, менее структурированном пуле данных, связанных с конкретными целями или темами.Структурированный профиль пользователя (Structured User Profile): Этот паттерн организует записи в виде набора ключевых фактов о пользователе, подобно карточке контакта, которая постоянно обновляется новой, стабильной информацией. Он предназначен для быстрого поиска важной фактической информации, такой как имена, предпочтения и детали учётной записи.«Скользящая сводка» (Rolling Summary): В отличие от профиля, этот паттерн объединяет всю информацию в единую, постоянно развивающуюся запись, которая представляет собой сводку на естественном языке обо всём взаимодействии пользователя и агента. Вместо создания новых, отдельных записей, менеджер непрерывно обновляет этот один «мастер-документ». Этот паттерн часто используется для сжатия длинных Сессий, сохраняя жизненно важную информацию при управлении общим количеством токенов.Архитектуры храненияКроме того, выбор архитектуры хранения — это критически важное решение, которое определяет, насколько быстро и интеллектуально агент сможет извлекать данные из памяти. Выбор архитектуры определяет, в чём агент будет преуспевать: в поиске концептуально схожих идей, в понимании структурированных взаимосвязей или и в том, и в другом.Записи в памяти обычно хранятся в векторных базах данных (vector databases) и/или графах знаний (knowledge graphs).Векторные базы данных помогают находить записи, концептуально схожие с запросом.Графы знаний хранят записи в виде сети сущностей и их взаимосвязей.Векторные базы данных — самый распространённый подход, позволяющий выполнять поиск на основе семантического сходства, а не точных ключевых слов. Записи в памяти преобразуются в векторные представления (embedding vectors), и база данных находит наиболее близкие по смыслу совпадения с запросом пользователя. Этот метод отлично подходит для извлечения неструктурированных, естественно-языковых записей, где ключевую роль играют контекст и значение (т.е. “atomic facts”. базовые факты/элементарные).Графы знаний используются для хранения записей в виде сети сущностей (узлов) и их взаимосвязей (рёбер). Извлечение данных включает в себя обход этого графа для поиска прямых и косвенных связей, что позволяет агенту делать выводы о том, как связаны между собой различные факты. Этот метод идеален для структурированных, реляционных запросов и понимания сложных связей внутри данных (т.е. «триплеты знаний»¹⁵).Можно также объединить оба метода в гибридный подход, обогатив структурированные сущности графа знаний векторными представлениями. Это позволяет системе выполнять одновременно и реляционный, и семантический поиск. Такой подход сочетает структурированные рассуждения графа с тонким концептуальным поиском векторной базы данных, предлагая лучшее из обоих миров.Механизмы созданияМы также можем классифицировать память по тому, как она была создана, включая способ получения информации.Явные воспоминания (Explicit memories) создаются, когда пользователь даёт агенту прямую команду что-то запомнить (например, «Запомни, что моя годовщина — 26 октября»).Неявные воспоминания (Implicit memories), напротив, создаются, когда агент сам делает вывод и извлекает информацию из диалога без прямой команды (например, «На следующей неделе у меня годовщина. Можешь помочь мне найти подарок для моей второй половинки?»).Память также можно различать по тому, где находится логика её извлечения — внутри (internal) фреймворка агента или снаружи (external).Внутренняя память (Internal memory) относится к управлению памятью, встроенному непосредственно во фреймворк агента. Это удобно для быстрого старта, но часто в таких решениях отсутствуют продвинутые функции. Внутренняя память может использовать внешнее хранилище, но сам механизм генерации записей находится внутри агента.Внешняя память (External Memory) предполагает использование отдельного, специализированного сервиса, предназначенного для управления памятью (например, Agent Engine Memory Bank, Mem0, Zep). Фреймворк агента делает API-вызовы к этому внешнему сервису для хранения, извлечения и обработки данных. Такой подход предоставляет более сложные функции, такие как семантический поиск, извлечение сущностей и автоматическая суммаризация, перекладывая сложную задачу управления памятью на специально созданный для этого инструмент.Область видимости памяти (Memory scope)Вам также необходимо определить, кого или что описывает память. Это влияет на то, какую сущность (например, пользователя, сессию или приложение) вы используете для агрегации и извлечения данных из памяти.Уровень пользователя (User-Level scope): Это самая распространённая реализация, предназначенная для создания непрерывного, персонализированного опыта для каждого отдельного человека. Например: «Пользователь предпочитает место в середине ряда». Данные привязаны к конкретному user ID и сохраняются между всеми его сессиями, что позволяет агенту формировать долгосрочное понимание его предпочтений и истории.Уровень сессии (Session-Level scope): Этот уровень предназначен для сжатия длинных диалогов. Например: «Пользователь ищет билеты из Нью-Йорка в Париж на даты с 7 по 14 ноября 2025 года. Он предпочитает прямые рейсы и место в середине ряда». Здесь создаётся постоянная запись с ключевыми выводами из одной сессии, что позволяет агенту заменить подробную, «тяжёлую» по токенам стенограмму на краткий набор фактов. Крайне важно, что эта память отличается от «сырого» лога сессии; она содержит только обработанные выводы из диалога, а не сам диалог, и её контекст изолирован в рамках этой конкретной сессии.Уровень приложения (Application-level scope) (или глобальный контекст): Это данные, доступные всем пользователям приложения. Например: «Кодовое название XYZ относится к проекту...». Эта область видимости используется для предоставления общего контекста, распространения общесистемной информации или формирования базового уровня общих знаний. Распространённый сценарий использования памяти на уровне приложения — это процедурная память, которая предоставляет агенту инструкции «как делать». Такие записи обычно предназначены для улучшения логики агента для всех пользователей. Критически важно, чтобы эти данные были очищены от любой конфиденциальной информации, чтобы предотвратить утечки данных между пользователями.Мультимодальная память (Multimodal memory)«Мультимодальная память» — это ключевая концепция, описывающая, как агент обрабатывает нетекстовую информацию, такую как изображения, видео и аудио. Здесь важно различать данные, из которых извлекается память (источник), и данные, в виде которых она хранится (содержимое).Память из мультимодального источникаЭто наиболее распространённая реализация. Агент может обрабатывать различные типы данных — текст, изображения, аудио, — но создаваемая им запись в памяти представляет собой текстовый вывод, извлечённый из этого источника.Например, агент может обработать голосовую заметку пользователя для создания «воспоминаний». Он не хранит сам аудиофайл; вместо этого он расшифровывает аудио и создаёт текстовую запись, например: «Пользователь выразил недовольство недавней задержкой доставки».Память с мультимодальным содержимымЭто более продвинутый подход, при котором сама запись в памяти содержит нетекстовые медиафайлы. Агент не просто описывает контент, а хранит его напрямую.Например, пользователь может загрузить изображение и сказать: «Запомни этот дизайн для нашего логотипа». Агент создаёт запись в памяти, которая напрямую содержит файл изображения, связанный с запросом пользователя.Большинство современных менеджеров памяти сосредоточены на обработке мультимодальных источников с созданием текстового содержимого. Это связано с тем, что генерация и извлечение неструктурированных двоичных данных, таких как изображения или аудио, для конкретной записи в памяти требует специализированных моделей, алгоритмов и инфраструктуры. Гораздо проще преобразовать все входные данные в единый, удобный для поиска формат: текст.Например, вы можете генерировать память из мультимодальных входных данных¹⁶, используя Agent Engine Memory Bank. Выходные данные будут представлять собой текстовые выводы, извлечённые из контента:В следующем разделе мы рассмотрим механику генерации памяти, подробно описав два ключевых этапа: извлечение новой информации из исходных данных и её последующее объединение с уже существующим корпусом памяти.  Генерация памяти: извлечение и объединениеГенерация памяти — это процесс, который автономно преобразует «сырые» данные из диалога в структурированные, осмысленные выводы. Представьте это как ETL-конвейер (Extract, Transform, Load), управляемый LLM и предназначенный для извлечения и сжатия данных для памяти.Именно этот ETL-конвейер отличает менеджеры памяти от RAG-систем и традиционных баз данных. Вместо того чтобы требовать от разработчиков вручную прописывать операции с базой данных, менеджер памяти использует LLM, чтобы решать, когда добавлять, обновлять или объединять записи.Эта автоматизация — ключевое преимущество менеджера памяти. Он абстрагирует (скрывает) всю сложность, связанную с управлением содержимым базы данных, выстраиванием цепочек LLM-вызовов и развёртыванием фоновых сервисов для обработки данных.Хотя конкретные алгоритмы различаются в зависимости от платформы (например, Agent Engine Memory Bank, Mem0, Zep), высокоуровневый процесс генерации памяти обычно проходит следующие четыре этапа:Приём данных (Ingestion): Процесс начинается, когда клиент предоставляет менеджеру памяти источник «сырых» данных, обычно — историю диалога.Извлечение и фильтрация (Extraction & Filtering): Менеджер памяти использует LLM для извлечения значимого контента из исходных данных. Ключевой момент в том, что эта LLM извлекает не всё подряд; она фиксирует только ту информацию, которая соответствует заранее определённым темам (topic definition). Если в полученных данных нет информации, соответствующей этим темам, запись в памяти не создаётся.Объединение (Consolidation): Это самый сложный этап, на котором менеджер памяти разрешает конфликты и устраняет дубликаты. Он выполняет процесс «саморедактирования», используя LLM для сравнения новой извлечённой информации с уже существующими записями. Чтобы база знаний пользователя оставалась целостной, точной и развивалась со временем на основе новой информации, менеджер может принять решение:Объединить новую информацию с существующей записью.Удалить существующую запись, если она стала неактуальной.Создать совершенно новую запись, если тема новая.Хранение (Storage): Наконец, новая или обновлённая запись сохраняется (persists) в надёжный слой хранения (to a durable storage layer) (например, в векторную базу данных или граф знаний), чтобы её можно было извлечь в будущих взаимодействиях.Управляемые менеджеры памяти, такие как Agent Engine Memory Bank, полностью автоматизируют этот конвейер. Они предоставляют единую, целостную систему для превращения «разговорного шума» в структурированные знания, позволяя разработчикам сосредоточиться на логике агента, а не на создании и поддержке базовой инфраструктуры данных.Например, для запуска генерации памяти с помощью Memory Bank требуется всего лишь один простой API-вызов¹⁷:Процесс генерации памяти можно сравнить с работой заботливого садовника, ухаживающего за садом.Извлечение (Extraction) — это как получение новых семян и саженцев (новая информация из диалога). Садовник не просто беспорядочно разбрасывает их по участку. Вместо этого он выполняет Объединение (Consolidation): вырывает сорняки (удаляет избыточные или конфликтующие данные), подрезает разросшиеся ветви, чтобы улучшить здоровье существующих растений (уточняет и обобщает существующие записи), а затем аккуратно высаживает новые саженцы в оптимальном месте.Такой постоянный, вдумчивый уход гарантирует, что сад остаётся здоровым, организованным и продолжает процветать с течением времени, а не превращается в заросшие, непроходимые дебри. Этот асинхронный процесс происходит в фоновом режиме, обеспечивая, чтобы сад всегда был готов к следующему «визиту».Теперь давайте подробнее рассмотрим два ключевых этапа генерации памяти: извлечение и объединение.Глубокое погружение: извлечение памяти (Memory Extraction)Цель извлечения памяти — ответить на фундаментальный вопрос: «Какая информация в этом диалоге достаточно значима, чтобы стать „воспоминанием“?» Это не простое суммирование; это целенаправленный, интеллектуальный процесс фильтрации, предназначенный для отделения сигнала (важные факты, предпочтения, цели) от шума (любезности, слова-«наполнители»).«Значимость» — это не универсальное понятие; она полностью определяется целью и сценарием использования агента. То, что нужно помнить агенту поддержки (например, номера заказов, технические проблемы), кардинально отличается от того, что нужно помнить персональному велнес-коучу (например, долгосрочные цели, эмоциональное состояние). Поэтому настройка того, какая информация сохраняется, является ключом к созданию по-настоящему эффективного агента.LLM менеджер памяти решает, что извлекать, следуя тщательно продуманному набору программных ограничений (guardrails) и инструкций, обычно встроенных в сложный системный промпт. Этот промпт определяет, что значит «значимый», предоставляя LLM набор определений тем (topic definitions).Извлечение на основе схемы или шаблона: LLM предоставляется заранее определённая JSON-схема или шаблон, использующий такие функции LLM, как структурированный вывод¹⁸. LLM получает инструкцию сформировать JSON, используя соответствующую информацию из диалога.Определение тем на естественном языке: LLM руководствуется простым описанием темы на естественном языке.Промптинг с несколькими примерами (Few-shot prompting): LLM «показывают», какую информацию извлекать, на примерах. Промпт включает несколько образцов входного текста и идеальную, качественную запись, которую следует извлечь. LLM изучает желаемый паттерн извлечения по примерам, что делает этот метод высокоэффективным для нестандартных или тонких тем, которые трудно описать с помощью схемы или простого определения.Большинство менеджеров памяти «из коробки» ищут распространённые темы, такие как предпочтения пользователя, ключевые факты или цели. Многие платформы также позволяют разработчикам определять свои собственные темы, адаптируя процесс извлечения к своей специфической области.Например, вы можете настроить, какую информацию Agent Engine Memory Bank должен считать значимой для сохранения, предоставив свои собственные определения тем и примеры¹⁹:Хотя само по себе извлечение памяти — это не «суммаризация», алгоритм может включать в себя суммаризацию для концентрации информации.Для повышения эффективности многие менеджеры памяти встраивают «скользящую сводку» (rolling summary) диалога непосредственно в промпт для извлечения памяти²⁰. Эта сжатая история предоставляет необходимый контекст для извлечения ключевой информации из самых последних взаимодействий. Это устраняет необходимость на каждом шаге (turn) заново обрабатывать полный, подробный диалог для поддержания контекста.После того как информация извлечена из источника данных, существующий корпус памяти должен быть обновлён, чтобы отразить эту новую информацию, — через процесс объединения (consolidation).Глубокое погружение: объединение памяти (Memory Consolidation)После того как «воспоминания» извлечены из подробного диалога, процесс объединения (consolidation) должен интегрировать эту новую информацию в целостную, точную и развивающуюся базу знаний. Это, пожалуй, самый сложный этап в жизненном цикле памяти, который превращает простую коллекцию фактов в тщательно курируемое понимание пользователя. Без объединения память агента быстро превратилась бы в «зашумлённый», противоречивый и ненадёжный лог каждой когда-либо зафиксированной крупицы информации.Это «само-курирование», обычно управляемое LLM, и есть то, что возвышает менеджер памяти над простой базой данных.Объединение решает фундаментальные проблемы, возникающие при работе с данными из диалогов, в том числе:Дублирование информации: Пользователь может упомянуть один и тот же факт разными способами в разных диалогах (например, «Мне нужен рейс в НЙ» и позже «Я планирую поездку в Нью-Йорк»). Простой процесс извлечения создал бы две избыточные записи.Противоречивая информация: Состояние пользователя меняется со временем. Без объединения память агента содержала бы противоречивые факты.Эволюция информации: Простой факт может стать более детальным. Изначальная запись «пользователь интересуется маркетингом» может развиться в «пользователь возглавляет маркетинговый проект, сфокусированный на привлечении клиентов в 4 квартале».Устаревание памяти (Memory Relevance Decay): Не все воспоминания остаются полезными вечно. Агент должен уметь «забывать» — проактивно удалять старые, неактуальные или маловероятные записи, чтобы поддерживать базу знаний релевантной и эффективной. «Забывание» может происходить путём инструктирования LLM отдавать предпочтение более новой информации во время объединения или через автоматическое удаление по истечении срока жизни (TTL).Процесс объединения — это управляемый LLM рабочий процесс, который сравнивает вновь извлечённые данные с существующими записями пользователя.Сначала он пытается найти существующие записи, похожие на новые. Эти записи становятся кандидатами на объединение. Если существующая запись противоречит новой информации, она может быть удалена. Если дополняется — обновлена.Затем LLM предоставляются и существующие, и новые данные. Её основная задача — проанализировать их вместе и определить, какие операции следует выполнить. Основные операции включают:UPDATE (Обновить): Изменить существующую запись, добавив новую или исправленную информацию.CREATE (Создать): Если новая информация совершенно уникальна и не связана с существующими записями, создать новую.DELETE / INVALIDATE (Удалить / Сделать неактуальной): Если новая информация делает старую запись полностью нерелевантной или неверной, удалить или пометить её как неактуальную.Наконец, менеджер памяти преобразует решение LLM в транзакцию, которая обновляет хранилище памяти.Происхождение данных в памяти (Memory Provenance)Классическая аксиома машинного обучения «мусор на входе — мусор на выходе» (garbage in, garbage out) ещё более актуальна для LLM, где результат часто описывается как «мусор на входе — уверенный бред на выходе» (garbage in, confident garbage out).Чтобы агент мог принимать надёжные решения, а менеджер памяти — эффективно объединять данные, они должны уметь критически оценивать качество своих собственных «воспоминаний». Эта надёжность напрямую зависит от происхождения (provenance) записи в памяти — детальной истории её источника и изменений.Процесс объединения (consolidation) памяти — слияние информации из нескольких источников в единую, развивающуюся запись — создаёт необходимость отслеживать её «родословную» (lineage). Как показано на диаграмме выше, одна запись в памяти может быть результатом смешения нескольких источников данных, а один источник — разделён на несколько записей.Чтобы оценить надёжность, агент должен отслеживать ключевые детали по каждому источнику, такие как его происхождение (тип источника) и возраст («свежесть»). Эти детали критически важны по двум причинам: они определяют вес, который имеет каждый источник во время объединения памяти, и они подсказывают агенту, насколько он должен полагаться на эту память во время логических выводов.Тип источника — один из самых важных факторов в определении доверия. Источники данных делятся на три основные категории:Начальные данные (Bootstrapped Data): Информация, предварительно загруженная из внутренних систем, таких как CRM. Эти данные с высоким уровнем доверия могут использоваться для инициализации памяти пользователя, чтобы решить проблему «холодного старта» — сложность предоставления персонализированного опыта пользователю, с которым агент никогда раньше не взаимодействовал.Данные от пользователя (User Input): Сюда входят данные, предоставленные явно (например, через форму, что заслуживает высокого доверия), или информация, извлечённая неявно из диалога (которая, как правило, менее надёжна).Результаты работы инструментов (Tool Output): Данные, возвращённые после вызова внешнего инструмента. Создание записей в памяти из результатов работы инструментов, как правило, не рекомендуется, потому что такие «воспоминания» имеют тенденцию быть хрупкими (brittle) и быстро устаревать (stale), что делает этот тип источника более подходящим для краткосрочного кэширования.Учёт происхождения данных при управлении памятьюТакой динамический, многоисточниковый подход к памяти создаёт две основные операционные проблемы при её управлении: разрешение конфликтов и удаление производных данных.Объединение памяти (consolidation) неизбежно приводит к ситуациям, когда один источник данных противоречит другому. Происхождение (provenance) записи позволяет менеджеру памяти установить иерархию доверия для своих источников информации. Когда записи из разных источников противоречат друг другу, агент должен использовать эту иерархию в стратегии разрешения конфликтов. Распространённые стратегии включают:Приоритет наиболее надёжного источника.Предпочтение самой свежей информации.Поиск подтверждения из нескольких источников.Ещё одна сложность возникает при удалении данных. Одна запись в памяти может быть производной от нескольких источников. Когда пользователь отзывает доступ к одному из них, данные, полученные из этого источника, также должны быть удалены. Удаление каждой записи, «затронутой» этим источником, может быть слишком агрессивным подходом. Более точный, хотя и вычислительно затратный, метод — пересоздать затронутые записи с нуля, используя только оставшиеся, валидные источники.Помимо статических данных о происхождении, уверенность (confidence) в записи должна меняться со временем. Уверенность растёт через подтверждение (corroboration), например, когда несколько надёжных источников предоставляют согласующуюся информацию.Однако эффективная система памяти должна также активно курировать свои существующие знания через «чистку» памяти (memory pruning) — процесс, который выявляет и «забывает» записи, которые больше не являются полезными. Эта «чистка» может быть вызвана несколькими факторами:Устаревание со временем (Time-based Decay): Важность записи может со временем уменьшаться. Запись о встрече двухлетней давности, скорее всего, менее релевантна, чем о встрече на прошлой неделе.Низкая уверенность (Low Confidence): Запись, созданная на основе слабого логического вывода и так и не подтверждённая другими источниками, может быть удалена.Нерелевантность (Irrelevance): По мере того как агент глубже понимает пользователя, он может определить, что некоторые старые, тривиальные записи больше не соответствуют текущим целям пользователя.Сочетая реактивный конвейер объединения с проактивной «чисткой», менеджер памяти гарантирует, что база знаний агента — это не просто растущий лог всего, что когда-либо было сказано. Вместо этого, это тщательно курируемое, точное и релевантное понимание пользователя.Учёт происхождения данных во время логических выводов (inference)Помимо учёта происхождения (lineage) при курировании содержимого корпуса, надёжность записи также следует учитывать во время логических выводов (inference time). Уверенность агента в записи не должна быть статичной; она должна меняться на основе новой информации и течения времени.Уверенность растёт через подтверждение (corroboration), например, когда несколько надёжных источников предоставляют согласующуюся информацию. И наоборот, уверенность уменьшается (или затухает) со временем, когда старые записи устаревают, а также падает, когда появляется противоречивая информация. В конечном итоге система может «забыть» данные, архивируя или удаляя записи с низкой степенью уверенности.Этот динамический показатель уверенности (confidence score) критически важен во время логических выводов. Записи из памяти и, если доступны, их показатели уверенности, внедряются в промпт, позволяя LLM оценивать надёжность информации и принимать более тонкие решения.Вся эта система оценки доверия служит для внутреннего процесса рассуждений агента. Сами записи и их показатели уверенности обычно не показываются пользователю напрямую. Вместо этого они внедряются в системный промпт, позволяя LLM взвешивать доказательства, учитывать надёжность своей информации и, в конечном счёте, принимать более взвешенные и заслуживающие доверия решения.Запуск генерации памятиХотя менеджеры памяти автоматизируют извлечение и объединение данных после запуска генерации, агент всё равно должен решать, когда следует пытаться сгенерировать запись. Это критически важный архитектурный выбор, балансирующий между свежестью данных, вычислительными затратами и задержками (latency).Это решение обычно управляется логикой агента, которая может использовать несколько стратегий запуска. Генерация памяти может быть инициирована на основе различных событий:Завершение сессии: Запуск генерации в конце многоходовой сессии.Периодичность по шагам (Turn Cadence): Запуск процесса после определённого количества шагов (например, каждые 5 шагов).В реальном времени (Real-Time): Генерация после каждого шага диалога.Явная команда (Explicit Command): Активация процесса по прямой команде пользователя (например, «Запомни это»).Выбор триггера — это прямой компромисс между стоимостью и точностью (fidelity). Частая генерация (например, в реальном времени) гарантирует, что записи будут очень подробными и свежими, фиксируя каждый нюанс. Однако это влечёт за собой самые высокие затраты на LLM и базу данных и может вызывать задержки, если не обрабатывается должным образом. Нечастая генерация (например, по завершении сессии) гораздо более экономична, но рискует создавать менее точные записи, поскольку LLM приходится суммировать гораздо больший блок диалога за один раз. Также следует убедиться, что менеджер памяти не обрабатывает одни и те же события несколько раз, так как это приводит к ненужным затратам.Память как инструмент (Memory-as-a-Tool)Более продвинутый подход — позволить агенту самому решать, когда создавать запись в памяти.В этом паттерне генерация памяти представляется в виде инструмента (например, create_memory). В описании этого инструмента должно быть указано, какие типы информации следует считать значимыми. Агент затем может анализировать диалог и автономно принимать решение вызвать этот инструмент, когда он обнаружит информацию, которую стоит сохранить.Это перекладывает ответственность за определение «значимой информации» с внешнего менеджера памяти на самого агента (и, следовательно, на вас как на разработчика).Например, в ADK вы можете сделать это, упаковав ваш код для генерации памяти в Инструмент (Tool)²¹, который агент будет вызывать, когда сочтёт, что диалог содержит что-то важное для сохранения. Вы можете отправить Session в Memory Bank, и Memory Bank извлечёт и объединит данные из истории диалога:Другой подход — использовать внутреннюю память, где агент активно решает, что запомнить из диалога. В этом рабочем процессе агент сам отвечает за извлечение ключевой информации.Опционально, эти извлечённые «воспоминания» затем могут быть отправлены в Agent Engine Memory Bank для их объединения (consolidation) с существующими данными пользователя²²:Фоновые и блокирующие операцииГенерация памяти — это ресурсоёмкая операция, требующая LLM-вызовов и записей в базу данных. Для агентов в продакшене генерация памяти почти всегда должна выполняться асинхронно, как фоновый процесс²³.После того как агент отправил ответ пользователю, конвейер генерации памяти может работать параллельно, не блокируя пользовательский опыт. Такое разделение (decoupling) необходимо, чтобы агент оставался быстрым и отзывчивым. Блокирующий (синхронный) подход, при котором пользователю пришлось бы ждать, пока запись в память будет завершена, прежде чем получить ответ, создал бы недопустимо медленный и разочаровывающий пользовательский опыт. Это требует, чтобы генерация памяти происходила в сервисе, архитектурно отделённом от основной среды выполнения (runtime) агента.Извлечение памяти (Memory Retrieval)Когда механизм генерации памяти настроен, ваше внимание может переключиться на критически важную задачу — извлечение (retrieval). Интеллектуальная стратегия извлечения необходима для производительности агента и включает в себя решения о том, какие «воспоминания» следует извлекать и когда это делать.Стратегия извлечения сильно зависит от того, как организована память.Для структурированного профиля пользователя извлечение — это, как правило, простой поиск всего профиля или конкретного атрибута.Для коллекции «воспоминаний», однако, извлечение — это гораздо более сложная задача поиска. Цель состоит в том, чтобы найти наиболее релевантную, концептуально связанную информацию в большом пуле неструктурированных или полуструктурированных данных.Стратегии, обсуждаемые в этом разделе, предназначены для решения этой сложной задачи извлечения из коллекций памяти.Извлечение памяти (memory retrieval) — это поиск наиболее релевантных «воспоминаний» для текущего диалога. Эффективная стратегия извлечения критически важна: предоставление нерелевантной информации может запутать модель и ухудшить её ответ, в то время как идеально подобранный фрагмент контекста может привести к поразительно интеллектуальному взаимодействию. Основная задача — сбалансировать «полезность» памяти в рамках строгого бюджета по задержкам (latency).Продвинутые системы памяти выходят за рамки простого поиска и оценивают потенциальные «воспоминания» по нескольким параметрам, чтобы найти наилучшее совпадение.Релевантность (семантическое сходство): Насколько концептуально эта запись связана с текущим диалогом?Свежесть (временной фактор): Как давно была создана эта запись?Важность (значимость): Насколько критична эта запись в целом? В отличие от релевантности, «важность» может быть определена ещё на этапе генерации.Распространённая ошибка — полагаться исключительно на релевантность, определённую на основе векторов. Оценки сходства могут «поднять наверх» записи, которые концептуально похожи, но стары или тривиальны. Наиболее эффективная стратегия — это смешанный подход, который комбинирует оценки по всем трём параметрам.Для приложений, где точность имеет первостепенное значение, извлечение можно улучшить с помощью таких подходов, как переформулирование запроса (query rewriting), переранжирование (reranking) или специализированные ретриверы. Однако эти техники вычислительно затратны и добавляют значительные задержки, что делает их неподходящими для большинства приложений, работающих в реальном времени. В сценариях, где эти сложные алгоритмы необходимы, а данные устаревают не слишком быстро, эффективным решением может стать слой кэширования. Кэширование позволяет временно сохранять дорогостоящие результаты запроса, избегая высоких задержек при последующих идентичных запросах.При переформулировании запроса LLM может использоваться для улучшения самого поискового запроса. Это может включать преобразование неоднозначного ввода пользователя в более точный запрос или расширение одного запроса до нескольких связанных, чтобы охватить разные аспекты темы. Хотя это значительно улучшает качество первоначальных результатов поиска, это добавляет задержку ещё одного LLM-вызова в начале процесса.При переранжировании первоначальный поиск извлекает широкий набор кандидатов (например, топ-50 результатов) с помощью поиска по сходству. Затем LLM может повторно оценить и переранжировать этот меньший набор, чтобы составить более точный итоговый список²⁴.Наконец, можно обучить специализированный ретривер с помощью дообучения (fine-tuning). Однако это требует доступа к размеченным данным и может значительно увеличить затраты.В конечном счёте, лучший подход к извлечению начинается с качественного формирования самой памяти. Обеспечение того, чтобы корпус памяти был высококачественным и свободным от нерелевантной информации, — это самый эффективный способ гарантировать, что любой набор извлечённых «воспоминаний» будет полезен.Выбор времени для извлечения (Timing for retrieval)Заключительный архитектурный вопрос при работе с извлечением — это когда именно извлекать данные из памяти.Один из подходов — проактивное извлечение (proactive retrieval), при котором данные из памяти автоматически загружаются в начале каждого шага (turn) диалога. Это гарантирует, что контекст всегда доступен, но приводит к ненужным задержкам (latency) на тех шагах, которые не требуют доступа к памяти. Поскольку данные в памяти остаются статичными в течение одного шага, их можно эффективно кэшировать, чтобы компенсировать эти потери в производительности.Например, в ADK вы можете реализовать проактивное извлечение, используя встроенный PreloadMemoryTool или кастомный коллбэк (custom callback)²⁵:Альтернативный подход — реактивное извлечение (reactive retrieval), или «Память как инструмент». В этом случае агенту предоставляется инструмент для запроса к своей памяти, и он сам решает, когда ему нужно извлечь контекст.Этот метод более эффективен и надёжен, но требует дополнительного LLM-вызова, что увеличивает задержки (latency) и стоимость. Однако память извлекается только тогда, когда это действительно необходимо, поэтому дополнительные затраты на задержку возникают реже.Кроме того, агент может не знать, существует ли релевантная информация, которую можно было бы извлечь. Эту проблему можно смягчить, проинформировав агента о типах доступных «воспоминаний» (например, в описании инструмента, если вы используете кастомный инструмент), что позволит ему принимать более обоснованное решение о том, когда делать запрос.Использование памяти при формировании ответа (Inference with Memories)После того как релевантные данные извлечены из памяти, заключительный этап — их грамотное размещение в контекстном окне модели. Это критически важный процесс, поскольку то, где именно находятся эти данные, может значительно повлиять на логику рассуждений LLM, операционные затраты и, в конечном итоге, на качество финального ответа.Данные из памяти в основном представляются двумя способами: добавлением в системные инструкции или внедрением в историю диалога.На практике гибридная стратегия часто оказывается наиболее эффективной.Используйте системный промпт для стабильных, глобальных «воспоминаний» (таких как профиль пользователя), которые должны присутствовать всегда.В остальных случаях используйте внедрение в диалог или подход «Память как инструмент» для временных, эпизодических данных, которые релевантны только для текущего контекста диалога.Это позволяет сбалансировать потребность в постоянном (persistent) контексте с гибкостью извлечения информации по мере необходимости.Данные из памяти в системных инструкцияхПростой способ использовать память при формировании ответа — добавить (append) извлечённые данные в системные инструкции.Этот метод сохраняет историю диалога чистой, поскольку извлечённые «воспоминания» добавляются напрямую в системный промпт вместе с преамбулой, представляя их как фундаментальный контекст для всего взаимодействия.Например, вы можете использовать Jinja для динамического добавления данных из памяти в ваши системные инструкции:Включение данных в системные инструкции наделяет их высоким приоритетом, чётко отделяет контекст от диалога и идеально подходит для стабильной, «глобальной» информации, такой как профиль пользователя. Однако существует риск чрезмерного влияния, когда агент может пытаться привязать любую тему к информации из своих базовых инструкций, даже если это неуместно.Этот архитектурный паттерн накладывает несколько ограничений.Во-первых, он требует, чтобы фреймворк агента поддерживал динамическое формирование системного промпта перед каждым вызовом LLM, а такая функциональность доступна не всегда.Во-вторых, паттерн несовместим с подходом «Память как инструмент», поскольку системный промпт должен быть сформирован до того, как LLM сможет решить вызвать инструмент для извлечения данных.Наконец, он плохо справляется с нетекстовыми данными: большинство LLM принимают в качестве системных инструкций только текст, что усложняет прямое встраивание мультимодального контента, такого как изображения или аудио.Данные из памяти в истории диалогаПри этом подходе извлечённые данные внедряются (inject) непосредственно в пошаговый диалог. Их можно поместить либо перед всей историей диалога, либо непосредственно перед последним запросом пользователя.Однако этот метод может создавать информационный шум, увеличивая затраты на токены и потенциально путая модель, если извлечённые данные нерелевантны. Основной риск — это «внедрение в диалог» (dialogue injection), когда модель может ошибочно принять данные из памяти за реплику, которая действительно была произнесена в диалоге. Также нужно быть осторожнее с тем, от чьего лица сформулированы внедряемые данные. Например, если вы используете роль «user» и данные уровня пользователя, они должны быть написаны от первого лица.Частным случаем внедрения в историю диалога является извлечение данных через вызовы инструментов. В этом случае «воспоминания» будут включены непосредственно в диалог как часть ответа от инструмента.Процедурная памятьЭтот материал был в основном сосредоточен на декларативной памяти, что отражает текущее состояние рынка решений для управления памятью. Большинство платформ также спроектированы для этого декларативного подхода, преуспевая в извлечении, хранении и получении ответа на вопрос «что?» — фактов, истории и данных о пользователе.Однако эти системы не предназначены для управления процедурной памятью — механизмом для улучшения рабочих процессов и логики рассуждений агента. Хранение ответа на вопрос «как?» — это не задача по извлечению информации, а задача по расширению возможностей рассуждения (reasoning augmentation).Управление этим «знанием как» требует совершенно отдельного и специализированного алгоритмического жизненного цикла, хотя и с похожей высокоуровневой структурой²⁶:Извлечение: Извлечение процедурных данных требует специализированных промптов, предназначенных для того, чтобы «выкристаллизовать» переиспользуемую стратегию или «план действий» из успешного взаимодействия, а не просто зафиксировать факт или значимую информацию.Объединение (Consolidation): В то время как декларативное объединение сливает связанные факты («что»), процедурное объединение курирует сам рабочий процесс («как»). Это активный процесс управления логикой, сфокусированный на интеграции новых успешных методов с существующими «лучшими практиками», исправлении ошибочных шагов в известном плане и удалении устаревших или неэффективных процедур.Извлечение (Retrieval): Цель состоит не в том, чтобы получить данные для ответа на вопрос, а в том, чтобы получить план, который будет направлять агента при выполнении сложной задачи. Поэтому процедурные данные могут иметь иную схему, нежели декларативные.Эта способность агента к «самоэволюции» своей логики, естественно, напрашивается на сравнение с распространённым методом адаптации: дообучением (fine-tuning) — часто с помощью Reinforcement Learning from Human Feedback (RLHF)²⁷.Хотя оба процесса направлены на улучшение поведения агента, их механизмы и применение коренным образом различаются.Дообучение — это относительно медленный, оффлайн-процесс тренировки, который изменяет веса модели.Процедурная память обеспечивает быструю, онлайн-адаптацию, динамически внедряя правильный «план действий» в промпт и направляя агента через обучение в контексте (in-context learning) без необходимости какого-либо дообучения.Тестирование и оценкаТеперь, когда у вас есть агент, оснащённый памятью, вам следует проверить его поведение с помощью всестороннего тестирования и оценки качества.Оценка памяти агента — это многоуровневый процесс. Необходимо убедиться, что агент запоминает правильные вещи (качество), что он может найти эти «воспоминания», когда они нужны (извлечение), и что их использование действительно помогает ему достигать своих целей (успех задачи). В то время как академическая среда фокусируется на воспроизводимых бенчмарках, на практике оценка сосредоточена на том, как память напрямую влияет на производительность и удобство использования агента в рабочей среде.Метрики качества генерации памятиЭти метрики оценивают содержимое самих «воспоминаний», отвечая на вопрос: «Запоминает ли агент то, что нужно?». Обычно это измеряется путём сравнения сгенерированных агентом данных с вручную созданным «эталонным набором» (golden set) идеальных «воспоминаний».Точность (Precision): Какой процент созданных агентом записей является точным и релевантным? Высокая точность защищает от «слишком усердной» системы, которая засоряет базу знаний нерелевантным шумом.Полнота (Recall): Какой процент всех релевантных фактов, которые агент должен был запомнить, он действительно зафиксировал? Высокая полнота гарантирует, что агент не упускает критически важную информацию.F1-Score: Гармоническое среднее между точностью и полнотой, предоставляющее единую, сбалансированную метрику качества.Метрики производительности извлечения памятиЭти метрики оценивают способность агента находить нужную запись в нужное время.Recall@K: Когда требуется «воспоминание», находится ли правильное среди K верхних результатов поиска? Это основная метрика точности системы извлечения.Задержка (Latency): Извлечение находится на «критическом пути» (hot path) ответа агента. Весь процесс должен выполняться в рамках строгих временных рамок (например, менее 200 мс), чтобы не ухудшать пользовательский опыт.Итоговые метрики успеха задачи (End-to-End)Это главная проверка, отвечающая на вопрос: «Действительно ли память помогает агенту лучше выполнять свою работу?». Это измеряется путём оценки производительности агента на последующих задачах (downstream tasks) с использованием его памяти. Часто для этого используется LLM в роли «судьи», который сравнивает итоговый результат агента с эталонным ответом.Оценка — это не разовое событие, а механизм для постоянного совершенствования. Приведённые выше метрики предоставляют данные, необходимые для выявления слабых мест и систематического улучшения системы памяти с течением времени.Хотя метрики выше сосредоточены на качестве, готовность к продакшену (production-readiness) также зависит от производительности. Для каждой области оценки критически важно измерять задержку базовых алгоритмов и их способность масштабироваться под нагрузкой. Извлечение на «критическом пути» может иметь строгие рамки по времени отклика, не превышающие долей секунды. Генерация и объединение, хотя часто и асинхронны, должны иметь достаточную пропускную способность, чтобы справляться с пользовательской нагрузкой. В конечном итоге, успешная система памяти должна быть интеллектуальной, эффективной и надёжной для использования в реальных условиях.Требования к Памяти в рабочей среде (Production)Помимо производительности, перевод агента с памятью от прототипа к продакшену требует особого внимания к архитектурным аспектам корпоративного уровня. Этот переход выдвигает критические требования к масштабируемости, отказоустойчивости и безопасности. Система промышленного уровня должна быть спроектирована не только для интеллектуальной работы, но и для обеспечения надёжности корпоративного класса.Чтобы пользовательский опыт никогда не блокировался из-за вычислительно затратного процесса генерации памяти, надёжная архитектура должна отделять (decouple) обработку памяти от основной логики приложения. Хотя это событийно-ориентированный an event-driven паттерн, он обычно реализуется через прямые, неблокирующие API-вызовы к выделенному сервису памяти, а не через самостоятельно управляемую очередь сообщений. Процесс выглядит так:Агент отправляет данные: После релевантного события (например, завершения сессии) приложение агента делает неблокирующий API-вызов к менеджеру памяти, «отправляя» ему для обработки «сырые» исходные данные (например, стенограмму диалога).Менеджер памяти обрабатывает данные в фоновом режиме: Сервис менеджера памяти немедленно подтверждает получение запроса и помещает задачу на генерацию в свою собственную, внутреннюю управляемую очередь. После этого он берёт на себя всю ресурсоёмкую асинхронную работу: делает необходимые LLM-вызовы для извлечения, объединения и форматирования данных. Менеджер может отложить обработку до тех пор, пока не пройдёт определённый период неактивности.Данные сохраняются: Сервис записывает итоговые «воспоминания» — будь то новые записи или обновления существующих — в выделенную, надёжную базу данных. В управляемых менеджерах памяти хранилище является встроенным.Агент извлекает данные: Основное приложение агента может затем напрямую запрашивать это хранилище, когда ему нужен контекст для нового взаимодействия с пользователем.Такой сервисный, неблокирующий подход гарантирует, что сбои или задержки в конвейере обработки памяти не влияют напрямую на приложение, с которым работает пользователь, делая систему гораздо более отказоустойчивой (resilient). Это также определяет выбор между онлайн-генерацией (в реальном времени), идеальной для поддержания актуальности диалога, и оффлайн-обработкой (пакетной), которая полезна для наполнения системы историческими данными.По мере роста приложения система памяти должна справляться с высокочастотными событиями без сбоев. При одновременных запросах система должна предотвращать взаимоблокировки (deadlocks) или состояния гонки (race conditions), когда несколько событий пытаются изменить одну и ту же запись. Снизить риск состояний гонки можно, используя транзакционные операции с базой данных или оптимистичные блокировки (optimistic locking).Сервис памяти также должен быть устойчив к временным ошибкам. Если LLM-вызов завершается неудачно, система должна использовать механизм повторных попыток с экспоненциальной задержкой (exponential backoff) и направлять постоянные ошибки в очередь «мёртвых» сообщений (dead-letter queue) для анализа.Для глобальных приложений менеджер памяти должен использовать базу данных со встроенной межрегиональной репликацией для обеспечения низких задержек и высокой доступности. Репликация на стороне клиента нецелесообразна, поскольку для объединения данных требуется единое, транзакционно-целостное представление данных, чтобы предотвратить конфликты.Управляемые системы памяти, такие как Agent Engine Memory Bank, должны помочь вам решить эти задачи, чтобы вы могли сосредоточиться на основной логике агента.Риски для конфиденциальности и безопасностиДанные в памяти создаются на основе пользовательских данных и включают их, поэтому они требуют строжайших мер контроля конфиденциальности и безопасности. Полезная аналогия — представить память системы как защищённый корпоративный архив, которым управляет профессиональный архивариус, чья работа — сохранять ценные знания, защищая при этом компанию.Кардинальное правило для этого архива — изоляция данных. Точно так же, как архивариус никогда не смешает конфиденциальные файлы из разных отделов, память должна быть строго изолирована на уровне пользователя или клиента (tenant). Агент, обслуживающий одного пользователя, никогда не должен иметь доступа к «воспоминаниям» другого. Это обеспечивается с помощью ограничительных списков контроля доступа (ACL). Кроме того, пользователи должны иметь программный контроль над своими данными, с чёткими опциями для отказа от генерации памяти или запроса на удаление всех их «файлов» из архива.Перед тем как подшить любой документ, архивариус выполняет критически важные шаги.Во-первых, он тщательно просматривает каждую страницу, чтобы удалить (redact) конфиденциальную личную информацию (PII), гарантируя, что знания сохраняются, не создавая при этом юридических рисков (liability).Во-вторых, архивариус обучен распознавать и отбрасывать подделки или намеренно вводящие в заблуждение документы — это защита от отравления памяти (memory poisoning)²⁸.Точно так же система должна проверять и очищать (sanitize) информацию, прежде чем сохранять её в долговременную память, чтобы не позволить злоумышленнику повредить постоянные знания агента через prompt injection. Система должна включать такие защитные механизмы, как Model Armor, для валидации и очистки информации²⁹.Кроме того, существует риск утечки (exfiltration), если несколько пользователей совместно используют один и тот же набор «воспоминаний», как в случае с процедурной памятью (которая учит агента, как что-то делать). Например, если процедурная память от одного пользователя используется в качестве примера для другого — подобно распространению служебной записки по всей компании — архивариус должен сначала выполнить строгую анонимизацию, чтобы предотвратить утечку конфиденциальной информации между пользователями.ЗаключениеВ этом материале мы рассмотрели дисциплину Context Engineering (инженерии контекста), сосредоточившись на двух её центральных компонентах: Сессиях и Памяти. Путь от простого шага в диалоге до фрагмента постоянного, действенного знания (actionable intelligence) управляется именно этой практикой, которая заключается в динамической сборке всей необходимой информации — включая историю диалога, «воспоминания» и внешние знания — в контекстном окне LLM. Весь этот процесс опирается на взаимодействие двух разных, но взаимосвязанных систем: краткосрочной Сессии и долговременной Памяти.Сессия управляет «настоящим моментом», выступая в роли хронологического контейнера с низкой задержкой (low-latency) для одного диалога. Её основные вызовы — это производительность и безопасность, требующие быстрого доступа и строгой изоляции. Чтобы предотвратить переполнение контекстного окна и задержки, необходимо использовать такие методы извлечения, как усечение по токенам или рекурсивная суммаризация, для сжатия содержимого.Память — это двигатель долгосрочной персонализации и основной механизм для сохранения данных между несколькими сессиями. Она идёт дальше, чем RAG (который делает агента экспертом по фактам), и делает агента экспертом по пользователю. Память — это активный, управляемый LLM ETL-конвейер, отвечающий за извлечение, объединение и получение данных, который «дистиллирует» самую важную информацию из истории диалога. Чтобы поддерживать отзывчивость пользовательского интерфейса, генерация памяти должна работать как асинхронный фоновый процесс, запускаемый после того, как агент ответил.Отслеживая происхождение (provenance) данных и применяя защитные меры от таких рисков, как отравление памяти, разработчики могут создавать надёжных, адаптивных помощников, которые действительно учатся и развиваются вместе с пользователем.Теги:googlechatgptагентыагенты ииrag aidatabaseХабы:DevOpsМашинное обучениеPythonSQL",568,0,0,58 мин,https://habr.com/ru/articles/965662/,92844,11350,4
"Небольшой экскурс в историю, или почему хз самый частый ответ в мире ИИ",vvvphoenix,2025-11-13T11:01:49.000Z,"['Машинное обучение *', 'Искусственный интеллект', 'Читальный зал']","vvvphoenix 10 минут назадНебольшой экскурс в историю, или почему хз самый частый ответ в мире ИИУровень сложностиПростойВремя на прочтение5 минКоличество просмотров87Машинное обучение * Искусственный интеллектЧитальный залИз этой главы любознательный читатель извлечет несколько фактов по истории вопроса, поймет за что нынче дают Нобелевские премии по физике и узнает почему на почти любой вопрос в мире ИИ - ответ ""да хрен его знает""...О мыслящих машинах человечество мечтало с давних времен. Известный факт 18го века - шахматный автомат ""Турок"" (который оказался человеко-машинной системой). Но сама идея,очевидно, зародилась гораздо раньше. Затем тема искусственного интеллекта занимала центральное место в произведениях Айзека Азимова, Фрэнка Герберта (""Дюна"" и продолжения), Джеймса Кэмерона (""Терминатор"") и других титанов популярной культуры. Но обзор научной фантастики, посвященной ИИ не входит в мои сегодняшние планы (возможно, как- нибудь потом). Я сосредоточусь на том пути, который привел ИИ в его нынешнее состояние. И отмечу три важных вехи.Модель перцептрона Розенблатта.В 50х годах прошлого века американский ученый Фрэнк Розенблаттпредложил первую модель ""искусственного разума"". А поскольку Розенблатт был нейрофизиологом, то за основу он взял устройство человеческого мозга. Последний, как известно, состоит из клеток, называемых нейронами. Нейроны способны принимать, а также, находясь в возбужденном состоянии, передавать электрические импульсы по своим отросткам. Собственно, таким образом осуществляется то, что мы называем высшей нервной деятельностью. Именно это Розенблатт простейшим образом формализовал и назвал свою модель перцептроном.  Довольно быстро выяснилось, что уже простейшие перцептроны способны осуществлять некоторые полезные функции - например, различать простые геометрические фигуры. Как они работают, я расскажу в одной из следующих глав. Вдохновленный этим фактом Розенблатт даже построил громоздкий полумеханический компьютер ""Марк-1"". Однако, потом дело надолго застопорилось, по ряду причин, одной из которых, стала ранняя смерть ученого. Наряду с этим стали очевидны серьезные проблемы с обучением такого рода сеток. По большому счету никто тогда толком не знал, как их обучать, да и вычислительной мощности катастрофически не хватало... Люди тогда упражнялись с ""игрушечными"" (по нынешним временам) модельками в несколько нейронов. Хорошая новость, однако, в том, что на таких модельках они пытались понять математику процесса. Чего в позднейшей истории, не случалось почти никогда... Но и на этом пути разочарований было больше, чем побед.Так или иначе, теория эта пришла в упадок, практически на 50 лет. И, как мы увидим, этот паттерн ""забили/забросили/забыли"" довольно частый в истории искусственного интеллекта. Если идея ""не взлетает"" сразу, это отнюдь не значит, что она плохая. Возможно, время ее просто не пришло...Глубокое обучениеОднако, спустя примерно полвека (в начале 2000х) в теме произошел новый прорыв. Группа, под руководством профессора университета Торонто Джеффри ХинтонаДжефф Хинтонразработала метод обучения многослойных нейронных сетей методом ""обратного распространения ошибки"".  Ту абракадабру, которая приведена на этой картинке, я попробую объяснить простыми словами в одной из следующих глав. А пока же отмечу, что прорыв произошел именно в начале 2000х в силу двух обстоятельств. К этому моменту уже был разработан математический аппарат для многослойных сетей нейронов, получивший название ""глубокого обучения"". Да и вычислительная мощность в те времена уже позволяла работать с сетками совсем не ""игрушечных"" размеров.И вот с тех пор индустрия искусственного интеллекта начала свой экспоненциальный рост, который не прекращается по сей день. С помощью глубокого обучения были решены многие задачи компьютерного зрения, распознавания речи, анализа данных. Так что открытие Хинтона оказало огромное влияние на развитие науки и техники в различных областях. И поэтому я считаю, что Нобелевская премия по физике, которую он получил в 2024м году абсолютно заслужена, хотя она и вызвала много споров. Сложнее ответить на вопрос почему именно по физике. Ну да, в глубоком обучении есть некоторое количество аналогий с термодинамикой. И я даже буду периодически ими пользоваться. Но истина скорее в том, что Нобелевской премии в области компьютерных наук просто нет. Потому что во времена Нобеля компьютеров еще не было. Может стоит уже ввести таковую? :)""Революция искусственного интеллекта"", начавшаяся с открытия Хинтона, также оказала серьезное влияние на мир бизнеса. Приведу лишь один пример. В начале 2000х безоговорочным лидером на рынке вычислений была компания Intel. На которую автор этих строк проработал без малого четверть века. Но Intel не сумел вовремя распознать потенциал глубокого обучения, о чем я писал в своей книге. Лидером ""революции искусственного интеллекта"" и главным ее бенефициаром(пока?) стала компания NVidia. И это позволило ей вырваться в лидеры в области микроэлектроники, а Intel остался на задворках. Сейчас по капитализации Nvidia превосходит Интел более чем в 30 раз. А еще 20 лет назад все было ровно наоборот. Вот так одно научное открытие может все перевернуть в мире бизнеса. И счастлив тот, кто оказывается на ""правильной стороне истории""...Большие языковые моделиЭтот новый прорыв в мире искусственного интеллекта произошел совсем недавно. Первые большие языковые модели (LLM - Large Language Models) появились в широком доступе всего то 5-7 лет назад. Наиболее известной является GPT (Generative Pretrained Transformer) от OpenAI. Или, говоря простым языком Сэм Альтман, Грэг Брокман, Илья Суцкевер (он, кстати ученик Хинтона) и примкнувший к ним Андрей Карпатый сумели научить железку ""говорить на человеческом языке"". Технология эта быстро завоевала популярность и сейчас LLMки появляются буквально как грибы после дождя (хотя это все еще стоит их создателям немалых денег). Как работают LLM и главное, как они меняют нашу жизнь мы также подробно разберем в будущем. А пока спрошу - как вы думаете, до OpenAI никто не пытался обучить компьютер говорить ""по -человечьи""? Конечно пытались, уж скоро 100 лет будет как. Просто еще 10 лет назад большие языковые модели были невозможны все по той же причине нехватки вычислительной мощности. Да и архитектуру предобученного трансформера придумали лишь в 2017м. Как мы можем видеть, из этого коротенького обзора развитие ИИ идет совсем даже не прямыми путями. Скорее это можно назвать ""методом проб и ошибок"". Мы пробуем натренировать нейронку на решение какой то задачи и у нас не получается. В ответ на вопрос ""почему?"" иногда приходится слышать - ""данные неочищенные"" или ""градиенты разваливаются"" (что значат эти заклинания мы тоже будем разбирать). Но самый частый ответ - ""да хрен его знает"". И, как всегда - забили/бросили/забыли... А потом мы возвращаемся к этой задачей с новыми данными и вычислительными возможностями и у нас все ""взлетает"". Но опять же- Почему? - Да хрен его знает...Второй вывод который можно сделать - осмысление искусственного интеллекта на математическом, этическом, законодательном и чисто практическом уровне далеко отстает от развития самой технологии.Вот такими вопросами мы в меру данных нам Богом сил и будем заниматься в этом цикле.Оставайтесь со мной.Больше -  в моем канале об ИИ на Дзен.Теги:искуственный интеллектдля новичковХабы:Машинное обучениеИскусственный интеллектЧитальный зал",87,0,0,5 мин,https://habr.com/ru/articles/966068/,7394,1031,3
Подборка фанатских 3D-сцен по популярным играм с ArtStation,ggsel,2025-11-12T12:54:20.000Z,"['Блог компании ggsel.net', 'Игры и игровые консоли', 'Разработка игр *']","ggsel 22 часа назадПодборка фанатских 3D-сцен по популярным играм с ArtStationВремя на прочтение4 минКоличество просмотров660Блог компании ggsel.netИгры и игровые консолиРазработка игр * ДайджестИгры живут не только на экране — они продолжают существовать в памяти игроков и в творчестве художников. Фанатские 3D‑сцены становятся своеобразными «вторыми жизнями» виртуальных миров: кто‑то бережно воссоздает знакомые локации, кто‑то добавляет собственные детали, а кто‑то превращает их в полноценные исследования окружения.Для разработчиков такие проекты становятся индикатором вовлеченности аудитории, для художников — источником идей и ориентиром по качеству, а для игроков — возможностью снова погрузиться в любимые места.«Средневековая деревня» из «Ведьмака 3», Владислав КолдаевЭта сцена вдохновлена миром «Ведьмака» и передает атмосферу тихой деревни, где жизнь течет размеренно и без спешки. Автор сосредоточился на простых, но выразительных деталях: деревянные дома с покатыми крышами, каменные подпорки, хозяйственные постройки и тропинки, уходящие вглубь.Все это создает ощущение места, где можно услышать скрип телеги или лай собак за оградой.Ссылка на Artstation: https://www.artstation.com/artwork/1xE6goОкружение из «Ведьмака 3», Абрар АхмедЭта сцена показывает лесную дорогу, уходящую вглубь зеленого массива. Узкая тропа обрамлена густой растительностью, а легкий туман добавляет глубины и создает ощущение прохлады. Свет мягкий, утренний, он подчеркивает фактуру влажной земли и древесины, делая окружение живым и правдоподобным.Атмосфера здесь спокойная и немного загадочная: зритель словно готов отправиться в путь, где за каждым поворотом может скрываться новая история.Ссылка на Artstation: https://www.artstation.com/artwork/QK3bOrОкружение из Elden Ring, Томас СюкеЗрителя встречают монументальные каменные постройки — башни, стены и арки. Каменные башни и стены показаны так, будто они пережили века: грубая кладка, следы разрушений и мрачная фактура создают ощущение древности.Автор использует контраст света и тени: яркие акценты выделяют ключевые элементы композиции, а остальное пространство погружается в полумрак, создавая атмосферу угрозы и величия.Ссылка на Artstation: https://www.artstation.com/artwork/WX83VJЛокация Лазурный простор из World of Warcraft, Дилан БортонЭта работа посвящена одной из ключевых локаций World of Warcraft: Dragonflight — Azure Span. Эта работа ценна как пример левел-дизайна — как художественные решения соединяются с игровой логикой, создавая цельный опыт для игрока.Сцена делится на несколько зон: вход в лагерь, боевые площадки, рыбацкие и художественные лагеря, башни и тропы. Каждая зона имеет свое настроение и функцию — от спокойного отдыха до напряженных боевых столкновений.Ссылка на Artstation: https://www.artstation.com/artwork/rJYdoeУбежище из Fallout, Стивен ДаггЭта работа переносит нас в атмосферу постапокалиптического мира Fallout. Стивен Дагг воссоздает интерьер убежища — пространство, где сочетаются строгая утилитарность и следы человеческой жизни. Металлические стены, массивные двери и технические панели создают ощущение защищенности, но при этом холодности.Автор уделяет внимание деталям: старые плакаты, предметы быта, светильники и оборудование делают сцену живой и правдоподобной. Освещение играет ключевую роль — оно жесткое и направленное, подчеркивает текстуры металла и создает контраст между зонами.Ссылка на Artstation: https://www.artstation.com/artwork/5WXYN8Бар «Посмертие» из Cyberpunk 2077, Лоренцо СкарафияБар «Посмертие» из Cyberpunk 2077 показан как культовое место встреч наемников в Найт‑Сити. Лоренцо Скарафия акцентирует внимание на визуальном шуме: экраны, лампы, рекламные панели и хаотичные надписи создают ощущение перегруженного, но живого пространства.Металл и бетон здесь оживлены светом — холодные поверхности отражают разноцветные огни, превращаясь в фон для бесконечных сделок и разговоров. Композиция строится вокруг барной стойки и зон отдыха, где каждый элемент — от экранов до лампочек — работает на атмосферу города и его рискованную жизнь.Ссылка на Artstation: https://www.artstation.com/artwork/rJ6PJ5Киберпанковская улица, Хаолянг ЯнгВ этой сцене внимание сосредоточено на одной из улиц Найт-Сити — тесной, насыщенной деталями, где каждый элемент работает на общее ощущение плотной городской среды. Нависающие конструкции и узкие проходы формируют замкнутую перспективу, а световые акценты задают направление взгляда. В центре композиции — девушка на мотоцикле. Она становится частью улицы, отражая ее энергию и визуальный шум, и именно через нее сцена ощущается живой и напряженной.cdn.artstation.comСсылка на Artstation: https://www.artstation.com/artwork/Vyzn58Центр города из Stray, КукиЭта работа воссоздает район Midtown из игры Stray. Атмосфера сцены — тихая и немного меланхоличная: людей нет, но пространство выглядит обжитым, наполненным признаками жизни роботов, которые стали новыми жителями города.Композиция построена камерно: внимание сосредоточено на деталях — табличках, витринах, кабелях и влажном асфальте, отражающем свет. Все это создает ощущение замкнутого, но живого места, где кот‑герой Stray мог бы исследовать каждый уголок.Ссылка на Artstation: https://www.artstation.com/artwork/qe11meКомната Гарри Дюбуа из Disco Elysium, Джонджо ХемменсФанатская реконструкция комнаты Гарри Дюбуа из Disco Elysium. Автор переносит зрителя в знакомое пространство: тесная, слегка хаотичная комната, где каждый предмет отражает состояние героя. Старая мебель, разбросанные вещи, приглушенный свет и следы беспорядка создают атмосферу упадка и усталости.Композиция строится вокруг фигуры персонажа — не героической, а уязвимой. Взгляд, поза и свет подчеркивают его внутреннюю раздвоенность: человека, который живет на грани между падением и прозрением.Ссылка на Artstation: https://www.artstation.com/artwork/DvWkRGЦерковь из Final Fantasy VII, Дон ФамЭта работа вдохновлена культовой сценой из Final Fantasy VII, где церковь в секторе 5 становится символом надежды среди разрухи. Высокие арки, мягкий свет, пробивающийся сквозь разбитые витражи, и зелень, проросшая сквозь каменный пол.Атмосфера сочетает тишину и легкую магию — это место, где природа и вера противостоят индустриальному миру Мидгарда.Ссылка на Artstation: https://www.artstation.com/artwork/PX03eZТеги:геймдев3d-сценылевел-дизайн3d-графика3d-графика и игрыartstationХабы:Блог компании ggsel.netИгры и игровые консолиРазработка игр",660,0,0,4 мин,https://habr.com/ru/companies/ggsel/articles/965716/,6435,781,3
"Зачем я все еще нанимаю копирайтеров, хотя умею писать с ChatGPT",studio33bukvy,2025-11-13T09:16:32.000Z,"['Искусственный интеллект', 'Контент и копирайтинг *', 'Поисковая оптимизация *', 'Копирайт']","studio33bukvy 2 часа назадЗачем я все еще нанимаю копирайтеров, хотя умею писать с ChatGPTУровень сложностиПростойВремя на прочтение3 минКоличество просмотров158Искусственный интеллектКонтент и копирайтинг * Поисковая оптимизация * КопирайтИз песочницыИскусственный интеллект ускоряет процесс. Но смысл и интонацию все еще задает человек.Недавно на Хабре вышел материал с громким заголовком — «Как я генерирую тексты без копирайтеров (и почему поисковики это не замечают)». Признаюсь честно, как специалист по коммуникациям, я прочитала его с удовольствием. Текст автора — искренний, системный и вдохновляющий. Lastman рассказал, как с помощью нейросети создаёт статьи, которые выглядят естественно и даже проходят SEO-проверку.Автор явно знает, о чём пишет — чувствуется насмотренность, понимание SEO-логики, работа с LSI-запросами, структурами, микро-редактурой. Из чего я делаю логичный вывод, что человек находится внутри ремесла, профессии, контекста. Просто он сам стал для себя и копирайтером, и редактором, и SEO-специалистом одновременно. Это заслуживает уважения.Но всё же — давайте чуть подискутируем. Не потому что кто-то «за» или «против» копирайтеров, а потому что тема, на мой взгляд, требует дополнения.Нейросеть — не волшебная палочка. Если вы внутри рынка, я на 1000% уверена, что хоть раз, но вам приходилось слышать от заказчика: «только не нейросеть, пожалуйста». Не потому что они против технологий — а потому что получали и продолжают получать на руки тексты без редактуры, с шаблонными оборотами и фразами «по инструкции». И это действительно проблема: в руки к заказчику попадает не результат работы с нейросетью, а просто её сырой отклик: не отредактированный, с не красивыми оборотами, с не богатым русским языком текст.И вот тут-то и нужен специалист, профессионал, эксперт — человек, который умеет работать с текстами, понимает разницу между рекламным, нативным, продающим, SEO-оптимизированными текстами, владеет техниками соблюдения определенного темпо-ритма, умеет работать под ToV, у которого есть вкус, логика, начитанность и насмотренность, чтобы сделать из AI-черновика контент, который хочется дочитать до конца.Сегодня AI действительно умеет многое: собирать структуру, находить LSI-запросы, формировать логичный текст. Нейронка может и написать текст, и текст будет хорош. Но, как правило, без редактуры не обойтись. Как я всегда говорю, нейросеть надо править. Машина может воспроизвести смысл, но не интонацию. Она может выдать ответ, но не почувствовать момент. Я обожаю нейросеть и постоянно ее использую: ChatGPT, Claude, Алиса, Шедеврум, Leonardo, Kandinsky, DeepSeek и другие. Пару месяцев назад по сети гулял промт, который можно ""скормить"" нейросети и на основе анализа ваших запросов и манерой коммуницировать, AI создает изображение, как он вас видит. Мое изображение было очень суровым. Потому что я часто спорю и критикую, а еще проверяю и подвергаю сомнениям выводы нейронки. ИИ выдает слова. Человек придает им смысл.Если продолжать мысль, увы, ни один из этих инструментов не знает, что такое «брендовый голос», как реагирует аудитория, и почему одни тексты вызывают доверие, а другие — раздражение. Можно идеально подобрать LSI-запросы и всё равно не попасть в аудиторию. Потому что алгоритм не знает боли клиента, ее знает копирайтер.В чем я убеждена, и меня не убедить в обратном, нет смысла бороться с нейросетями. Но есть огромный плюс в натренированных нейросетках.AI помогает ускорить этапы — анализ, черновики, структуру. А человек добавляет интонацию, эмпатию, голос бренда и опыт. И еще в придачу, выверенный слог и красивый язык.Хорошая нейросеть — как ассистент, но, увы, не дирижёр оркестра.Возвращаясь к автору материала, я не могу не сказать, что, процесс, который описывается: анализ выдачи, LSI-запросы, структура, доработка, — уже работа копирайтера, редактора, пиарщика и маркетолога. Просто сделанная своими руками. И работа, отличная, надо признать.Не вместо, а вместе. Алгоритмы считывают данные, копирайтеры - смыслы.В итоге мы пришли к главной мысли моего повествования — не «вместо», а «вместе». Нейросети не «убивают» профессию и не «сотрут» копирайтеров как вид работников в HR‑запросах. Как и в любой ремесленной профессии, главный навык — не писать, а понимать, что и зачем ты пишешь.Пока искусственный интеллект не умеет чувств овать, понимать контекст и юмор. Но однозначно, AI сделал нашу профессию умнее. Как Photoshop не убил художников, а Excel — бухгалтеров. Искусственный интеллект может помочь написать текст. Но только человек умеет сделать его живым, эмоциональным, легким и образным. Работа копирайтера, редактора и PR-стратегa всё ещё нужна. Просто теперь она стала умнее.Теги:генерация текстанейросетиискусственный интеллекткопирайтингконтент-маркетингseo-оптимизациянейросетьтекстыредакторХабы:Искусственный интеллектКонтент и копирайтингПоисковая оптимизацияКопирайт",158,0,0,3 мин,https://habr.com/ru/articles/965998/,4866,661,4
Почему мы все еще используем SASS в 2025 году,AlexEp,2025-11-12T12:47:14.000Z,"['CSS *', 'HTML *', 'Разработка под e-commerce *', 'Веб-разработка *']","AlexEp 22 часа назадПочему мы все еще используем SASS в 2025 годуУровень сложностиПростойВремя на прочтение5 минКоличество просмотров2.5KCSS * HTML * Разработка под e-commerce * Веб-разработка * ОбзорПривет, Хабр! На связи Герман, TL Frontend-разработчик в Webest, и сегодня хочу поделиться тем, почему мы продолжаем использовать препроцессор SASS/SCSS в наших проектах, несмотря на растущую популярность Tailwind и CSS-in-JS решений.К слову, мы не «олдскульные фанаты» SASS, и Tailwind тоже используем, но в зависимости от типа проекта. Комбинированный подход дает гибкость, особенно в масштабируемых фронтенд-системах.Что такое SASS/SCSSSASS — это препроцессор CSS, который позволяет использовать переменные, функции, условия, циклы и другие конструкции, отсутствующие в стандартном CSS.SASS имеет два синтаксиса:SASS — оригинальный синтаксис с отступами вместо фигурных скобок.SAASSCSS — более современный и популярный вариант, максимально похожий на CSS (добавляет расширения без изменения привычной структуры).SCSSМы в своей практике используем исключительно SCSS — он проще для вхождения и легко интегрируется с современными фреймворками и сборщиками.Зачем используем в 2025SASS появился в 2007 году и сразу стал прорывом: позволял использовать переменные, вложенность и миксины еще до того, как что-то похожее появилось в стандарте CSS. В 2010-е он стал де-факто стандартом в крупных проектах.Позже появились альтернативы: LESS, Stylus, еще позже — CSS-in-JS и Tailwind. Многие стали отказываться от SASS, когда в нативном CSS появились переменные, @custom-media, clamp(), вложенность и другие возможности. Поэтому сейчас SASS воспринимается не как «обязательный инструмент», а как осознанный выбор для архитектуры и масштабируемости.Мы используем комбинированный подход, в зависимости от проекта. Где-то используем SASS в «классическом виде»: написал стили – скомпилировал – готово. Где-то он используется в связке с CSS Modules, в частности в крупных проектах на Vue/Nuxt.Например, в разработке интернет-магазина для проекта X с тысячами карточек товаров Tailwind оказался удобен на старте, но при масштабировании код стал тяжело поддерживать. Вернувшись к SCSS-модулям, мы сократили дублирование кода и сделали поддержку проще.Несмотря на то, что в CSS появляется все больше нативных возможностей (например, CSS variables, container queries, nesting и т.д.), SASS все еще дает разработчикам:Быструю разработку с DRY-принципами.Чистую архитектуру за счет деления на модули.Возможность писать более выразительный и масштабируемый код.Преимущества для масштабированияНачнем с возможностей, которых пока нет в CSS.МиксиныМиксин – это конструкция которая может содержать определенный набор свойств, @extend, вызовов функций и циклов. Все, что будет объявлено внутри миксина можно переиспользовать, указав название миксина с ключевым словом @include.Простой пример, миксин для добавления анимации css-свойствам:// Создаем сам миксин который будет добавлять анимацию для css-свойств
@mixin transition(
  $property: all,
  $duration: .3,
  $function: ease,
  $delay: null
) {
  transition-duration: $duration;
  transition-property: $property;
  transition-timing-function: $function;

  @if $delay {
    transition-delay: $delay;
  }
}

// Применяем миксин к селектору
.some-selector {
    @include transition((color, background-color));
}С таким кодом нет необходимости создавать для каждого блока transition-свойство, просто используем миксин, передаем нужные свойства для анимации и готово!Пример как выглядит gif-изображение с применением миксина и безМиксины здорово экономят время. Мы сами в этом убедились, когда в e-commerce проекте нужно было единообразно управлять анимацией карточек. Вместо копипаста transition-свойств по всему коду вынесли в миксин и смогли централизованно обновлять поведение.Условия (@if, @else)SASS позволяет использовать условные конструкции. Например, в примере о миксинах был следующий блок с задержкой анимации.@if $delay {
    transition-delay: $delay;
  }В нем мы можем указать нужное свойство или несколько свойств. Например, миксин который добавляет внешние и внутренние отступы элементу, если переменная $needOffset содержит истинное значение:@if ($needOffset) {
	margin-block: 10px;
	margin-inline: 20px;
	padding: 8px;
}В данном случае если четвертым аргументом передаем не null-значение, то для анимации появится задержка. Конечно, значение должно быть валидным, например, 0.3s, 500ms.Дебаггинг для поиска проблемной вёрсткиИногда на проекте появляется проблема: «вёрстка поехала» или неожиданно появился горизонтальный скролл, хотя все элементы находится в контейнере. Проверили разметку, все элементы на месте, а верстка все равно едет. В таких случаях помогает быстрый отладочный прием — включить режим дебага через SCSS-переменную:$debug: true;

@if ($debug) {
   * {
        outline: 1px solid red;
    }
}Важно, что здесь используется свойство  outline, а не border — оно не влияет на размеры элементов (помним о box-sizing) и не смещает layout. Такой приём позволяет очень быстро находить проблемные участки верстки и экономит время на дебаг.ПеременныеSASS-переменные – одна из базовых возможностей препроцессора. Но с приходом CSS-переменных мы почти полностью перешли на них. Именно от переменных SASS в пользу CSS Variables, так как последние работают в runtime, с таким работать удобнее и могут быть случаи когда с переменными приходится работать через JS.Однако, если значение не должно изменяться в runtime, старые добрые SASS-переменные остаются удобными.Мы используем оба подхода. для темизации интерфейса удобнее CSS Variables, ведь ими легко управлять через JS прямо в runtime. Но там, где значения не должны меняться (брейкпоинты, базовые размеры), оставляем SCSS-переменные.Пример кода создания переменныхИмпортыSASS обрабатывает импорты на этапе компиляции, в отличие от @import в CSS, который создаёт дополнительные HTTP-запросы. Для больших проектов это критично: мы можем разбивать код на модули, но при этом не перегружаем сеть.В SCSS импорты срабатывают ещё на стадии сборки, поэтому на выходе всегда один CSS-файл (или несколько, если так задумано архитектурой). Это удобно: код структурирован, итог лёгкий.Импорты в SCSS не раз спасали архитектуру в больших проектах. Например, в корпоративной системе с десятками страниц мы разделили стили на отдельные модули — хедер, футер, карточки, таблицы. А на продакшн все собиралось в один оптимизированный файл без лишних запросов.АдаптивыЧаще всего в адаптиве нужно плавно изменять свойства. Например, размер шрифта или отступы в зависимости от ширины экрана. Вместо множества медиа запросов удобно использовать CSS-функцию clamp(). Она создает не фиксированное значение, а «резиновое», которое растет или сжимается в зависимости от ширины экрана. Мы внедряем ее во всех проектах с адаптивом, например, для типографики и отступов.В SCSS это можно автоматизировать с помощью функции:@function fluid($min, $max, $min-vw: 360, $max-vw: 1440) {
    $slope: math.div(($max - $min), ($max-vw - $min-vw));
    $y-intercept: $min - $slope * $min-vw;

    @if $min > $max {
        @return clamp(
            #{$max}px,
            #{$y-intercept}px + #{$slope * 100}vw,
            #{$min}px
        );
    }

    @return clamp(
        #{$min}px,
        #{$y-intercept}px + #{$slope * 100}vw,
        #{$max}px
    );
}Функция fluid принимает четыре аргумента:$min — минимальное значение свойства (например, 16 для шрифта в 16px).$max — максимальное значение свойства (например, 24 для шрифта в 24px).$min-vw — минимальная ширина экрана, при которой применяется $min (по умолчанию 360px).$max-vw — максимальная ширина экрана, при которой применяется $max (по умолчанию 1440px).То есть мы задаем «коридор» значений, где между ними изображение плавно изменяется. И на выходе получаем clamp-функцию с автоматически рассчитанными параметрами.ЗаключениеДа, SASS уже не на хайпе, как раньше. Но он по-прежнему остается мощным инструментом, особенно в проектах с жесткими требованиями к архитектуре, масштабируемости и стабильности.Мы комбинируем SASS с современными практиками и не видим причин полностью от него отказываться, по крайней мере, пока нативный CSS не догонит его по возможностям.А вы используете SASS  или другие препроцессоры в современной разработке или решили отказаться в пользу других инструментов?Теги:saasscssfrontend-разработкаверстка сайтовХабы:CSSHTMLРазработка под e-commerceВеб-разработка",2500,0,0,5 мин,https://habr.com/ru/articles/965698/,8421,1075,4
Чем же крут язык Zig?,interpres,2025-11-12T13:01:00.000Z,"['Блог компании RUVDS.com', 'Zig *', 'Программирование *', 'Компиляторы *']","interpres 22 часа назадЧем же крут язык Zig?Уровень сложностиСреднийВремя на прочтение17 минКоличество просмотров3.3KБлог компании RUVDS.comZig * Программирование * Компиляторы * ОбзорПереводАвтор оригинала: Nilo StolteНе думаю, что за мою 45-летнюю карьеру какой-то другой язык удивлял меня сильнее, чем Zig. Могу с уверенностью сказать, что Zig — это не только новый язык программирования, но и, на мой взгляд, совершенно новый способ написания программ. Задача этого языка — далеко не только замена C или C++.В этой статье я расскажу о фичах, показавшихся мне самыми привлекательными в Zig, а также вкратце опишу их. Моя цель — представить простые фичи, которые позволят программистам быстро приступить к работе с языком. Однако имейте в виду, что у него есть ещё множество других фич, влияющих на его популярность в нашей отрасли.КомпиляторОн компилирует C и выполняет кросскомпиляциюНаверно, самое невероятное достоинство компилятора Zig — способность компиляции кода на C. Оно связано с возможностью кросскомпиляции кода для запуска его на архитектуре, отличающейся от архитектуры машины, на которой он компилировался изначально, и это уже само по себе уникальная особенность. Уже лишь эти фичи, изначально доступные в языке, невероятно сильно повлияли на отрасль. Впрочем, мы будем в первую очередь говорить о том, как писать программы на Zig, и почему следует выбрать его вместо какого-то другого языка.Установка компилятораУстановить компилятор достаточно просто. На странице скачивания Zinglang можно найти компилятор во множестве разных форматов для разных процессоров и операционных систем:Например, в случае Windows 10 следует выбрать zip-файл x86_64 и скопировать его распакованное содержимое в нужную папку, скажем, в Program Files. Я изменил имя корневой папки на zig-windows-x86_64, потому что благодаря этому можно просто скопировать другую версию компилятора без необходимости изменения пути в переменной окружения Path.Дальше нужно добавить путь к этой корневой папке в переменную окружения Path в «Дополнительных свойствах системы» (нажать на 1-4, вставить путь в 5 и нажать на OK в 6-8):Сразу после указания пути к корневой папке Zig уже можно использовать компилятор в режиме CLI. Благодаря своей простоте это рекомендуемый способ использования.Собираем программу «Hello World!» на Zig Рекомендуется собирать программу «Hello World!» по инструкциям из раздела «Getting Started» сайта руководства. Также там представлена альтернативная процедура установки, в том числе для MacOS и Linux (крайне рекомендуется использовать «Installing manually»). Также рекомендуется изучить раздел «Language Basics» для более глубокого понимания языка Zig.Основные концепции и командыВ приведённых ниже разделах будет представлено общее описание языка Zig. Их задача — позволить программисту научиться быстро приступить к программированию на Zig, освоив лишь некоторые базовые концепции и команды.Далее представлено краткое описание того, как собирать программы и тестовые модули.В конце приводится более глубокая информация о низкоуровневом программировании на Zig с более подробным описанием примеров.Объявление переменныхОбъявления переменных на Zig потенциально могут состоять из трёх частей. Первая часть содержит степень доступности (pub или ничего), за ней идёт слово var или const, а затем имя переменной. Вторая часть, отделённая от первой двоеточием, содержит объявление типа переменной. Третья часть — это инициализация переменной. В Zig обязательны только первая и третья части, что довольно непривычно для людей, программирующих на Java или C. Можно задаться вопросом, как же компилятор определяет тип переменной. В данном случае тип выводится из инициализации.var sum : usize = 0;         // объявление переменной из трёх частейПеременные, для которых степень видимости не указана явным образом как pub, являются локальными для модуля, то есть доступ к ним невозможен снаружи файла исходников, в котором она объявлена (точно так же, как для переменных static в C). Не рекомендуется объявлять переменные, как pub, и рекомендуется создавать в модуле лишь небольшое количество функций, объявленных, как pub, чтобы снизить связанность и повысить целостность. Функции pub ведут себя, как API модуля.Struct, анонимные struct и тестовые блокиВ Zig символ .{, закрываемый } — это литерал анонимной struct. Анонимные struct в основном используются для инициализации элементов другой структуры или для создания новой структуры с инициализированными элементами. .{ } — это пустой литерал анонимной struct. Слово struct, за которым идёт {, закрывающийся } — это объявление struct. Можно инициализировать переменную с типом, действующую в качестве псевдонима типа. Обратите внимание на тестовый блок, позволяющий компилировать и исполнять тесты без необходимости исполняемого файла.Битовые поляБитовые поля (bitfield) — это объявляемые поля с типами, имеющие конкретные размеры в packed struct. Указатели могут указывать на конкретное битовое поле следующим образом:Циклы ForСинтаксис Zig понятнее, чем у C, только там, где обычно используется [0..8], в нём указывается открытый интервал: [0..9). Преимущество Zig: объявление типа i, её инициализация, проверка и инкремент выполняются автоматически.Массивы[_] определяет массив неизвестного размера. За ним должен следовать тип элементов (в данном случае u8, unsigned byte) и его инициализация между { и }. В показанном ниже примере инициализация гласит, что это массив неизвестного размера ([_]), в котором каждый элемент имеет тип u8 (unsigned byte) и инициализирован нулями ({0} ** 81). Стоит отметить, что размер также выводится из показателя повторения (81) инициализации ({0}).var grid = [_]u8{0} ** 81;На иллюстрации ниже показано тестовое окружение с циклом, взаимодействующим с массивом и складывающим его элементы. Конструкция try expect проверяет корректность sum.Слово byte — это не тип и не зарезервированное слово в Zig. Здесь это переменная для хранения каждого из элементов массива на каждом шаге цикла. Переменная, объявленная между двумя | с массивом между скобками цикла for, всегда считается имеющей тот же тип, что и элементы массива.Также стоит отметить, что usize — это естественный unsigned integer для платформы. Это значит, что на 64-битных машинах он имеет размер u64, а на 32-битных — u32.Указатели на множественные элементы в ZigУказатели на массивы могут использовать арифметику указателей, только если указатель явным образом объявлен как указатель на множественные элементы, в данном случае [*]const i32. Обратите внимание, что массив является const, то есть не может быть изменён, но указатель является var.Разыменование указателейВ случае привязки к адресу отдельной позиции массива указатель невозможно изменять арифметикой указателей. В данном случае, const предотвращает лишь изменение значения другой прямой привязкой к адресу. Для разыменования указателей в Zig используется ptr.*, как показано ниже:Break с меткамиZig может выполнять множество операций на этапе компиляции. Например, давайте инициализируем массив. Здесь применяется break с меткой. Блок помечен : после своего имени init и значение возвращается из блока при помощи break.Этот синтаксис может показаться запутанным. 0.. означает бесконечный диапазон, начинающийся с нуля. В for переменные pt и i инициализуются адресом массива initial_value и нулём. В течение цикла инкремент обеих выполняется автоматически, и цикл останавливается сразу после последней позиции массива. Также обратите внимание, как разыменовывается указатель pt: pt.*.Любопытен и способ объявления массива в блоке с меткой. Массив называется initial_value и содержит 10 позиций типа Point (объявленного позже). В Zig переменные объявляются обязательно. Явным образом можно отказаться от инициализации при помощи зарезервированного слова undefined.Функции в ZigФункции в Zig объявляются при помощи fn; по умолчанию они являются статическими (не могут импортироваться в другие файлы) в файле, где были объявлены, за исключением случаев, когда перед fn идёт pub. Функцию можно «встроить». Указателям функций предшествует const, а за ними идёт прототип функции.ООП в ZigStruct могут иметь функции. Ниже реализован простой стек. Этот стек объявляется и используется только внутри модуля, где он был определён; он получает доступ к другими структурам данных, не относящимся к нашему примеру. Стек может содержать не больше 81 элемента (как указано в объявлении stk), каждый с типом StkNode. Учтите, что операторов ++ и -- в Zig нет. Вместо них следует использовать эквиваленты += и –=. Указатель стека — это просто integer, используемый в качестве индекса массива stk.Обратите внимание, что указатель self (self — это не зарезервированное имя, а стандартное наименование) не передаётся в явном виде как параметр, как можно было бы ожидать. Косвенно подразумевается, что это указатель на экземпляр стека, из которого вызывается функция. В примере ниже извлечение из стека можно вызвать при помощи stack.pop();. В этом случае указатель self соответствует указателю на stack. Этот указатель частично эквивалентен this в Java или C++.Функция init() — это конструктор стека.Обратите также внимание, что функции pop и push «встроены».Сборка и исполнение программ на ZigСборка исполняемого файлаДля сборки исполняемой программы необходима функция main, которая, как и в программах на Java или C, обозначает точку входа в программу. Если эта функция существует, множество модулей можно скомпилировать в код исполняемого файла. Простая программа может содержать функцию main в том же файле, что и остальная часть программы. Во многих случаях также можно вставить функцию main в конец модуля, чтобы создать исполняемый файл для отдельной отладки модуля. После завершения отладки эту функцию можно закомментировать.В таких ситуациях можно использовать следующую команду для компиляции program.zig и генерации исполняемой программы (в Windows это program.exe):zig build-exe -O ReleaseFast program.zigЧтобы избежать опечаток, команду можно сохранить в пакетный файл.Исполнение тестовых блоков в модулеНаверно, это самая лучшая фича Zig как языка программирования. Это окружение обычно используется для тестирования, но в нём можно и прототипировать.Блок в Zig схож с блоком в C или Java, то есть это некий код между { и }. Тестовый блок — это блок, начинающийся с test ""message"" { и заканчивающийся на }, где ""message"" — это строка, содержащая сообщение, отображаемое при выполнении теста (в данном случае только слово message).Тестовые блоки исполняются независимо от исполняемого файла. Готовый исполняемый файл не включает в себя тесты. Тестовые блоки в конкретном module.zig компилируются вместе со всем кодом этого файла и выполняются следующей командой:zig test module.zigПримерВ качестве реального примера покажем тестовый блок из модуля example.zig:test "" => testing set and print functions"" {
    set(
      ""800000000003600000070090200"" ++
      ""050007000000045700000100030"" ++
      ""001000068008500010090000400""
    );
    std.debug.print(""\n"" ++
      ""===================\n"" ++
      ""Input Grid\n"" ++
      ""===================\n"",
      .{}
    );
    print();
}Обратите внимание, что в example.zig нет функции main, а потому он не может сгенерировать исполняемый файл, но его тестовый блок можно выполнить следующей командой:zig test example.zigКак и написано в сообщении, показанный выше тестовый блок тестирует функции set и print. Как видно из кода, set передаёт в качестве параметра строку десятичных цифр, за которой следует print (выводящий заголовок «Input Grid» ), за которым следует вызов функции print.Результат выполнения будет следующим:Вывод в ZigДавайте обратим внимание на строку std.debug.print. Это вызов функции print в debug.zig в стандартной библиотеке Zig std. Первый параметр — это форматирующая строка, а вторая — анонимная struct (перед которой стоит точка), содержащая список переменных, отображаемых при помощи форматирующей строки. Так как в форматирующей строке формат отсутствует, struct пуста. Так и выполняется форматированный вывод. В данном случае он по умолчанию отобразится в stderr.Всё это выглядит, как код на C, но здесь есть фундаментальное отличие. В C функция printf динамически интерпретирует форматирующую строку во время исполнения, а в Zig можно работать с литеральной строкой и списком переменных во время компиляции. Поначалу этот принцип трудно уяснить. Многое можно исполнять во время компиляции.Отладка исполняемого файлаРабота с отладчиком обычно непроста, если в IDE не интегрирован отладчик (например, как в IDE Java наподобие Eclipse и Intellij IDEA) или отсутствуют интегрированные комплекты разработки (наподобие w64devkit для C/C++).Большое неудобство при работе с отладчиками заключается в том, что необходимо интегрировать символы, что не только раздувает код бесполезной для программы информацией, но и требует компиляции в режиме отладки, при которой генерируется гораздо менее эффективный исполняемый код. Люди, имеющие опыт в сложных системах, знают, что это может быть очень длительным процессом.Для устранения подобных проблем в Zig есть довольно удобный «хак»:Встроенная @breakpointКогда исполняемый файл запускается в отладчике, эта встроенная функция останавливает программу в точке, где в исходный код вставлен @breakpoint();. Это полезная фича для отладки оптимизированного кода на Zig без необходимости символов.Достаточно лишь трассировать переменные, за которыми нужно наблюдать, при помощи std.debug.print сразу после @breakpoint(); Благодаря этому мы сможем знать, каковы значения переменных в данный момент.ПримерВ качестве примера сгенерируем исполняемый файл для модуля debug_example.zig, содержащего следующую функцию main:pub fn main() !void {
    set(
      ""800000000003600000070090200"" ++
      ""050007000000045700000100030"" ++
      ""001000068008500010090000400""
    );
}Чтобы дважды выполнить сравнение с результатами example.zig, параметр, передаваемый функции set в этой main , совпадает со строкой, передаваемой set в тестовом блоке в example.zig, но на этот раз в функцию set вставляется следующий код:        if (c != 0) {
           print();
           std.debug.print(
              ""Current digit {}\nposition in string {}\n"" ++
              ""line {}\ncolumn {}\ncode {b}\n"", 
              .{c, k, i, j, code}
           );
           @breakpoint();
        }Можно сгенерировать исполняемый файл debug_example.exe при помощи следующей команды:zig build-exe debug_example.zigДальше мы используем отладчик для вызова debug_example.exe. В этом случае я использовал gdb — отладчик C/C++ из w64devkit, но это мог быть любой отладчик для исполняемых программ. Внутри gdb необходимо запустить программу при помощи команды r и после этого нажать Enter. Обратите внимание, что программа вывела сетку с содержимым, а также переменными, остановившись в ожидаемом месте. Если затем ввести команду c и нажать на Enter, то продолжится трассировка содержимого сетки и переменных. После этого можно просто продолжать нажимать Enter (при этом повторяется последняя команда, то есть в данном случае c). После многократного нажатия на Enter до завершения программы значения в сетке будут соответствовать значениям, показанным в тестовом блоке в example.zig, потому что оба примера передают set одинаковый параметр.Низкоуровневое программирование на ZigОзнакомившись с введением и примерами, вы уже можете приступать к написанию общих программ на Zig. Ниже представлен более глубокий анализ интересных низкоуровневых фич языка, уже использованных в примерах.Смысл показанных выше примеров заключается в создании модуля, задающего (инициализирующего) и отображающего сетку 9x9. В этой матрице будет храниться сетка судоку, то есть в ней будут содержаться только десятичные цифры. Инициализация сетки должна гарантировать, что сетка удовлетворяет условиям игры в судоку, то есть не содержит ошибок.В то же время это будет отличная возможность показать низкоуровневые фичи Zig (по крайней мере, самые выдающиеся), и наши примеры хорошо для этого подходят.Гипотеза, лежащая в основе этих примеров, заключается в том, чтобы представить цифру сетки в качестве бита в позиции, заданного её значением. Это представление достаточно удобно для определения того, присутствует ли уже цифра в сетке (это основное правило сеток судоку). При всём этом, она будет зашифрована так, что её можно использовать только внутри модуля.Представление матрицыЧтобы значения легко понимались человеком, цифры хранятся в матрице как стандартные целые u8. Хоть входная сетка в примерах задаётся в строковом формате, символы ASCII преобразуются в целые u8. Хранение цифр в сетке организовано линейно, строка за строкой, в массиве с 81 позицией, называемом в примерах grid:var grid = [_]u8{0} ** 81;        // Линейно хранящаяся сетка судокуДля проверки корректности сетки необходимо получать доступ к элементам по соответствующей строке и столбцу. Иными словами, необходимо получать доступ к элементам в матрице. Стратегия заключается в создании массива указателей с 9 позициями, каждая из которых указывает на начало каждой строки сетки. Блоки кода в принципе не могут возвращать значение, но в Zig это возможно благодаря break с метками:const matrix = fill9x9: {         // массив матрицы, обеспечивающий доступ  
   var m : [9][*]u8 = undefined;  // к элементу сетки, как матрицы, 
   var pt : [*]u8 = &grid;        // то есть: элемент = matrix[i][j]
   for (0..9) |i| {               //
      m[i] = pt;                  // хранит указатели на каждую строку
      pt += 9;                    // в каждой позиции матрицы
   }                              //
   break :fill9x9 m;              // инициализирует матрицу с помощью m
};В конце цикла m возвращается снаружи блока при помощи команды break :fill9x9 m;. Следует учесть, что fill9x9 соответствует имени метки, расположенной сразу после начала блока.Если i и j — это строка и столбец элемента, то к любому элементу сетки можно получить доступ при помощи следующей записи:element = matrix[i][j]Представление десятичных цифр в виде битовКлючевая концепция, используемая здесь — это замена целочисленной десятичной цифры i целочисленным code:      i ∈ [1,9]  →  code = 2ⁱ⁻¹
      i = 0      →  code = 0Иными словами, единственный бит code, которому присваивается 1 — это бит в позиции i-1, если i находится между 1 и 9, в противном случае все биты code равны нулю.Значения code для каждой цифрыВ таблице ниже показаны отношения между цифрами и их двоичным представлением:Вычисление code в ZigЗначение code вычисляется в функции set при помощи оператора сдвига влево, только если c не равно нулю:code = @as(u9,1) << (c-1);Чтобы операция могла скомпилироваться, и результат операции мог быть привязан к переменной, в Zig константы должны иметь нужный размер. В данном случае, code имеет объявленный тип u9. Это ещё одно фундаментальное качество Zig — возможность наличия переменных с произвольным битовым размером. Как видно из таблицы выше, максимальное значение code равно 256, а для его представления нужно не менее 9 битов. Встроенная функция @as позволяет привести константу 1 к типу u9.Представление сетки при помощи битовых полейПредставив цифры в виде битов, можно отобразить сетку гораздо более простыми способами.Сетка битовых полей строкМассив lines отражает всю сетку, представляя каждую строку 9-битным integer, где каждый бит представляет десятичную цифру, которую можно представить в строке:var   lines   = [_]u9{0} ** 9; Благодаря этому, просто получая доступ к строке i элемента в сетке 9x9, можно понять, присутствует ли конкретная цифра в этой строке, выполнив побитовое и ( & ) c code цифры:lines[i] & codeЕсли результат этой операции равен нулю, то цифра ещё не представлена в строке i. Противоположное означает наличие дубликата.Сетка битовых полей столбцовМассив columns отражает всю сетку, представляя каждый столбец как 9-битный integer:var   columns = [_]u9{0} ** 9;Благодаря этому для проверки наличия конкретной цифры в столбце j  достаточно простого доступа к этому массиву с этим столбцом элемента в сетке 9x9. Для этого нужно всего лишь выполнить побитовое и ( & ) с code цифры:columns[i] & codeЕсли результат этой операции равен нулю, это означает, что цифры ещё нет в столбце j . Противоположное означает наличие дубликата.Правила судокуПредположим, что у нас есть пустая сетка судоку, как это было, когда мы заполняли в примерах сетку входной строкой. Новая цифра, вставляемая в пустой элемент, не должна присутствовать во всей строке, столбце или ячейке, содержащей новый элемент.Допустим, что эта сетка уже инициализирована:Ячейка — это каждая из девяти сеток 3x3, ограниченных жирными линиями. Здесь важно понять, что каждый элемент в сетке 9x9 имеет уникальную строку, столбец и ячейку, содержащую этот элемент.Например, первая ячейка сетки содержит значения 3, 5, 6, 8 и 9. Следовательно, в ней отсутствуют значения 1, 2, 4 и 7. Предположим, что мы хотим поместить значение 7 в одно из пустых мест первой ячейки. Очевидно, что нельзя поместить его в единственный пустой элемент первой строки, потому что в этой строке уже присутствует 7. Нельзя её разместить и в единственном пустом месте первого столбца, потому что 7 уже есть в этом столбце. Можно поместить 7 только в один из двух пустых элементов второй строки. Но мы не можем знать точно, какой из них подходящий.Давайте теперь изучим вторую ячейку, содержащую значения 1, 5, 7 и 9. Мы видим, что единственный возможный элемент в этой ячейке, куда можно поместить 8 — первая строка в пустой позиции справа от значения 7.Массивы lines и columns проверяют наличие дубликатов в строках и столбцах. Значит, требуется новый массив для проверки дубликатов в ячейках.Сетка битовых полей ячеекМассив cells отражает всю сетку, представляя каждую ячейку в виде integer из 9 бит:var   cells   = [_]u9{0} ** 9;    // все элементы каждой ячейкиИ здесь всё становится сложнее. Нельзя получить доступ к cells непосредственно при помощи строки или столбца. Было бы проще, если бы мы могли получать доступ к cells, как к матрице 3x3. Это можно сделать имитацией того, что мы сделали для матрицы 9x9, то есть заполнив массив cell следующим образом:const cell = fill3x3: {           // массив cell обеспечивает доступ
   var m : [3][*]u9 = undefined;  // к элементам ячейки, как к матрице,
   var pt : [*]u9 = &cells;       // cell[cindx[i]][cindx[j]]
   for (0..3) |i| {               // 
      m[i] = pt;                  // сохраняет указатель на каждую строку
      pt += 3;                    // в каждой позиции ячейки
   }                              //
   break :fill3x3 m;              // инициализирует ячейку с помощью m
};                                //Но теперь нужно определить строку и столбец в матрице cell по строке и столбцу элемента в исходной сетке 9x9. Можно использовать целочисленное деление, чтобы поделить строку и столбец на 3 и получить индексы, но известно, что операция деления медленная. Два деления усугубляют ситуацию. Можно использовать следующий массив, чтобы представить результат деления:const cindx   = [_]usize{ 0,0,0, 1,1,1, 2,2,2 };Благодаря этому, просто получив доступ к этой матрице со строкой i и столбцом j элемента в сетке 9x9, можно понять, присутствует ли конкретная цифра в ячейке этого элемента, выполнив побитовое и ( & ) c code цифры:cell[cindx[i]][cindx[j]] & codeЕсли результат этой операции равен нулю, то цифра ещё не присутствует в ячейке, в противном случае есть дубликат.Проверка дубликатов элементаПолная проверка наличия дубликатов элемента выполняется сочетанием побитового или ( | ) всех предыдущих элементов в той же строке, столбце и ячейке с последующим побитовым и ( & ) с code элемента:if (((lines[i]|columns[j]|cell[cindx[i]][cindx[j]])&code) !=  0) {
    unreachable;
}Если результат равен нулю, значит, элемент пока не существует в этой строке, столбце или ячейке. Если результат ненулевой, то программа останавливает работу, потому что пытается выполнить команду unreacheable. Это простейший способ явного обозначения ошибки исполнения в Zig. Обратите внимание, что код в функции set также выводит подробности о том, где произошла ошибка.Например, если в строке, передаваемой set, заменить '0' сразу после первой '8' на '5', то при тестировании example.zig будет выведена следующая ошибка:Это вызвано тем, что в столбце 1 уже есть значение 5 в строке 3, как и написано в сообщении об ошибке. Причина ошибки заключается в панике, вызванной попаданием в недостижимый код в функции set.Изменение структур данныхВ функции set двойной цикл for строка за строкой копирует каждый новый элемент из входной строки s в сетку, как показано ниже (в переменной k хранится индекс нового входящего символа в строке s):   for ( 0..9 ) |i| {
      line = matrix[i];
      for ( 0..9 ) |j| {
        c = @intCast(s[k]-'0');
        if (c != 0) {
          code = @as(u9,1) << (c-1);
          ⋮  // остальная часть кода
        }
        line[j] = c;
        k+= 1;  
      }
   }Символ преобразуется в u4 (переменная c) вычитанием из него '0'. Если новый элемент, вставляемый в сетку, не равен нулю ( c != 0 ), то code (вычисленный командой сдвига влево) копируется в каждую из сеток выполнением побитового или ( |= ) с соответствующей сеткой:    lines[i] |= code;
    columns[j] |= code;
    cell[cindx[i]][cindx[j]] |= code;Никакой явной проверки того, что значение c находится в интервале от 1 до 9, не требуется, потому что при выполнении операции сдвига произойдёт переполнение. Например, заменив '0' сразу после первой '8' входной строки на ':' в строке, передаваемой set, мы получим при тестировании example.zig следующую ошибку:Если заменить тот же '0' на '/', то возникнет схожая ошибка исполнения. Программа будет работать, только если все значения находятся в интервале от 1 до 9, то есть если входная сетка содержит только десятичные цифры.В вебе во многих сетках судоку '0' также обозначается как '.'. Именно поэтому в функции set есть следующая строка:if (s[k] == '.') c = 0;Это позволяет удобно обойти операцию сдвига, потому что значение c равно нулю.Прототипирование и устойчивостьПринудительные ошибки, показанные в двух предыдущих разделах, демонстрируют важные фичи Zig, которые могли бы оказаться незамеченными. Одна из них — это устойчивость Zig. Как мы видели, в случае операции сдвига не допускается никакое ошибочное поведение, и ситуация перехватывается во время исполнения.Кто-то может подумать, что все усилия по развитию Zig направлены на эффективность, но это типичный случай того, как производительностью жертвуют в пользу устойчивости. Если ваша главная цель — производительность, это может показаться спорным выбором. Например, в C потеря бита при операции сдвига — это проблема программиста, зато это повышает производительность определённой ассемблерной команды.Ещё одна фича, продемонстрированная пару разделов назад — возможность использования тестовых блоков для прототипирования. Потенциал этой фичи огромен, хотя в нашем примере она используется лишь для отладки определённых ситуаций в случаях ошибок.Уже сами по себе эти фичи демонстрируют большую мощь, редко встречающуюся в языках программирования, и в особенности в компилируемых.ЗаключениеВсё это довольно неожиданно и заставляет меня задуматься о том, что множество преимуществ, которые ранее встречались только в интерпретируемых языках, постепенно мигрируют в компилируемые языки с целью обеспечения повышенной производительности.Схожесть Zig с интерпретируемыми языками потрясает, и в особенности его концепция исполнения во время компиляции, к сожалению, рассмотренная в этой статье недостаточно подробно. Этот аспект Zig, с одной стороны, делает его уникальным и мощным, с другой стороны, усложняет его освоение.В основном я сделал упор на знакомство с основами и простыми аспектами Zig, благодаря которым он настолько крут; однако существуют и другие довольно впечатляющие аспекты.Показанные в статье примеры — это упрощённые версии более сложных программ для решения судоку, написанных также на Java и C. Документация в репозитории подробно объясняет большинство структур и алгоритмов, использованных для решения задачи.Теги:zigязыки программированияruvds_переводХабы:Блог компании RUVDS.comZigПрограммированиеКомпиляторы",3300,0,0,17 мин,https://habr.com/ru/companies/ruvds/articles/964856/,28089,3881,4
Тимбилдинг здорового человека: как фасилитация помогает формировать и развивать команды,polisha_kr,2025-11-13T10:36:53.000Z,['Блог компании КОРУС Консалтинг'],"polisha_kr 38 минут назадТимбилдинг здорового человека: как фасилитация помогает формировать и развивать командыВремя на прочтение12 минКоличество просмотров102Блог компании КОРУС КонсалтингАнастасия Криулина Фасилитатор и ведущая командных сессий в «КОРУС Консалтинг»Представьте себе оркестр без дирижера, ребенка, развивающего сложные навыки без поддержки взрослого или нового сотрудника в крупной корпорации без наставника. Во всех этих случаях развитие будет сложным и запутанным, а результат — не гарантирован. С командами такая же история: без должного сопровождения они рискуют так и не стать по-настоящему сплоченными.Меня зовут Анастасия Криулина, я фасилитатор и ведущая командных сессий в КОРУСе. Сегодня расскажу о том, как с помощью фасилитации можно эффективно управлять развитием команд и превращать группу людей в настоящую команду.Что в материале: Чем отличается настоящая команда от команды формальной, что командность дает бизнесу и всегда ли нужно к ней стремиться; Как фасилитация помогает создать эффективную команду, как выглядят фасилитируемые встречи и какие результаты приносят; Как фасилитация помогает на каждом этапе развития команд. Команда или группа лиц? В бизнесе мы привыкли любое объединение сотрудников вокруг задачи или проекта называть командой, но давайте взглянем на определение «команды». Даже Википедия предлагает нам следующую формулировку: «Команда — это группа лиц, объединенная общими мотивами, интересами, идеалами, действующая сообща. Участники команды объединены поддержкой друг друга и несут коллективную ответственность за результат деятельности всей команды». Обратите внимание на важные слова «объединение общими мотивами», «действующая сообща», «поддержку» и «коллективную ответственность». А теперь задумайтесь, все ли команды, которые вы встречаете в корпоративном мире, действительно такие? Ведь на деле многим недостает той самой синергии и взаимной поддержки, а ответственность часто остается в руках руководителя, равно как и возможность влиять на принятие решений. Что же такое «командная» команда? Это сплоченная группа, где люди работают над общим делом — задачей, проектом — в обстановке взаимного доверия, уважения и поддержки. В такой среде безопасно задавать вопросы (даже если они кажутся самому вопрошающему глупыми), экспериментировать, не боясь совершать ошибки (ведь на ошибках учатся), давать обратную связь как коллегам, так и руководителю. В «командной» команде спокойный эмоциональный фон, есть ощущение общности и понимание, что ты действительно можешь опереться на коллег, в том числе для того, чтобы исследовать свою ближайшую зону развития и творчески себя проявлять. Ответственность за результат несут все в команде, а не только руководитель. Командность оказывает прямое влияние на бизнес. Думаю, большинство читателей по собственному опыту знают, насколько легче и продуктивнее работается в сплоченной команде. Эмпирические выводы подтверждают данные исследований. Например, совместное исследование Google и проекта «Аристотель» говорит о том, одним из ключевых факторов эффективности команд в том числе является психологическая безопасность — знание, что тебя не накажут на совершенную ошибку или высказанное мнение. McKinsey в своих отчетах отмечают, что качество отношений с коллегами и руководителем является одним из ключевых факторов удержания сотрудников.Всем ли нужна «командная» команда или группа лиц — это тоже хорошо?На самом деле «некомандная» команда тоже работает и со своими функциями справляется. Просто эта самая командность дает добавленную стоимость к продуктивности. При этом есть случаи, когда работа над командной синергией не является приоритетом. Во-первых, не всегда нужно собирать команду из группы экспертов. Многие центры компетенций так работают: люди работают в одной среде, делятся экспертизой друг с другом, помогают или участвуют в запуске проектов, оставаясь при этом достаточно атомарными. Во-вторых, полноценно заниматься командообразованием — проводить сессии на целый день, вообще планировать такие активности, как отдельный трек развития — нет смысла, если вы формируете команду для краткосрочного проекта, например, меньше 3 месяцев. Это трата ресурсов (временных, эмоциональных и организационных), которая не окупится. При этом отдельные командообразующие механики, которые помогут лучше узнать друг друга и синхронизироваться по целям, использовать стоит.  Наконец, несмотря на то, что сонастройкой команд хорошо бы заниматься с самого их основания, это не должно быть приоритетом, пока вы не решили несколько «гигиенических» задач, к которым относятся: настройка регулярного менеджментаопределение ролей и функционала каждого в командекоммуникационные каналы: настроенная система встреч, не хаотичная или под запрос, а существующая регулярно понятное планирование задач для каждого сотрудника  балансировка компетенций — чтобы в команде хватало специалистов для закрытия всех стоящих задач. Можно «опустить» гигиенические процедуры и вытянуть все за счет командности. Однако это будет очень дорого (из-за критических ошибок, потраченного времени, текучки и пр.) Как чаще всего занимаются здоровьем команд На самом деле, очень многие заинтересованы в создании «командной» команды: интуитивно чувствуют, что что-то в этом направлении предпринимать нужно, и даже занимаются этим на доступном уровне. Что уже радует!Чаще всего делают следующее: Личное развитие. Индивидуальные планы развития специалистов, курсы, экзамены и прочие активности. С ними все хорошо: они действительно помогают людям развиваться, находить себя и расти. Но этот рост и развитие фиксируются в рамках индивидуального плана или связаны с очень конкретными проектными или производственными потребностями. А проектные потребности не есть потребности командные. Тимбилдинги. Помогают создать ощущение единства, эмоциональной связи, хорошо работают, если тимбилдинг заточен под конкретную команду. Однако в таких мероприятиях нет предметной деятельности и реального контекста того, над чем именно трудятся люди. Даже если вы отлично справились со сложной трассой в веревочном парке, это не значит, что при планировании спринта на проекте вы будете также эффективны. В результате люди чаще всего развиваются атомарно. За счет общих мероприятий эмоциональная составляющая может и усиливается, но конкретных механизмов, улучшающих взаимодействие, нет. С течением времени команда действительно становится более сонастроенной, слаженной, учится разделять ответственность, но это не всегда случается, сильно зависит от личности руководителя, поведения и личностных характеристик остальных членов команды.  Представьте себе учеников, у которых нет классного руководителя, или же он есть, но не делает из детей дружный коллектив, не проводит связывающие активности, которые бы показали им силу объединения. В таком случае никто не гарантирует, что за 11 лет такие ученики станут единым организмом, а не останутся группой ребят, сидящих за партами в одном помещении. Фасилитация — это тот самый «классный руководитель» для команды. Эта практика помогает найти общие цели, увидеть общий смысл и превратиться из разрозненной группы в единое целое. «Одна из важных задач фасилитации — эмоциональная настройка. Люди в незнакомой группе чувствуют себя небезопасно. Фасилитация же помогает прийти в состояние оптимального эмоционального состояния, чтобы справляться с вызовами и стресса», — Мария Курякова, HRD компании ДАР (входит в ГК Корус Консалтинг).Что такое фасилитация, в чем ее сила и особенностиФасилитация — это проведение командных встреч и совещаний особым образом, когда инструментами решения бизнес-задач становятся групповая работа и вовлечение в принятие решения всех участников. Это структурированное обсуждение с заранее подготовленными вопросами, таймингом, вовлечением всех участников без исключения в обсуждение и в само принятие решения о том, как именно дальше будет действовать команда. Отличительные черты фасилитируемых встреч Есть сценарий встречи с точными формулировками вопросов, заточенных под решение бизнес-задачи. Есть приоритизация, благодаря которой можно в моменте оценить критичность того или иного фактора для решения задачи. Вопросы формируются групповые и задаются каждому — отсидеться нельзя. Даже если принимается решение по вопросу достаточно специфическому и в команде есть сотрудник с сильной экспертизой, его просто просят подготовить экспертный доклад, но используют для дальнейшей командной работы и обсуждения, а не как единственно верный взгляд на проблему. Существуют четкие правила обсуждения: уважение времени, возможность высказаться, стимуляция ОС, толерантность ко всем идеям. Отвязывание идеи от ее автора. В фасилитации используются стикеры, списки идей, которые собираются со всех участников без указания авторства. Делается это для того, чтобы высказанная кем-то идея не давила своим авторитетом, и, напротив, не была проигнорирована. К решению все приходят в группе. Через обсуждение, голосование и так далее.Формируется план действий, люди сами себе формулируют и ставят задачи и сроки. Для удобства собрала табличку, по которой видно, чем фасилитируемые встречи отличаются от обычных.Элементы встречи Без фасилитацииС фасилитациейКто вводит в курс делаРуководитель или избранный эксперт Каждый вносит дополнения в контекст, исходя из своей роли и опыта Кто ведет встречуРуководитель Специально обученный ведущий Есть ли правила, регламенты Чаще всего нет, есть общее понимание «как у нас обычно проходят встречи»Да, есть регламент и правила, которые касаются поведения, обратной связи, времени на ответы и обсуждения Тайминг В голове у руководителяЧетко прописан фасилитаторомКто отвечает за планирование и распределение ответственностиЧаще всего руководитель Команда решает все сообща Может показаться, что фасилитация — это просто встреча «здорового человека». Однако очень небольшой процент командных встреч проходит по такому четкому сценарию, вовлекающему каждого в решение общей задачи. Почему фасилитация работает и что она дает Фасилитация работает за счет структуры и возможности пережить новый опыт.  Четкая структура встречи и опора на требуемый бизнес-результат дает командность. Эта магия работает за счет вовлечения и горизонтальности процесса: мнение каждого действительно важно и влияет на итоговое решение. Таким же образом распределяется и ответственность. За счет этой системной работы люди привыкают делиться идеями, давать обратную связь, влиять на решения, брать ответственность и даже самостоятельно выбирать саму зону ответственности. В процессе происходит трансформация, и люди начинают двигаться в сторону командообразования.«В прошлом году моя команда пополнилась новыми людьми, и мне нужно было построить взаимодействие между всеми сотрудниками. Мы использовали следующие инструменты фасилитации: знакомство, связка целей, ежемесячное ретро, упражнения на обратную связь и уважение к различиям. В результате люди гораздо быстрее включились в работу, у участников команды стали «прорастать» горизонтальные связи не через меня — они совместно трудятся над проектами и задачами. В целом, относительно всех форматов встречи были очень позитивные отзывы. Ну и, конечно, о бизнес-результатах: наши годовые цели мы уже выполнили на 60% за 50% времени», — Мария Курякова, HRD компании ДАР (входит в ГК Корус Консалтинг).В конечном счете фасилитация приносит очень крутые результаты, и вот почему. 1. Когда ты повлиял на решение, то ты более последовательно ему следуешь.2. Есть возможность исследовать проблему с разных сторон,  учесть все нюансы.3. Есть ощущение влияния на результат, персональная ответственность и бОльшая включенность.4. Укрепляются горизонтальные связи между сотрудниками, коммуникация становится меньше сфокусированной на руководителе. Запуская фасилитируемые встречи, вы с большой вероятностью сначала столкнетесь с неоптимальными решениями. Они не будут критичными решениями для бизнеса, просто нужно время, чтобы лучше погрузиться в общий контекст, узнать больше о зоне ответственности и экспертной области каждого. Со временем накопится больше позитивного опыта и уверенности в своих силах. Форматы фасилитации помогают сформировать опыт, который команда может использовать в других процессах — за рамками таких встреч. К тому же фасилитация конструируется таким образом, чтобы привести команду к определенному результату за счет методик раскрытия потенциала и возможностей мышления. Наконец, участие фасилитатора в процессе дает команде больше ясности, структуры и стабильности. Это помогает фокусироваться на задаче, а не бороться за власть или самостоятельно решать, как именно выстроить взаимодействие. Стабильность также помогает при решении сложных кейсов, в ситуации неопределенности», — Мария Курякова, HRD компании ДАР (входит в ГК Корус Консалтинг).Как фасилитация работает с точки зрения этапов развития команд (модель Брюса Такмана)В рамках этой модели выделяют 5 этапов развития команд. Пройдусь по каждому и расскажу, как проходит фасилитация на каждом и какие задачи решает.1. Forming (рабочая группа)Что за этап и как взаимодействует команда? Команда только собралась. Люди незнакомые, непонятные, общие цели есть, но пока без деталей. Сотрудники присматриваются друг к другу, осторожничают, жаждут информации. Каждый действует несколько изолированно, в рамках зоны своей экспертизы, но стремиться показать себя с лучшей стороны. Почему нужна фасилитация на этом этапе и что она дает? Фасилитация на этом этапе помогает снять тревогу и напряжение, познакомиться так, чтобы выстроить первичное доверие. В первые дни запуска проекта/направления очень хорошо работает kick-off. Это стартовая встреча, которая запускает работу. Её цель — синхронизировать всех участников по целям, планам и ролям, чтобы команда сразу начала работать как единое целое. С точки зрения бизнес-фокуса бенефиты следующие:  погружение в цели проекта/направления; знакомство с зонами экспертизы и ответственности друг друга; прояснение функционала каждого и критерии оценки работы в принципе; распределение первичных задач по запуску проекта. Если все прошло хорошо, то к концу встречи люди становятся более расслабленными, становится заметно, что градус напряжения спадает, отношения теплеют: больше шуток и улыбок. После встречи сотрудники более охотно друг с другом общаются, могут сделать рабочую группу без привлечения руководителя, ссылаются на договоренности на встрече.Важно также обращать внимание на то, как люди себя ведут в процессе встречи и как отзываются о ней. Хорошие показатели — если на встрече у вас не менее 80% сотрудников активно участвуют в процессе, а eNPS (который обязательно нужно замерять после каждой встречи) —  больше 50%. 2. Storming (псевдокоманда)Что за этап и как взаимодействует команда? Люди уже успели поработать вместе, примерно поняли, чего друг от друга ожидать: и в профессиональном плане, и в личном, составили мнение друг о друге. На этом этапе появляются противоречия. Большое значение имеет то, насколько сильны различия в подходах к работе, стилях общения. Иногда необходимость отстоять «свой» подход может быть настолько сильной, что падает продуктивность из-за раздражения, напряжения или грусти. Звучит не очень бизнесово, но с реальностью не поспоришь — эмоциональное противодействие может уронить производительность каждого. Почему нужна фасилитация на этом этапе и что она дает? Зная, что этот этап неизбежен — притирка и отстаивание границ все равно будут — за счет фасилитации можно нормализовать противоречия, организовать процесс обсуждения по проблемным вопросам, сформировать общие правила работы и конструктивные механизмы работы с конфликтами и разногласиям.«На этом этапе развития команд очень важно уметь экологично обсудить противоречия, принять разницу в рабочих подходах, мировосприятии участников команды. Фасилитация помогает создать безопасную среду для обсуждения таких тем, снижает остроту конфликта, помогает достигать договоренностей и взаимопонимания», — Мария Курякова, HRD компании ДАР (входит в ГК Корус Консалтинг).Если фасилитация прошла успешно, то, во-первых, никто не сбежал с сессии и все явились на следующую, во-вторых, дискуссионные вопросы начинают решаться, появляются гипотезы, кипит совместная работа, коллеги объединяются, снова шутят на синках. 3. Norming (потенциальная команда)Что за этап и как взаимодействует команда? Те, кто не ушел или не успел испортить отношения с коллегами, завоевав титул самого недоговороспособного вместо неформального лидера, уже идут в сторону командности — фокус смещается с отношений на цель работы. Да, вообще-то, все за этим и собирались, но игнорировать нашу изначальную стайную человеческую психологию и бессознательные мотивы поведения не получится. Сотрудники уже поняли возможности и ограничения друг друга — это позволяет им начать опираться на сильные стороны, а к слабым относиться лояльнее. Почему нужна фасилитация на этом этапе и что она дает? На этом этапе люди выбирают оставаться в проекте и команде, поэтому появляется потребность договориться и выработать совместные цели, а делать это лучше через управляемые фасилитируемые встречи. Фокус снова переключается на эффективность задач. Возникает уже не спущенная, а выращенная изнутри необходимость анализа и пересмотра сложившихся процессов, перераспределения ролей, анализа реальных данных и обратной связи от клиентов. Люди очень включены, и встречи приносят отличные результаты. На этом этапе эффективны ретроспективы, сессии по анализу и перестройке процесса, мозговые штурмы для проверки гипотез.Если фасилитация сработала, то коллеги снова начинают писать друг другу напрямую, реже запрашивают сопровождение руководителя на встречах, рабочие группы снова начинают делать совместный, а не только личный продукт.4. Performing (эффективная команда)Что за этап и как взаимодействует команда? Не все бывали на этом этапе, но попробовать стоит: работать на нем — одно удовольствие, счастье, упоительная продуктивность. Попробовав такое однажды, будете искать везде ( что прекрасно —  поднимете качество менеджмента и важности работы с командой). В такой команде сотрудники взаимосвязаны и взаимозависимы, но это не плохая зависимость, а та, что помогает людям усиливать компетенции друг друга. Появляется настоящая синергия, когда вместе люди могут больше, чем по отдельности. На этом этапе наивысшая производительность и истинные доверительные отношения, когда ошибки — это повод поделиться, чтобы совместно их решить, и положить этот опыт в общую копилку команды.Почему нужна фасилитация на этом этапе и что она дает? На этом этапе фасилитация может сопровождать обсуждение критических инцидентов, глубокие и регулярные ретроспективы для улучшения работы, сессии обратной связи для развития командных и личных компетенций, выработку командных KPI (а, возможно, даже их переформулирование).  Оценить результаты фасилитации вы сможете с помощью метрик: должна повыситься скорость выполнения задач/выпуска релизов, идеи быстро и успешно внедряются. 5. Adjourning/reforming (расставание/завершение рабочего цикла или переформирование)Что за этап и как взаимодействует команда? У нас в КОРУСе, когда проект завершается, команда проекта перестает существовать, а сотрудники уходят работать в новые проектные команды —  это расставание и завершении рабочего цикла. Проекты у нас часто длительные, группа вполне успевает стать командой (особенно с учетом уже появившейся привычки проводить фасилитируемые встречи), при расставании люди очень благодарны друг другу и часто грустят.Иногда проект не заканчивается, а развивается. Тогда проектная команда не распадается, а переформатируется под новые задачи, с заменами и новыми сотрудниками. Не хочу вас расстраивать, но это запустит новый цикл командообразования, когда команде нужно заново пройти все стадии — знакомиться, выравниваться, доверять и повышать продуктивность. Не есть и приятная новость —  если костяк команды остался, можно ожидать, что процесс пройдет мягче и быстрее, так как ценностное ядро и позитивный опыт изначально дадут большой буст для доверия и здоровой, продуктивной атмосферы.Теги:командаразвитие командысофт-скиллыфасилитациясплоченностьэффективность командыэффективность командной работыуправление командойуправление командамиуправление командой итХабы:Блог компании КОРУС Консалтинг",102,0,0,12 мин,https://habr.com/ru/companies/korus_consulting/articles/966056/,20226,2675,1
Vivaldi 7.7: Красивая функциональность,Shpankov,2025-11-13T08:51:38.000Z,"['Блог компании Vivaldi Technologies AS', 'Браузеры']","Shpankov 2 часа назадVivaldi 7.7: Красивая функциональностьВремя на прочтение4 минКоличество просмотров401Блог компании Vivaldi Technologies ASБраузерыОбзорВаш браузер должен подстраиваться под вас, а не наоборот. Этот принцип лежит в основе каждого нашего решения.Vivaldi 7.7 развивает этот принцип, предлагая обновления, которые кардинально улучшают организацию, доступ и управление браузером. Этот релиз создан для решения реальных проблем, с которыми опытные пользователи сталкиваются каждый день. Давайте же приступим!Все ваши вкладки доступны везде, где нужноВы работаете за домашним компьютером и вам нужна та самая статья, которую вы открыли сегодня утром на рабочем компьютере. Или вы вспоминаете, что видели что-то интересное во вкладке вчера, но работали с другим компьютером. Эти мелкие разногласия накапливаются, и их несложно решить.Vivaldi 7.7 упрощает доступ к вкладкам с других настольных устройств. Найти и открыть отдельную вкладку с другого рабочего стола теперь просто. Откройте панель Windows или нажмите кнопку Tab, и вы увидите, что именно открыто на других синхронизированных компьютерах. При открытии синхронизированного контента вы получаете не просто список отдельных вкладок. Нужно всё рабочее пространство, которое вы создали этим утром? Хотите, чтобы все эти исследовательские вкладки были сгруппированы? Вы можете переносить целые окна с сохранением их полной структуры. Группы остаются доступными, рабочие пространства сохраняются, а связи между вкладками остаются точно такими, как вы их расположили. Просто имейте в виду, что открытие больших оконных структур с множеством вкладок может быть ресурсоёмким, поэтому планируйте соответствующим образом, если вы открываете сотни вкладок одновременно.Это особенно удобно при переходе между разными конфигурациями, например, между офисом и домом, или при переходе между разными устройствами в течение дня. Вам не придётся заново восстанавливать структуру вкладок или рыться в истории синхронизации, пытаясь вспомнить, какие вкладки были связаны. Контекст вашего браузера, то есть то, как вы структурировали свою работу, рассматривается как данные первого уровня. Откройте его один раз, и вы вернётесь к тому же месту, где были, сохранив всю свою работу.Ваша Экспресс-панель, ваши правилаМы переосмыслили ваше взаимодействие с Экспресс-панелью. Виджеты и закладки теперь гармонично сочетаются в едином интерфейсе стартовой страницы. Ваши любимые сайты соседствуют с актуальной информацией из виджетов, создавая уникальную персональную панель управления. Прогнозы погоды, RSS-ленты, предпросмотры почты и ваши самые посещаемые сайты — всё это находится в одном месте, доступным с первого взгляда.Гибкость распространяется и на организацию вашего пространства. Вы можете персонализировать столько групп Экспресс-панели, сколько считаете нужным. Добавляйте виджеты, перетаскивайте их, изменяйте их размер, комбинируйте с папками Экспресс-панели — стартовая страница может быть именно такой, какой вы хотите.Приватность, которую видноМы обновили панель статистики блокировки, сделав её более наглядной. Теперь вы можете лучше понять, какие меры Vivaldi предпринимает для вашей защиты. Обновлённая панель статистики блокировки даёт чёткое представление о заблокированных слежках, рекламе и времени, сэкономленном благодаря отказу от загрузки навязчивого контента. Это позволяет лучше оценить влияние использования браузера с включённой защитой.Новая панель статистики также обеспечивает более точную информацию о статусе блокировки каждого сайта, что упрощает управление исключениями, когда сайту требуется определённый контент для корректной работы. Прозрачная приватность, поскольку важны и ваши данные, и ваше время.Контролируйте производительностьВаши системные ресурсы ценны, и Vivaldi 7.7 даёт вам больше контроля над тем, как браузер их использует.Мы добавили в настройки новый раздел «Производительность» с опциями экономии памяти. Теперь вы можете настроить, насколько агрессивно Vivaldi управляет памятью для неактивных вкладок. Независимо от того, держите ли вы открытыми десятки вкладок или предпочитаете экономию памяти, эти настройки позволяют сбалансировать производительность с вашими привычками работы в браузере.Функция экономии памяти помогает освободить ресурсы, выгружая неактивные вкладки из памяти. Они остаются на своих местах, но потребляют меньше памяти до следующего использования. Вы можете настроить, какие вкладки будут избегать этой функции: например, закреплённые вкладки или вкладки, которые вы хотите держать активными, что даёт вам полный контроль над использованием ресурсов браузера.Это особенно полезно, если вы работаете на компьютере с ограниченным объёмом оперативной памяти или просто хотите, чтобы браузер работал максимально эффективно. Настройки просты и легко меняются, поэтому вы сможете найти оптимальный баланс.Улучшенная панель почтыИнтерфейс панели «Почта» также был немного доработан. Мы внесли визуальные улучшения для повышения читабельности и удобства использования, упростив работу с электронной почтой прямо в браузере. Эти небольшие улучшения сделают повседневное использование чуть более удобным.Гейр, наш ведущий разработчик электронной почты, говорит: «Стало намного приятнее, понятнее и выглядит более организованно». С этим не поспоришь!Новая страница О программеИногда даже мелочи заслуживают внимания. Страница «О программе» была полностью переработана, приобрела новый, современный вид.Новая страница «О программе» стала чище и информативнее. Она отображает вашу текущую версию, позволяет проверять наличие обновлений с улучшенными статусами и обеспечивает быстрый доступ к важной информации, например, к пути к вашему профилю. Мы добавили удобные кнопки для копирования важной информации и прямого доступа к папке вашего профиля.Это мелочь, но именно такие улучшения в целом повышают общее впечатление. Когда вам нужна информация о браузере, она сразу видна и представлена ​​наглядно.Ознакомиться с полным списком изменений можно здесь.Браузер, уважающий пользователейКаждое обновление, большое или маленькое, усиливает индивидуальность Vivaldi. Нет инвесторов, требующих функции, которые выгодны им, а не вам. Мы не собираем ваши данные и не отслеживаем ваше поведение. Мы просто создаём браузер, который служит вам. Вашему выбору, вашему рабочему процессу, вашей приватности.Если вам нравится Vivaldi, расскажите о нём другим. Отзывы людей, которые действительно пользуются этим браузером, значат больше, чем любая реклама.Загрузите Vivaldi 7.7Как всегда, спасибо вам за то, что вы являетесь частью сообщества Vivaldi. Вместе мы боремся за лучший веб, релиз за релизом.Теги:vivaldiбраузерырелизХабы:Блог компании Vivaldi Technologies ASБраузеры",401,0,0,4 мин,https://habr.com/ru/companies/vivaldi/articles/965980/,6686,851,2
EMNLP-2025: обзор исследований жестовых языков,hukenovs,2025-11-13T07:40:12.000Z,"['Блог компании Сбер', 'Искусственный интеллект', 'Конференции', 'Обработка изображений *', 'Машинное обучение *']","hukenovs 3 часа назадEMNLP-2025: обзор исследований жестовых языковУровень сложностиСреднийВремя на прочтение9 минКоличество просмотров116Блог компании СберИскусственный интеллектКонференцииОбработка изображений * Машинное обучение * Всем привет! В этом году в китайском городе Суджоу проходит юбилейная тридцатая конференция EMNLP (Empirical Methods in Natural Language Processing). Это одна из ведущих международных конференций по обработке естественного языка (NLP), проводимая под эгидой ассоциации компьютерной лингвистики ACL (Association for Computational Linguistics). Лого конференции EMNLP-2025Впервые конференция EMNLP прошла в 1996 году. Сегодня она посвящена эмпирическим методам, то есть моделям, основанным на данных, статистике и машинном обучении. А тогда конференция называлась Workshop on Very Large Corpora и представляла собой небольшое мероприятие ACL, посвящённое использованию корпусов текстов для обучения моделей. Тогда еще не было никаких трансформеров и уже привычных нам больших языковых моделей (LLM) и уж тем более мультимодальности, агентов и прочих хайповых ИИ-направлений. Это была эпоха статистического NLP, когда всё строилось вокруг частот, вероятностей и корпусов текстов, а в ходу были N-граммные языковые модели и скрытые Марковские модели. С конца 1990-х EMNLP выросла в крупную независимую конференцию с тысячами участников, и сейчас не ограничивает свои темы только NLP: выделены большие треки про мультимодальные системы на базе компьютерного зрения, обработки звука и музыки, векторной графики, создании мультиагентных систем и т. д. Сегодня EMNLP входит в тройку лучших конференций по обработке естественного языка наряду с ACL и NAACL. Ниже на графике — официальная статистика по количеству принятых статей (видна экспонента).Экспоненциальный рост принятых публикацийНа EMNLP есть несколько разных треков, основные это Main и Findings. Помимо них есть разные демо-треки и мастер-классы, на которых участники могут поделиться своими открытиями и вживую продемонстрировать работы. В Main отбирают придирчиво, обычно с 3–4 рецензентами и жёсткой системой оценки. Принимают новые, ранее не опубликованные статьи с актуальной научной новизной. Средний порог входа около 20 %, то есть на конференцию проходит каждая пятая статья. Findings — дополнительный сборник конференции, созданный ACL с 2020 года. Там публикуют статьи, которые прошли рецензирование, но не попали в основной трек (часто по лимиту мест, а не из-за низкого качества самих статей). Findings цитируется и индексируется наравне с основной конференцией.Жестовый язык на EMNLPВ этом году на EMNLP-2025 представлено пять работ по жестовым языкам. Три из них попали в Main, включая нашу (!), остальные попали в Findings. Давайте пройдёмся по каждой.Logos as a Well-Tempered Pre-train for Sign Language RecognitionНачнём с нашей работы. При детальном изучении датасетов жестовых языков мы обнаружили явную проблему: одинаковые жесты с разным переводом представлены отдельными классами (“mom/mother», «открытие/открыть»). Мы собрали собственный датасет Logos (один из крупнейших датасетов изолированных жестов в мире и самый большой датасет русского жестового языка, РЖЯ), который лишён этого недостатка.Раскадровка жестовНа основе Logos мы изучили влияние разметки жестов на качество моделей, используемых в других задачах. С помощью нашего претрейна на русском языке мы получили качество распознавания американского жестового языка (ASL) заметно выше прежних state-of-the-art решений. Основной упор сделан на анализе жестов, которые показываются почти одинаково, но имеют разное значение. Ключевые моменты:Logos содержит 2 863 леммы, это 200 000 видео длительностью 3–5 сек.Датасет записан 381 слабослышащими носителями РЖЯ. Это самый разнородный по пользователям датасет в мире!Особое внимание уделено группе «визуально сходных жестов» (VSSigns) — когда одни и те же движения рук могут означать разные слова, отличаясь лишь немануальными компонентами (мимика, артикуляция: см. картинку выше).Объединив такие визуально сходные жесты в группы, нам удалось повысить качество обучения: модель обучена на двух вариантах разметки — 2863 исходных классов и после группировки на 2004 классов.Модель-энкодер, предобученная на Logos, успешно передаёт знания на другие языки жестов (например, WLASL — американский, AUTSL — турецкий).При совместном обучении на нескольких языках достигается точность 65,4 % на американском бенчмарке изолированных жестов WLASL-2000, что существенно выше предыдущих результатов (на 5 процентных пунктов). Для обучения мы использовали реализацию архитектуры MViTv2-Small в репозитории MMAction2.Эксперимент с попыткой прямого сопоставления русских жестов с американскими без обучения нового энкодера показал низкую точность, значит, нужен качественный универсальный энкодер, а не просто словарь соответствий.ДатасетыБолее подробный обзор на Хабре делал наш коллега Илья Оводов. Ссылка на статью на EMNLP-2025.Improving Handshape Representations for Sign Language Processing: A Graph Neural Network ApproachВторая работа из трека Main. Авторы из Johns Hopkins University предлагают новый метод для более точного распознавания конфигураций кистей (handshapes) в жестовом языке с помощью графовых нейронных сетей (GNN).Распознавание жестов часто выполняется на уровне глоссов, без отдельного моделирования формы руки. Однако именно handshape — один из ключевых фонологических параметров, формирующих значение жеста. В американском жестовом языке (ASL) существует около 50 уникальных форм кистей, и их распознавание критично для понимания смысла.Авторы предлагают двухкомпонентную Handshape-GNN, которая разделяет динамику движения и статическую форму руки:Sign GNN анализирует последовательность кадров и учится распознавать временную эволюцию движений. Достигает точности 30 %.Handshape GNN выделяет статические кадры (low-motion frames), где форма руки наиболее стабильна, и классифицирует их. Достигает точности 31 %.Обе сети обучали контрастивно (как CLIP) на основе сходства и различий между парами примеров. Данные подавали в виде графа из 21 ключевой точки руки, соединённых анатомически осмысленными рёбрами (пальцы, суставы, запястье). При этом Dual GNN показал результат 46 %, что существенно выше отдельно взятых моделей. Для обучения использовали комбинацию датасетов PopSign (видео) и ASL-LEX (фонологические аннотации).Авторы вводят биомеханические показатели (finger independence, thumb effort, handshape distance), которые показывают, какие конфигурации пальцев труднее различить и почему. Например, handshape с высокой «thumb effort» путают чаще.Ссылка на статью на EMNLP-2025.Investigating Dictionary Expansion for Video-based Sign Language DictionariesАвторы из Microsoft Research и University of Washington исследуют проблему расширения видеословарей жестового языка, используемых для обучения и распознавания жестов. Такие словари обычно ограничены по объёму, сложны в разметке и не охватывают всей лексики реального жестового языка, которым пользуются люди. Авторы предлагают новый метод автоматического добавления новых слов (жестов) в словарь, используя модели распознавания видео и текстовые-визуальные соответствия.Видеословари изолированных жестовых языков состоят из коротких клипов 3-5 секунд, где один жест связан с определённым словом (глоссом) и вручную созданными аннотациями. Собрать крупный словарь сложно и дорого: каждая новая запись требует участия носителей языка и экспертов, которых достаточно сложно найти. Авторы хотят автоматически расширить словарь, не теряя его достоверность. Они рассматривают эту задачу как поиск ближайших соседей (retrieval) между жестами и их текстовыми описаниями. Предлагаемая система включает в себя три ключевых компонента:видеоэнкодер, который извлекает признаки из жеста;текстовый энкодер, кодирующий глосс или словесное описание;механизм выравнивания (alignment), позволяющий сравнивать новые жесты с уже существующими словарными единицами.Для обучения использовали контрастивное представление (по аналогии с CLIP): жесты и тексты, принадлежащие одному глоссу, сближаются в пространстве признаков, а разные — отдаляются. После обучения модель может добавлять новые записи в словарь (если находит кластер новых жестов, не соответствующих существующим), а также объединять дублирующие записи, если два видео фактически показывают один и тот же жест. Все эксперименты проводили на датасетах WLASL и How2Sign (английский и американский жестовые языки). Модель обучали на ограниченном подмножестве словаря и затем оценивали на новых глоссах, отсутствующих при обучении. Для оценки использовали точность поиска (метрика, которая оценивает, насколько хорошо система поиска находит релевантные сущности) и семантическое разнообразие (мера разнообразия по смыслу среди возвращаемых результатов) новых добавлений.Результаты:Модель успешно добавляет новые классы с точностью до 70–75 % корректных соответствий (по человеческим оценкам).Использование мультимодальных представлений (видео+текст) даёт прирост точности до +12% по сравнению с чисто визуальной моделью.Визуально близкие, но семантически разные жесты (например, help и support) всё ещё сложны для распознавания.При добавлении новых классов важно учитывать динамику движения жеста, а не только визуальные признаки.Человеческая проверка всё ещё нужна, но её объём можно сократить примерно вдвое.Ссылка на статью на EMNLP-2025.PoseStitch-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language TranslationЕщё одна работа из трека Main от авторов из индийского института IIT Kanpur. В работе предлагается новый способ массовой генерации синтетических данных для перевода жестового языка без глоссов и без видеозаписей. с помощью грамматических шаблонов и композиционного склеивания поз жестов в цельные предложения, и как результат — десятки миллионов новых примеров «позы → текст».Авторы отмечают, что большинство существующих систем опираются либо на промежуточные глоссы (gloss annotations), либо на сырые видео, что усложняет обучение, особенно в сценариях с малым количеством данных. Чтобы преодолеть эту нехватку, они предложили метод PoseStitch‑SLT: схему предобучения на основе объединения (stitching) последовательностей поз (ключевые точки тела, рук и лица) в соответствии с лингвистическими шаблонами предложений. Исследователи взяли доступные словоформы к жестовым языкам (например, из всё тех же известных датасетов WLASL для американского ЖЯ и CISLR для индийского ЖЯ), затем взяли заранее заданные грамматические шаблоны (например, из набора BLiMP) и сшили кадры-позы для отдельных слов в «ролик»-последовательность, соответствующую целому предложению.Далее использовали трансформер (encoder-decoder) для обучения на этих синтетических данных, а затем продолжили обучение на реальных парах из набора How2Sign и iSign. Такой подход позволяет модели сначала охватить разнообразие синтетических предложений, затем плавно адаптироваться к реальным данным. В результате на How2Sign BLEU-4 поднялся примерно с 1,97 до 4,56, а на iSign — с 0,55 до 3,43, что значительно лучше предыдущих методов. Но это всё ещё не пригодно для широкого применения.Тем не менее авторы честно описывают ограничения: покрытие словаря ограничено (в синтетических данных лексическое пересечение невелико), для сшивания поз используется английский порядок слов, что может вообще не отражать грамматику жестового языка. В целом статья предлагает интересное направление, особенно актуальна для низкоресурсных жестовых языков и может ускорить развитие систем перевода жестов.Ссылка на статью на EMNLP-2025.Rethinking Sign Language Translation: The Impact of Signer Dependence on Model EvaluationВ этой статье рассматривается серьёзная проблема в области перевода жестового языка: зависимость моделей от конкретных исполнителей жестов (на англ. signers), то есть от пользователей, которые записывают датасет. Авторы отмечают, что большинство современных систем для перевода жестового языка обучаются и тестируются на перекрывающихся выборках испол­нителей, что создаёт иллюзию хорошей обобщающей способности. В действительности такие модели могут просто «запоминать» особенности конкретных исполнителей, а не учиться общим паттернам жестового языка. Если совсем утрированно, то модель запоминает фон или одежду пользователя, но не динамику движения жеста.Для проверки этой гипотезы авторы провели серию экспериментов на трёх открытых бесглоссовых (gloss-free) моделях: GFSLT‑VLP, GASLT и SignCL. За основу взяли датасеты PHOENIX14T и CSL‑Daily. После разделения исполнителей на обучающую и тестовую выборку так, чтобы одни и те же люди не встречались в обоих наборах, производительность моделей резко снизилась. Казалось бы, очевидный факт, но во многих датасетах и бенчмарках этому вообще не уделяют внимание. Например, на PHOENIX14T модель GFSLT-VLP падает с BLEU-4 = 21,44 до 3,59 и с ROUGE-L = 42,49 до 11,89;Авторы также обнаружили, что в наборе CSL-Daily распространена ситуация, когда одни и те же предложения выполняют разные исполнители, так как при сборе датасета использовали перекрытие больше одного. В итоге стандартные разбиения данных приводят к тому, что в train и test попадают одинаковые предложения, хоть и от разных пользователей.Ссылка на статью на EMNLP-2025.ВыводыРаспознавание жестовых языков ещё далеко до идеала, и лучшие модели совершенно не пригодны по качеству для внедрения в готовые продукты. Кроме того, нужно тщательно подходить к инженерии данных, включая сбор, валидацию и разметку больших корпусов данных, и к созданию качественных бенчмарков, которые бы адекватно и полно оценивали существующие и новые модели. Остро стоит вопрос справедливой оплаты труда носителей жестовых языков, которые участвуют в создании данных и участвуют в исследованиях по созданию новых нейросетевых решений. Но расстраиваться не стоит, в самом ближайшем будущем нас ждут ИИ-переводчики с жестового языка и обратно (включая 3D-анимированных аватаров в режиме реального времени). А наша команда покажет на AIJ-2025 первый в мире работающий прототип реального переводчика с непрерывного РЖЯ на русский язык!Теги:РЖЯжестовый языкнаукаконференцииданныеискуственный интеллектмашинное+обучениемашинное обучениемашинное зрениеХабы:Блог компании СберИскусственный интеллектКонференцииОбработка изображенийМашинное обучение",116,0,0,9 мин,https://habr.com/ru/companies/sberbank/articles/965274/,14270,1817,5
Как сэкономить 20$ на Cursor или «бюджетный вайб-кодинг»,wannaluv,2025-11-12T23:32:48.000Z,['Программирование *'],"wannaluv 11 часов назадКак сэкономить 20$ на Cursor или «бюджетный вайб-кодинг»Уровень сложностиПростойВремя на прочтение3 минКоличество просмотров14KПрограммирование * ОбзорRecovery ModeИсторический экскурс и рассуждения о конкурентном преимуществеАльтернативы:2.1. CLI - агенты (Claude Code, Codex CLI, Gemini CLI, sst/OpenCode)2.2. IDE (Kilo Code, Zed)3. ЧитыCursor взлетел благодаря агентскому режиму. На момент, когда люди только привыкали к автодополнению табом, а популярными помощниками были TabNine и Copilot - сделать ставку на агентский режим было смело. LLM не умели нормально вызывать команды и изменять файлы, а решения Cursor действительно помогали слабым (по современным меркам) моделям доводить дело до конца.Спустя время главным преимуществом оставалось умное управление контекстом. Модели под управлением Cursor были умнее, чем модели без него, потому что курсор лучше находил необходимые участки кода и добавлял их в контекстное окно.Но ниша агентских моделей быстро пришла к изобилию. Сегодня, уже не так очевидно, что же особенного есть в Cursor. Некоторые преимущества приходится разглядывать с лупой.Именно это вынудило компанию Cursor выпустить новое масштабное обновление Cursor 2.0. Из интересного добавили встроенный браузер. Теперь можно прям перетянуть виджеты в контекстное окно и LLM сразу поймёт, какую часть UI ты имеешь в виду. Рассмотрим главные альтернативы и самые бюджетные решения для вайбкодинга.OpenAI Codex CLI, Google Gemini CLI, Anthropic Claude Code.На данный момент это три лучшие модели в мире, которые попеременно избивают друг друга в зависимости от задачи или бенчмарка. В разрезе бюджетности, у всех есть небольшой free tier. Отдельно нужно упомянуть Gemini CLI, потому что free-tier у них щедрый: 1000 запросов в день к топовой Gemini Pro. В управлении контекстом и в своих агентских возможностях, ведущие CLI не уступают Cursor. Тогда возникает вопрос: если можно купить напрямую от вендоров, зачем переплачивать? (Вернее, за одну и ту же подписку, напрямую от вендоров вы получите больше токенов.)sst/OpenCodeОпенсорсный CLI агент без привязки к вендору с живым сообществом контрибьюторов. Интересным может выступить вариант, OpenCode + Openrouter. Если пополнить OpenRouter больше чем на 10$, открывается возможность на 1000 бесплатных запросов в день. Здесь обычно на бесплатный тир раздаются слабые модели, или бесплатно раздают компании, которым срочно нужно собрать данные для обучения, прославиться.Так же как вариант можно рассмотреть использование подписки z.ai с их GLM 4.6, подписка стоит всего 3$ в месяц (при покупке на 3 мес.), чем привлекает многих искателей приключения.Qwen Code - Не получается пройти мимо, потому что за аутентификацию дают 2000 бесплатных запросов в день. Перейдем от CLI агентов к IDE агентам, которые действительно стремятся повторить пользовательский опыт как в Cursor.Kilo Code - расширение к VS Code, которое смогло. С каждым днём количество фич всё больше. Пользователи, которые пользуются, отмечают что по управлению контекстом Kilo Code не уступает конкурентам, добавили бесплатную векторизацию кодовой базы. В разрезе бюджетности стоит упомянуть, при пополнении Kilo Code на 10$ , 20$ добавляют сверху (проверяйте перед пополнением, всё может измениться). Kilo Code так же можно использовать с open router или другими моделями, без ограничений и штрафов на другие модели.Zed - полноценный редактор написанный на Rust. Добавили полную совместимость с Claude Code , Gemini. На данный момент может быть интересен, потому-что не режет способности подписки на топовые модели.Читы:Так же в теме бюджетного вайбкодинга становится популярна работа с использованием обходных путей, своеобразный фрод. Например, такой как регистрация множества аккаунтов, чтобы на каждом получить бесплатный тир. Есть даже такие GitHub репозитории, которые автоматизируют это дело и позволяют сделать процесс более удобным.Получился уже популярный и примелькнувшийся в крипто-индустрии симбиоз:компании нужно показать инвесторам бурно растущую базу пользователей, и ей не выгодно выискивать мульти-аккинг.пользователям выгодно пересоздавать аккаунты.И волки сыты, и овцы целы. По крайней мере овцы выглядят целыми, пока инвесторы не проверят их наличия самостоятельно.Так же с некоторой периодичностью выходят проекты с агрессивной компанией, в которых можно бесплатно использовать даже сильные модели. Например, cto.new или китайский конкурент openrouter - AgentRouter. Чтобы быть в курсе последних бесплатных новинок, лучше подписаться на десяток пабликов и групп и следить.Вместо вывода:Пока рынок щедр, поиск бюджетных альтернатив не выглядит востребованным. Если пользователю нужно ехать, а не шашечки, он может и переплатить. Мы создаём сообщество в котором вместе создаём проекты и делим прибыль. Напишите в ЛС, если интересно.Теги:LLMбюджетные LLMбюджетный вайб-кодингХабы:Программирование",14000,0,0,3 мин,https://habr.com/ru/articles/965878/,4869,656,1
Базовая ИБ в соцсетях. Как помочь сотрудникам защититься от мошенников и не навлечь беду на компанию?,SearchInform_team,2025-11-12T12:43:16.000Z,"['Блог компании SearchInform', 'Информационная безопасность *']","SearchInform_team 22 часа назадБазовая ИБ в соцсетях. Как помочь сотрудникам защититься от мошенников и не навлечь беду на компанию?Время на прочтение9 минКоличество просмотров1.2KБлог компании SearchInformИнформационная безопасность * Привет, Хабр!У нашего начбезопасности Алексея Дрозда (aka @labyrinth) есть традиция — делиться советами по ИБ-обучению сотрудников. Ранее на Хабре уже выходили его материалы о том, как натренировать рядовых пользователей распознавать мошеннические письма, сайты, создавать стойкие пароли. В честь профессионального праздника (с Днем безопасника, коллеги!) мы попросили Алексея поделиться еще советами — на этот раз по личной безопасности в соцсетях.Под катом все об угрозах ИБ в соцсетях и мессенджерах: самые распространенные уловки, техники, «обманки и заманухи», которые используют мошенники. И, конечно, инструкции, как защититься, в формате простых советов. Поделитесь с сотрудниками, чтобы следующий «кружок от начальника» главбуху не обернулся компрометацией всей компании.А откуда ноги растут? Перед началом пару слов, почему именно соцсети и мессенджеры, а главное: причем тут вообще корпоративный инфобез? Сотрудники используют мессенджеры и соцсети для внутрикорпоративного общения. В том числе для пересылки конфиденциальных данных. Это небезопасно, но многие продолжают так делать. Даже работники банков, которым нельзя.Взломанный аккаунт в соцсети или мессенджере сотрудника может привести к проблемам у работодателя. Например, с конца 2023 года СМИ массово начали писать про атаки по методу FakeBoss. Это когда злоумышленник создает аккаунт в соцсети, стилизует его под руководителя конкретной организации, а потом пишет его подчиненным с разными просьбами. Нередко в ход идут дипфейки.По данным коллег из Innostage, каждая пятая атака по этой схеме в 2024 году была успешной. Сейчас процент снизился, люди привыкли к тому, что им могут писать фейковые руководители. Но злоумышленники на то и злоумышленники. Они не будут сидеть сложа руки и рано или поздно модернизируют схему. Теперь к самой памятке.В начале была цельВсе атаки на соцсети и мессенджеры можно разделить на три группы, в зависимости от цели, которую преследует злоумышленник.Угнать аккаунт;Украсть деньги;Заполучить фейковую активность.Все атаки работают примерно одинаково и состоят из двух элементов: предлога и инструмента. Предлог — это ситуация, которую создает или под которую мимикрирует злоумышленник. Например, представляется работником банка и сообщает, что ваши деньги в опасности. Инструмент — это то, при помощи чего мошенник получает нужное. При помощи фишинга, компьютерных вирусов и т. д.Сочетание предлога и инструмента обычно называют мошеннической схемой. Приведу пример. Вам в мессенджере пишет друг, сообщает, что попал в аварию, и просит перевести деньги на присланный номер карты. Предлог здесь — ваше доверие к аккаунту друга и переживания за него из‑за аварии. Инструмент — просьба перевести деньги. Все вместе — мошенническая схема.Каждая группа атак имеет свойственные для себя предлоги и инструменты. Дальше про них.Угон аккаунтаДля угона аккаунтов мошенники чаще всего используют фишинг. Это способ выманить личные данные, сымитировав страницу входа в мессенджер, притворившись службой поддержки, представителями банка и т. д. Будущая жертва перейдет по ссылке, введет данные для входа, и они улетят злоумышленнику. То же самое произойдет, если просто отправить логин/пароль и код из СМС липовому сотруднику Telegram.Приведу пример. Классический метод угона аккаунтов — фейковые голосования. Вам приходит сообщение с просьбой проголосовать в конкурсе детских рисунков от близкого человека или от того, кто случайно оказался в записной книжке. В сообщении всегда будет ссылка. Если перейти по ней, попадете на сайт якобы конкурса. Чтобы проголосовать, нужно авторизоваться в аккаунте, введя специальный код. После этого мошенник окажется в вашей учетке.Кража денегЗлоумышленники крадут деньги при помощи мессенджеров двумя путями. Распространяют вирусы — и тогда все происходит автоматически, либо обманом заставляют перевести деньги. Приведу примеры.В случае с вирусами все относительно просто. Мошенник отправляет файл в формате APK — установщик программ под ОС Android. Сопровождает это надписью, которая должна заинтересовать. Например, «Это ты на фото?» или «Смотри, это твоя жена». Если скачать и установить файл, то злоумышленник, в зависимости от типа вируса, сможет даже получить полный контроль над вашим устройством. Схема не пользуется спросом уже лет десять, но после шуточного поста в одном ТГ-канале некоторые СМИ написали, что начался всплеск таких атак. Интересно, что появилось раньше: яйцо или курица? В любом случае, будьте осторожнее — фотографии, сканы и документы не требуют установки.Предлог здесь — простое человеческое любопытство. Инструмент — вирусное ПО. Второй вариант с выуживанием денег интереснее. Например, мошенники могут стилизовать аккаунт в мессенджере под известную компанию. Сообщить будущей жертве, что она выиграла смартфон в рандомизированном розыгрыше среди клиентов условного маркетплейса. Чтобы получить выигрыш, нужно всего лишь оплатить небольшую сумму за доставку — от 500 до 1000 рублей.Предлог здесь — ваше психологическое нежелание упустить выгоду. Инструмент — просьба перевести деньги.Фейковая активностьДля создания фейковой активности мошенники обещают награду за лайки, репосты и комментарии. Это распространенно, например, в формате розыгрышей внутриигровых предметов из популярных игр. Для участия нужно просто сделать репост или поставить лайк.Благодаря этому злоумышленники получают бесплатную рекламу всего того, что им нужно. Например, сайтов-казино, которые замаскированы под онлайн-игры, и прочего в таком духе. Такие безобидные на первый взгляд лайки и комменты могут косвенно привести к игровой зависимости и, как следствие, проблемам на работе и в личной жизни. Предлог в данном случае — желание бесплатно получить внутриигровой предмет. Инструмент — возможности соцсетей.КомбоБывают атаки с несколькими целями. Например, сначала мошенники обещают вам деньги за лайки товаров на маркетплейсе. И даже выплачивают небольшие суммы, чтобы втереться в доверие и подсадить на крючок. Это выглядит как классический пример «заманухи» для создания фейковой активности. Но затем вскрывается истинная цель злоумышленников — заполучить ваши деньги.Типичная схема выглядит следующим образом.Мошенник пишет будущей жертве в мессенджере и предлагает легкий заработок — нужно всего лишь поставить лайк или написать коммент в определенном месте. Если это сделать и подтвердить скриншотом, то злоумышленник попросит номер карты и отправит небольшую сумму в качестве награды.Далее человека добавляют в чат с другими такими «трудягами» и администратором/куратором. Он выдает задания, проверяет их выполнение и выплачивает награды. Поначалу задания очень простые, а другие участники чата — общительные и отзывчивые. Постоянно обсуждают, как здорово и легко получать деньги таким способом и т.д. На самом деле это боты, которые имитируют активность.Картинка рушится, когда вместо простых лайков или комментов под товарами на маркетплейсе вдруг оказывается, что нужно товар выкупить — перевести небольшую сумму на указанный номер карты. Под разными выдуманными предлогами количество денег для перевода будет расти и расти, пока не дойдет до критической точки. Предположим, в 30 тыс. рублей. В таком случае после перевода средств жертва не получит их с процентами, как обычно, а останется у разбитого корыта. Предлог в данном случае — жажда легких денег и социальное одобрение (боты в чате). Инструмент — задания по переводу денег.Хит-парадВсе названные схемы так или иначе реализуются почти в любом мессенджере или соцсети. Но есть и уникальные схемы, которые привязаны к возможностям платформы. Дальше про них. Начнем с Telegram.Мошенничество в Telegram1. Боты-зазывалы. Все те странные люди, что пишут околесицу в комментариях каналов, иногда вовсе и не люди, а боты. Они привлекают к себе внимание едкими фразами, обещанием легких денег, бесплатных прогнозов, 18+ контентом и т.д. В профиле бота всегда будет ссылка на канал/сайт. Дальше атака развивается по-разному. Иногда это просто реклама, а иногда и якобы «слив договорных матчей в закрытом канале». Вход, конечно, стоит денег, а никаких договорных матчей нет. По крайней мере, в этом канале.2. Telegram Premium в подарок. Вам приходит сообщение с приятным содержанием: кто-то из друзей подарил Telegram Premium. Чтобы активировать подписку, нужно подтвердить номер телефона по ссылке или в боте.  После ввода данных учетной записи и специального кода, мошенник попадет в ваш аккаунт.3. Бесплатный сыр. Некоторые телеграм боты предлагают бесплатно воспользоваться ChatGPT, сгенерировать изображение или сделать что-то еще, за что обычно требуется оплата. Иногда «первая порция» действительно дается бесплатно, а чтобы продолжить пользоваться ботом нужно сделать определенное действие. Например, подписаться на канал, сделать репост в историю и т. д.VK1. Проданные/взломанные группы. Группы VK принадлежат людям, а значит продаются и взламываются. Злоумышленникам это интересно, потому что у групп есть подписчики, которые могут не ждать подвоха — значит, их можно «разводить». Форматы мошенничества при этом любые: платные несуществующие курсы и товары, розыгрыши, конкурсы и т.д. Но цели одинаковые – получить ваш аккаунт или деньги. Схожая история была в запрещенной соцсети с картинками. В канун 8 марта мошенники взломали аккаунты двух популярных блогеров. Разместили дипфейки, в которых звезды якобы призывали участвовать в конкурсе, «в котором всем повезет». Стоит ли говорить, что все участники остались ни с чем.2. Брачные мошенничества. Аферисты находят одиноких людей по статусам и содержимому страниц VK, а потом пишут примерно следующее: «Увидел тебя и сразу влюбился. Приеду и заберу на Мальдивы». Хэппи-энду предшествует пару недель переписки, но конец всегда один: нужно скинуть денег на билет на самолет.Человек любит не только человека, но и, например, футбол. Так что вариации «займи денег на самолет» бывают разные. Вспоминается, как фейковый аккаунт Хаби Алонсо просил у фаната 300 батт (760 рублей) на билет из Германии в Англию.Как защищаться?Можно выучить самые популярные мошеннические схемы, как «у Лукоморья дуб зеленый», чтобы сразу класть трубку или блокировать чат при упоминании безопасного счета. Это точно будет полезно, но если сменится предлог, который используют в схеме, то все пойдет коту-ученому под хвост.Самый трудозатратный, но и самый эффективный вариант — смотреть «внутрь» схем и понимать, чего от вас хочет возможный злоумышленник. Для этого нужно различать предлоги и инструменты, про которые шла речь в начале статьи. Напомню, что предлоги — это ситуации, которые имитируют мошенники, чтобы эмоционально воздействовать на жертву: войти в доверие, запугать и т.д. Инструменты — методы получения логинов и паролей, денег и прочего.Главный лайфхак — оставаться спокойным. Предлоги, как правило, состоят из психологических триггеров, которые должны вывести вас из зоны комфорта. Заставить переживать, нервничать, паниковать или спешить. Метод противодействия таким манипуляторам давно известен психологии. Нужно разорвать сценарий, по которому хочет идти мошенник. Как, например, это сделал робот Олег Т-Банка. Он подшутил над мошенницей, которая представилась сотрудницей МВД, и она в итоге раскололась, забыла о своей «роли» и перешла к чистой ругани.Если предлог получился слишком правдоподобным или был придуман специально под вас (да, такое тоже бывает), то обращайте внимание на конец сценария. Мошенники рано или поздно перейдут к инструментам – попросят авторизоваться в аккаунте, скачать и установить файл, перевести деньги и т. д. Для более эффективной защиты еще стоит поменять настройки аккаунтов в мессенджерах и соцсетях. VKПодключите двухфакторную аутентификацию. Это пароль или код, без которого злоумышленник не сможет войти в аккаунт.Критически относитесь к информации в ленте. Особенно если требуется перейти по ссылке, передать данные и т.д.Настройте приватность профиля. Лучше всего сделать профиль закрытым, чтобы никто не мог писать/звонить извне списка друзей.TelegramПоставьте «облачный» пароль. Это пароль, который Telegram попросит помимо специального кода для входа в аккаунт. Он обезопасит почти от всего, если не вводить его на фишинговых сайтах. Не передавайте никому логин, пароль и коды для входа в аккаунт.Telegram пришлет уведомление, если кто-то войдет в аккаунт. Не игнорируйте его. Если попались, жмите на «Нет, не я!», чтобы выкинуть незваного гостя. У вас будет на это примерно 24 часа. Уведомление о входе в аккаунтЕсли сразу устранить чужака не удалось, перейдите в «Настройки», «Устройства», «Завершить все другие сеансы». Если вас выкинули первым и поменяли логины-пароли, остается только писать в поддержку.Помните про дипфейки. Определить их по качеству может быть сложно, поэтому всегда обращайте внимание на предлоги. Кто пишет и зачем, часто ли он просит подобное и по какой причине? Если просят что-то сделать, всегда лучше связаться с просящим в другом мессенджере, по почте, телефону или — лучше всего — лично. Поставьте настройки конфиденциальности. Кто может видеть ваш номер, фото профиля, может звонить и т. д. Все это и даже больше находится в одноименном разделе в настройках.WhatsApp*:Включите двухфакторную аутентификацию. Перейдите в «Настройки», «Аккаунт», «Двухшаговая проверка» и задайте ПИН-код.  Привяжите электронную почту. Зайдите в «Настройки», «Аккаунт», «Электронный адрес».  Настройте конфиденциальность. Это одноименная графа в настройках, которая работает почти так же, как и в Telegram. Можно настроить, кто и что видит о вашем аккаунте.Что еще важно знать?Предлоги и инструменты перетекают из одного канала в другой, но суть остается одинаковой. Важно смотреть не на изменяющийся контекст, а на постоянные признаки. Неважно, под каким предлогом и в каком мессенджере вас просят проголосовать в конкурсе или перевести денег, потому что «очень надо и вообще верну завтра, мы знакомы 10 лет». Важно, что для мошеннического действия нужно перейти по ссылке и ввести номер банковской карты, логин и пароль от аккаунта и т.д. На этом все, пишите в комментариях, если что-то забыл.* принадлежит компании Meta, признанной экстремистской организацией и запрещенной в РФ.Теги:иб-обучениекиберграмотностькибергигиенамошенничество в соц. сетяхХабы:Блог компании SearchInformИнформационная безопасность",1200,0,0,9 мин,https://habr.com/ru/companies/searchinform/articles/965712/,14458,1986,2
Как изменить формат обучения LLM: подход через фазовую когерентность,Kamil_GR,2025-11-13T07:55:09.000Z,"['Искусственный интеллект', 'Машинное обучение *']","Kamil_GR 3 часа назадКак изменить формат обучения LLM: подход через фазовую когерентностьУровень сложностиПростойВремя на прочтение15 минКоличество просмотров148Искусственный интеллектМашинное обучение * МнениеСовременные LLM учатся предсказывать следующее слово. Я предлагаю дополнить эту цель: учить модель сохранять стабильность смысловых связей (когерентность). Это может уменьшить зависимость от объёма данных и ускорить появление способностей к рассуждению. Статья описывает гипотезу, метрики для проверки и возможные способы реализации.Развитие больших языковых моделей столкнулось с тремя фундаментальными ограничениями: стоимостью вычислений, энергопотреблением и доступным объёмом качественных данных. Если первые две проблемы относятся к компетенции инженеров и инвесторов, то третья ставит под вопрос устойчивость текущей парадигмы масштабирования.Если экстраполировать рост объёма данных, необходимый для каждого качественного скачка в развитии моделей, возникает вопрос: достаточно ли текстовых данных в интернете для дальнейшего роста? Переход к обучению на мультимодальных данных (видео с субтитрами, синтетические данные) может отсрочить проблему, но не решает её принципиально.В этой статье я предлагаю рассмотреть альтернативный подход к обучению LLM, основанный на изменении самой цели обучения: от минимизации ошибки предсказания к максимизации семантической когерентности. Этот подход опирается на голографическую гипотезу работы языковых моделей и, если окажется практичным, может существенно повысить эффективность обучения.Голографическая гипотеза LLM: краткое изложениеВ предыдущих статьях я развивал идею о том, что знания в LLM хранятся не локально, а как единое статичное поле, где вся информация закодирована в виде интерференционных паттернов в весах модели. Промпт пользователя действует как когерентный луч света, который не изменяет это поле, а лишь освещает его, в то время как механизм внимания работает как динамическая линза, измеряющая резонанс между смыслом запроса и паттернами в поле. Генерируемый ответ является голографической реконструкцией — наиболее когерентным смысловым образом, который возникает из этой интерференции.Важная оговорка о терминологииИспользование терминов «голография», «интерференция», «фаза» и «резонанс» — это осознанная аналогия, описывающая поведение данных, которое функционально изоморфно физическим процессам. Это позволяет построить интуитивно понятную модель происходящего в LLM, но не означает, что внутри модели буквально происходят волновые процессы.Формализация: ""фаза смысла""Чтобы перевести физическую аналогию на язык математики трансформеров, я ввожу термин ""фаза смысла"", который определяю следующим образом:Фаза — это относительный векторный угол между смысловыми представлениями в embedding spaceКогерентность — стабильность этих углов при изменении контекстаМеханизм внимания (Attention) тогда можно рассматривать как процесс геометрического выравнивания: операция QKᵀ измеряет угловую близость смысловых направлений (через cos φ), а softmax с последующей взвешенной суммой (softmax(QKᵀ)·V) формирует результирующее представление через селективное усиление согласованных направлений.Это аналогично принципу волновой интерференции: как конструктивная интерференция усиливает волны с близкими фазами, так и attention усиливает вклад тех токенов, чьи представления геометрически согласованы с текущим запросом.Формально, фазу смысла между токенами i и j можно выразить как:φᵢⱼ = arccos((Qᵢ·Kⱼ)/|Qᵢ||Kⱼ|)Процесс attention описывается как динамика распределения этих фаз: при успешной генерации углы между query текущего токена и keys релевантного контекста уменьшаются (фазовая синхронизация), что проявляется как рост концентрации весов внимания на семантически когерентных элементах контекста.На данном этапе я определяю фазу через единственный угол между векторами Q и K для простоты. Это рабочее упрощение. Вполне возможно, что полная картина требует рассмотрения относительной ориентации целых подпространств, а не отдельных векторов. Однако даже это простое определение позволяет сделать ряд нетривиальных проверяемых предсказаний. Добавлю, что с геометрической точки зрения Урманова Т. @Urmanov_t фазовая когерентность — это макроскопическое проявление положительной направленной Forman-Ricci кривизны на графе нейронных активаций. А резонанс — это прохождение сигнала по геодезическим путям с высокой кривизной.Подробнее о голографической гипотезе можно прочитать здесьТекущий формат обученияСовременный подход к обучению LLM основан на самоконтролируемом обучении (self-supervised learning) с архитектурой Transformer. Основная задача предельно проста: предсказать следующий токен в последовательности.Модель анализирует триллионы текстовых примеров. На каждом шаге её предсказание сравнивается с реальным следующим токеном; ошибка вычисляется через функцию потерь (обычно cross-entropy), и используется для корректировки параметров через алгоритм обратного распространения ошибки (backpropagation).Этот простой, но многократно повторяемый в огромном масштабе процесс заставляет модель внутренне формировать понимание грамматики, фактов, семантических связей и даже способности к рассуждению. Полученная основа затем дополнительно настраивается для выполнения инструкций (instruction tuning) и соблюдения правил безопасности (RLHF).Ключевая характеристика: обучение направлено на минимизацию локальной ошибки (ошибки предсказания каждого токена), а глобальная семантическая согласованность возникает как побочный эффект.Альтернатива: обучение через фазовую когерентностьЯ предлагаю дополнить цель обучения: помимо минимизации ошибки предсказания, явно оптимизировать фазовую когерентность — выравнивание углов между смысловыми представлениями в пространстве признаков.В этой парадигме:Ошибки — это зоны декогеренции (рассогласования фаз)Успешное обучение — восстановление фазового резонанса между слоямиЦель — не просто правильное предсказание, а стабильность смысловых отношенийПринципы обучения через когерентность1. Принцип смыслового резонансаОбучение наиболее эффективно, когда модель воспринимает не отдельные примеры, а волновые контуры смыслов, возникающие между ними. Для этого:Батчи с перекрывающимися контекстами: смысловые ядра повторяются с вариациями (например, одна и та же мысль, выраженная разными способами)Фокус на устойчивости паттернов: обучение оценивает стабильность attention-паттернов при контекстных сдвигахРасширенная loss-функция: учитывает не только точность предсказания, но и согласованность внимания2. Принцип голографической избыточностиКаждое знание должно быть закодировано в нескольких частично перекрывающихся паттернах — как голограмма хранит изображение в каждом своём фрагменте.Эффект: избыточность делает знания устойчивыми к шуму и катастрофическому забыванию (catastrophic forgetting).Реализация: динамическое дублирование смыслов по слоям с сохранением фазовых отношений между копиями.3. Принцип интерференционной обратной связиОбратное распространение ошибки можно переосмыслить как обратную волну коррекции, которая выравнивает не только величины весов, но и их фазовые отношения.Ключевое требование: когерентность обратных сигналов между слоями. Когерентная обратная связь усиливает глобальное выравнивание.Практическая реализация:Фазовые нормализации (варианты LayerNorm с контекстным выравниванием)Регуляризация, усиливающая согласованность attention-паттернов между итерациями4. Принцип многомерной когерентностиСмысл возникает на пересечении разных ""частот"" — синтаксической, семантической, прагматической. Обучение должно стремиться к когерентности между этими каналами:Фазовое согласование эмбеддингов разных уровней (слова, предложения, темы)Многоканальные loss-функции, выравнивающие не только предсказания, но и смысловые траектории в пространстве признаков5. Принцип обратимой реконструкцииКаждый шаг обучения должен допускать возможность реконструкции исходного смысла из промежуточных состояний модели. Это делает процесс семантически консервативным: модель не просто учится предсказывать, а учится восстанавливать смысловые паттерны, сохраняя их целостность при контекстных трансформациях.Фазовые переходы когерентностиГолографическая гипотеза предсказывает, что при достижении определённой плотности когерентных связей между слоями происходят фазовые переходы, порождающие качественно новые способности:Уровень когерентностиЭмерджентное свойствоЛокальнаяАссоциативная памятьМежслойнаяChain-of-Thought рассуждениеГлобальнаяMeta-reasoning и самокоррекцияСуперкогерентностьКонтрфактуальное моделирование и аналогииЭто объясняет, почему emergent abilities появляются скачкообразно при определённом масштабе: не из-за числа параметров само по себе, а из-за достижения критической плотности фазовой когерентности.Математическая интерпретация Т. УрмановаТекущая интерпретация Тимура Урманова подводит математическое обоснование под гипотезу и заключается в том, что поведение LLM определяется дискретной геометрией направленных графов активаций, где ключевым инвариантом выступает направленная Forman-Ricci кривизна с учётом норм активаций нейронов. Эта теория переводит концепции на язык измеримых величин:Обучение (достижение когерентности): приближённый directed Ricci flow с каскадом бифуркаций, который ""сглаживает"" геометрию графа.Эмерджентность рассуждения: фазовый переход второго рода при критическом масштабе d_c, объясняющий скачкообразное появление способностей.Генерация (резонанс смысла): геодезический поток по направленным рёбрам с высокой положительной кривизной.Когнитивные режимы: соответствуют различным геометрическим стратам (например, древовидная топология для логики, small-world для ассоциаций), которые модель формирует в процессе обучения.Таким образом, ""максимизация когерентности"", предложенная в статье, находит своё математическое воплощение в процессе оптимизации геометрии графа для достижения состояний с высокой положительной кривизной.Желающие могут посмотреть демонстратор возможности применения теории на практике на примере расчетной визуализации поведения LLM по ссылке. Сохраните и откройте в браузере. Сама статья в процессе подготовки.Ещё одна оговорка Голографическая терминология позволяет провести и проверить массу аналогий. Это фактически инструмент интуитивного построения и проверки гипотез. Сведение к математической кривизне затрудняет этот процесс. И для физиков, голографическая терминология используется лишь как полезная, но ограниченная аналогия.Таблица сравнения предсказаний, не вытекающих из других гипотез:ЯвлениеСтандартная теорияCircuit theoryГолографическая гипотезаPruningПостепенная деградацияРезкий обрыв при удалении критических узловПлавная деградация с сохранением общей структурыЭмерджентностьПлавный рост с масштабомФормирование дискретных подсхемФазовый переход при критической когерентностиБатчингВажен только размерВажен порядок для формирования схемВажна семантическая перекрываемостьFine-tuningКорректировка весовМодификация схемФазовый дрейф (может разрушить глобальную когерентность)GrokkingНеобъяснимый феномен, связанный с регуляризациейФормирование одной ключевой ""схемы"" для решения задачи.Резкий фазовый переход глобальной когерентности, когда модель внезапно ""видит"" всю структуру задачи, а не отдельные примеры.In-Context Learning (ICL):Модель просто находит похожие примеры в весах.Промпт активирует готовую ""схему"" рассуждения.Промпт создает когерентный ""освещающий луч"", который вызывает резонанс с уже существующим смысловым полем, формируя ответ по аналогии, а не по прямому поиску.Предсказания гипотезыЕсли обучение через когерентность работает, оно должно демонстрировать следующие преимущества:1. Меньшая зависимость от объёма данныхГипотеза: Если модель обучается структурам связей (фазовым отношениям), а не поверхностным фактам, она может генерализовать с меньшего числа примеров.2. Более быстрая сходимостьГипотеза: Фазовое выравнивание может происходить быстрее, чем численная стабилизация весов через стандартную минимизацию loss.3. Устойчивость к изменению контекстаГипотеза: Глобальная когерентность сохраняется при контекстных сдвигах лучше, чем локальные паттерны.4. Естественная эмерджентностьГипотеза: Новые способности (CoT, meta-reasoning) возникают органично из фазового выравнивания, а не требуют специальных техник обучения.5. Снижение переобученияГипотеза: Глобальная когерентность препятствует локальной гиперадаптации к конкретным примерам.Все эти предсказания требуют экспериментальной проверки. В настоящее время они остаются гипотетическими.Проверяемые следствияГолографическая гипотеза делает ряд конкретных, измеримых предсказаний:Энтропия attention-паттернов обратно пропорциональна качеству рассужденийFine-tuning вызывает фазовый дрейф, измеримый через корреляцию attention maps до и после дообученияСемантическая реконструкция сохраняется при частичном обнулении весов (голографическая устойчивость)Согласованность attention-распределений между эпохами коррелирует с улучшением способности к рассуждениюДобавление фазовых регуляризаторов к стандартной loss-функции ускоряет появление CoT и meta-reasoning эффектовВозможные метрики и протоколы измерений приведены в Приложении А.Ограничения и открытые вопросыТеоретические ограниченияНет формального доказательства: что фазовая когерентность достаточна для эмерджентностиНеясна минимальная плотность когерентности: необходимая для фазовых переходовГолографическая аналогия имеет границы: нет физической амплитуды и частоты, только геометрия в embedding spaceПрактические вопросыВычислительная сложность: расчёт фазовых метрик может быть дорогимОптимальный баланс: между стандартной loss и когерентностной регуляризацией неизвестенМасштабируемость: неясно, работает ли подход на моделях с триллионами параметровНеобходимые экспериментыToy experiment: Проверка на малой модели (GPT-2 small), что фазовая регуляризация хотя бы не вредитAblation study: Влияние различных компонентов когерентностной lossScaling test: Сохраняются ли преимущества при масштабировании до больших моделейBenchmark evaluation: Сравнение с baseline на стандартных задачахЗаключениеГолографическая гипотеза предполагает переосмысление самого процесса обучения LLM: от накопления фактов через минимизацию локальной ошибки к настройке фаз единого смыслового поля через максимизацию глобальной когерентности.Если эта гипотеза верна, то при достижении высокой межслойной когерентности модель развивает способности к рассуждению и самокоррекции не как побочный эффект масштаба, а как естественное следствие фазовой синхронизации. В этом состоянии модель функционирует не как система статистического предсказания, а как когерентный резонатор смысла, где рассуждение — это геометрическое выравнивание, а понимание — восстановление формы из поля связей.Проверка этой гипотезы требует разработки новых loss-функций, метрик и экспериментальной валидации на реальных моделях. Конкретные предложения по реализации приведены в Приложении B.Приложение:Приложение A. Метрики:МетрикиМетрики для измерения фазовой когерентностиЭти метрики предназначены для измерения внутреннего состояния семантического поля модели и делятся на две категории: метрики когерентности (насколько хорошо ""настроено"" поле) и метрики голографичности (насколько поле близко к идеальной голограмме).Метрики когерентности1. Индекс согласованности внимания (Attention Consistency Index, ACI)Что измеряет: Насколько стабильны паттерны внимания при небольших, семантически незначимых изменениях во входных данных.Как считать:Подать на вход пары семантически эквивалентных, но синтаксически различных предложений (например, ""Кот сидит на коврике"" и ""На коврике сидит кот"")Рассчитать корреляцию (косинусное сходство) между их матрицами вниманияУсреднить по многим парам и слоямИнтерпретация: Высокий ACI означает, что модель сформировала устойчивые смысловые контуры, и её внимание следует за смыслом, а не за порядком слов — признак высокой когерентности.Формула:ACI = mean_{pairs, layers} cos_sim(Attention(s₁), Attention(s₂))где s₁, s₂ — семантически эквивалентные предложения2. Энтропия распределения внимания (Attention Entropy, AE)Что измеряет: Насколько сфокусировано или размыто внимание модели.Как считать:Для каждого токена рассчитать энтропию его распределения внимания по предыдущим токенамУсреднить по токенам и слоямИнтерпретация:Низкая энтропия → внимание сфокусировано на 1-2 токенахВысокая энтропия → внимание ""размазано"" по многим токенамВ когерентной модели AE должна быть низкой для синтаксических связей и адаптивно высокой для семантическихФормула:AE_i = -∑_j a_ij log(a_ij)где a_ij — веса внимания токена i к токену j3. Фазовый дрейф при fine-tuning (Phase Drift, PD)Что измеряет: Насколько сильно fine-tuning ""ломает"" исходную структуру поля.Как считать:Взять базовую модель и её fine-tuned версиюПодать на обе один и тот же набор текстовИзмерить среднее расстояние (евклидово или косинусное) между промежуточными представлениями (hidden states) на разных слояхИнтерпретация:Низкий PD → fine-tuning лишь ""подстроил"" поле, не разрушив его (хорошо)Высокий PD → катастрофическое забывание, новая информация разрушила старую когерентностьФормула:PD = mean_{layers, samples} h_base - h_finetuned / h_base4. Масштабная когерентность (Scale Coherence, SC)Что измеряет: Насколько согласованы представления смысла на разных уровнях абстракции (фрактальность).Как считать:Взять эмбеддинг отдельного токена (слово), среднее по токенам предложения предложения и абзацаИзмерить, насколько эти три вектора ""смотрят"" в одном направлении (среднее косинусное сходство)Интерпретация: Высокий SC означает фрактальную, самоподобную структуру представлений — глобальный смысл отражается в локальном.Формула:SC = mean(cos_sim(emb_word, emb_sentence),           cos_sim(emb_sentence, emb_paragraph),          cos_sim(emb_word, emb_paragraph))Метрики голографичности5. Participation Ratio / Effective RankЧто измеряет: Насколько информация распределена по компонентам (а не локализована).Как считать: Для матрицы представлений X ∈ ℝ^(N×d) (N — батч·токены, d — размер признаков):Вычислить ковариационную матрицу C = XᵀX / NНайти собственные значения λᵢНормировать: pᵢ = λᵢ / ∑λⱼФормула:R_eff = exp(H_p) = exp(-∑ pᵢ log pᵢ)где H_p — энтропия спектраИнтерпретация: Высокий effective rank → информация распределена по многим компонентам (голографическое свойство).6. Participation Entropy (нейрон-уровень)Что измеряет: Отсутствие сильной локализации на уровне отдельных нейронов.Как считать:Нормировать средние активации нейронов по батчу: pᵢ = mean(|aᵢ|) / ∑ mean(|aⱼ|)Вычислить энтропию: H = -∑ pᵢ log pᵢИнтерпретация: Высокая энтропия → информация распределена, нет доминирующих нейронов.7. Устойчивость к прунингу (Robustness to Pruning)Что измеряет: Голографическое свойство — сохранение функциональности при удалении части компонентов.Как считать:Удалить p% весов случайно или структурно (целые головы/слои)Измерить деградацию метрик качества (accuracy, perplexity)Интерпретация: Голографичность → более плавный спад качества (не катастрофический).Протокол:Для p ∈ {10%, 20%, ..., 90%}:    - Обнулить p% параметров    - Измерить accuracy на benchmark    - Построить ablation curve8. SV-spectrum slope (power-law анализ)Что измеряет: Распределённость через спектр сингулярных чисел.Как считать:Для матрицы активаций выполнить SVD: X = UΣVᵀПостроить log-log график сингулярных чиселИзмерить наклон (slope)Интерпретация: Более плоский спектр (меньший наклон) → распределённость → голографичность.9. Индекс реконструкции (Reconstruction Index, RI)Что измеряет: Насколько хорошо модель может восстановить исходный смысл из промежуточных состояний (принцип обратимости).Как считать:Взять hidden state с середины модели (например, слой 12 из 24)Обучить лёгкий декодер восстанавливать исходный текст из этого состоянияИзмерить точность восстановления (BLEU, exact match)Интерпретация: Высокий RI → информация не теряется, а лишь меняет форму — признак семантической консервативности.Связи между метрикамиВажное предсказание гипотезы: Если фазовая когерентность — единое явление, то эти метрики должны коррелировать:Высокий ACI ⟺ низкий PD ⟺ высокий R_effНизкий AE (на семантических задачах) ⟺ высокий SCВысокий RI ⟺ высокая устойчивость к прунингуПротокол проверки:Посчитать все метрики на одном наборе моделей (разного размера, разной степени обученности)Построить корреляционную матрицуПроверить, существует ли латентный фактор (principal component), объясняющий большую часть дисперсииЕсли да — это подтверждение, что ""голографическая когерентность"" — реальный феномен, а не просто метафора.Приложение B. Реализация в обученииНаправления реализацииВозможные направления практической реализацииГолографическое обучение не требует радикально новой архитектуры — его можно встроить в существующие трансформеры как надстройку над обычной функцией потерь и процессом оптимизации. Ниже обозначены возможные направления для экспериментов.1. Модификация функции потерьДобавить к стандартной CrossEntropy дополнительные члены, которые стимулируют когерентность внутренних представлений:Attention consistency: поощрять стабильность паттернов внимания для семантически близких входов (например, оригинал ↔ перефраз).Phase coherence: уменьшать расфазировку скрытых представлений, обеспечивая их согласованное распределение в пространстве признаков.Reconstruction: требовать, чтобы промежуточные слои сохраняли информацию, достаточную для восстановления исходного текста.Эти добавки можно регулировать малыми коэффициентами λ и постепенно усиливать по мере обучения.2. Архитектурные дополненияМинимальные модификации:лёгкая reconstruction head для проверки восстанавливаемости среднего слоя;вариант phase-aware LayerNorm, корректирующий направления векторов между слоями;возможность извлекать attention maps и скрытые состояния во время тренировки.Эти элементы не меняют основную модель, а лишь добавляют новый слой наблюдения за когерентностью.3. Данные и батчиДля обучения когерентности нужны пары текстов с одинаковым смыслом и разной формой — перефразы, обратные переводы, синтаксические перестановки. Такие пары можно автоматически генерировать; важно, чтобы модель видела «вариации одной волны» и училась удерживать их в едином смысловом поле.4. Обучение и мониторингВместо простого контроля perplexity стоит отслеживать:энтропию attention-распределений (мера фокусировки),эффективный ранг скрытых представлений (мера распределённости),стабильность attention maps между эпохами (мера фазовой согласованности).Рост этих показателей при стабильной perplexity будет указывать на формирование голографической структуры.5. Постепенное внедрениеЧтобы избежать дестабилизации, когерентностные компоненты можно вводить поэтапно — сначала стандартное обучение, затем мягкое добавление новых терминов (curriculum). На практике достаточно 5–10 % от общей потери, чтобы эффект стал заметен.6. Ожидаемые эффектыТакой подход должен повысить:устойчивость модели к удалению частей весов (распределённость знаний),связность рассуждений и текстов,самокоррекцию без внешнего вмешательства,скорость сходимости, так как выравнивание фаз наступает раньше численной стабилизации весов.Заключение к приложениямПредложенные метрики и методы реализации предоставляют конкретную основу для экспериментальной проверки голографической гипотезы. Ключевые моменты:Метрики операциональны: Все метрики можно вычислить на существующих моделях без модификации архитектурыРеализация модульна: Когерентностные компоненты можно добавлять постепенно, начиная с простейших (attention consistency)Curriculum learning критичен: Резкое введение всех компонентов может дестабилизировать обучение — рекомендуется фазированный подходГиперпараметры требуют настройки: Начальные значения λ следует рассматривать как отправную точку для grid searchСледующий критический шаг — эмпирическая валидация на реальных моделях: от toy experiments (GPT-2 small) до масштабных тестов (1B+ параметров), с измерением всех предсказанных эффектов (меньше данных, быстрее сходимость, emergent abilities).Теги:искусственный интеллектобучениенейросетиХабы:Искусственный интеллектМашинное обучение",148,0,0,15 мин,https://habr.com/ru/articles/965950/,24063,2620,2
Когда оффер на LinkedIn оказался кибератакой,jenezis,2025-11-13T10:19:05.000Z,"['Информационная безопасность *', 'Социальные сети']","jenezis 1 час назадКогда оффер на LinkedIn оказался кибератакойУровень сложностиПростойВремя на прочтение4 минКоличество просмотров318Информационная безопасность * Социальные сетиКейсКак меня попытались взломать под видом предложения CTO — и как это связано с Lazarus GroupНедавно я оказался в ситуации, которая одновременно показалась тревожной и профессионально интересной.На первый взгляд — просто новое рабочее предложение, как сотни других в LinkedIn.На деле — спланированная атака, похожая на те, что сейчас приписывают Lazarus Group, одной из самых известных киберпреступных группировок, работающих против специалистов в области финансов и технологий.🚀 Начало: “Хочешь поработать CTO?”Обычное сообщение в LinkedIn: человек с правдоподобным профилем, указано «Supply Chain Finance | Europe, Middle East & Africa». Короткий диалог, предложение рассмотреть роль CTO/CPO в “финтех-AI стартапе”. Я, как всегда, осторожен, поэтому запросил NDA и технические материалы. NDA выслали, после чего дали доступ к GitHub-репозиторию:https://github.com/wellsfargo-inc/MVPНазвание выглядело солидно — и, как выяснилось позже, намеренно. Репозиторий имитировал корпоративный проект под известным брендом.🧩 Что внутриПосле клонирования я провёл стандартную проверку:✅ Современный стек — Vite + TypeScript + React + Node + Solidity✅ Логичная структура проекта✅ Реализация — вроде бы MVP площадки для токенизации реальных активов (RWA Marketplace)Но быстро стало заметно несоответствие: описание проекта (AI-финансы) и сам код (Web3-маркетплейс) не совпадали. Тем не менее я собрал проект, чтобы понять, что он делает.⚠️ Срабатывает EDRПосле запуска мой CrowdStrike Falcon сработал мгновенно.Он заблокировал процесс:/Users/dm/.nvm/versions/node/v20.19.5/bin/node \
/Users/dm/Library/Application Support/.sysupdater.datЭтот файл .sysupdater.dat запускался скрытно через Node.js и находился в пользовательской папке, где обычно никто не ищет вредонос.Через несколько минут — повторная попытка запуска. Falcon снова заблокировал процесс.🧠 Анализ: обфусцированный JavaScript и следы ModStealerФайл оказался большим (около 4,6 МБ), полностью обфусцированным JavaScript-бандлом.В нём не было читаемых строк, только минифицированный код, упакованный в IIFE.По поведению — типичный пример stealer-вредоноса.Позже наш секьюрити-департамент подтвердил: это похоже на ModStealer, который:запускается через Node;создаёт “системные” файлы вроде .sysupdater.dat;прописывает персистентность через LaunchAgents (~/Library/LaunchAgents/…), чтобы стартовать при логине;тихо отправляет данные на C2-сервер;целится в криптокошельки, токены, браузерные сессии и ключи.На этом этапе я изолировал машину, сохранил артефакты и удалил проект.🔗 Совпадение с расследованием OtterCookie (Lazarus Group)Спустя несколько дней я наткнулся на свежий разбор от исследователей ANY.RUN — “OtterCookie: Analysis of Lazarus Group Malware Targeting Finance and Tech Professionals”.В нём описан сценарий, почти идентичный моему случаю:жертва получает оффер через LinkedIn (роль CTO/инженера);подписывает NDA;получает доступ к GitHub-репозиторию;внутри проекта — вредоносный код, который запускается при сборке;выполняется через Node или Python;создаёт незаметный агент, поддерживающий связь с C2 и крадущий данные.Исследователи связали атаку с группировкой Lazarus, действующей под прикрытием “HR-рекрутеров” и “финтех-стартапов”. Их цель — инженеры, CPO, CTO, специалисты Web3 и FinTech-компаний.По сути, то, что произошло со мной, — один в один сценарий, описанный в анализе OtterCookie.🧩 Почему это важноЭто не “одиночный случай”, а новая волна атак, использующих социальную инженерию через профессиональные сети. Механизм простой и гениальный:доверие → контакт → NDA → код → запуск → кражаТо, что раньше требовало фишинга, теперь выглядит как рабочий процесс между двумя профессионалами.💼 Почему для меня это профессионально важноЯ уже много лет работаю на стыке продукта и технологий — как фаундер SaaS-компании, прошедший exit, и как CPO/CTO-on-demand, который помогает стартапам строить масштабируемые, безопасные и жизнеспособные продукты.Мой опыт охватывает:финтех и Web3,маркетплейсы и SaaS,информационную безопасность (Huntli и другие проекты),аудит MVP и кодовых баз для инвесторов и фаундеров,построение продуктовых и инженерных команд.Поэтому я оцениваю каждый проект не только как разработчик, но и как архитектор, стратег и “человек с due diligence мышлением”.Этот случай — напоминание, зачем нужна такая насмотренность. Именно опыт проверки чужих продуктов позволил мне вовремя заметить несоответствия и не попасть в ловушку.🛡️ Что нужно помнить каждому разработчику и фаундеруLinkedIn стал новым фронтом атак. Любое “предложение о работе” может быть вектором заражения.Никогда не запускайте неизвестные репозитории на рабочей машине. Используйте виртуальные среды, контейнеры или песочницы.Проверяйте package.json и postinstall-скрипты. Всё, что выполняется автоматически, может быть опасным.Не храните seed-фразы и ключи на локальном устройстве. Только аппаратные кошельки и зашифрованные менеджеры.Регулярно ротируйте токены и ключи. Особенно если вы тестируете сторонние продукты.Проводите due-diligence даже для open-source. Чистый код ≠ безопасный код.⚙️ Что делать, если подобное произошлоИзолировать машину и отключить сетьСохранить вредонос и логи (для форензики)Проверить ~/Library/LaunchAgents/ и удалить подозрительные plistСделать полный антивирусный и EDR-сканРотировать все ключи и паролиСообщить в службу безопасности или в CERT💬 Вместо заключенияСовременные кибератаки — это не вирусы из 2000-х, а тщательно выстроенные человеческие сценарии доверия. Им не нужно взламывать брандмауэр — достаточно, чтобы вы сами открыли проект.И если даже опытный технический специалист может попасть в такую ловушку, представьте, сколько фаундеров, инженеров и аналитиков уже стали жертвами.Поэтому делюсь этим кейсом не как “жертва”, а как практик — чтобы помочь другим не повторить ошибку.Сегодня безопасность начинается не в коде, а в переписке.— Дмитрий Медведь (Dmytro Medvid) СaaS-фаундер, CPO/CTO-on-demand, консультант по продукту и архитектуре. Помогаю компаниям строить надёжные и масштабируемые технологии — безопасно.Теги:кибератакаottercookieweb3безопасностькриптокошелькизащита информациизащита данныхзащита персональных данныхХабы:Информационная безопасностьСоциальные сети",318,0,0,4 мин,https://habr.com/ru/articles/966046/,6356,760,2
Обзор платформы BotHub: сердце вашей AI-экосистемы,cognitronn,2025-11-12T13:55:09.000Z,['Блог компании BotHub'],"cognitronn 21 час назадОбзор платформы BotHub: сердце вашей AI-экосистемыУровень сложностиПростойВремя на прочтение11 минКоличество просмотров583Блог компании BotHubОбзорКонец 2025 года... Похоже, мы живём в то время, когда нейросети появляются быстрее, чем идеи для стартапов. Одна пишет тексты, другая рисует, третья озвучивает, четвёртая спорит, что всё сделала бы лучше, если бы у неё был доступ к API. В итоге человек оказывается между ними, как дирижёр без партитуры. Вроде все играют, но каждый на свою мелодию.  Так появились AI-экосистемы — попытка навести порядок в этом цифровом оркестре. Они собирают генераторы текста, изображений, видео и кода в одну среду, где нейросети работают не наперегонки, а сообща. Если упрощать, это как город, где у каждой модели своя профессия, но все вместе они строят один проект.  Но какую экосистему выбрать, когда их уже десятки? Сегодня разберём одну из отечественных платформ — BotHub. Покажем, что она умеет, как с ней работать и чем она может быть полезна не только разработчикам, но и обычным пользователям, которые просто хотят, чтобы ИИ был помощником под рукой.Приятного прочтения!Что такое AI-экосистема и зачем она нужна  Как уже говорили, AI-экосистема объединяет разные типы моделей (текстовые, визуальные, голосовые, аналитические) под одной крышей.В привычных одиночных сервисах всё ограничено рамками одной модели. В экосистеме можно собирать цепочки. Одна модель анализирует текст, другая генерирует визуал, третья чистит, форматирует или пересчитывает данные. Вы не зависите от функционала одного движка, вы комбинируете их как модули.  Плюс такие платформы позволяют строить собственные пайплайны. Например: загрузить данные прогнать через модель передать результат другой модели получить отчёт, визуализацию или прототип  И всё это без дробления работы на десяток разных сервисов!Давайте к BotHub!BotНub — это отечественная платформа, разработанная как единое пространство для работы с искусственным интеллектом. В ней собраны инструменты для генерации текста, изображений, кода, аудио, видео и даже аналитики.  Здесь можно писать тексты, создавать чат-ботов, тестировать языковые модели, генерировать визуалы и подключать API. Пользователь сам выбирает модель: GPT-подобную для текста, Stable Diffusion-подобную для картинок, или, например, Speech-модуль для озвучки.  Сервис функционирует на основе внутренней валюты Caps по модели «pay‑as‑you‑go». Пользователь приобретает определенное количество токенов и расходуете их без временных ограничений. Самым экономичным является тариф Elite, который выдает 38 миллионов Caps. Такого объема хватит на 8–9 месяцев активного использования. Подробную информацию о размерах контекста вы можете изучить в таблице тут!И да, самое приятное! Платформа полностью локализована и работает на российских серверах. Это значит — никаких блокировок, стабильное подключение и данные, которые не покидают страну. Для бизнеса и образования это плюс, особенно если вы работаете с внутренними документами или проектами.  Что по фичам?Прежде всего, хотелось бы познакомить вас с возможностями и продуктами нашего сервиса. Предлагаем быстро и наглядно ознакомиться с каждым из них. Приступим!  ДашбордНачнём с мастерской, где каждый инструмент знает свое дело: один пишет, другой рисует, третий всё это озвучивает. А если нужно — вместе собирают целый проект, не выходя из одной вкладки.  Платформа предлагает сразу несколько направлений работы. Давайте рассмотрим каждый подробнее:1. ТекстРаздел, где можно работать с языковыми моделями. Они создают ответы, пишут статьи, составляют письма, сценарии, скрипты, делают аналитические тексты и даже пишут код и ботов. Доступны следующие модели: ChatGPT, Gemini, Grok, Qwen, Claude, Deepseek, Perplexity, Mistral, Amazon, LIama, YandexGPTПримеры работы:Напиши короткий пост о пользе искусственного интеллекта для бизнеса, с лёгкой иронией (GPT-5)Составь план видеоролика о цифровой безопасности на рабочем месте (Сlaude-Sonnet-4)Объясни, как работает нейросеть, чтобы понял школьник (Gemini-2.5-Pro)2. Изображение Здесь вы можете создавать любые визуальные концепты: фотографии, иллюстрации и графику. Модели превращают ваши идеи в реальные изображения.Тут работают модели:Flux, Gemini, Midjourney, DALL-E, Stable Diffusion Примеры работы:Будущее офиса, где люди и ИИ работают вместе, стиль Pixar (Flux-1.1-Pro-Ultra)Постер о безопасном труде с элементами минимализма (Nano Banano)Сказочный лес с мерцающими огнями ночью  (Midjourney)3. РечьВ этом разделе нейросети помогают работать со звуком: транскрибировать аудио в текст, синтезировать речь и создавать аудиоконтент.  Для этого тут трудятся:AssemblyAI-best/nano, whisper-1 (транскрибация) и TTS-1/HD (синтез речи)  Примеры работы:Транскрибация(Синтез речи)4. ВидеоС помощью этих инструментов можно генерировать и редактировать видео, создавая ролики и анимацию на основе ваших идей.  Моделей тут две:Runway, VeoПример работы:An elderly woman in a floral headscarf and oversized sunglasses, confidently walking through a typical Soviet courtyard with cracked asphalt and rusty playground swings. She is holding two leashes: on the left a giant brown bear, on the right a heavy gray rhinoceros. Both animals wear hanging wooden signs — the rhino’s says “Review”, the bear’s says “of Veo 3”. The mood is epic and surreal. (Veo-3)Кроме того, весь этот функционал также адаптирован для использования в нашем Telegram-боте.  Возможность API   Если вам мало кнопок, интерфейсов и красивых дашбордов, BotHub предлагает следующий уровень взаимодействия с AI. Это возможность подключаться к моделям напрямую через API. То есть вы можете автоматизировать генерацию текстов, обрабатывать массивы данных, синтезировать изображения или подключать модели к CRM, веб-сайтам или бизнес-процессам.  Например:подключить автоматическую генерацию описаний товаров в интернет-магазине встроить интеллектуальный помощник в корпоративный портал выдавать клиентам мгновенные ответы без участия сотрудника запускать аналитические пайплайны по расписаниюПолная документация лежит здесь. Даже если вы не разработчик, базовое использование можно освоить за вечер.   Также мы предоставляем полный доступ к API OpenAI через наш агрегатор. Все наши конечные точки идентичны конечным точкам OpenAI. Вы также можете получить доступ ко всем нашим моделям (Claude, Midjourney, Mistral, LLaMA, Mythomax)Вы можете использовать наш API и соответствующие конечные точки при использовании плагинов или при разработке собственного программного обеспечения через SDK.Наш шлюз поддерживает завершения чата, функцию Vision, генерацию изображений, модерацию, аудиотранскрипциюEasy WriterДалее хочу представить сервис, созданный для тех, кто работает со словами каждый день. Он не обещает вдохновения, но помогает сделать структуру и экономит времяРедакторы используют Easy Writer для ускорения подготовки коротких материалов и аннотаций. Маркетинговые агентства подключают его при разработке описаний к рекламным кампаниям. В некоторых компаниях его тестируют как инструмент внутреннего документооборота, чтобы составлять письма и пресс-релизы.Инструмент позволяет настраивать стиль и тон повествования, выбирая, например, нейтральный, научный или свой стиль. Easy Writer поможет создать статьи, обзоры, посты для блога и другие типы контента. Пример работы:ИИ-бот-модератор BotHubИ напоследок — бот. Он представляет собой решение для борьбы со спамом и флудом в чатах. Технически бот построен на стеке: Python 3.13 с асинхронной архитектурой, сервером на FastAPI и библиотекой Aiogram для работы с Telegram API, данные хранятся в PostgreSQL (с Alembic‑миграциями) и Redis используется для кэша и сессий, прокси и балансировку берёт на себя Nginx. В области машинного обучения бот использует модель RUSpam (на базе BERT для русского языка) и интеграцию с LLM через BotHub API, а также библиотеки scikit‑learn и Transformers для контекстного анализа. По части безопасности предусмотрены: запуск под non‑root пользователем в Docker, шифрование токенов, ограничения скорости (rate limiting), проверка на заблокированных пользователей, кэширование на час. Функционально бот позволяет владельцу чата отслеживать статистику активности: дата добавления бота, статус защиты, порог спама, данные за последние 24 ч и отчёт о производительности. Можно включать или отключать режим защиты и уведомления: когда защита включена — бот активно реагирует и банит/мутит; когда отключена — он просто присутствует как участник. Есть возможность настраивать системный промт модели (то есть задавать сценарий работы ИИ‑модели и её поведение) и управлять белым списком и заблокированными пользователями (иммунитет участникам, снятие бана вручную). Бот различает спам (например массовые рассылки типа «куплю‑продам», «перейди по ссылке») и флуд (много подряд сообщений): за флуд накладывается мут, за спам — бан. Результат работы:Настраиваем работу и стартуем без вложений!Да, вам не показалось — без вложений. Чтобы настроить наши продукты, вам нужен аккаунт. BotHub раздаёт 100 000 токенов всем, кто зарегистрируется по этой ссылке. А это значит, что вы сможете приступить к первым задачам уже сейчас! Регистрируемся и продолжаем двигаться по списку!  Результат:ДашбордBotHub не требует ручной настройки API или дополнительных ключей. Все модели доступны из коробки:1. Откройте дашборд.2. Раскройте список, чтобы выбрать модель. Нужный ИИ можно найти по названию или типу задачи.3. Чат‑боты находятся на вкладке Текст, а другие модели — во вкладках Изображение, Речь и Видео.4. После выбора модели напишите промпт. Это может быть текст любой длины, от нескольких символов до тысяч и десятков тысяч символов.Например, gpt-5 имеет контекстную память длиною в 400 000 токенов (0,8...1,2 млн рус. символов/1,6 млн англ. символов).Также, BotHub предоставляет доступ к библиотеке промптов. Это готовые заготовки для рекламных текстов, заголовков, рассылок, сценариев, постов и статей. Всё работает в пару кликов и идеально подходит тем, кто раньше ничего не слышал о нейросетях.  5. Все модели позволяют прикреплять файлы кнопкой ＋6. В правой части интерфейса находятся параметры модели. Для текстовых чат‑ботов можно оставить их по умолчанию, а вот для изображений и видео нужно заглянуть. Там настраиваются стиль, разрешение и прочие параметры. Панель настроек для модели GPT-57. После ввода промпта нажмите Enter или кнопку отправки. Чат‑бот начнет отвечать уже в процессе генерации, а у размышляющих моделей будет виден процесс рассуждения. Интерфейс интуитивно понятный, но если у вас возникнут вопросы - вы всегда можете посмотреть гайд для работы в BotHub!Как получить доступ к API  1. Авторизуйтесь в своем профиле BotHub по ссылке выше. 2. Переходите по этому адресу.  3. Нажмите кнопку «Добавить ключ».  4. Скопируйте его и положите в безопасное место. Ключ — это доступ ко всем вашим квотам. 5. Используйте ключ в запросах через заголовок Authorization: Bearer <ваш_ключ_API>  6. Проверьте работу на одном тестовом запросе. Вводите промпт, отправляете запрос и получаете ответ в JSON. Если JSON прилетел — значит интеграция состоялась.   Пример для Pythonmodels = requests.get(  'https://bothub.chat/api/v2/model/list?children=1',  headers={    'Content-Type': 'application/json',    'Authorization': 'Bearer <your access token>',  },).json()  Подставляете свой ключ в заголовок — и всё работает!Easy WriterНа главной странице мы можем либо ввести свою тему, либо сгенерировать случайную, выбрав предварительно и нажав кнопку «Случайная тема». Затем нам нужно выбрать формат текста из доступных вариантов.Допустим, мы выбрали модель Grok и сгенерировали тему «Как малому бизнесу выжить в эпоху цифровизации». В качестве формата выберем «Пошаговое руководство» и перейдем к следующему шагу.На втором этапе нам нужно составить план статьи. Мы можем написать его самостоятельно или воспользоваться помощью Easy Writer. Инструмент предложит вам идеи для плана, которые мы сможем использовать в своей статье. Здесь также доступен ползунок креативности, который отвечает за уникальность текста. Чем выше значение, тем менее предсказуемым будет результат.  После того как план готов, переходим к настройкам. Здесь мы можем задать настроение статьи (например, юмористическое, научное или свой собственный), добавить ссылки на источники (прямые или просто упоминания), выбрать модель генерации, количество символов, язык текста и прописать ключевые слова для SEO-оптимизации. Мы также можем использовать референс, загрузив его или указав ссылку на источник.  Результат:ИИ-бот-модератор BotHub1. Перейдите к боту в Telegram: @bothub_community_bot, отправьте команду /start и получите приветствие и инструкции.    2. Добавьте бота в нужный чат и назначьте его администратором с необходимыми правами.    3. Зарегистрируйтесь на сайте BotHub и получите API-токен BotHub.   4. В личных сообщениях бота введите команду /bothub и вставьте полученный ключ/API-токен.    5. Выберите модель работы бота (например GPT-4 или другую из доступных).    6. Из меню бота в чате установите порог спама, настройте системный промпт, белый список и черный список пользователей (по необходимости).    7. Проверьте работу. Бот начнёт мониторинг сообщений, при превышении порога - мут либо бан. Вы можете переключать режим защиты и уведомлений из меню.  Бот готов к работе!  BotHub для бизнесаТеперь рассмотрим, чем сервис может помочь бизнесу. Вы получаете рабочую систему, которую можно включить в процесс и получить результат буквально в первый же день. Здесь важно понимать принцип: бизнес получает экосистему, где AI работает не сам по себе, а встроен в рабочие процессы. Он понимает контекст, обучается на ваших данных, работает с документами, обрабатывает запросы и помогает команде выполнять рутину быстрее. И если вы уже представляете, как это выглядит, то дальше вопрос в выборе конкретных возможностей. Чтобы не потеряться, разбираем на практических ситуациях, которые могут случиться в любой компании. Представьте отдел поддержки, который тонет в сообщениях. Клиенты пишут на всех площадках сразу, ответы запаздывают, менеджеры устают. В BotHub вы настраиваете AI‑агента, который понимает ваши правила, условия, частые вопросы и типичные ситуации. Он автоматически берет на себя поток запросов, распределяет их по сложности и оставляет сотрудникам только те случаи, где нужна человеческая логика. Менеджеры освобождают рабочее время, клиенты получают быстрый ответ, а контроль нагрузки становится прозрачным. Кроме того, в корпоративном доступе вы контролируете, кто что делает внутри системы. Можете ограничить расход токенов по отделам, отслеживать активность, видеть отчеты. По необходимости подписывается NDA, а каналы передачи данных закрыты. Это позволяет внедрять ИИ не хаотично, а с нормальными корпоративными правилами. И еще пример — аналитика. Руководителю нужно показать динамику продаж за месяц. Сотрудник подключает BotHub к CRM через API, выгружает данные, получает сводку, анализ и графики. AI не просто строит таблицу, он объясняет изменения и формирует удобный текстовый отчет. Сотруднику остается только проверить выводы и отправить презентацию дальше. И вот здесь появляется еще один интересный момент. В BotHub можно работать командой. Это значит, что вы не один на один с инструментом. Можно подключить нескольких сотрудников, давать им разные уровни доступа, вести совместные проекты, следить за расходованием токенов и обмениваться файлами. А сейчас к тарифам! Теперь, когда мы разобрались, как работает сервис и зачем вообще нужны Caps, можно смело переходить к теме планов. Да, тот самый бесплатный пакет на сто тысяч капсов уже мелькал выше, и если вы испытали  платформу и не разочаровались, у вас вполне может появиться мысль о продолжении работы.Поэтому предлагаю взглянуть на тарифы:Тариф   Что входит?Подходит...Free (по ссылке) 100 000 Caps Новичкам, для тестирования, мелкая задача  Basic    2 000 000 Caps за 3 $   Специалисту, который хочет больше мощности Premium   5 000 000 Caps за 7 $  Если задачи регулярные, контент-производствоDeluxe   10 000 000 Caps за 14 $  Для командного использования, когда нужен значительный объёмElite   35 000 000 Caps за 49 $  Для крупных проектов или агентства Enterprise   Индивидуальный тариф, обсуждается отдельно Корпоративным клиентам, с большим штатомСрок действия пакета не ограничен, а токены не сгорают.РезюмируяЧтобы подытожить нашу экскурсию по BotHub, хочется добавить, что у нас есть блог на Хабре, где мы регулярно рассказываем, что обновилось в сервисе, пишем новости из мира AI и обозреваем другие платформы. А если что-то не работает или хочется спросить, как лучше реализовать идею, есть чат поддержки. Там вы сразу получите ответ на все, что вас интересует.Спасибо, что дошли до конца! А теперь очередь за вами. Расскажите, что вы думаете о нашем сервисе? Интересно прочитать ваше мнение. Давайте будем расти вместе! Теги:иинейросетимашинное+обучениеchatgptgeminigrokclaudeopenaianthropicgoogleХабы:Блог компании BotHub",583,0,0,11 мин,https://habr.com/ru/companies/bothub/articles/964432/,16918,2275,1
SwiftUI-Adapter: поддерживаем новые модификаторы SwiftUI на старых версиях iOS,den_apps,2025-11-12T13:27:30.000Z,['iOS *'],"den_apps 21 час назадSwiftUI-Adapter: поддерживаем новые модификаторы SwiftUI на старых версиях iOSУровень сложностиПростойВремя на прочтение1 минКоличество просмотров224iOS * Из песочницыSwiftUI-AdapterДрузья, привет! Сегодня хочу представить вам свою библиотеку - SwiftUI-Adapter, которая избавит вас от головной боли при работе с новыми модификаторами SwiftUI.Недавно я наткнулся на удобную Android-библиотеку, которая упрощает работу с разными версиями API и подумал: «Почему бы не сделать что-то подобное для SwiftUI?». После этого родилась идея разработать инструмент, который избавит вас от бесконечных проверок available и сделает код чище.Ссылка на библиотеку в GitHubУстановка библиотеки выполняется через Swift Package Manager.Зачем это нужно?Каждый раз, когда Apple выпускает новый модификатор в SwiftUI, нам приходится писать такие конструкции:if #available(iOS 15.0, macOS 12.0, *) {
  YourView()
    .badge(5)
} else {
  YourView()
}SwiftUI-Adapter делает эту рутину за вас! Просто используйте единый синтаксис – проверки версий останутся под капотом:YourView()
  .adapter.badge(5)Преимущества:Не повлияет на производительность: все проверки производятся на этапе компиляции.Чистая кодовая база: больше никаких available в каждом втором файле.Простота интеграции: добавляется за пару минут через SPM.Открытый исходный код: полная прозрачность, возможность вносить правки и участвовать в развитии.Библиотека поддерживает модификаторы доступные с iOS 15 и выше, а так же macOS 12 и выше.SwiftUI-Adapter поддерживает большое количество модификаторов, включая новые модификаторы, которые были добавлены в iOS 26 и macOS 26. Описание каждого из модификаторов доступно на странице в GitHub.Ознакомиться с другими моими публикациями и разработками можно в моем телеграм канале.Теги:swiftswiftuiмобильная разработкаios разработкаios разработка swiftswift  разработкаxcodeios-разработкаios-developmentбиблиотекиХабы:iOS",224,0,0,1 мин,https://habr.com/ru/articles/965732/,1925,226,1
Как мы превратили цифровое ТВ в радар,astronotius,2025-11-13T10:18:49.000Z,"['Open source *', 'Любительская радиосвязь', 'Python *']","astronotius 1 час назадКак мы превратили цифровое ТВ в радарУровень сложностиСреднийВремя на прочтение3 минКоличество просмотров542Open source * Любительская радиосвязьPython * Из песочницы Open Source проект по мониторингу воздушного пространства на SDR  Зачем вообще делать радар из телевизора?Традиционные радиолокационные системы (РЛС) — это огромные антенны, киловатты мощнос��и, разрешения на частоты и бюджеты уровня «военного отдела». Нам же хотелось видеть небо, не нарушая ни законов физики, ни законодательства.Так родилась идея собрать пассивную когерентную локацию (PCL) — систему, которая ничего не излучает, а просто слушает уже существующие сигналы в эфире. FM-радио, LTE, цифровое ТВ — всё это мощные «осветители», которые и так покрывают территорию. Почему бы не использовать их?Мы выбрали сигнал DVB-T2 (546  МГц) — стабильный, мощный, и что особенно приятно — с известной структурой (OFDM).Результат? Получился радар без передатчика, который можно запустить хоть на балконе. И да, всё это — на полностью open source стеке.Архитектура: что мы собралиЧтобы превратить ТВ-сигнал в радар, нам понадобилось не так уж много железа, но с хитростью.Capture Unit (RPi 5 + KrakenSDR) Снимает IQ-потоки с 5 приёмных каналов, синхронизированных по PPS и GPSDO.Processing Server Получает UDP-потоки, вычисляет CAF (Cross-Ambiguity Function) и CFAR-детекции, объединяет треки с помощью  Kalman Filter и  Hungarian algorithm WebSocket API Рассылает JSON-треки в реальном времени клиентам — веб-панелям, картам и системам визуализации.КомпонентНазначениеПримечаниеKrakenSDR (5 каналов)Приём сигналов с антеннМногоканальность нужна для фазовых измерений и пеленгацииАнтенный массив (5× Yagi-Uda)1 референсная антенна + 4 следящиеРеференсная ловит прямой сигнал DVB-T2, остальные — отраженияRaspberry Pi 5Вычислительный блокОбрабатывает IQ-потоки в реальном (или почти реальном) времениОсветитель (Illuminator)Сеть DVB-T2Некооперативный, но стабильный источник сигналаНа практике это выглядит довольно забавно: несколько антенн на крыше, Pi5 с Kraken’ом на столе, и ноутбук с Python’ом, который «слушает телевизор» и находит дроны самолетного типа.Пайплайн DSP: как из шума извлечь движениеГлавная задача — отделить динамические отражённые сигналы от всего остального: земли, домов и гор. Сырые IQ-потоки проходят через несколько ключевых этапов цифровой обработки.1. CAF: превращаем эфир в «кадр радара»Мы вычисляем Cross Ambiguity Function (CAF) — по сути, это двумерная карта, где:ось задержки (delay) показывает расстояние,ось Доплера — скорость объекта.Результат похож на тепловую карту, где почти весь центр — это «статический столб» (clutter).2. MTI: избавляемся от статикиЧтобы выделить движущиеся объекты, мы применяем MTI-фильтр (Moving Target Indication). Принцип простой: вычитаем предыдущий кадр из текущего.# MTI: удаляем статику, оставляем движение
filtered_caf = caf_frame_t - caf_frame_t_minus_1Если сигнал не изменился — исчезает. Если что-то движется — появляется всплеск.После этого “тишина эфира” превращается в живую картину с подвижными точками.3️3. CFAR + Kalman Filter — из вспышек в устойчивые трекиCFAR (Constant False Alarm Rate) — адаптивный детектор, который ищет цели, не увеличивая ложные срабатывания. А Kalman Filter помогает связать отдельные кадры в треки и предсказать движение объектов.Чтобы не путать цели, используем Hungarian algorithm для сопоставления детекций между кадрами.🧭 Результат: каждая цель получает свой ID и непрерывно отслеживается, даже если сигнал временно пропадает.Технические вызовыСейчас проект активно развивается, и вот с чем мы боремся:🔹 Фазовая когерентность KrakenSDR. Как синхронизировать четыре приёмных канала, если источник (DVB-T2) никак с нами не связан? Пока пробуем софтверную компенсацию.🔹 Оптимизация DSP на Raspberry Pi 5. Расчёт CAF прожорлив: NumPy и SciPy не всегда успевают. Пробуем переносить части расчётов на GPU через OpenCL или CUDA.🔹 Особенности OFDM. Хотим минимизировать боковые лепестки и артефакты при обработке DVB-T2 — поле для экспериментов и дискуссий. Open Source и масштабированиеВесь проект — open source. KrakenSDR и Raspberry Pi 5 стоят не дороже роутера, поэтому можно собирать целые сети пассивных приёмников.Они могут совместно покрывать  территории на сотни километров, естественно там где присутствует FM, LTE,📦 Репозиторий: 👉 github.com/Stanislav-sipiko/passive-sdr-radar💬 ПрисоединяйтесьЕсли вы разбираетесь в SDR, цифровой обработке, оптимизации Python или просто хотите помочь улучшить алгоритмы — будем рады вашей помощи.Теги:радиотехникаПрограммированиеsdr-приёмникпеленгацияХабы:Open sourceЛюбительская радиосвязьPython",542,0,0,3 мин,https://habr.com/ru/articles/966044/,4633,590,3
Пошаговая настройка вывода логов из .NET-автотестов в ELK (Filebeat → Logstash → Elasticsearch → Kibana),webrise,2025-11-13T10:11:45.000Z,"['.NET *', 'IT-инфраструктура *', 'C# *', 'Веб-разработка *', '*nix *']","webrise 1 час назадПошаговая настройка вывода логов из .NET-автотестов в ELK (Filebeat → Logstash → Elasticsearch → Kibana)Уровень сложностиПростойВремя на прочтение4 минКоличество просмотров82.NET * IT-инфраструктура * C# * Веб-разработка * *nix * ТуториалВсем привет, с вами Юрий Ковальчук, backend разработчик в ВебРайз. В этой статье разберем процесс вывода логов из приложения c автотестами на .NET в ELK с последующей визуализаций в Kibana.ELK представляет из себя достаточно массивный инструмент для сбора, хранения, обработки и анализа логов, организации мониторингов. С наскоку разобраться с ним вряд ли получится, поэтому подготовили небольшую инструкцию с примерами - на базе простого теста прокинуть результаты до Kibana.  Отправка логов из .NET-тестаНиже — обезличенный пример e2e-/UI-теста на .NET (xUnit + Playwright), который:Запускает браузерВыполняет действия на страницеФормирует объект с результатом тестаПишет лог в Serilog, откуда его потом заберёт Filebeat[Fact]
public async Task SubmitPaymentForm_EmptyForm_ShouldShowValidationErrors()
{
    string baseUrl = $""{_baseUrl}payment"";
    var testName = ""SubmitPaymentForm_EmptyForm_ShouldShowValidationErrors"";

    var context = await _browser.NewContextAsync(new() { IgnoreHTTPSErrors = true });
    var page = await context.NewPageAsync();

    var result = new TestResult
    {
        Test = testName,
        Url = baseUrl,
        Timestamp = DateTime.UtcNow,
        ErrorText = """",
        ErrorStyle = """",
        Success = false
    };

    try
    {
        if (string.IsNullOrEmpty(baseUrl))
            throw new Exception(""BaseUrl not configured"");

        await WaitForSiteReady(page, baseUrl, 60);
        await page.GotoAsync(baseUrl);

        await page.ClickAsync(""button.nf-button--primary"");
        await page.ClickAsync(""button.js-pay-button-submit"");

        var error = await page.WaitForSelectorAsync("".cell__error-message"", new() { Timeout = 30000 });
        var text = await error.InnerTextAsync();
        var style = await error.EvaluateAsync<string>(""el => el.getAttribute('style')"");

        Assert.Contains(""Заполните"", text);
        Assert.Contains(""display: block"", style);

        result.Success = true;
    }
    catch (Exception ex)
    {
        Log.Error(ex, $""Test {testName} failed"");
        result.ErrorText = ex.Message;
        result.ErrorStyle = ""danger"";
    }
    finally
    {
        await context.CloseAsync();
        LogTestResult(result);
    }
}Пояснения к ключевым строкам:baseUrl — конечная точка тестируемой страницыtestName — удобное имя теста, которое попадёт в логиresult — объект, который вы будете сериализовать/логировать (его потом легко разобрать в Filebeat)try/catch/finally — в catch пишем ошибку, в finally — пишем структурированный результатLog.Error(...) — классический Serilog-вызов, который попадёт в файлFilebeat: сбор логов тестов и отправка в LogstashПример filebeat.yml:  filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/www/project/tests/bin/Debug/net6.0/Logs/e2e*.log
    fields:
      type: e2e_tests
    fields_under_root: true
    scan_frequency: 5s
    processors:
      - decode_json_fields:
          fields: [""message""]
          process_array: false
          max_depth: 3
          target: """"
          overwrite_keys: true
      - rename:
          fields:
            - from: ""Properties.TestResult.Timestamp""
              to: ""test_timestamp""
            - from: ""Properties.TestResult.Test""
              to: ""test_name""
            - from: ""Properties.TestResult.Success""
              to: ""test_success""
          ignore_missing: true
      - drop_fields:
          fields: [""Properties"", ""MessageTemplate"", ""Level""]

output.logstash:
  hosts: [""111.18.100.38:5044""]Что важно:paths — путь до логов приложения/тестовdecode_json_fields — разбираем Serilog JSON, чтобы получить плоские поляrename — переименовываем вложенные поля Serilog в удобные (test_name, test_success и т.д.)drop_fields — удаляем лишний технический шумoutput.logstash — указываем, куда отправлять (Logstash)Logstash: приём и отправка в ElasticsearchМинимальный pipeline Logstash (пример logstash.conf):input {
  beats {
    port => 5044
  }
}

filter {
  if [type] == ""e2e_tests"" {
    mutate {
      add_field => { ""[@metadata][index]"" => ""e2e-tests-%{+YYYY.MM.dd}"" }
    }
  }
}

output {
  elasticsearch {
    hosts => [""http://localhost:9200""]
    index => ""%{[@metadata][index]}""
  }
  stdout { codec => rubydebug }
}
input.beats — то, что получает от Filebeatfilter — можно добавлять/нормализовывать поляoutput.elasticsearch — конечная точка, индекс называется e2e-tests-YYYY.MM.ddПоиск логов в Kibana/Dev ToolsСтартуем с простого запроса:
GET _cat/indices?v
Так проверяем, что индекс e2e-tests-* вообще есть. Дальше — простой поиск:
GET e2e-tests-*/_search
{
  ""size"": 10
}
Чтобы находить именно тестовые логи Serilog со вложенным TestResult, используем match_phrase по message:
GET e2e-tests-*/_search
{
  ""size"": 10,
  ""query"": {
    ""match_phrase"": {
      ""message"": ""\""Properties\"":{\""TestResult\""""
    }
  },
  ""sort"": [
    { ""@timestamp"": ""desc"" }
  ]
}
size — сколько документов вернутьmatch_phrase — ищем конкретный фрагмент JSON, характерный для наших тестовых логовsort — сортируем по времени прихода документаВизуализация в KibanaНиже показано, как создать визуализации в Kibana для отображения результатов e2e-тестов .NET. Будем использовать Filebeat и Logstash для отправки логов в Elasticsearch, а Kibana — для построения графиков.1. Список визуализаций Список визулизаций2. Создание новой визуализации Создание новой визуализацииНажимаем «Create visualization». Для простоты используем тип Lens — он подходит для построения базовых графиков.  3. Настройка Lens визуализации Настройка Lens визуализацииВ окне Lens выбираем индекс e2e-tests*. На оси X указываем @timestamp, на оси Y — уникальное количество test_success. Сверху можно добавить фильтр test_name, чтобы отображать результаты только конкретного теста.4. Сохранение визуализации Сохранение визуализацииПосле настройки графика сохраняем его с понятным названием, чтобы использовать при создании дашборда.  5. Открытие Dashboard Открытие DashboardВ меню Kibana переходим в раздел Dashboard — здесь создаются панели мониторинга, состоящие из нескольких визуализаций.6. Создание нового Dashboard Создание нового DashboardМожно создать новый дашборд («Create dashboard») или открыть существующий. В примере используется дашборд «Тесты_Site.ru».  7. Добавление визуализаций Добавление визуализацийЧтобы добавить визуализацию, нажмите Add → Lens Visualization и выберите нужный график из списка.  8. Итоговый Dashboard Итоговый DashboardПосле добавления всех графиков на панель вы получите наглядный дашборд, отображающий состояние e2e-тестов в реальном времени.9. Пример визуализации теста Пример визуализации тестаЭтот график показывает результаты теста во времени: успешные и неуспешные прогоны.Теперь дашборд можно использовать для контроля состояния и стабильности тестов, а также для анализа проблем при падении отдельных сценариев.  По вопросам, телеграм @webrise1Теги:elkelasticsearchlogstashkibanafilebeatлогированиелоги.net.net corec#Хабы:.NETIT-инфраструктураC#Веб-разработка*nix",82,0,0,4 мин,https://habr.com/ru/articles/966038/,7237,775,5
Сравнение технологий аппаратного транскодирования,vmetrix,2025-11-12T12:17:13.000Z,"['Блог компании RUTUBE', 'Компьютерное железо', 'Сжатие данных *', 'Работа с видео *', 'Видеокарты']","vmetrix 23 часа назадСравнение технологий аппаратного транскодированияУровень сложностиПростойВремя на прочтение13 минКоличество просмотров770Блог компании RUTUBEКомпьютерное железоСжатие данных * Работа с видео * ВидеокартыАналитикаМожно ли чем-то заменить NVIDIA? Если уж не для нейросетей, то для транскодирования видео, которое в медиапроизводстве занимает очень значительное место и требует больших вычислительных ресурсов. В этой статье попытаемся выяснить, есть ли у аппаратной платформы NVIDIA альтернативы в задачах обработки и кодирования видео, и можно ли её заменить чем-то более доступным во всех смыслах: и по возможности закупки на рынке РФ, и по цене. Меня зовут Дмитрий Митяев, я почти 30 лет в IT, работал в области авиастроения, занимался анализом качества видеокодирования в NVIDIA, продолжаю интересоваться этим и сейчас — в RUTUBE TECH. В этой статье покажу результаты сравнения качества кодирования видео на различных аппаратных платформах: CPU, GPU NVIDIA, Intel, Mac и Rockchip. Этот анализ будет полезен, если вы, как и мы, исследуете, на что можно заменить NVIDIA, чтобы оптимизировать затраты на железо и нивелировать риски, связанные с недоступностью определенных карт. Если такая задача вам пока не актуальна, то, возможно, вам будет любопытно узнать о метриках сравнения относительно субъективного параметра «качество» видео и способах тестирования кодеков. Пригодится для стримингов, кинотеатров, видеохостингов или домашней лаборатории. Основные понятия и теоретическая базаПод процессом кодирования видео подразумевается сжатие видеопотока с определёнными параметрами. Например, для того, чтобы сделать архивное видео используются одни параметры — можно сжимать подольше, но с большим качеством. Чтобы транслировать игру, нужна меньшая нагрузка на систему, быстрое кодирование и допустимый ущерб качеству. В зависимости от используемых параметров результирующее видео может отличаться по объёму в десятки раз.Кодек — это реализация спецификации алгоритма сжатия, а также, что немаловажно, спецификации алгоритма декодирования, то есть раскодирования сжатого потока в формат, подходящий для отображения. Разные реализации одного и того же стандарта, вообще говоря, при декодировании должны давать бинарно одинаковые кадры.Основные стандарты и их реализации:СтандартРеализации (библиотеки)H.264x264, NVENC H264, RKMPP H264, QSV, VideotoolboxHEVCx265, NVENC HEVC, RKMPP HEVC, QSV HEVCVP8/VP9libvpxAV1libaom, svtav1, Rav1e, Dav1d (декодирование)H.266Uvg266, Fraunhofer VVEnc, x266В рамках данного исследования мы ограничимся H.264 — достаточно старым, но по-прежнему популярным стандартом, который занимает значительную часть рынка и поддерживается максимальным количеством устройств.Библиотека может быть написана как только для процессора, так и под разнообразное железо: видеокарты, специализированные карты FPGA (программируемые матрицы), ASIC (специализированные микросхемы). Например, в аппаратных комплексах видеофиксации могут быть свои реализации, оптимизированные конкретно под задачи хранения видео с камер наблюдения.Библиотеки под процессоры обычно выигрывают по качеству кодирования, так как имеют больше степеней свободы и возможностей оптимизации. При неограниченном бюджете и наличии множества серверов, которые можно занять видеокодированием, в общем случае по качеству лучшим вариантом будет libx264. Однако аппаратные реализации как правило дают лучшие результаты по соотношению цена-качество, плотности упаковки в датацентре, энергопотреблении. Какие из них будут выгоднее в каком случае, мы как раз и хотим выяснить. В сравнении мы не рассматриваем решения на базе ASIC и FPGA (Netint и Xilinx) по причине их практической недоступности и относительно высокой стоимости. То же самое касается Apple M4 — на сегодняшний день его стоимость приближается к картам NVIDIA, хотя и есть ожидания, что по качеству картинки новый кодек будет лучше, чем M2, и скорее всего в будущем мы сделаем новое исследование и опубликуем его результаты. Метрики качестваЧтобы сравнить разные способы кодирования, нам нужно оценить качество перекодированного видео, и сопоставить затраченные на это ресурсы: производительность, цену самого оборудования, размер видео и т.д. Но как минимум, нужна методика оценки качества видео. Методы оценки качества видео делятся на методы с использованием оригинала и без. В первом случае у нас есть исходное видео, которое мы сжимали с определёнными параметрами, и мы можем сравнивать с ним полученный результат. Во втором — просто набор готовых кадров, качество которых необходимо оценить по каким-то эвристическим методикам. Последние сейчас в основном реализуются с помощью моделей машинного обучения, самые заметные из них: NR-VMAF, NIQE, PIQE, BRISQUE. Методы без оригинала в рамках данной статьи мы рассматривать не будем.Среди методов с оригиналом самое большое распространение получили: VMAF, PSNR, SSIM и их производные, созданные под решение конкретных проблем присущих стандартным реализациям. Реже используется CIEDE2000 — цветовая дистанция между сравниваемыми кадрами. В рамках исследования использовались различные методы оценки качества, как правило результаты валидировались по нескольким метрикам. Рассмотрим, каждую из них.PSNR — пиковое соотношение сигнал/шум. Размерность — дБ.где  — максимальное значение уровня сигнала. В случае 8-битного кодирования видео PSNR показывает, какое количество шума внесено кодеком при сжатии видео. Фактически представляет собой среднеквадратическую ошибку: берутся два кадра (из оригинала и получившегося видео) и попиксельно вычитается одно значение из другого, возводится в квадрат и делится на количество пикселей в кадре. Конечно, берутся значения по каналам. В зависимости от формата кодирования пикселя при усреднении используются веса по каналам. Например, для yuv444 у каждого канала вес –1, так как количество кодируемой информации в каждом канале (плоскости, если говорить о структуре хранения информации по кадру) одинаковое — используется одинаковое количество бит. А вот для yuv420 под яркостный канал отводится большее количество бит и вес при усреднении больше, чем у каналов цветности (подробнее с кодированием пикселей можно ознакомится, например, в этой статье на википедии).Например, так выглядит фрагмент кода для подсчёта среднеквадратической ошибки по плоскостям в реализации фильтра PSNR в FFMPEG:...
    for (int c = 0; c < s->nb_components; c++)
        mse += comp_mse[c] * s->planeweight[c];SSIM — показывает, насколько структурно один кадр близок к другому. Фактически это корреляция между двумя случайными величинами (между средними значениями окон сравниваемых кадров). — средние значения окон X и Y, — дисперсия значений окон X и Y, — ковариационный момент значений окон X и Y,— коэффициенты для балансирования отношения.VMAF — метрика качества, разработанная Netflix. Основная цель VMAF — оценить качество видео максимально приближенно к субъективной, человеческой оценке. Она состоит из трёх компонент: Visual Information Fidelity (VIF) — степень искажения; Detail Loss Metric (DLM) — степень потери деталей; Mean Co-Located Pixel Difference (MCPD) — степень разницы между кадрами.Выход из трёх компонент загоняется в регрессионную модель. В поставке входят уже обученные модели для разных размеров кадров. При необходимости можно обучить модель на своих видео.Работает гораздо дольше, чем более простые с вычислительной точки зрения PSNR или SSIM, но есть реализация на GPU.Сравнение на диапазоне битрейтов — RD-CurvesДля корректного сравнения, конечно, недостаточно просто закодировать два видео и получить финальную метрику качества. Необходимо рассматривать как разные сцены, так и разные битрейты. А для полной картины — диапазон битрейтов, для того, чтобы определить поведение кодека на интересующем диапазоне. Ведь обычно, конечный пользователь в течение просмотра может получать видео с разным битрейтом в зависимости от пропускной  способности сети в данный конкретный момент.Предположим, что мы уже провели кодирование исходного видео  в разные битрейты с помощью программной реализации x264 и одной из аппаратных платформ и нарисовали две кривые, проходящие через точки, полученные после подсчёта качества с помощью VMAF.Красная кривая — кодирование с помощью x264, зелёная — аппаратная реализация H.264. Более высокая кривая означает, что кодеку необходимо больше битрейта для того же качества видеоЕсли вы уже работали с кривыми RD, то скорее всего обратили внимание, что график перевёрнут: по оси X — качество, по оси Y — битрейт. Обычно графики строят относительно битрейта (по оси X) и смотрят на качество при фиксированном битрейте. Однако у нас обратная задача: определить, какое количество битрейта тратит тот или кодек для кодирования с определённым качеством. Чтобы не поворачивать голову и не подвергать шейные позвонки риску, перевернём сам график. На нём более высокая кривая означает, что кодеку необходимо больше битрейта для получения того же качества.Визуальная оценка — это хорошо и наглядно. Но что можно сделать, чтобы получить объективные числа для сравнения? Например, первое, что приходит на ум — значения на границах измеряемого диапазона качества. Это даст представление о разнице реализаций в самом худшем качестве и в самом лучшем. Значения на границах: 20,6% и 25,9%. Но, пожалуй, объективного представления о разнице два числа не дадут — на всём диапазоне могут быть совсем иные соотношения. Посмотрим ближе к середине: при значении VMAF = 65 разница между двумя способами кодирования составляет 29,4%. при значении 80 — 30,1%. И так далее — можно до бесконечности добавлять отношения на интересующих точках.Чтобы перевести этот набор значений в разных точках в один показатель, используем подход Bjontegaard Delta Bitrate (BD-BR), предложенный в 2001 году норвежским учёным Йисле Бьонтегором (Gisle Bjontegaard).Метод Бьонтегора наглядно: область, обозначенная на графике оранжевым говорит о разнице в качестве, количественно — это отношение площадей под кривымиПо сути это отношение площадей под этими кривыми, рассчитывается следующим образом:Для каждого набора точек конкретного кодека используем полином для получения функции, описывающей поведение этой кривой.Интегрируем на общем отрезке, т.е. получаем площадь под каждой кривой.Вычисляем отношение между этими площадями.Метод даёт ответ на вопрос, во сколько раз битрейт одного кодека должен отличается от другого при кодировании с одинаковым качеством. Таким образом, если, например, в маркетинговом проспекте написано, что AV1 на 30% лучше, чем HEVC, то это значит, что кодеку AV1 нужно на 30% меньше битрейта для достижения такого же качества (оценённого по какой-либо методологии).В этом примере мы использовали метрику VMAF, но, очевидно, что под капотом может быть любая другая метрика, которую мы сочтём показательной для нашей задачи.Метод Бьонтегора не лишён недостатков. Он хорошо работает, когда функция зависимости битрейта от качества монотонно возрастает. Если же на графике много выбросов, то метод не даст адекватных результатов. Чтобы с этим справиться, можно дополнительно поработать с данными. Например, команда лаборатории компьютерной графики и мультимедиа при факультете ВМК МГУ (проект compression.ru) использует метод BSQ-Rate. Однако в целом вопрос, нужно ли «лечить» некорректные данные, на мой взгляд, дискуссионный. Если кодек при увеличении битрейта не даёт возрастающие величины качества, то насколько можно вообще оценить его качество? Ведь он фактически работает непредсказуемо.Условия измеренийДля сравнительного анализа использовались следующие платформы:libx264 (CPU) в качестве эталона;Orange Pi 5 Plus, Rockchip RK3588, кодек RKMPP, цена ≈ 12 000 ₽;Intel N100, кодек QSV, цена ≈ 18 000 ₽; Mac Mini, кодек Videotoolbox, цена ≈ 30 000 ₽;NVIDIA RTX A4000, кодек NVENC, цена ≈ 120 000 ₽.В качестве тестовых образцов мы выбрали 21 видео, в которых присутствуют разные особенности, например, большое количество деталей, быстродвижущиеся объекты, контрастные или наоборот очень близкие цвета и т. д. Исходные видео в разрешении 4K перекодировались в 8 меньших размеров кадров по 10 вариантов битрейтов, равномерно распределённых внутри характерного для данного разрешения отрезка.Размер кадраБитрейты2160p[1 Мбит/с, ..., 13 Мбит/с]1140p[671 кбит/с, ..., 8,3 Мбит/с]1080p[461 кбит/с, ..., 5,7 Мбит/с]720p[260 кбит/с, ..., 3,2 Мбит/с]480p[148 кбит/с, ..., 1,8 Мбит/с]360p[95 кбит/с, ..., 1,2 Мбит/с]232p[49 кбит/с, ..., 611 кбит/с]144p[27 кбит/с, ..., 343 кбит/с]Сделано это для того, чтобы:получить более полную картину распределения качества по диапазону;захватить разные битрейты, которые соответствуют разным условиям просмотра, режимам контроля битрейта и пропускной способности сети в стриминге.Параметры кодированияРезультат кодирования может отличаться в зависимости от параметров. Так как наш тест независимый и мы хотим узнать максимальные возможности каждой платформы, подбирались такие настройки кодеков, которые были бы оптимальны на тестовом наборе. Использовался FFMPEG версии 7.1.1. (кроме QSV), остальные параметры ниже (жирным выделено основное, на что стоит обратить внимание):libx264: -c:v libx264 -preset fast -g 50 -b:v {BITRATE} -bufsize {BITRATE/FRAMERATE} -maxrate {BITRATE} -minrate {BITRATE} -x264opts no-sliced-threads:no-psy=1:aq-mode=0 -tune zerolatency -profile:v {PROFILE} -level:v {LEVEL} -vsync passthroughNVENC: -c:v h264_nvenc -no-scenecut 1 -g 50 -preset p4 -tune ll -b:v {BITRATE} -profile:v {PROFILE} -level:v {LEVEL} -vsync passthroughRKMPP: -c:v h264_rkmpp -g 50 -b:v {BITRATE} -bufsize {BITRATE}*1.5 -maxrate {BITRATE}*1.3 -level:v {LEVEL} -profile:v {PROFILE} -rc_mode VBR -vsync passthroughQSV (кодирование с помощью утилиты в поставке к SDK): h264 -b {BITRATE} -f {FRAMERATE} -u veryslow -hwVideotoolbox (нет ручек, за которые можно было бы подёргать для настройки кодека, использовались только параметры для установки битрейта): -c:v h264_videotoolbox -g 50 -b:v {BITRATE} -profile:v {PROFILE} -level:v {LEVEL} -vsync passthroughПресеты для x264 и NVENC были выбраны как «средние» по качеству/скорости. Для QSV был выбран самый медленный и самый качественный veryslow, так как только он смог как-то тягаться с остальными.Хочу обратить внимание на параметр -vsync passthrough (ныне заменённый на -fps_mode passthrough). Параметр важен для последовательности кадров на выходе кодека. Алгоритм кодека может решать пропускать или добавлять кадры — в таких местах метрики качества будут проседать. Параметр заставляет кодек использовать ту же последовательность кадров на выходе, что и на входе.Результаты сравненияСначала посмотрим на усреднённые результаты сравнения разных платформ кодирования по метрике VMAF, а далее рассмотрим частные случаи и углубимся в детали. В таблице ниже представлено сравнение кодеков с эталонным libx264 для размеров кадров 360p, 1080p и 2160p. Значения приведены относительно libx264, т. е. отрицательное значение в таблице означает, что для получения такого же качества сравниваемому кодеку нужно на столько процентов больше битрейта. Положительное значение — кодек справился лучше libx264 и сэкономил x% битрейта. BD-BR VMAF 360pBD-BR VMAF 1080pBD-BR VMAF 4KNVENC-12.9%-3.7%-1.9%RKMPP-19.9%-18.4%-15.3%QSV-20.0%-23.4%-21.2%VideoToolbox-30.3%-28.4%-18.0%Если не вдаваться в частности отдельных сценариев, то кодеки расположились в таком порядке по увеличению битрейта относительно libx264: NVENC, RKMPP, QSV, VideoToolbox. Ожидаемо, намного более дорогие NVIDIA выигрывают по качеству кодирования.Теперь рассмотрим детальнее отдельные характерные сцены и то, насколько кодеки с ними справились. Пример 1: смена относительно статичных сцен с движением посередине.КодекBD-BR VMAF (libx264) - 360pNVENC-11%RKMPP-29%QSV-23%VideoToolbox-50%В принципе характерная картина для данного исследования: лучше других выглядит NVENC, хуже — VideoToolbox на Mac. Ниже графики сравнения, где кривая ниже — лучше (требуется меньше битрейта для того же качества).Пример 2: примитивная графика, плоские цвета. Это, пожалуй, самый простой вариант для кодеков и все они справились примерно одинаково. КодекBD-BR VMAF (libx264) - 360pNVENC-2.31%RKMPP-17.50%QSV-5.74%VideoToolbox-10.73%NVENC опять лучше других, хуже всех — RKMPP.Пример 3: практически статическая картина, но с большим количеством деталей.КодекBD-BR VMAF (libx264) - 360pNVENC-21.48%RKMPP8.27%QSV-43.85%VideoToolbox-57.37%Здесь RKMPP обогнал даже libx264, VideoToolbox в аутсайдерах.Пример 4: много размытых деталей, градиенты. Очень часто на таких сценах, а также на водной ряби у кодека NVIDIA появляется много артефактов и пикселизация. Что мы и видим в данном тесте. КодекBD-BR VMAF (libx264) - 360pNVENC-33.29%RKMPP-21.03%QSV-8.10%VideoToolbox-14.48%Пример 5: как и в примере 4 тут дым, но видео с относительно статичным фоном. На этот раз сравниваем кодирование для 2160p. КодекBD-BR VMAF (libx264) - 2160pNVENC-24.22%RKMPP-23.26%QSV-38.90%VideoToolbox-57.10%NVENC и RKMPP справляются примерно одинаково, VideoToolbox — хуже всех и с приличным отставанием. Пример 6: статичный фон и много движения в центре кадра, разрешение 2160p.КодекBD-BR VMAF (libx264) - 2160pNVENC5.65%RKMPP-11.10%QSV-17.65%VideoToolbox-1.23%NVIDIA даже лучше libx264, QSV — в аутсайдерах. Интересно, что VideoToolbox на Mac в этом случае тоже справился хорошо.Пример 7: большая область с подвижными объектами, 1080p.КодекBD-BR VMAF (libx264) - 1080pNVENC2.56%RKMPP-35.32%QSV-11.17%VideoToolbox-19.06%NVIDIA как и в прошлом примере справляется даже лучше, чем софтверный кодек. RKMPP — не справляется.Пример 8: много деталей, много движения — сложная сцена для всех кодеков.КодекBD-BR VMAF (libx264) - 1080pNVENC14.03%RKMPP-18.58%QSV-12.99%VideoToolBox-12.49%NVENC опять на высоте. Остальные кодеки ведут себя примерно одинаково и требуют больше битрейта для того же качества. Пологая кривая на графике говорит о том, что на более низких битрейтах у всех будет с качеством не очень.ЭнергопотреблениеДля оценки отношения производительности и энергопотребления рассмотрим только крайние случаи: CPU, GPU и SoC. Для корректного замера производительности кодирование запускалось в несколько потоков, чтобы загрузить все кодеры. Количество параллельных потоков подбиралось так, чтобы общая производительность была максимальной.Ниже диаграмма с относительным энергопотреблением, рассчитанным на произведённый кадр. Ожидаемо система на процессоре потребляет гораздо больше энергии для сжатия такого же количества кадров, чем GPU и SoC (System on Chip). Меньше всего энергии тратит Rockchip — пиковая нагрузка составила порядка 5 Вт (только кодирование, т.е. никаких других задач на процессоре не выполнялось), энергопотребление при полной нагрузке не превышает 20Вт. Интересно отметить, что по производительности Rockchip достаточно близок к A4000.Декодирование: также проверили, насколько кодеки корректно работают при декодировании. Выяснилось, что все выдают бинарно идентичные кадры на выходе из декодера. Это важно, если для оценки качества используются сжатые входные видео. Если какой-то декодер будет выдавать отличные от других кадры на вход расчёта метрики качества, то корректность такого сравнения под большим вопросом.ИтогоВерхнеуровневые выводы нашего исследования следующие.КодекПреимуществаНедостаткиlibx264Почти всегда качество вышеВысокое энергопотреблениеNVENCХорошо справляется с большинством сценЕсть проблемы с размытыми объектами, ценаRKMPPСтоимость,скоростьКачество нижеQSVСтоимостьИнтеграция с FFMPEGVideoToolboxКачествоЧто и когда выбрать:Libx264 — если у вас много серверов и их некуда приспособить, либо есть жёсткое требование вытянуть максимум качества при ограниченном битрейте. NVENC — лидер в реализации кодека в железе, сбалансированное решение по качеству и быстродействию.RKMPP — самый дешёвый и самый энергосберегающий вариант. Имеет смысл использовать, если нет жёстких требований к качеству картинки или сцены в основном статичны. QSV — близок по качеству к RKMPP, но дороже и медленнее.VideoToolbox — самый дорогой из альтернатив и в то же время даёт самое низкое качество. Можно использовать, если он у вас уже есть для других целей и качество картинки не очень важно.Представленные результаты хоть и не являются полновесными, но достаточно хорошо отражают общее поведение рассмотренных кодеков. За кадром остался интересный вариант с Apple M4, который ожидается, что должен показать более высокое качество, чем его предшественник. Также не хотелось бы сбрасывать со счетов аппаратные реализации вроде NetInt или Xilinx. И, конечно, крайне интересно будет взглянуть на результаты работы разных реализаций AV1 и H.266. Единичные тесты качества последнего меня лично сильно удивили и есть интерес провести полноценный анализ — подписывайтесь на этот блог и канал, чтобы узнать о результатах будущих исследований.В канале Смотри за IT рассказываем о создании медиасервисов, инженерных тонкостях, продуктовых находках и полезных мероприятиях, делимся видео выступлений и кадрами из жизни команд Цифровых активов «Газпром-Медиа Холдинга» таких, как RUTUBE, PREMIER, Yappy.Теги:видеотранскодированиекачествоанализ изображенийкодекиh.264nvidiaХабы:Блог компании RUTUBEКомпьютерное железоСжатие данныхРабота с видеоВидеокарты",770,0,0,13 мин,https://habr.com/ru/companies/habr_rutube/articles/965288/,21127,2728,5
Подсчёт энергопотребления освещения в Home Assistant,linux2000,2025-11-12T14:21:11.000Z,['Умный дом'],"linux2000 21 час назадПодсчёт энергопотребления освещения в Home AssistantУровень сложностиПростойВремя на прочтение3 минКоличество просмотров3KУмный домТуториалВ прошлой статье «Многотарифный счётчик электричества для умного дома Home Assistant» я рассказал, как считать общее потребление электричества в доме на основе счётчика Zigbee и его интеграции в Home Assistant.В этот раз хочу поделиться конфигурацией, которая была у меня до установки общего счётчика — а именно, для подсчёта потребляемого электричества выключателем света.Одно из первых устройств, с которых начинается проект создания умного дома, у большинства это розетки и выключатели. И мне стало интересно считать, сколько электричества потребляется на освещения каждой комнаты, при этом в базовых моделях выключателей нет готовой функции подсчёта энергопотребления, тем более с поддержкой многотарифности.В итоге я разработал конфигурацию для Home Assistant, в которой указана мощность всех ламп, подключённых к выключателю, и которая считает всю потреблённую энергию с разбивкой по временным тарифам.График потребления энергии выключателем в Home AssistantКонфигурацияДля создания счётчика создадим три новых сенсора класса Power. У меня выключатель трёхклавишный Tuya Zigbee, поэтому он будут называться с префиксом gang3 switch.Создадим сенсор под каждую из кнопок, из параметров, которые нужно изменить — это значение мощности одной лампы и их количество.Например, у меня к первой клавише подключено 6 ламп по 3 Вт, ко второй — 6 ламп по 5,5 Вт, к третьей — 4 лампы по 5,5 Вт.
template:
 - sensor:
     - name: ""gang3 switch switch 1 energy power""
       unique_id: ""gang3_switch_switch_1_energy_power""
       device_class: power
       state_class: measurement
       unit_of_measurement: ""W""
       state: ""{{ 3.0 * 6 if is_state('switch.gang3_switch_switch_1', 'on') else 0 }}""

     - name: ""gang3 switch switch 2 energy power""
       unique_id: ""gang3_switch_switch_2_energy_power""
       device_class: power
       state_class: measurement
       unit_of_measurement: ""W""
       state: ""{{ 5.5 * 6 if is_state('switch.gang3_switch_switch_2', 'on') else 0 }}""

     - name: ""gang3 switch switch 3 energy power""
       unique_id: ""gang3_switch_switch_3_energy_power""
       device_class: power
       state_class: measurement
       unit_of_measurement: ""W""
       state: ""{{ 5.5 * 4 if is_state('switch.gang3_switch_switch_3', 'on') else 0 }}""

     - name: ""gang3 switch total energy power""
       unique_id: ""gang3_switch_total_energy_power""
       device_class: power
       unit_of_measurement: ""W""
       state: >-
         {{
           (states('sensor.gang3_switch_switch_1_energy_power') | float(0)) +
           (states('sensor.gang3_switch_switch_2_energy_power') | float(0)) +
           (states('sensor.gang3_switch_switch_3_energy_power') | float(0))
         }}
Cенсор gang3_switch_total_energy_power суммирует мощность всех трёх клавиш, чтобы получить общее потребление выключателя.Далее для перевода нашей общей энергии из просто Ватт в кВт/ч создаём еще один сенсор.
sensor:
- platform: integration
 source: sensor.gang3_switch_total_energy_power
 name: gang3_switch_total_energy_usage
 unit_prefix: k
 round: 2
 unit_time: h
 max_sub_interval:
   minutes: 1
Далее как я говорил, у меня трёхтарифный счетчик и для распределения потребления по ним, создадим сенсор типа utility_meter, который будет хранить суточное потребление по каждому из тарифов.
utility_meter:
 daily_gang3_switch_total_energy_usage:
   source: sensor.gang3_switch_total_energy_usage
   cycle: daily
   tariffs:
     - t1
     - t2
     - t3
И дополнительно, создаём автоматизацию с указанием в какой период времени, какой действует тариф для utility_meter.
- alias: Set tariff
 trigger:
   - platform: time
     at:
       - ""07:00:00""
       - ""17:00:00""
     variables:
       tariff: ""t1""
   - platform: time
     at:
       - ""23:00:00""
     variables:
       tariff: ""t2""
   - platform: time
     at:
       - ""10:00:00""
       - ""21:00:00""
     variables:
       tariff: ""t3""
 action:
   - service: select.select_option
     target:
       entity_id:
         - select.daily_gang3_switch_total_energy_usage
     data:
       option: ""{{ tariff }}""
Каждую минуту автоматизация запускается и устанавливает правильный текущий тариф для daily_gang3_switch_total_energy_usage.РезультатНа этом конфигурация готова, теперь в Home Assistant доступен новый сенсор daily_gang3_switch_total_energy_usage из которого можно получать суточное потребление электричества с выключателя по каждому из тарифов.Статистика потребления энергии выключателем в Home AssistantДругие полезные конфигурации для умного дома и обзоры умных устройств, можно найти в моём Tg канале.Теги:умный домhome assistanttuyazigbeematterХабы:Умный дом",3000,0,0,3 мин,https://habr.com/ru/articles/965722/,4781,529,1
Мой первый опыт инвестиций: почему я проваливаю план и как тикер FMMM спасает счёт,Finam_Broker,2025-11-13T08:31:24.000Z,"['Блог компании Финам', 'Финансы в IT', 'IT-компании', 'Читальный зал', 'Интервью']","Finam_Broker 2 часа назадМой первый опыт инвестиций: почему я проваливаю план и как тикер FMMM спасает счётУровень сложностиПростойВремя на прочтение3 минКоличество просмотров312Блог компании ФинамФинансы в ITIT-компанииЧитальный залИнтервьюКейсТерминал FinamTradeДисклеймер: в основу этого материала легло интервью с частным инвестором, Николаем Негораевым (май 2025 г.) Решил поделиться своим личным (и пока не слишком успешным) опытом погружения в мир инвестиций. Для многих начинающих, как я, всё начинается с постановки целей. Мне сразу посоветовали: прежде чем делать первую сделку, определи, зачем ты здесь, сколько и на какой срок готов вложить.Цели VS Реальность: Почему я пока проигрываюМои цели были довольно скромными, но ясными:Главная цель: Разобраться в инвестициях, понять, как работают инструменты и механики рынка.Долгосрочная ставка: Некоторые бумаги, вроде акций ""Сбера"", я сразу определил для долгосрочного удержания, в первую очередь ради хороших дивидендов.План-минимум: Не уйти в минус.Годовая цель: Получить доходность выше, чем по обычному банковскому вкладу.С первым пунктом я справляюсь, но, честно говоря, годовую финансовую цель я пока с треском проваливаю. И вот почему: часть покупок я совершал совершенно импульсивно, без чёткого понимания, зачем они мне нужны. Результаты этих спонтанных сделок, мягко говоря, неутешительные.FMMM: Мой ""накопительный счёт"" на биржеЧтобы хоть как-то поддержать общий счёт в плюсе и сохранить ликвидность, я вложил часть средств в надёжные инструменты. Об облигациях я уже рассказывал, а ещё одним спасательным кругом стал Биржевой паевой инвестиционный фонд (БПИФ) ""Финам - Денежный рынок"" с забавным тикером FMMM.Как это работает (простыми словами):По сути, это аналог накопительного счёта, но на бирже. Фонд инвестирует через сделки обратного РЕПО с Национальным клиринговым центром (ЦК). ЦК занимает деньги под залог ценных бумаг, а потом возвращает их с процентами.Главная фишка, почему БПИФы денежного рынка стали такими популярными: они приносят доход, близкий к ключевой ставке. Поскольку ставка сейчас на исторических максимумах, доходность у них очень привлекательная. Хотя, как сообщает ЦБ, ожидание скорого понижения ставки уже замедлило приток активов в такие фонды.Мой опыт использования фонда:Ежедневный прирост: Паи фонда немного прирастают каждый день — это приятно.Низкий порог входа: Можно инвестировать совсем небольшие суммы, буквально от 10 рублей.Ликвидность: Деньги можно вывести в любой момент. Я использую его как безопасное ""временное хранилище"" свободных средств, пока жду новую инвестиционную идею.По моим личным прикидкам, исходя из текущей динамики, доходность получается где-то 19-20% годовых. Да, это больше, чем по большинству накопительных счетов. Но есть нюанс: доход от БПИФ, в отличие от вклада, подлежит налогообложению. Однако, для меня возможность мгновенно вывести деньги и реализовать инвестидею на бирже полностью компенсирует этот недостаток.Валютные грабли: покупка долларов на внебиржевом рынкеПосле того, как в июне 2024 года Мосбиржа попала под санкции и торги долларами и евро там остановились, я решил опробовать покупку валюты на внебиржевом рынке (ОТС) через приложение FinamTrade. Инструмент там называется USD/RUB в разделе ""ОТС Валюта"".Я, конечно, тот ещё стратег: купил доллары по 92 рубля, то есть ровно в тот момент, когда рубль начал резко укрепляться.До сих пор не понимаю, почему рубль так упорно держится в районе 85 за доллар и ниже. Виденные мной объяснения не кажутся убедительными. В итоге мои доллары пока подешевели. Я уверен, что рано или поздно доллар вернётся выше 100 рублей, но, честно говоря, я думал, что это произойдёт гораздо быстрее.В общем, пока моя инвестиционная карьера — это смесь долгосрочных ставок, спасительных БПИФов и импульсивных провалов. В прошлых записках я упоминал, что даже успел получить первую прибыль. В следующей части я обязательно расскажу, как умудрился растерять большую её часть, и почему в этом виновен... Дональд Трамп! Теги:трейдертрейдингтрейдинг на фондовом рынкефондовый рынокфондовые рынкифондыфондовые индексыфондовая биржаХабы:Блог компании ФинамФинансы в ITIT-компанииЧитальный залИнтервью",312,0,0,3 мин,https://habr.com/ru/companies/finam_broker/articles/965760/,4170,562,5
Идеи vs бумажки: почему без документов ваше мероприятие обречено на хаос,kseniaevent,2025-11-13T06:42:45.000Z,"['Developer Relations *', 'Конференции', 'Управление персоналом *']","kseniaevent 4 часа назадИдеи vs бумажки: почему без документов ваше мероприятие обречено на хаосУровень сложностиПростойВремя на прочтение3 минКоличество просмотров122Developer Relations * КонференцииУправление персоналом * ТуториалПривет! Продолжаем говорить про организацию мероприятий для непрофессиональных ивентеров. HR, ассистенты, руководители проектов, маркетологи, пиарщики и все те, на чью долю выпало в нагрузку к основным рабочим обязанностям сделать мероприятие, эта статья для вас.Мы уже обсудили с чего начать подготовку к мероприятию, его упаковку, CJM участников, выбор площадки, организацию питания, как собрать программу, фото-/видеосъемку, организацию трансляции, спонсорские интеграции и работу с командой меропрития. Организация мероприятия – это не только креативные идеи и работа с подрядчиками, но и куча договоров, согласований и счетов. Правильный документооборот помогает избежать финансовых и юридических рисков, а также упрощает взаимодействие с партнерами, подрядчиками и командой.Документы — это не просто формальность. Они обеспечивают защиту прав и интересов всех сторон, участвующих в проекте, контроль и прозрачность расходов, а также юридическую уверенность, что все обязательства будут выполнены. Ошибка или отсутствие нужного документа могут привести к форс-мажорам, задержкам в работе и финансовым потерям. Поэтому грамотная система документооборота — залог успешного мероприятия. Здесь ваши первые помощники - юристы и бухгалтера. Первые - составят правильные документы, а вторые - не только будут все оплачивать, но и помогут проконтролировать закрывающие документы.На этапе подготовки вам понадобятся: Договоры с подрядчиками (аренда площадки, кейтеринг, технические службы, фотографы и видеографы, декораторы и др.). Уточняйте все условия: сроки выполнения работ, стоимость, форс-мажорные ситуации и ответственность сторон. Не работайте без договоров. Вам надо юридически зафиксировать все договоренности. Отсутствие договора = угроза срыва, ведь так легко что-то забыть и не выполнить. А вы даже компенсацию получить не сможете!Договоры с партнерами и спонсорами, где прописаны обязательства каждой стороны.Трудовые и волонтерские соглашения для команды и временного персонала на мероприятии.Сметы и бюджеты, где будут прописаны все расходы, чтобы вы понимали общий порядок сумм, а также мог ли бы после мероприятия сравнить с тем, что получилось. Некоторые документы обязательны для соблюдения закона и проведения мероприятия: Согласие на обработку персональных данных — необходимо при регистрации участников и работе с их контактной информацией.Согласие на фото- и видеосъемку — особенно важно для публичных мероприятий, где фото и видео будут использоваться в промо-материалах.Разрешения и согласования с властями — если мероприятие массовое или затрагивает общественные пространства.Лицензии — например, на использование музыкального контента с авторскими правами.Не пренебрегайте юридическими нюансами, чтобы избежать штрафов или отмены мероприятия.На этапе проведения мероприятия и после него важно:Контролировать выполнение всех договорных обязательств подрядчиков.Составлять акты приема-передачи оборудования и услуг.Финализировать закрывающие документы (счета, акты, отчеты) вовремя. Особенно обратите внимание на акты, если у вас в договоре есть автоприемка по истечению некоторого времени. Подготовить финальные отчеты по бюджету: сравнить план с фактомОтправить благодарственные письма партнерам и подрядчикам.Собрать обратную связь от участников и команды для улучшения процессов в будущем: на сколько быстры и эффективны ваши бюрократические процессы. Документооборот может показаться сложным, но без него организация мероприятия рискует стать хаотичной и рискованной. Создавайте структуру, используйте шаблоны и цифровые инструменты — и работа с документами станет системной и понятной. Тогда ваше мероприятие пройдет не только ярко, но и безупречно с точки зрения организации.Ксения КазаковаHR-проекты / Employer Brand / DevRelТеги:мероприятияконференциидокументооборотдоговорыподрядчикиhrХабы:Developer RelationsКонференцииУправление персоналом",122,0,0,3 мин,https://habr.com/ru/articles/964318/,4106,503,3
Как ускорить WebView в Android и доказать это цифрами,timkaopensoul,2025-11-12T21:03:30.000Z,"['Android *', 'Kotlin *', 'Google Chrome']","timkaopensoul 14 часов назадКак ускорить WebView в Android и доказать это цифрамиВремя на прочтение5 минКоличество просмотров564Android * Kotlin * Google Chromeили почему WebView-пререндер — не костыль, а инвестиция в UX и бизнесПроблема, с которой сталкивается каждый Android-разработчикWebView — самый непредсказуемый компонент Android: •	долго инициализируется  (особенно при первом вызове); •	потребляет память; •	часто показывает белый экран при загрузке; •	и, самое неприятное — нет очевидного способа измерить, насколько быстро пользователь увидел контент.На наших экранах WebView мы часто слышали от пользователей и QA:“Экран ""X"" иногда открывается моментально, а иногда — секунд через пять.”Это типичный случай, когда Android и Web сталкиваются в «серой зоне» UX.Поэтому мы решили не просто оптимизировать, а построить измеримую инфраструктуру.Что мы сделали?Мы решили построить полноценную инфраструктуру для: 1.	пререндеринга WebView — чтобы экран открывался мгновенно; 2.	измерения момента визуальной готовности (TTVR) — чтобы можно было доказать эффект в метриках.Чуть подробнее:WebViewPreloader — сервис, который греет и пререндерит WebView заранее (инициатор — что угодно: App Startup, фича, VM, эксперимент).WebViewReadyDetector — лёгкий детектор визуальной готовности (offscreen draw → «небелые» пиксели).CoreComposeWebView — контейнер, который умеет: взять готовый инстанс из пула, корректно пересоздать fresh, подключить детектор, управлять cookie-политикой и сам отправить метрики в аналитику⚙️ Архитектура (в 3 объектах)1. WebViewPreloader — “фоновый рендеринг без магии”Небольшой сервис, который создаёт WebView заранее (в App Startup) и прогружает нужные URL ещё до того, как пользователь откроет экран.Под капотом:Разогрев Chromium (пустой WebView + инициализация).prerenderUrlAsync API  использует обычный WebView (никаких приватных API)создаёт его в невидимом контейнере,вызывает loadUrl() заранее,сохраняет экземпляр в памяти в пуле (Map<String, WebView>),Cookie-политика:
	•	UsePreloaded — можно показать stale-контент (быстро, осторожнее с auth).
	•	DropAndFresh — строгая консистентность (медленнее, зато без рассинхрона).Когда пользователь реально открывает экран, наш инструмент просто берёт готовый экземпляр из пула — без повторной инициализации и без белого экрана.Пример вызова при старте приложения в AppStartup Initializer'eclass WebViewPreloadInitializer : Initializer<Unit>, KoinComponent {
    override fun create(context: Context) {
        val jobs = listOf(
            PrerenderJob(url = ""https://habr.com"", cookies = ""кука/куки"")
        )
        webViewPreloader.preloadWebviews(jobs)
    }
}💡 Ключевая идея — WebViewPreloader не привязан к AppStartup.Его можно вызвать в любой момент — из ViewModel, Experiment, Feature или Onboarding.По сути, он:Создаёт WebView в невидимом контейнере.Загружает URL в фоне (prerenderUrlAsync -> cм. дальше).Сохраняет готовый экземпляр в пул (Map<String, WebView>).Позже возвращает его при запросе через takePreloaded(url)Что за prerenderUrlAsync в AndroidX WebKit и как его правильно готовить?Коротко: это экспериментальный (alpha) API из AndroidX WebKit, который позволяет фоново подготовить рендер страницы до того, как вы откроете экр��н. Когда пользователь переходит на экран — мы “активируем” подготовленный рендер и показываем страницу без холодного старта WebView.Модель работы Запрос на пререндер: вы зовёте prerenderUrlAsync(url, options, callback) не имея видимого WebView на экране.Под капотом движок создаёт изолированный рендер-контекст и начинает загрузку.Готовность: в колбэк приходит сигнал “готов” (или таймаут/ошибка). Это значит, что движок может отдать первый кадр очень быстро.Активация: когда пользователь реально открывает экран, вы либо:берёте уже готовый WebView (если API умеет вернуть/передать его),либо вызываете “активацию” (если API работает как “подготовленный рендер”, а привязка к вашему WebView идёт при показе).Отмена: если экран не открылся — отменяете задачу, чтобы освободить память.Важно: в разных версиях Android System WebView (Chromium) поведение может отличаться (что именно исполняется до активации, как ведут себя тяжёлые операции, когда проматываются таймеры и т.д.). Поэтому — замеряйте TTVR в своём окружении и держите fallback.Почему это лучше, чем просто держать невидимый WebViewПодходПлюсыМинусыНевидимый WebView Предсказуемо и работает везде; вы контролируете экземплярНужен реальный View в иерархии/контексте (даже offscreen); риск утечек; overhead по памятиprerenderUrlAsyncСистемный путь: движок сам решает, что и когда готовить; меньше шансов на баги уровня View; потенциально лучше по памятиНовое API (alpha), поддержка не везде; поведение зависит от версии WebView; нужно аккуратно кодить fallbackПрактические советыТаймауты и отмена: ставьте таймаут ~6–10 с. Если не успели — падайте на fallback и логируйте причину (timeout, unsupported, error).Лимиты: не пререндерьте всё подряд. Заводите список кандидатов (главный WebView-экран, часто посещаемые сцены).Cookie-политика: если auth-cookie изменился — не используйте старый пререндер (или маркируйте его “stale” и осознанно решайте “UsePreloaded” против “DropAndFresh”).Метрики: Сохраняйте причину выбора стратегии (API/Fallback/Timeout) — это сильно помогает в анализе регрессий.QA-панель: добавьте в “debug overlay” строку с текущей стратегией и статусом пререндеринга (API/Fallback/Skipped).2. WebViewReadyDetector - Как понять, когда страница реально отрисовалась?Мы написали мини-инструмент, который следит за первыми кадрами и определяет момент, когда контент стал видимым.Вкратце:делает offscreen-рендер в Bitmap размером 48×48 пикселей;анализирует, сколько пикселей не белые;когда “заполненность” кадра превышает порог (например, 2%) — считает, что контент визуально готов.3. CoreComposeWebView — умный контейнерНадстройка над обычным AndroidView(WebView) в Compose, которая:умеет брать готовый пререндер из WebViewPreloader;создаёт fresh экземпляр, если нет пререндера;подключает WebViewReadyDetector;следит за cookie-политикой (UsePreloaded / DropAndFresh);шлёт метрики автоматически, без участия экранов.CoreComposeWebView(
    url = ""https://be-friendly.com"",
    providerName = WebViewProviderName.FRIENDLY,
    expectedCookie = authCookie,
    attachChromeClient = { webView -> /* ... */ },
    attachJsBridges = { webView -> /* ... */ },
)Таким образом, каждый WebView-экран становится самодостаточным компонентом, который:быстро открывается,и сам отправляет свою UX-метрику в аналитику.Типичные грабли и как мы их обошлиБелые UI страницы → false-negative. Решение: порог 2%, 3 подряд кадра, тонкая настройка (и логирование ratio).Cookie mismatch → «быстро, но не тот контент». Решение: политика DropAndFresh для auth-чувствительных экра��ов; метрика REASON=COOKIE_MISMATCH.SPA/лоадеры → быстрые «скелетоны» обманывают восприятие. Решение: фиксированная тёмная заливка фона у контейнера, дополнительный порог по непрозрачности (alpha).Alpha-API prerenderUrlAsync → на части устройств недоступно/ведёт себя по-разному. Решение: feature-gate + fallback + метрика “почему фолбэк”.Что получилось?Теперь у нас в Grafana:есть метрика TIME_TO_VISUAL_READY_MS для каждого WebView;видно, насколько пререндер реально ускоряет экран.Пример из боевых данных:SourceMedian TTVRp90Без пререндеринга3100 ms4900 msС пререндерингом1200 ms1900 ms🔥 В среднем экран загружается в 2,5 раза быстрее.И самое главное — теперь это измеримо и прозрачно.💰 Как это продать бизнесу💰Для менеджеров и аналитиков мы перевели TTVR в понятный язык:“Мы экономим пользователю ~2 секунды при каждом открытии WebView.”Если считать, что WebView-экран открывают 5 млн раз в месяц,то суммарная экономия времени — ~2800 часов пользовательского внимания.Плюс — меньше оттока на “белом экране”, выше вовлечённость.A/B-методология (чтобы было чему верить)Делим трафик на устройстве (персистентный флаг), чтобы не путать прогрев движка.Сегментируем: сети (Wi-Fi/Cell, RTT если есть), девайсы (классы по CPU/RAM), платформа (SDK/Chromium/WebView).Холгоут: часть трафика держим без пререндера даже после раскатки.Меряем медиану и p90 отдельно — бизнесу важны хвосты.Не смешиваем: разные провайдеры, разные экраны.Почему этот подход масштабируетсяЛюбой новый WebView можно включить одной строкой.Вся аналитика централизована в Clickhouse → Grafana.Код не зависит от ChromeClient или JS-интеграций.ИтогМы перестали “на глаз” судить, быстро ли грузится WebView.Теперь у нас есть инструмент, метрика и цифры, доказывающие, что UX стал лучше.Спасибо большое за внимание, за деталями можете обращатся в linkedIn  :)Теги:kotlibandroidwebviewperformanceoptimizationprerenderХабы:AndroidKotlinGoogle Chrome",564,0,0,5 мин,https://habr.com/ru/articles/965866/,8657,1027,3
Без интернета и шпионов: как мы собрали локального голосового ассистента,augap,2025-11-13T07:10:57.000Z,"['Блог компании Wiren Board', 'Умный дом', 'DIY или Сделай сам', 'Искусственный интеллект', 'Голосовые интерфейсы *']","augap 4 часа назадБез интернета и шпионов: как мы собрали локального голосового ассистентаУровень сложностиПростойВремя на прочтение8 минКоличество просмотров1.2KБлог компании Wiren BoardУмный домDIY или Сделай самИскусственный интеллектГолосовые интерфейсы * Облачные ассистенты вроде Алисы, Google Assistant и Siri давно стали привычными. Но у всех у них одни и те же слабые места: зависимость от быстрого интернета и риск утечки данных. И речь не только о персональной информации — дома нередко обсуждают темы, которые можно отнести к коммерческой или даже военной тайне. Неудивительно, что многим некомфортно говорить в присутствии микрофона, который каждое слово отправляет куда-то «в облако» (один из наших заказчиков прямо сказал: «никаких Алис в доме не будет»).На Хабре уже появлялись статьи про попытки заменить Алису на полностью локальные решения. Но почти всегда все сводилось к стандартной схеме: ESP32-микрофон → Home Assistant → intent recognition. Такая связка работает, но до действительно «умного» ассистента ей далеко.Мы пошли дальше и собрали свой голосовой ассистент, который:работает локально, на доступном и нетребовательном «железе»;использует wake word, идентификацию человека по голосу, а также нейросети и эмбеддинги, чтобы понимать команды в свободной форме;при этом отвечает голосом, без обращения к облаку.Что такое wake word и эмбеддингиWake word — это ключевая фраза, по которой ассистент «просыпается» (например, «Окей, Google» или «БАРИ!»). Система постоянно слушает, но реагирует только на это слово.Эмбеддинги — это способ представить слова в виде векторов чисел, чтобы нейросеть могла сравнивать выражения и находить близкие по смыслу. Благодаря этому ассистент понимает, что «вруби свет» и «включи лампу» означают одно и то же.Система заработала, но быстро стало понятно: готовые эмбеддинги из больших языковых моделей ограничивают ее возможности. Тогда мы полностью переработали архитектуру и сделали собственную модель эмбеддингов, обучаемую прямо на Orange Pi.Результат — новая версия Ассистента BARY, которая понимает естественную речь, справляется с ошибками распознавания и по-прежнему работает полностью оффлайн.Как все начиналосьПервая версия Ассистента BARY появилась, можно сказать, из чистого любопытства — как инженерный эксперимент: можно ли построить голосового помощника, который работает полностью офлайн, без интернета и облачных API.Архитектура получилась простой и понятной:wake word активировал систему по ключевому слову «БАРИ»;VOSK отвечал за распознавание речи (Speech-to-Text);эмбеддинги из LLM (предобученной языковой модели) превращали текст в вектор;выполнялся поиск ближайшего по смыслу совпадения среди заранее описанных команд;TTS-движок (Text-to-Speech) озвучивал ответ, чтобы ассистент мог подтверждать команды и говорить с пользователем.Ассистент действительно работал: можно было сказать «включи свет на кухне» — и реле срабатывало. Он даже понимал синонимы вроде «вруби» или «подсветку сделай».Но довольно быстро проявились ограничения этой схемы:готовые эмбеддинги из LLM нельзя было адаптировать под конкретный дом или манеру речи;база команд оставалась статической — ассистент не понимал, что «в спальне темно» означает то же самое, что «включи свет»;и главное — он не мог обучаться, а значит, не становился умнее со временем.Ассистент BARY работал стабильно и предсказуемо, но в какой-то момент стало ясно: чтобы по-настоящему «понимать» речь, ассистенту нужна своя обучаемая модель, а не внешние эмбеддинги.Что изменилось: новая версия Ассистента BARYВо второй версии Ассистента BARY мы полностью переработали ядро. Теперь у него есть собственная модель эмбеддингов — компактная embedding model на основе Transformer. Она не просто создает числовые векторы для слов, а учится понимать взаимосвязи между командами, формируя собственное векторное пространство смыслов.Модель небольшая — около 5 млн параметров и 128-мерный вектор на выходе. Этого достаточно, чтобы различать и связывать десятки тысяч фраз. Главное — она работает целиком на Orange Pi 5 Pro, без внешних API и без интернета.ОбучениеТеперь обучение модели возможно прямо локально.При небольшом наборе (до 5 000 команд) процесс занимает примерно 10–15 минут.На полном датасете (около 16 000 команд) — чуть больше часа.В будущем обучение, возможно, будет выноситься в облако — но только как опция. Само распознавание останется полностью локальным, и количество команд почти не влияет на скорость отклика.РезультатТеперь BARY не просто ищет совпадения по вектору, а интерпретирует сказанное в рамках огромного набора команд и их вариантов. Фразы вроде «в спальне душно», «тут что-то жарковато» или «открой шторы» срабатывают потому, что модель обучена на широком наборе фраз — включая разговорные, эмоциональные и даже не совсем литературные формулировки. Модель эмбеддингов размещает близкие выражения рядом и уверенно находит нужное действие.И все это работает на обычном контроллере стоимостью около 15 тысяч рублей.Orange Pi 5 ProESP32-S3-BOXКак это работаетПолная цепочка выглядит так:Wake Word → STT (VOSK) → Embedding Model → поиск по сходству → TTS (Piper) → действие / ответ.Что здесь происходитWake Word активирует систему по ключевой фразе — «БАРИ».VOSK преобразует речь в текст (Speech-to-Text).Embedding Model создает векторное представление команды, обученное на большом наборе примеров. Она не просто переводит слова в числа, а строит собственное «пространство смыслов», где «вруби свет в зале», «включи потолок на кухне» и «сделай поярче, а то темно» — близкие выражения.Далее используется поиск по сходству — команда сравнивается с базой векторов, и выполняется наиболее подходящее действие.Если нужен голосовой ответ, включается Piper, который синтезирует речь локально. Голосовые ответы применяются, когда ассистент сообщает данные или уточняет действия: «Температура в спальне 22 градуса» или «Закрыть шторы?»В остальных случаях Ассистент BARY отвечает короткими звуковыми сигналами — как в Алисе или Google Home. Это быстрее, понятнее и не раздражает при частом использовании.Как обучается Embedding ModelМодель обучается на автоматически сгенерированном датасете из пяти уровней:Базовые команды — включи/выключи, яркость, цвет, режим и т. п.Синонимы — генерация вариантов через перефразирование.Склонения и формы — вариации по помещениям, контексту и типичным ошибкам STT. Например, если VOSK часто путает «кухня» и «в кухне» или «светло» и «свет», эти варианты тоже добавляются в датасет.Эмоциональные и разговорные формулировки — чтобы ассистент понимал естественную речь, а не только строго заданные команды.Контекстные короткие фразы — с памятью до 20 секунд, что позволяет понимать уточнения вроде «а потусклее».Дисклеймер. Эмоциональные (и нецензурные) варианты добавились не сразу — об этом вспомнили, когда команд уже было «овердофига». После генерации набор команд вырос примерно в 2,5 раза, но именно эти фразы значительно повысили устойчивость модели к живой речи.В итоге наш примерный датасет охватывает 49 устройств, 18 помещений и ≈ 16 000 команд, из которых более половины содержат эмоциональные или разговорные формы.При этом модель эмбеддингов остается компактной, но покрывает почти все сценарии домашней автоматизации.Почему это важноГлавная цель у нас не изменилась — создать голосового ассистента, который работает без интернета и облака, но при этом понимает речь по смыслу, а не по заранее заданным шаблонам. Однако новая архитектура изменила все.Раньше локальные решения выглядели одинаково: Home Assistant, микрофон и поиск совпадений по словарю. Такой подход позволял включать свет или розетки, но любая фраза вне списка ставила систему в тупик.Теперь Ассистент BARY действительно интерпретирует речь. Он понимает естественные формулировки вроде:«в детской душно» → включить приточку сильнее;«надоел трек» → переключить музыку;«жарковато на кухне» → усилить работу кондиционера;«пора спать» → выключить свет во всей квартире.Новая модель стала гораздо устойчивее к ошибкам распознавания. Если VOSK немного искажает слова, модель эмбеддингов все равно правильно определяет намерение.Визуализация эмбеддинговКроме того, благодаря идентификации пользователя по голосу ассистент теперь может подстраиваться под каждого человека. Это не только про музыку — система запоминает личные предпочтения: кому комфортно при +22°, а кому при +25°, кто любит приглушенный свет, а кто потолочный на максимум. Такая персонализация решает «вечную войну» «мне холодно — мне жарко» и делает автоматизацию по-настоящему удобной.В перспективе этот механизм можно развивать бесконечно. Ассистент сможет добавлять продукты в личный список покупок, напоминать о привычках, предлагать сценарии вроде «пора спать, выключить свет?». Функциональность, которую крупные компании реализуют через облачные профили пользователей, BARY делает локально — прямо у вас дома.По сути, теперь это не просто офлайн-распознавание речи, а понимание контекста и поведения.Результаты и планыСегодня Ассистент BARY — уже не прототип, а стабильная рабочая система, которая управляет умным домом полностью офлайн. За последние месяцы мы довели архитектуру до состояния, когда ей можно пользоваться каждый день.Текущие результатыСкорость обработки команды: — около 150 мс на Orange Pi 5 Pro; — менее 20 мс на MacBook M1 Max при тестах.Контекстная память: около 20 секунд — ассистент понимает уточнения вроде «а потусклее», если их произнести в течение этого времени.Покрытие: 100 % подключенных устройств (49 устройств, 18 помещений).Работа без интернета: система полностью автономна — от wake word до озвученного ответа.Реакция на естественную речь: уверенно обрабатывает синонимы, разговорные и эмоциональные формулировки, включая нецензурные варианты.Что дальшеДальнейшее улучшение поддержки протокола Yandex Smart Home API — не для подключения к облаку, а для совместимости.Это позволит встроить Ассистент BARY в любые системы, которые уже умеют работать с устройствами Яндекса, сохранив локальную логику.Развитие Voice ID — персональные сценарии для членов семьи, запоминание предпочтений по голосу.Генерация датасетов — добавляем реалистичные диалоги, короткие уточнения и естественные формы речи.Голосовые ответы на базе Piper — улучшение тембра, пауз и скорости, чтобы ассистент звучал естественнее.Облачное обучение при локальном распознавании — пользователи смогут обновлять модели без тяжелого оборудования, при этом все распознавание останется офлайн.Предиктивная аналитика — следующий шаг к действительно «умному» дому.Ассистент учится замечать закономерности и предлагать действия заранее.Например, если хозяин каждый вечер в 22:30 выключает свет и включает кондиционер, BARY сможет предложить это сам, без команды.Или другой сценарий: в помещении установлены датчик движения и реле, но нет правил для автоматического включения света. Система анализирует поведение, видит, что свет включается вскоре после входа, и предлагает сделать это автоматическим правилом.Проблемы с микрофонамиИз всех узких мест у голосовых ассистентов самое непредсказуемое — звук. С нейросетью можно разобраться, с распознаванием — подружиться, а вот стабильное качество микрофонов остается лотереей.Мы пробовали разные варианты:Ноутбук использовался только на этапе разработки и тестов.Планшеты, встроенные в стены как стационарные панели управления, стали полноценными «ушами» системы. Но тут есть нюанс: когда планшет стоит в корпусе или за стеклом, чувствительность микрофона падает в разы — голос из дальнего угла комнаты он уже не слышит.ESP32-S3-BOX с двумя микрофонами показал себя лучше, но почти без шумоподавления и с сильной зависимостью от расстояния до говорящего.USB-микрофоны и китайские массивы — результаты непредсказуемы: в одной комнате работают отлично, в другой — плохо.Пока универсального решения нет: где-то помогает усиление на уровне драйвера, где-то — дополнительная фильтрация шума на стороне VOSK.Мы решили обратиться к сообществу: какие микрофоны, модули или схемы реально работают для голосовых ассистентов в «боевых» домашних условиях?Если вы делали похожие проекты — поделитесь, что выбрали и почему.Возможно, вместе мы соберем список проверенных решений и избавим будущих энтузиастов от этой боли.Планшет в настенном крепленииЗаключениеАссистент BARY начинался как эксперимент — можно ли сделать голосового помощника, который работает без интернета и при этом действительно понимает команды. Первая версия доказала, что это возможно. Вторая показала, что локальный ассистент может быть не просто «реактивным», а по-настоящему гибким и адаптивным.Теперь у Ассистента BARY есть собственная модель эмбеддингов, автогенерация команд, понимание естественной речи и учет контекста. Он не подключен к облаку и не отправляет данные наружу — все обрабатывается локально.Можно сказать, что мы перенесли «думалку» внутрь самой модели: теперь именно она отвечает за интерпретацию фраз и выбор нужного действия. Никакой магии — просто аккуратная работа с языковыми представлениями.Ассистент BARY пока не распознает все подряд и не способен мыслить, но с задачами управления домом он справляется отлично. Если Алиса живет в облаке, то BARY — у вас дома.А если дома есть свободный компьютер с нормальной видеокартой — можно подключить локальную LLM-интеграцию. В этом режиме ассистент начнет отвечать на общие вопросы и вести простые диалоги — тоже полностью офлайн, без отправки данных наружу.Пусть BARY пока не рассуждает в полном смысле, но уже уверенно понимает, чего вы от него хотите — и теперь способен не просто реагировать, а действительно помогать.Теги:Wiren BoardBARYАлисаголосовой ассистентраспознавание речиvoskPiperEmbeddingWake Wordумный домХабы:Блог компании Wiren BoardУмный домDIY или Сделай самИскусственный интеллектГолосовые интерфейсы",1200,0,0,8 мин,https://habr.com/ru/companies/wirenboard/articles/965856/,13702,1820,5
Доказательство гипотезы Коллатца,Sayman22,2025-11-12T19:18:22.000Z,['Математика *'],"Sayman22 15 часов назадДоказательство гипотезы КоллатцаУровень сложностиСреднийВремя на прочтение7 минКоличество просмотров1.5KМатематика * МнениеRecovery ModeГипотеза Коллатца (также известная как сиракузская проблема) — одна из самых известных нерешённых задач в теории чисел. Она формулируется следующим образом:Возьмём любое натуральное число. Затем будем применять к нему следующие правила рекуррентно:- если число чётное — разделим его на 2;- если число нечётное — умножим его на 3 и прибавим 1.Повторяя этот процесс, гипотеза утверждает, что независимо от начального значения последовательность неизбежно достигнет числа 1, после чего зациклится в последовательности 4 → 2 → 1 → 4 → …Несмотря на простоту формулировки и огромное количество численных проверок (вплоть до чисел, превышающих 2⁶⁸), строгое математическое доказательство этого утверждения до сих пор не найдено.Эта задача известна с 1 июля 1932 года и считается одной из старейших нерешенных задач теории чисел. Несмотря на простоту формулировки, гипотеза Коллатца оказалась чрезвычайно устойчивой к попыткам доказательства или опровержения. Были проведены различные исследования и проверки с целью доказательства гипотезы Коллатца, включающие:·             эмпирические исследования и вычислительные подходы, например, подход включает в себя проверку гипотезы Коллатца для огромного количества чисел с помощью компьютеров, хотя такие проверки могут подтвердить гипотезу для определенных диапазонов чисел, они не могут предоставить общего доказательства;·             аналитические подходы, например, метод включает в себя использование инструментов математического анализа, таких как теория чисел, динамические системы и математическая индукция, для доказательства гипотезы, но такие подходы до сих пор не привели к полному доказательству гипотезы;·             вероятностные и статистические подходы, например, подход использует вероятностные и статистические методы для анализа поведения последовательностей Коллатца, где пытаются показать, что вероятность того, что последовательность достигнет 1, близка к 1. Такие методы не предоставляют строгого доказательства гипотезы;·             использование машинного обучения и анализа данных, например, метод  с применением машинного обучения для выявления скрытых закономерностей и структур в данных, сгенерированных последовательностями по гипотезе Коллатца. Однако, такой подход не предоставляет математического доказательства. В данной статье предлагается доказательство гипотезы Коллатца (сиракузской проблемы), основанное на:- подмножествах вычетов по модулю шесть;- параметризации нечетных чисел;- ориентированном графе переходов под действием функции f(n);- закономерности последовательностей четных чисел;- единственном цикле в системе под действием функции f(n). Параметризация нечётных чисел на основе подмножеств вычетов по модулю шестьРанее в статье «Параметризация нечётных чисел на основе подмножеств вычетов по модулю шесть» было подробно рассказано о:·       подмножествах вычетов по модулю шесть;·       параметризации нечетных чисел.Сделано это было умышлено, так как в совокупности весь материал довольно большой, а эти две части возможно было представить, как отдельные пункты. Главное, что нужно понять в изложенном материале:- числа представленные, как подмножества вычетов по модулю 6, за счет представленной формулы параметризации нечетных чисел имеют фрактальную структурную детерминированность;- числа от 1 до 384 описывают 98, 4375% уровня масштабирования во фрактальной структуре. Так на «кадрах повторения» представлены расположения нечетных чисел на одном уровне масштабирования и их взаимосвязь с подпространствами. Диапазон в 384 числа получается от наибольшего значения периода изменения индекса внутри уровня для «кадров повторения» (6*64);- числа от 1 до 24576 описывают 100% уровня масштабирования во фрактальной структуре. Так на «фрагментах повторения» представлены расположения «исключительных» элементов для нечетных чисел на одном уровне масштабирования и их взаимосвязь с подпространствами. В совокупности с описанными «кадрами повторения» «фрагменты повторения» помогают полностью описать один из уровней масштабирования. Диапазон чисел до 24576 получается от наибольшего значения периода изменения индекса внутри уровня для «кадров повторения» с учетом «исключительных» элементов (6*64*64); Ориентированный граф переходов под действием функции f(n)Граф рассматриваемой числовой системы, состоящей из подмножеств вычетов по модулю шесть, является связным.G = (V, E)где V = {0,1,2,3,4,5} – вершины, соответствующие классам вычетов по модулю 6, то есть множествам чисел вида 6*k+r, где r ϵ V,E – ребра отражают возможные переходы между классами под действием функции f(n).Анализ действия функции на каждом из классов вычетов позволяет построить следующую таблицу переходов (Таблица 1).Таблица 1 – Переходы между классами вычетов по модулю 6.Номер подмножестваФорма чиселПереходы06*k0   или   316*k+1426*k+21 или 436*k+3446*k+42 или 556*k+54 На основе этой таблицы строится граф G, визуализированный на рисунке 1. Граф является связным: из любой вершины существует ориентированный путь в вершину 4, а оттуда — в вершину 2, затем в 1 и, в конечном счёте, в цикл, ассоциированный с последовательностью, соответствующей завершающему циклу гипотезы Коллатца.Таким образом, структура графа отражает глубокую внутреннюю упорядоченность динамики отображения Коллатца на уровне классов вычетов. Связность графа указывает на то, что все классы вычетов по модулю 6 взаимодействуют в единой динамической системе и, следовательно, ни один из них не может порождать изолированных или расходящихся траекторий.Рис. 1- Граф связанности подмножеств вычетов по модулю шесть числовой системы.Необходимо уточнить, что:·       из подмножества 0 система всегда ведет в подмножество 3;·       все нечетные подмножества (1, 3, 5) напрямую ведут в подмножество 4;·       после выхода из подмножеств 0 и 3 вернуться в них обратно нельзя;·       четные подмножества (0 и 2) тоже через несколько шагов ведут в подмножество 4;·       из подмножества 4 система движется к магистрали, а следовательно, к числу 1.В подтверждение утверждений о переходах между подмножествами в таблицах 3,4,5 показаны примеры переходов от начальных чисел в подмножествах, где показана четкая структурированность и полное покрытие чисел рассматриваемой числовой системы. Таблица 2 –Переходы из подмножества 3Число   из подмножества 3Переходв подмножество31049284154642164427824331004391184Таблица 3 –Переходы из подмножества 0Число   из подмножества 0Переходв подмножество6331260189324120301533618042213Таблица 4 –Переходы из подмножества 2Число   из подмножества 2Переходподмножество211844147120104261313216438191Таблица 5 –Переходы из подмножества 4Число   из подмножества 4Переходподмножество4221055168222115281423417540202Таблица 6 –Переходы из подмножества 1Число   из подмножества 1Переходподмножество144722413404195842576431944371124Таблица 7 –Переходы из подмножества 5Число   из подмножества 5Переходподмножество516411344175242370429884351064411244Закономерности последовательностей четных чиселПоследовательность четных чисел. Любое четное число n можно представить как n=m*2^k, где m — стартовое нечётное число, а k≥1. Такой набор {m, 2m, 4m, 8m, ...} называется последовательность четных чисел, начинающимся с m.Главная последовательность четных чисел — это множество всех натуральных чисел 2^s, гдекоторые сводятся к числу 1, то естьЕдинственность последовательности четных чисел. Каждое число принадлежит ровно одной последовательности четных чисел. Для любого существует одно и только одно нечетное число m, такое что:    где T(m) – последовательность четных чисел, имеющих одно стартовое нечетное число.Это следует из того, что:·             любое четное число n можно представить, как n=m*2^k·             m – уникально для данного n.Следовательно последовательности четных чисел не пересекаются.Полнота последовательных четных чисел. Объединение всех последовательностей четных чисел исчерпывает все множество четных чисел:гдеТаким образом:·             все четные числа организованы в последовательности четных чисел;·             каждый последовательность четных чисел имеет свое начало – нечетное число;система последовательностей четных чисел полностью покрывает четные N.Доказательство1)          Любое нечетное число из подмножеств 1, 3, 5 в соответствии с графом связанности выводит систему к подмножеству 4;2)          В соответствии с графом связанности только из 4 подмножества возможен вход в главную последовательность четных чисел, а попадание в главную последовательность четных чисел, приведет к циклу 4→2→1.3)          Рассмотренные последовательности четных чисел непересекающиеся и исчерпывающе покрывают все четные числа, сводя их к нечетным числам под действием функции f(n);4)          Параметризация нечетных чисел на основе подмножеств вычетов по модулю шесть подтверждает их фрактальную структурную детерминированность. Полное разнообразие расположения нечетных чисел в одном уровне масштабирования помещается в диапазоне значений 24576.5)          Существует единственный цикл под действием функции f(n), так как в противном случае нетривиальный цикл должен был бы содержать нечетное число:·             но тогда оно было бы из одного из подпространств k=1..6 подмножеств 1,3,5, так как в соответствии с последовательностями четных чисел, все они выводят к нечетным числам;·             и под действием функции f(n) в соответствии с графом связанности система перейдет к подмножеству 4, а после чего к главной последовательности четных чисел, а следовательно, к циклу 4→2→1; В соответствии с указанными пунктами, доказав, что все числа меньше числа 24576 по гипотезе Коллатца под действием функции f(n) (в соответствии с одним уровнем масштабирования) попадут в главную последовательность четных чисел, а следовательно, спустятся к единственно возможному циклу 4→2→1, мы докажем, что все остальные натуральные числа также сделают это, так как они являются частью последующих уровней масштабирования в фрактальной структуре и ведут себя подобно первому уровню в диапазоне чисел от 1 до 24576.Методом перебора такая задача для существующих ЭВМ решается довольно быстро. И для таких чисел (меньше 24576) это было сделано уже давно, как было сказано в начале публикации, в настоящее время проверено более 2⁶⁸ чисел.Таким образом, за счет: подмножеств вычетов по модулю шесть, графа связанности, последовательностей чётных чисел, параметризации нечетных чисел и единого цикла мы смогли свести доказательство гипотезы Коллатца к перебору конечного числа значений, показав поведение системы на одном из уровней масштабирования.Спасибо за внимание!Теги:гипотеза коллатцадоказательствотеория чиселХабы:Математика",1500,0,0,7 мин,https://habr.com/ru/articles/965844/,10754,1317,1
РЕД ВРМ. Как мы создали VDI-решение с кроссплатформенным подключением РЕД ОС и Windows,RED_SOFT,2025-11-13T08:56:03.000Z,"['Блог компании РЕД СОФТ', 'Виртуализация *', 'Системное администрирование *', 'Настройка Linux *']","RED_SOFT 2 часа назадРЕД ВРМ. Как мы создали VDI-решение с кроссплатформенным подключением РЕД ОС и WindowsВремя на прочтение7 минКоличество просмотров214Блог компании РЕД СОФТВиртуализация * Системное администрирование * Настройка Linux * Привет, Хабр! Меня зовут Артём, и я менеджер продукта РЕД ВРМ. В сегодняшнем материале я расскажу, почему мы всё-таки решили разработать отечественный VDI на базе протокола RED DIRECT, что РЕД ВРМ уже умеет, а чему мы научим его в ближайших редакциях.Как мы поняли, что пришло время РЕД ВРМПри проработке концепции мы изучали рынок и то, как живётся админам в условиях перехода с зарубежных VDI. Нашли боли, которые, возможно, откликнутся и у читателя — от мук выбора из open-source протоколов до нюансов, вылезающих после обновления продукта, отсутствие технической поддержки для западных решений и обилие технологий на отечественном рынке с несовместимыми протоколами доступа. Смотрим по порядку:Подводит дефолтный RDP протоколКогда начинается процесс импортозамещения, привычного прокола RDP недостаточно, поскольку он работает только винда-винда. Сочетание зарубежных решений и отечественных операционных систем для серверов и рабочих станций требуют больших усилий. Страдают технологии RemoteApp, возникают проблемы при работе с классическими виртуальными рабочими местами. Половина отваливается, половина не подключается, периферия не работает, ограничен функционал.В open-source не так всё гладкоСложность интеграции и настройки. Большинство open-source решений требуют ручной настройки компонентов на уровне брокера, гипервизоров, протоколов  доступа, что увеличивает сроки внедрения и риск ошибок.Ограниченная поддержка и сопровождение. При возникновении проблем приходится разбираться самому с минимальной поддержкой сообщества, порой без документации и объяснений.Отсутствие в реестре отечественного ПО: open source-решения не включены в Реестр российского ПО и не допустимы к использованию в государственных организациях.Много нюансов у open-source протоколов в целом:У каждого протокола свои болячки. Как только организация мигрирует на любую отечественную операционную систему, возникает ситуация — нужен протокол подключения для того, чтобы VDI нормально работал. Когда мы идём в сторону любых Linux-подобных дистрибутивов, здесь встречается целый набор протоколов. У каждого протокола есть свои нюансы, с которыми нужно разбираться. И тут многое зависит от навыков админа. Либо он разберётся и настроит хорошо, либо... Частично работающая периферия. Или казусы, когда работает только микрофон, а аудиоканал в наушниках не поддерживается. Или не работает веб-камера. В общем, как кому повезёт.Относительная безопасность. В open-source не всегда всё очевидно с точки зрения безопасности: можно попасть на зловред или уязвимость. Если админ взял решение не из репозитория, где разработчики тестируют утилиты перед выгрузкой, можно наткнуться на вирус-вымогальщик.Непредсказуемость. Даже если заработала периферия и есть шифрование трафика, у каждого open-source проекта есть собственное видение развития продукта. Администратор на свой страх и риск внедряет решение, но никто не гарантирует, что не возникнет проблем с совместимостью, обновлениями, дополнительным функционалом, который может со временем потребоваться компании.Вопросы оптимизации. Стоит коснуться и качества передачи картинки. Есть протоколы, которые забивают картинкой весь канал. В итоге на одного пользователя тратится кратно больше ресурсов, нужен очень большой пропускной канал — вплоть до необходимости его расширения. Это лишние работы, которых можно было бы избежать.Что умеет РЕД ВРМ — и к чему стремитсяСостав нашего VDIИтак, мы пошли своим путём и изначально весь бек- и фронтенд сделали своими руками. Брокер подключений, компоненты клиентских и агентских приложений — самописные приложения в рамках нашего продукта. Это удобно: не нужно переключаться с ветки на ветку, можно самостоятельно развивать собственный протокол подключения RED DIRECT, а также функциональность самого брокера, сервисов мониторинга, клиентского приложения и других компонентов системы.Какие технологии РЕД ВРМ особенно примечательны:Протокол подключения RED DIRECTVDI-предоставление полноценного рабочего местаТерминальные сервера с сессиямиТехнология RemoteAppИм посвятим отдельные статьи на Хабр, однако можете посмотреть краткое описание продукта в презентации, там изложены основные момен��ы. Уточню, что все эти технологии будут появляться в РЕД ВРМ по мере выхода редакций — все возможности каждой редакции войдут в протокол RED DIRECT. Редакций запланировано три: Стандартная, Терминальная и Промышленная. Почему именно такое разделение? Так будет легче выбрать состав технологий для решения конкретных задач.Стандартная редакцияВышла в апреле. Предназначена не только для импортозамещения классических решений VDI, но и для замещения open-source.Что умеет?Создавать виртуальные рабочие места. Среди доступных форматов: удаленное подключение к физическому ПК или виртуальной машине, работа со статическими и динамическими пулами. Динамические пулы дают кучу возможностей по масштабированию рабочих мест, оптимизации ресурсов виртуализации и пользовательским сценариям.Автоматизировать и масштабировать. Процесс создания рабочих мест можно автоматизировать: подготовить типовой шаблон и развернуть нужное количество рабочих мест с требуемым функционалом и настройками, затем — выдать доступ пользователям к необходимым ВРМ.Работать кроссплатформенно. RED DIRECT обеспечивает взаимодействие между РЕД ОС и Windows, позволяя переключаться между средами. В том числе можно переносить файлы* между удаленным рабочим столом и текущим устройством простым копированием вне зависимости от типа используемой операционной системы.	Работать с доменными учётками. Можно гибко предоставлять доступ из LDAP-каталогов Microsoft Active Directory и РЕД АДМ как доменным пользователям, так и группам к нужным ВРМ, формировать списки доступа и управлять настройками подключения. Управлять правами подключения. Реализована поддержка периферии c возможностью настройки на уровне групп доступа*: смарт-карты, USB-устройства, принтеры, буфер обмена.* а эти фичи можно включать или отключать на уровне групп доступа через портал администратора. Например, если не хочется, чтобы пользователи таскали файлы из виртуалки на свой рабочий стол.Как поможет?В настоящий момент РЕД ВРМ может пригодиться при реализации не��кольких сценариев:На период миграции. Например, есть специфичное приложение на Windows, аналог которому ещё не найден или разработчик по разными причинам не готов переписывать код под Unix. А работать над задачами в организации нужно. Тогда, пока разработчики переписывают код под Unix, можно обеспечить решение бизнес задач с помощью ВРМ.Например: можно взять физический ПК, развернуть на нем Windows с нужным ПО, установить агент РЕД ВРМ и дать доступ к этому рабочему месту пользователям с нужными правами и настройками протокола.Масштабирование. Работа с шаблонами уменьшает количество головной боли админа, особенно в условиях роста компании. Можно быстро создавать типовые рабочие места, не нужно разрываться между несколькими устройствами у разных пользователей и заниматься индивидуальной настройкой.Распределение ресурсов. Например, требуется больше оперативной памяти для отдела разработки. Создаем динамический пул с расширенным количеством ресурсов на ВРМ.------> сейчас мы находимся здесь <-------Терминальная редакцияПоявится в ближайшее время. Предполагает терминальные сервера, где каждому пользователю выделяется рабочее место в рамках общего сервера. Включает поддержку серверных операционных систем РЕД ОС и Windows Server, а также многосессионный режим работы на уровне протокола, когда несколько пользователей совместно используют вычислительные ресурсы терминального сервера.Терминальная редакция позволит организовать грамотный расход ресурсов для работы над большими проектами — например, при инженерном моделировании. Тут не нужно создавать 100 высокопроизводительных машин для 100 пользователей и выделять каждому ресурсы. Здесь можно организовать единый сервер с необходимыми мощностями и масштабировать ресурсы на тех сотрудников, кто нуждается в них здесь и сейчас.Ещё одна фишка Терминальной редакции — реализация технологии RemoteApp. Это когда на удалённом сервере существует библиотека приложений, и пользователь может локально на своём виртуальном рабочем месте запускать любое приложение из библиотеки. Что, конечно, удобно и существенно оптимизирует использование вычислительных ресурсов.Также терминальная редакция умеет объединять группы терминальных серверов в общий пул для балансировки нагрузки. При этом поддерживаются два алгоритма балансировки нагрузки: Базовый по количеству сессий и Расширенный с учетом фактической нагрузки, которую создают пользователи при использовании ресурсов CPU и RAM на сервере.Промышленная редакцияЭта редакция объединит в себе все возможности Стандартной и Терминальной, а также предложит дополнительные фичи.Например:Отказоустойчивый кластер. Возможность использования нескольких экземпляров РЕД ВРМ как на уровне самого брокера, так и на уровне базы данных для исключения единой точки отказа.Оптимизацию графической передачи данных. Мы планируем поддержать ряд видеокодеков, качественно сжимающих изображение, и оптимизировать потоки данных, которые передаются от клиента к серверу.Поддержку тонких клиентов. Клиентское приложение, которое ставится на все операционные системы, в том числе и на тонкие клиенты, которые предназначены для запуска VDI-клиента. Чем станет РЕД ВРМ?Сегодня я поделился с вами результатами работы команды РЕД ВРМ, а также рассказал (проспойлерил) планы развития продукта. Планы серьёзные — стать VDI-решением, которое можно смело внедрять как в крупнейших организациях, так и в небольших компаниях с мобильными офисами по всей стране. Здесь важно не только создать своё, но и учесть ошибки проектов из open-source, а также обязательно слышать и слушать наших заказчиков, чтобы сделать качественный продукт для реалий российского рынка.Мы стремимся поддержать как можно больше периферии. Работаем и ищем новых на рынке производителей, открытых к сотрудничеству, а также взаимодействовать с заказчиками, расширяя состав совместимых решений.Мы также понимаем, что с экосистемой, когда всё легко и быстро интегрируется между собой, работать проще и приятнее. Поэтому РЕД ВРМ является логическим дополнением к тем решениям, которые уже предлагает РЕД СОФТ. Есть операционная система, есть контроллер домена, база данных, виртуализация. РЕД ВРМ из коробки интегрируется со всеми этими продуктами, закрывает ряд болей по организации удалённого доступа и предлагает гибкие возможности по автоматизации создания рабочих мест. Процессом разработки и результатами будем делиться с читателями на Хабр — кстати, пишите, если вам интересно узнать какие-либо детали. В ближайших планах рассказать более подробно о нашем протоколе RED DIRECT.Теги:vdivdi решения vmwareред врмред софткроссплатформенностьrdp протоколwindowsred directвиртуализацияcitrixХабы:Блог компании РЕД СОФТВиртуализацияСистемное администрированиеНастройка Linux",214,0,0,7 мин,https://habr.com/ru/companies/redsoft/articles/965544/,11124,1405,4
Электронный шепот пробуждает квантовую душу света,master_program,2025-11-13T05:22:24.000Z,"['Математика *', 'Физика', 'Научно-популярное', 'Квантовые технологии', 'Нанотехнологии']","master_program 6 часов назадЭлектронный шепот пробуждает квантовую душу светаУровень сложностиСреднийВремя на прочтение7 минКоличество просмотров1.6KМатематика * ФизикаНаучно-популярноеКвантовые технологииНанотехнологииУченые из Московского физико-технического института (МФТИ) и Всероссийского научно-исследовательского института автоматики им. Н.Л. Духова (ВНИИА), Евгений Андрианов и Олег Толстихин, разработали теорию, которая показывает, как казалось бы пассивное облако свободных электронов, рожденных при ионизации газа мощным лазером, способно кардинально изменять саму квантовую природу этого лазерного света. Их работа, опубликованная в журнале Physical Review A и отдельно особо отмеченная редакцией (Editors’ Suggestion), предсказывает формирование так называемых неклассических и негауссовых состояний света, включая состояния с кольцеобразной функцией Вигнера, что открывает новые пути к созданию и управлению светом для будущих квантовых технологий.Работа выполнена при поддержке Российского научного фонда (грант № 24-12-00055).Взаимодействие сверхмощных, ультракоротких лазерных импульсов с веществом – это передний край современной физики, область, давшая начало аттосекундной науке (отмеченной Нобелевской премией по физике в 2023 году), которая позволяет заглядывать в мир сверхбыстрых электронных процессов. В этом мире, где события разворачиваются за миллионные доли миллиардной доли секунды, фотоны – кванты света – играют роль и инструмента, и объекта изучения. Обычно, когда речь идет о мощном лазерном излучении, физики представляют его как когерентное состояние – наиболее близкое к классической электромагнитной волне, где фотонов так много, что квантовые флуктуации кажутся пренебрежимо малыми.Однако несколько лет назад научное сообщество было заинтриговано: оказалось, что процессы, инициируемые сильным полем, такие как генерация высоких гармоник – когда атомы переизлучают свет на частотах, многократно превышающих исходную, – или надпороговая ионизация – когда электроны поглощают больше фотонов, чем нужно для отрыва от атома, – могут изменять квантовое состояние самого возбуждающего лазерного поля. Это был сюрприз, ведь считалось, что столь интенсивное поле ведет себя сугубо классически. Стало ясно, что квантовая природа света может проявляться даже в таких экстремальных условиях.При этом оставался важный, ранее систематически не исследованный аспект. При любом из этих процессов в газовой мишени неизбежно образуется значительное количество свободных электронов, вырванных из атомов мощным лазерным полем. Влияние этого «электронного облака» на квантовое состояние проходящего сквозь него лазерного поля и роль этих «осиротевших» электронов в судьбе света, их породившего, представляли собой область, требующую глубокого изучения.Именно на эту неисследованную территорию и направили свой взгляд российские физики-теоретики Евгений Андрианов и Олег Толстихин. Их цель состояла в том, чтобы разработать теорию, описывающую, как взаимодействие сильного квантованного лазерного поля с ансамблем свободных электронов, возникших в результате ионизации газа этим же полем, влияет на квантовое состояние самого этого поля.Для решения этой сложной задачи авторы построили точно решаемую модель. Это означает, что основные уравнения модели можно решить аналитически, без грубых приближений, что дает глубокое понимание физики процесса. В основе модели лежит гамильтониан – математический оператор, описывающий полную энергию системы, состоящей из N свободных электронов и одной моды квантованного электромагнитного поля (представляющей лазерный импульс). Этот гамильтониан включает энергию электронов, энергию поля (описываемую через операторы рождения и уничтожения фотонов) и, что самое важное, член, описывающий их взаимодействие.Критически важным аспектом модели стал учет того, что газовая мишень, хотя и состоит из отдельных атомов, для которых применимо дипольное приближение (предположение, что размер атома много меньше длины волны света), как целое может иметь размер, сравнимый с длиной волны лазера или даже превышающий ее. Это учитывается введением фазовых множителей для каждого электрона, зависящих от его положения. Совокупный эффект этих фаз выражается через комплексный безразмерный параметр η (эта), который характеризует когерентность взаимодействия всего ансамбля электронов с полем. Другой ключевой параметр модели, ξ (кси), зависит от плотности свободных электронов и связанной с ней плазменной частоты (характерной частоты коллективных колебаний электронов).Центральным математическим шагом стал процесс диагонализации гамильтониана. Эта процедура была выполнена в два этапа. Сначала, с помощью элегантного математического приема, известного как преобразование Боголюбова, авторы преобразовали часть гамильтониана, отвечающую за поле. Физически это эквивалентно действию оператора сжатия. В результате такого «сжатия» неопределенность одной из характеристик светового поля (например, его амплитудной компоненты) может стать меньше стандартного квантового предела за счет увеличения неопределенности другой (фазовой компоненты). Исходные фотоны как бы «одеваются» взаимодействием, превращаясь в квазичастицы с измененной частотой. Затем, оставшиеся после сжатия члены в гамильтониане были устранены с помощью оператора смещения, который, как следует из названия, «сдвигает» квантовое состояние поля в фазовом пространстве (пространстве его амплитудных и фазовых компонент), не меняя его формы.После этих преобразований исследователи проанализировали, как начальное когерентное состояние лазерного поля эволюционирует во времени. Оказалось, что под влиянием взаимодействия со свободными электронами поле переходит в более сложное смещенное сжатое когерентное состояние. Его параметры смещения и сжатия зависят от времени, от параметров ξ и η, а также от импульсов электронов. Поскольку электроны рождаются с определенным распределением по импульсам, для получения наблюдаемых характеристик поля было проведено усреднение по этому распределению, что ввело в рассмотрение еще один параметр модели, χ (хи), характеризующий это распределение.Расчеты, проведенные Андриановым и Толстихиным, выявили ряд новых эффектов.Прежде всего, было предсказано рождение неклассических состояний света. Это такие состояния, которые не имеют аналога в классической физике. Одним из ярких признаков неклассичности является субпуассоновская статистика фотоотсчетов: если измерять число фотонов, приходящих на детектор, то их разброс (дисперсия) может быть меньше среднего числа. Авторы рассчитали соответствующий параметр δ: если δ < 0, то поле неклассическое. Оказалось, что δ действительно может становиться отрицательным!  Рисунок 1. Область параметров, приводящая к δ < 0. Источник: Physical Review A.Критическую роль здесь играет параметр η и особенно его фаза, которая зависит от соотношения размера газовой мишени и длины волны лазера. Если пренебречь протяженностью мишени, этот эффект формирования неклассических состояний в данной модели не проявляется так ярко.Еще одним захватывающим предсказанием стало то, что функция Вигнера поля – его своеобразный квантовый «портрет» в фазовом пространстве – может приобретать сложную негауссову, кольцеобразную структуру.  Рисунок 2. Функция Вигнера негауссовой формы. Источник: Physical Review A. Для обычного когерентного состояния функция Вигнера представляет собой простой симметричный гауссов «холм». Взаимодействие со свободными электронами «размазывает» этот холм в кольцо, что является явным свидетельством перехода в негауссово состояние. Формирование такой структуры также критически зависит от параметров η и ξ, становясь более выраженным при их увеличении. Рисунок 3. Этот график иллюстрирует ключевое предсказание ученых. Он показывает, как изменяется профиль функции Вигнера (квантового «портрета» света) в зависимости от плотности свободных электронов (параметр ξ). По мере увеличения плотности, изначально простое гауссово распределение (характерное для обычного лазера) превращается в сложную кольцеобразную структуру (синяя линия соответствует срезу кольца с рисунка 2(c)), указывая на формирование экзотического негауссова состояния света. Источник: Physical Review A.Самое важное в этой работе – это демонстрация того, что взаимодействие со свободными электронами, неизбежно присутствующими в экспериментах с сильными полями, является существенным и ранее недооцененным механизмом изменения квантового состояния самого лазерного поля. Этот эффект необходимо учитывать наравне с уже известными влияниями генерации высоких гармоник и надпороговой ионизации для построения полной квантовой картины взаимодействия света с веществом. Евгений Андрианов, старший научный сотрудник и доцент кафедры теоретической физики им. Л. Д. Ландау МФТИ, так прокомментировал полученные результаты: «Наше исследование открывает удивительную картину: даже сверхмощный лазерный импульс, который мы привыкли считать почти классической волной, оказывается чутко реагирует на созданное им же облако свободных электронов, его квантовая природа претерпевает глубокие трансформации. Мы показали, что эти электроны – не просто «побочный продукт» ионизации, но также  способны «сжимать» свет и придавать его квантовому портрету, функции Вигнера, причудливую кольцеобразную форму. Нами двигал не просто академический интерес. Понимание и умение управлять такими неклассическими и негауссовыми состояниями света открывает прямые пути к новым квантовым технологиям, от более мощных квантовых компьютеров до защищенных систем связи». Работа российских ученых имеет большое значение не только для фундаментальной физики, но для практических приложений, в первую очередь в области квантовых технологий. Так, сжатые состояния являются ключевым ресурсом для квантовых вычислений с непрерывными переменными (например, для создания универсальных квантовых вентилей), а негауссовы состояния (такие как предсказанные состояния с кольцеобразной функцией Вигнера) могут быть использованы  для достижения квантового вычислительного преимущества. Предложенный механизм может стать новым способом их генерации. Кроме того, понимание как различные параметры (плотность газа, интенсивность и длина волны лазера, геометрия взаимодействия) влияют на конечное состояние поля, может позволить целенаправленно «конструировать» желаемые квантовые состояния сильных световых полей.Сделанное открытие показывает дорогу для целого ряда новых исследований. Предстоит провести экспериментальную проверку предсказаний, измерив статистику фотоотсчетов и реконструировав функцию Вигнера лазерного поля после его взаимодействия с ионизованным газом в условиях, предсказанных теорией. Также важной задачей является создание единой теории, которая бы одновременно учитывала влияние на квантовое состояние поля всех трех ключевых процессов: генерацию высоких гармоник, надпороговую ионизацию и взаимодействие со свободными электронами. Перспективно исслед��вание более сложных систем, например, расширение модели на случай многомодовых лазерных полей или взаимодействия с электронами в других средах, таких как твердые тела. Не менее важен теоретический поиск оптимальных экспериментальных условий для целенаправленной генерации конкретных типов неклассических состояний света. Наконец, требуется более детальное изучение квантовой запутанности, которая может возникать между световым полем и коллективным состоянием свободных электронов.Научная статья: Evgeny S. Andrianov and Oleg I. Tolstikhin. «Formation of nonclassical and non-Gaussian states of a strong electromagnetic field due to its interaction with free electrons produced by ionization of a target gas.» Physical Review A 110, 023115 (2024). DOI: 10.1103/PhysRevA.110.023115.Теги:функция Вигнера полянадпороговая ионизациясжатые состояниянегауссовы состоянияколлективные колебания электроновХабы:МатематикаФизикаНаучно-популярноеКвантовые технологииНанотехнологии",1600,0,0,7 мин,https://habr.com/ru/articles/965058/,11864,1439,5
Как мы в MWS внедряем роль продуктового инженера: новый вариант развития для разработчиков,a4izhov,2025-11-13T07:46:12.000Z,"['Блог компании МТС', 'Управление разработкой *', 'Карьера в IT-индустрии', 'Управление персоналом *']","a4izhov 3 часа назадКак мы в MWS внедряем роль продуктового инженера: новый вариант развития для разработчиковВремя на прочтение5 минКоличество просмотров233Блог компании МТСУправление разработкой * Карьера в IT-индустрииУправление персоналом * КейсВсем привет! Меня зовут Антон Чижов, я руковожу в MWS центром практик agile. Вместе с Александром Демидовым, директором по разработке MWS, расскажем о продуктовом инженере — новой роли, которая открывает для опытных специалистов более гибкие варианты карьерного трека. В классических командах каждый участник развивается в своем направлении, но такой вариант подходит не всем. Специалистам в одной предметной области — например, архитектуре, разработке на Go и так далее — сложно охватить и постоянно держать в уме цели продуктов: у них не так много возможных карьерных треков. Убрать эти ограничения позволяет концепция продуктовых инженеров — специалистов, сочетающих в себе несколько технических компетенций, а также развитые продуктовые и софт скиллы. Сегодня подробно рассмотрим эту новую роль и покажем, чем она может быть интересна, кому подходит, какие варианты развития открывает и что дает компании.Почему классическая структура команд теряет эффективностьПривычная модель обеспечивает четкое разделение ролей и обязанностей, что действительно снижает дублирование работы, но требует постоянной координации и контроля. Это приводит к росту числа менеджеров и к проблемам на стыках этапов жизненного цикла ПО (SDLC, Software Development Life Cycle). Именно переходы между этапами становятся основной зоной риска. Специалистам, помимо своих основных задач, приходится погружаться в бизнес-цели продукта и понимать смежные области. Например, архитектору изучать DevOps, тестировщику — аналитика, фронтендеру — бэкенд. Как правило, это выливается в дополнительную нагрузку, которая в классической структуре команд не учитывается. Чтобы ее снять, вводятся новые роли — штат постепенно разрастается, и людям все сложнее взаимодействовать. Кроме того, работа в командах с такими специалистами приводит к простоям (временным потерям) между этапами. Например, время ожидания между разработкой и началом тестирования напрямую влияет на общий Lead Time продукта. Изменить этот процесс можно переходом к небольшим командам из продуктовых инженеров (микростартапам) — новой формализованной ветке развития российской ИТ-индустрии. Переход к кросс-функциональному карьерному трекуСовершенствуя процессы разработки, мы расширили роль для Middle, Senior, Senior+ и назвали ее «продуктовый инженер». В нашем понимании это эксперт, который сочетает несколько технических компетенций и обладает развитыми софт скиллами.Внедрение роли у нас идет по двум направлениям: Первый путь связан с трансформацией продуктов и переходом к малым инженерным командам (МИК). Мы берем существующую архитектуру, проектируем целевое состояние и формируем новые команды вокруг него. Сервисы при этом приходится выделять более четко, потому что они не всегда построены идеально.Второй путь — индивидуальный: любой разработчик может подать заявку на портале. Продуктовый инженер может работать и вне МИК, — это допустимо и нормально.В MWS в этой роли работают уже несколько сотен человек. Из них более 10 специалистов закрепили переход юридически. Они заявили смежную компетенцию, прошли интервью у технических мастеров и получили новый грейд. Даже внутри монолита роль несет пользу команде. SDLC остается тем же, но за счет смежных компетенций инженер работает заметно эффективнее. Мы это видим по цифрам. Например, если специалист подтвердил уровень сеньора и по QA, и по разработке, его метрики качества соответствует старшему уровню сразу в двух ролях. Новая роль — основа для концепции МИК МИК — это формат организации работы, когда небольшая группа специалистов (обычно до 10 человек) отвечает за создание, развитие и поддержку конкретного сервиса или продукта. Состав команды подбирается исходя из целевой архитектуры и набора необходимых компетенций. При этом ключевой принцип МИК — наличие критических навыков как минимум у двух человек, что снижает риск зависимости от отдельных сотрудников (bus factor).Основные преимущества такого подхода связаны с масштабом команды: небольшой размер сокращает время поставки инкремента и упрощает управление сложностью задач. Однако есть и обратная сторона. Универсальность неизбежно усложняет обучение специалистов, потому что им приходится одновременно удерживать фокус на нескольких направлениях и поддерживать уровень экспертизы в каждом из них. Со временем это может приводить к выравниванию навыков: глубина знаний снижается, а команда начинает тратить больше ресурсов на поддержание компетенций, чем на развитие продукта.Поэтому продуктовый инженер — ключевая роль для формирования МИК. Он понимает смежные области и развивается как специалист широкого профиля, потому что продуктовые навыки позволяют экспериментировать. Кроме того, основные инновации происходят на стыке дисциплин, потому именно там он наиболее полезен. Формат малых команд привлекателен и для самих специалистов. Небольшой состав требует от участников широты компетенций. Чтобы оставаться автономными и закрывать все ключевые направления, каждый осваивает смежные области. Такой подход ускоряет обмен опытом внутри группы и снижает зависимость от внешних ролей.Примеры такого специалиста — фулстек-разработчик, совмещающий работу с фронтендом и бэком, или сеньор-бэкендер, берущий на себя роль архитектора. Такой продуктовый инженер способен спроектировать микросервис с подходом API First, понимая, как он будет взаимодействовать с внешним миром и какое место займет в продукте. А если умеет автоматизировать тестирование, он сам прогонит базовые тесты и задеплоит результат. Благодаря совмещению навыков из разных направлений продуктовый инженер работает эффективнее специалистов с классическими выделенными ролям. Он понимает, как код будет писаться и тестироваться, поэтому может сформировать необходимые требования и предложить оптимальную архитектуру. Итак, какой он, идеальный продуктовый инженер? Имеет как минимум два технических навыка. Такие специалисты не боятся экспериментировать, быстрее находят рабочие решения и растут как эксперты широкого профиля. Логично, что именно они драйвят появление ключевых инноваций.Знает свою предметную область. Даже если речь идет о микросервисе как об отдельной реализации, он оценивает его ценность для других команд. Видит продукт целиком. Понимает, как его сервис используется другими и как он встраивается в более крупные решения. Имеет развитые софт скиллы: умение ставить цели, понимать мотивацию, видеть сильные стороны коллег. Без этого равноправные эксперты в команде легко уходят в конфликты.Какие варианты карьерного развития появляютсяСтать продуктовым инженером можно не только из позиции сеньора. Этот путь открыт и для мидлов. Чтобы переход был прозрачным и объективным, мы используем институт технических мастеров для подтверждения второй компетенцию.Процесс выглядит так. Инженер подает заявку на внутреннем портале, указывает дополнительную специализацию (например, DevOps) и проходит интервью. На встрече присутствуют два технических мастера, которые оценивают уровень кандидата. После этого проводится проверка знаний — фактически экзамен по новой компетенции. Если результат успешный, сотруднику присваивается роль продуктового инженера и открывается следующий грейд.Для джунов сложнее: сначала необходимо вырасти до мидла в своей области, а затем параллельно освоить еще одну на том же уровне. Но и для них этот трек остается возможным.Роль продуктового инженера расширяет классическую узкую специализацию (I-shape) и предлагает новый вектор роста. У него своя система грейдов, при этом уровень выше, чем у выделенных ролей. Так, мидл продуктовый инженер ценится выше, чем мидл разработчик, потому что его компетенции шире.Например, QA-инженер может развиваться по классическому пути — до сеньора и техлида, наращивая лидерские навыки. Но появляется возможность выбрать и другое направление: освоить архитектуру или системный анализ и перейти в продуктовые инженеры.Внутри MWS мы создаем все условия для такого развития: поддерживаем сотрудников, помогаем выбрать путь и даем возможность расти. И уже видим интерес к новой роли: коллеги стали чаще запрашивать курсы по смежным специальностям. В целом мы сейчас в начале пути и только обкатываем новый формат, но уже видим, что он востребован внутри компании. Теги:ит-кадрымалые инженерные командыкарьерный трекПродуктовый инженеркросс-функциональная команда#-shapeТрансформация разработкиэффективность командной работыХабы:Блог компании МТСУправление разработкойКарьера в IT-индустрииУправление персоналом",233,0,5,5 мин,https://habr.com/ru/companies/ru_mts/articles/965362/,8668,1133,4
"AMD заявила, что будет поддерживать старые видеокарты много лет",Xcom-shop,2025-11-13T08:56:10.000Z,"['Блог компании Группа компаний X-Com', 'Видеокарты']","Xcom-shop 2 часа назадAMD заявила, что будет поддерживать старые видеокарты много летУровень сложностиПростойВремя на прочтение4 минКоличество просмотров245Блог компании Группа компаний X-ComВидеокартыМнениеИ старые, и новые видюхи теперь будут получать обновления одновременно.Поддержка видеокарт предыдущих поколений — больная тема для геймеров. Купив производительную карту за приличные деньги, никто не хочет через пару лет остаться без оптимизаций для новых игр. AMD долгое время оставалась в тени конкурентов по этому вопросу, но недавняя ситуация с противоречивыми заявлениями о будущем RDNA 1 и RDNA 2 заставила компанию расставить все точки над i. Эта новость оказалась хорошей для миллионов владельцев Radeon RX 5000 и RX 6000.Изменение политики обновления видеокарт AMDТеперь почти с полной уверенностью можно заявить, что графические ускорители на базе архитектур RDNA 1 и RDNA 2 продолжат получать игровую оптимизацию наравне с новыми поколениями.Владельцы Radeon RX 5000 (RDNA 1) и RX 6000 (RDNA 2) могут не беспокоиться о своевременных обновлениях драйверов. Все четыре поколения RDNA (с первого по четвертое) получат оптимизации для свежих игр одновременно в рамках единых пакетов драйверов. Это касается как Day One патчей для крупных релизов, так и последующих улучшений производительности.Подтверждение нового подхода к поддержке появилось после обращения экспертов Hardware Unboxed напрямую в AMD. Журналисты интересовались конкретными примерами: как быстро старые карты получат оптимизацию для грядущих хитов вроде Call of Duty: Black Ops 7, Resident Evil Requiem и Crimson Desert. Ответ оказался однозначным — «Да, оптимизация игр и поддержка всех RDNA серий 1–4 будут реализованы одновременно в обоих пакетах драйверов».Как вы понимаете, такой вопрос мог быть не случайным, а подготовленным пресс-службой AMD для журналистов, чтобы те его задали и получили правильный ответ, который как бы случайно был произнесен со сцены, а не распространен в пресс-релизе.Старые видеокарты теперь станут более актуальнымиБолее ранние ответы AMD на подобные вопросы вызвали серьезное беспокойство в сообществе. Многие восприняли их как намек на то, что новые архитектуры получают приоритет в оптимизации, а старые — останутся без должного внимания или получают обновления с задержкой. Такой подход является нормой не только в индустрии компьютерного железа, но и в целом среди производителей электроники, получающей обновления.Многие эксперты и IT-журналисты уверены, что именно негативная реакция пользователей и давление сообщества заставили производителя пересмотреть стратегию и публично гарантировать равноценную поддержку всех актуальных поколений видеокарт. Это показывает, насколько действительно важна обратная связь от пользователей.Если говорить во временной ретроспективе, семейство RDNA охватывает период с 2019 года по настоящее время. Из основных нововведений каждого поколения можно отметить следующее:RDNA 1 дебютировала в видеокартах Radeon RX 5000 и была начальной реализацией архитектуры, заменившей GCNRDNA 2 была улучшением и доработкой первого поколения без каких-то существенных улучшений
RDNA 3 принесла аппаратную трассировку лучей в серию RX 6000, применение 5-нм техпроцесса и увеличение кэша второго уровня до 2 МБ на SE (всего 8 МБ на GPU)
RDNA 4 добавила использование техпроцесса TSMC N4C (оптимизированного для более низкой стоимости), поддержку шины PCI-E 5.0 с 16 линиями и рекордную плотность транзисторов (150 млн/мм²)
RDNA 5 (в разработке) должна получить ядра Radiance Core, нейронные массивы и технологию сжатия данных в реальном времени для снижения нагрузки на пропускную способность памяти. Возможно она будет применяться в консолях нового поколения (например, PlayStation 6)Почему важно обновлять драйвер видеокартыТеперь владельцы RX 5000 и RX 6000 получили четкий сигнал, что их видеокарты останутся актуальными значительно дольше того, на что они рассчитывали изначально. В эпоху, когда новые AAA-игры требуют всё больше ресурсов и нередко выходят в полусыром состоянии и только потом доводятся до ума, своевременная оптимизация драйверов может добавить от 10 до 20% производительности в отдельных тайтлах. Для карт среднего уровня это критическая разница между комфортной игрой на высоких настройках и необходимостью снижать детализацию до средних значений.Особенно важно это для владельцев карт начального и среднего ценового сегмента. Когда вы покупаете флагман вроде RX 7900 XTX или GeForce RTX 4090, запас производительности позволяет не переживать о каждом проценте FPS. Но если у вас RX 5600 XT или RX 6600, каждое обновление драйвера, выжимающее дополнительные кадры, продлевает жизнь вашей системы как минимум на несколько месяцев.Вряд ли AMD сильно беспокоится об этом, но единая политика поддержки также сильно упростит выбор видеокарты на вторичном рынке. Зная, что RX 6000 будет получать оптимизации наравне с новейшими моделями, многие будут более смело рассматривать покупку б/у карты предыдущего поколения. Это особенно актуально в условиях, когда разница в цене между поколениями может достигать 30-40%.Будут ли обновляться старые видеокартыРешение AMD поддерживать четыре поколения одновременно создает прецедент в индустрии. Если компания действительно будет последовательна в этом подходе, это может заставить прямых конкурентов и производителей других компонентов пересмотреть свою стратегию поддержки. Как думаете, кто от этого выиграет? Конечно мы с вами.Впрочем, радоваться рано и чуть выше я не зря использовал слово «почти» в контексте того, что можно быть уверенным в продлении жизненного цикла старых видеокарт. Теперь надо внимательно следить за тем, как обещания будут реализованы на практике. История знает немало случаев, когда подобные заявления производителей расходились с реальностью. Покажут ли RDNA 1 и RDNA 2 тот же прирост производительности от новых драйверов, что и RDNA 3 и RDNA 4, или разработчики всё же будут уделять больше внимания новым архитектурам — покажет время.Пока мы можем только ждать и скоротать время за обсуждением ситуации в комментариях. Напишите, какой подход к поддержке старых видеокарт вы считаете правильным — равная оптимизация для всех поколений или фокус на новинках с базовой поддержкой устаревших моделей?Теги:xcom-shopamdстарые видеокартыХабы:Блог компании Группа компаний X-ComВидеокарты",245,0,0,4 мин,https://habr.com/ru/companies/x-com/articles/965982/,6315,859,2
ИИ для юристов: Как мы неделю учили нейросеть работать с юридическими шаблонами,aipanda_ceo,2025-11-13T07:00:54.000Z,"['Блог компании Ai Panda', 'Искусственный интеллект', 'Развитие стартапа']","aipanda_ceo 4 часа назадИИ для юристов: Как мы неделю учили нейросеть работать с юридическими шаблонамиУровень сложностиПростойВремя на прочтение4 минКоличество просмотров185Блог компании Ai PandaИскусственный интеллектРазвитие стартапаКейсОднажды, к нам пришли наши клиенты — юристы и заявили, что наш B2C(на секундочку) агрегатор с правильным промптом обходит по эффективности их дорогие нейросетевые юридические сервисы. Но! Всегда ведь есть но. Говорят — «Ребята, продук»т классный, но нам нужно больше. Научите его работать с нашими внутренними шаблонами документов, искать актуальные нормы права, подбирать свежую судебную практику».Так родилась задача: создать ИИ ассистента — конструктора юридических документов, способного генерировать документы на основе проверенных шаблонов. Путь к ее решению оказался куда более извилистым, чем мы предполагали, и растянулся на семь дней интенсивной работы. Если кому интересен сразу результат, то вот онОбучение модели OpenAI(Fine-Tuning). Первый деньИзначально задача казалась технически простой. У нас на руках было 2000 актуальных юридических шаблонов, предоставленных нашими клиентами-юристами. Логика подсказывала прямой путь: Fine-Tuning. То есть, «дообучить» готовую большую языковую модель (LLM) на этом массиве данных. План был прост, как молоток:Загружаем шаблоны.Проводим тонкую настройку модели.Наслаждаемся результатом.Однако искусственный интеллект быстро напомнил нам, что прямолинейность в сложных вопросах редко приводит к успеху.Первые эксперименты с RAG. День 2–4Первый же эксперимент обнажил главную проблему. Юридические документы, особенно в рамках одной категории, структурно очень похожи. При обработке тысячи шаблонов модель начала путаться. Текст из одного договора незаметно «перетекал» в другой, реквизиты и условия смешивались, создавая юридическую кашу и галлюцинации. Сгенерированный документ мог быть безупречным с точки зрения грамматики, но абсолютно непригодным с позиции права. Попробовали вылечить промптами, но безуспешно. Вторая проблема упиралась в технологические ограничения. Эффективно дообучить можно было лишь устаревшие модели OpenAI, такие как GPT-3.5, или небольшие локальные LLM. А мы привыкли работать с DeepSeek — моделью, которая, как показала практика, справляется с русским языком и юридическими нюансами лучше продуктов от OpenAI и, при этом, за адекватные деньги. Менять его на менее мощный инструмент не хотелось.День четвертый: Новый подход — RAG-архитектураСтало ясно, что заталкивать все знания разом в модель — тупиковый путь. Мы обратились к более изящному решению — RAG (Retrieval-Augmented Generation). Его философия в том, чтобы не переучивать модель, а давать ей нужные знания в момент запроса.Первый блин вышел комом. Мы поступили по учебнику:Пропустили все документы через модель для создания эмбеддингов (векторных представлений текста).Сохранили эти вектора в специальную базу данных.При запросе пользователя система искала в базе наиболее релевантные фрагменты текста и подставляла их в промпт для DeepSeek.Стало лучше? Несомненно. DeepSeek в силу своей изначальной мощности выдавал более качественные результаты. Но старые проблемы никуда не делись: система по-прежнему находила неполные фрагменты документов, поиск был неточным, и нейросеть могла «склеивать» информацию из разных источников. С другой стороны, а чего мы ожидали? Fine-Tune примерно так и работает. То-есть, мы сделали тоже самое, но ожидали другого результата. Кто не ошибается, тот ничего не делает.Развитие мысли с RAG. Пятый деньМы поняли, что проблема — в качестве «сырья». Сваливать документы в кучу бессмысленно. Нужна интеллектуальная структура. Мы решили разбить каждый шаблон не на случайные чанки, а на логические разделы, обогатив их метаданными.Чтобы не тратить недели на рутинную работу, мы снова призвали на помощь API DeepSeek. Модель помогла нам создать 50 тестовых JSON-файлов, где каждый документ был аккуратно разобран и описан.Пример нашей разметки:json{
  ""document_id"": ""AGENT_DOGOVOR_2025_001"",
  ""metadata"": {
    ""document_type"": ""Агентский договор на привлечение клиентов"",
    ""primary_category"": ""Гражданско-правовые договоры"",
    ""legal_area"": ""гражданское""
  },
  ""search_tags"": [
    ""агентский договор"",
    ""привлечение клиентов"",
    ""вознаграждение агента"",
    ""ответственность сторон""
  ],
  ""document_text"": ""Тут текст документа""
}Результат превзошел все ожидания! На тестовой выборке все работало безупречно. Документы находились быстро и точно, нейросеть, получая четко структурированную информацию, генерировала идеальные проекты. Воодушевленные, мы прогнали через DeepSeek оставшиеся 1950 документов.И снова — кошмар. При масштабировании до полного объема данных старые «болезни» вернулись. Векторный поиск по метаданным и тексту снова начал давать сбои, документы перемешивались. Мы были на правильном пути, но финальный барьер оставался непокоренным.Очевидная гибридная модель. Дни с 6 по 7Мы уже готовились к сложной битве с использованием LangChain и построением многоуровневых workflow, как пришла ключевая мысль, настолько простая, что не понимали, как мы сразу об этом не подумали.А что, если в векторной базе хранить только «указатели» на документы, а сами тексты держать в нетронутом виде отдельно?Мы кардинально изменили архитектуру:Векторная база теперь хранит только легкие, но идеально описанные метаданные и, главное, — ссылку на документ.Сам документ в чистом виде, отформатированный в Markdown для удобства чтения нейросетью, хранится на сервере.Да, мы теряем возможность делать поиск по всему тексту документа, но зачем оно нам? Запросы то достаточно четкие, например «Сделай акт сдачи‑приемки квартиры». Итоговая структура записи в базе:json{
  ""document_id"": ""AGENT_DOGOVOR_2025_001"",
  ""metadata"": { ... },
  ""search_tags"": [ ... ],
  ""document_name"": ""Агентский договор на привлечение клиентов..."",
  ""document_url"": ""https://.../Agentskiy_dogovor_2025.txt""
}День седьмой: Триумф и работающий продуктМы загрузили все 2000 документов в новой архитектуре, затаили дыхание и начали тесты. И вот оно — сработало!Алгоритм стал работать безупречно:Пользователь формулирует запрос.Векторный поиск по метаданным и тегам находит 5 самых релевантных шаблонов.Юрист выбирает нужный ему вариант.Система, получив document_url, забирает с сервера полный, неизмененный текст выбранного шаблона.DeepSeek, используя этот эталонный текст как основу, аккуратно подставляет в него данные пользователя и генерирует безупречный документ, строго соответствующий внутренним стандартам юридической фирмы.Проблема «галлюцинаций» была решена. Нейросеть больше не выдумывала структуру, а строго следовала шаблону. Поиск стал точным, так как работал не с трудноуловимым смыслом целого документа, а с четкими категориями, тегами и названиями. Добавили к этому юридическую оценку самого документа нейросетью и получили прекрасный конструктор документов. Как работает — можете посмотреть сами на бета версии нашего сервиса ИИ ассистентов — expai.pro.Теги:ииии-ассистентюриспруденцияюристысервисынейросетидокументыХабы:Блог компании Ai PandaИскусственный интеллектРазвитие стартапа",185,0,0,4 мин,https://habr.com/ru/companies/aipanda/articles/965806/,7098,880,3
Обо всём и наболевшем,ssw733,2025-11-12T13:20:59.000Z,"['Веб-разработка *', 'Интервью', 'Карьера в IT-индустрии']","ssw733 22 часа назадОбо всём и наболевшемУровень сложностиПростойВремя на прочтение2 минКоличество просмотров1.3KВеб-разработка * ИнтервьюКарьера в IT-индустрииМнениеПолчаса назад закончилось очередное первичное собеседование, на котором HR задавал странные вопросы: работали ли вы с Kubernetes, работали ли вы с Postgres, работали ли вы с Redis и тому подобное. На собеседованиях я всегда отвечаю честно: с Postgres и SQL работал достаточно плотно, а Redis и Kubernetes знаю лишь теоретически — понимание есть, пару раз пробовал в песочнице, но полноценного опыта нет. В целом, мне кажется, что на определённом уровне для разработчика уже не существует «сложного нового инструмента»: вопрос лишь в глубине понимания, которая пропорциональна времени, проведённому с ним.Почему эти вопросы кажутся странными? Потому что непонятно, чего именно хочет HR. Уточнить уровень грейда? Он вряд ли поймёт это, ведь сам не разработчик. В итоге, чтобы пройти первый этап, нужно отвечать на всё «Да». Разберёмся что это значит:Вы имеете большой опыт работы с ЯП, с основными его инструментами, разбираетесь в кэшировании, экспертно владеете SQL но не имеете опыта с Redis? Не проходите, т. к. разобраться с key-value RAM инструментом это, видимо, очень сложно для разработчика.Или например вы знаете докер от начала до конца, создаёте контейнеры, docker-compose, возможно работали ещё с какими-либо системами контейнеризации/виртуализации, но так получилось, что не работали с Kubernetes. Тоже не проходите.И более того, HR даже не понимает, что в его суждениях не так, он просто следует инструкции «Надо чтобы умел редис, кубер, постгрес». Можно предположить что будет, если бы назначили 2 этап собеседования с разработчиками, повезёт, чтобы попались толковые ребята, но часто бывает, что тебя собеседует старший разработчик, который и сам не очень компетентен. А бывает и такое, что в ходе разговора со «старшим программистом» над теоретической задачей ты предлагаешь объективно наиболее лучшее решение, а собеседник отказывается слышать о таком, потому что это не соответствует тому как его учили, не по канону это.Был у меня аутсорс проект, на котором заказчик что‑то хотел сделать, ПМ и аналитик изложили всё совершенно по другому, понадобилось ещё 2 недели разбора, созвона, чтобы понять, что в итоге хотели делать то? На одном из созвонов пришлось ставить ультиматум: либо делаем всё нормально, либо не делаем совсем, на что ПМ в открытую высказал: да тебе не пофиг ли что делаем, давай, мол закроем задачу, а там дальше пусть сами разбираются. Можно закрывать глаза на это, если бы хоть минимально работающий продукт был, но там вообще ничего не работало. Ну то есть безумие ситуации: Работник уговаривает надзирающего делать работу нормально, хотя наоборот должно быть. И с подобным я сталкивался, когда работал в большой компании, ПМ брал проект, забивал на него месяца на 3, то есть даже не знаешь, что какая‑то работа должна вестись по проекту. И спустя 3 месяца тебе предъявляют, почему ничего не сделано по проекту, и мол ты виноват.Настолько всё стало абсурдом в IT‑трудоустройстве, наплыв непонятных людей, которые вообще к техническим специальностям отношения никакого не имеют. Вбухали денег в IT, так оно ведь лучше не стало, а наоборот.Теги:абсурдсобеседованиеhrХабы:Веб-разработкаИнтервьюКарьера в IT-индустрии",1300,0,0,2 мин,https://habr.com/ru/articles/965726/,3318,481,3
Как правильно рассчитать толщину печатной платы?,MarinaPro25,2025-11-13T04:42:22.000Z,"['Блог компании ЭЛЕКТРОконнект', 'Производство и разработка электроники *', 'Электроника для начинающих', 'Дизайн', 'Подготовка технической документации *']","MarinaPro25 6 часов назадКак правильно рассчитать толщину печатной платы?Время на прочтение2 минКоличество просмотров816Блог компании ЭЛЕКТРОконнектПроизводство и разработка электроники * Электроника для начинающихДизайнПодготовка технической документации * FAQДелая проект, разработчик не всегда понимает, что толщина стеклотекстолита не равна финальной толщине печатной платы. В этой статье мы решили разъяснить из чего формируется финальная толщина печатной платы и как правильно ее рассчитать.Финальная толщина печатной платы формируется из нескольких основных компонентов:1. Толщины диэлектрической подложки (FR-4, Rogers и т. д.)Толщина материала может варьироваться от 0.1 мм до 3.0 мм, в зависимости от требований к прочности и гибкости печатной платы.2. Толщины медного слоя, используемого для создания проводящего рисункаТолщина медной фольги, используемой для создания проводящих дорожек, обычно измеряется в в микронах (микроме́тр (мкм, µm), это единица измерения длины, равная одной миллионной доле метра (10⁻⁶ метра) или одной тысячной доле миллиметра). На нашем предприятии мы используем фольгу следующей толщины: 12 мкм, 18 мкм, 35 мкм, 50 мкм, 70 мкм, 105 мкм . Для печатных плат с высокими токами или тепловыми нагрузками используются более толстые слои фольги.3. Толщины защитного покрытия, финишного покрытия ( HASL, иммерсионное золото, олово или серебро)Финишное металлическое покрытие защищает медную поверхность контактных площадок печатной платы от окислений и прочих повреждений. Это позволяет сохранить их паяемость, обеспечить плоскостность покрытия и надежный монтаж электронных компонентов, паяных соединений.«ЭЛЕКТРОконнект» предлагает следующие варианты финишных покрытий:Горячее лужение (HASL).Толщина 15-25 мкм.Иммерсионное золото на никелевом подслое ( ENIG).Толщина (3 – 5,0) мкм Ni + (0,06 – 0,1) мкм Au.Иммерсионное олово (Immersion Tin - ISn).Толщина 0,8–1,2 мкм.4. Толщина препрегаДля склеивания слоев в многослойной печатной плате используется препрег. «ЭЛЕКТРОконнект» предлагает две толщины препрега: 0,180 мм, 0,0686 мм. Количество препрегов и их толщина будут зависеть от конструктива многослойной печатной платы, от толщины фольги и так далее.-2Помимо этого, нужно понимать, что все материалы имеют допуск +/- 10 %. То есть всегда будет небольшой люфт в расчете толщины.Так же, для расчета толщины нужно учитывать толщину паяльной маски в том случае, когда она покрывает элементы топологии ( проводники, отверстия).Ограничение по толщине печатной платы на нашем производстве - 3,5 мм.Как влияет толщина печатной платы на ее качество:Более толстая печатная плата обеспечивает большую прочность и жесткость, но увеличивает её вес.Токопроводимость. Толщина медного слоя определяет способность печатной платы пропускать ток. Более толстый медный слой позволяет пропускать больше тока без перегрева.Паяемость. Финишное покрытие обеспечивает хорошую паяемость и защиту от окисления. Толщина покрытия и его тип влияют на надежность паяных соединений.-3Таким образом общая толщина печатной платы — это сумма толщин диэлектрического материала, медных слоев, осажденной меди, финишного покрытия, толщина любых других слоев (например, слоев препрега между внутренними слоями многослойной печатной платы) +/- 10%.Для расчета толщины нужно учитывать толщину защитной паяльной маски в том случае, когда она покрывает элементы топологии ( проводники, отверстия).Больше информации о материалах, которые используются в производстве «ЭЛЕКТРОконнект» Вы можете найти на нашем сайте.Больше о нас здесь:Сайт ТГ ВК Дзен Youtube Rutube Теги:печатная платаэлектроникатолщина печатной платыподготовка проектапроект платыпроизводство печатных платдизайн печатной платыправила проектированияХабы:Блог компании ЭЛЕКТРОконнектПроизводство и разработка электроникиЭлектроника для начинающихДизайнПодготовка технической документации",816,0,0,2 мин,https://habr.com/ru/companies/electroconnect/articles/965888/,3847,478,5
Время дорого стоит,ilyaRyb,2025-11-13T09:58:25.000Z,"['Блог компании Контур', 'C# *', 'IT-компании', 'Алгоритмы *']","ilyaRyb 1 час назадВремя дорого стоитВремя на прочтение7 минКоличество просмотров216Блог компании КонтурC# * IT-компанииАлгоритмы * КейсУ Контура более 10 тыс сотрудников и очень-очень много групповых встреч: около 30 тыс ежемесячно, мы считали. 👀 И бывает так, что нужно собрать сразу нескольких ребят в наиболее удобное для всех время. И начинается вот это вот: зайти на страницу человечка > посмотреть, какое время у него свободно > сопоставить со своим > проверить, а могут ли в это время остальные участники > обнаружить, что нет, и идти заново по кругу смотреть другие слоты, забывая, чё там у кого. 🙄 Да блин!Мы решили остановить эту котовасию✋🚫и добавить в наш внутренний портал (в Контуре используется Стафф) рекомендацию свободных слотов для всех участников встречи. Рассказываем и показываем, как реализовали это.Но для начала, что такое Стафф.Это внутренний веб-портал для сотрудников Контура: комбинация новостной платформы и справочника. Здесь можно найти информацию о коллегах, формировать сообщества, получать новости от разных отделов, выпускать статьи и общаться в комментариях. Стафф используется во многих рабочих процессах, и, чтобы не «выбиваться из контекста», не выходя из инструмента удобно просматривать занятость коллег в календаре и планировать встречи. Чтобы быстро планировать созвоны с большим количество участников мы решили добавить в этот календарь рекомендацию слотов.  Вот так это выглядит: когда ты выбрал всех участников, планировщик предлагает слоты, которые свободны одновременно у всех этих людей. И ничего не надо идти проверять самостоятельно. 👇  Наша система:Встроена в процесс планирования встреч.Проверяет календари участников встречи и свободные переговорки.Сама предлагает слоты когда это нужно, анализируя ситуацию.Даёт возможность выбирать слоты в один клик и высвобождает время на другие дела.Самое популярное время — 60 минут, его мы и выбрали как максимальное для нашего рекомендательного слота. Можно сразу внести время встречи, например, с 17:00 до 17:15, выбрать участников. А система предложит все свободные слоты и переговорки, куда можно пойти.Что ещё умеет наша система:Работает для любого количества участников: от двух до бесконечности.Предлагает слоты только с одной переговоркой, не требующей согласования. Адрес переговорки определяется в контексте текущих настроек места, а приоритет в предложениях — ближайшая к вашему этажу или первая по порядку, если адрес не по месту работы.Предлагаемые слоты — это общее ближайшее свободное время для всех участников и переговорки с 10:00 по 18:00 по часовому поясу организатора. Поддерживаем только этот период как самый сложный в планировании.Слоты одного дня предлагает по одному до 14:00 и после, чтобы закрыть предпочтения по разным периодам проведения встречи в течение дня.Рабочие дни определяются в соответствии с производственным календарем, чтобы минимизировать ошибки в планировании.Дальность поиска слотов — 2 недели.Рекомендации работают только для разовых встреч, регулярные пока не поддерживаются.Система предлагает до четырёх ближайших слотов с переговоркой и/или без, а те, кому не подходят предложенные варианты, могут воспользоваться опцией «Поиск слотов», где можно выбрать слоты любой продолжительности и перебрать найденные варианты переговорок.А что же внутри??Есть два вида запросов: за календарём офиса, который учитывает занятость переговорок на 1 день, и за рекомендациями слотов, которые учитывают занятость на две недели вперёд.  Запрос за рекомендациями выглядит так: https://api/calendar/suggest.Вот немного метрик по обоим запросам: видно, что медианный запрос всего в два раза дольше.  Поиск слотов для участниковЗдесь всё кажется простым: есть список email-ов участников, стартовая дата, и конечная дата для поиска. По этим параметрам первым запросом к базе данных Стаффа, используя новый бэкенд календарей, получаем все встречи в указанном промежутке.// Получаем события календарей участников
var calendarItems = await calendarService.GetEvents(emails, searchStartDate, searchEndDate);
// Ищем свободные слоты
var freeIntervals = FindFreeIntervals(calendarItems.Values, searchStartDate, searchEndDate, chillTime);И ищем свободные интервалы — записываем в currentFreeInterval первый подходящий слот, сортируем все встречи по времени начала и идём по его возрастанию. Если очередная встреча пересекается с currentFreeInterval, то объединяем их и записываем в currentFreeInterval, иначе им становится новый промежуток, а разницу между временем начала и конца добавляем к результату.  var busyIntervals = calendarItems
   .SelectMany(s => s.Where(item => item.BusyType != BusyType.Free))
   .Select(s => new TimeInterval(s.StartDate, s.EndDate))
   .Concat(chillTime?? [])
   .ToList();

// Сортируем занятые интервалы по времени начала
busyIntervals.Sort((a, b) => a.StartDate.CompareTo(b.StartDate));
var firstInterval = busyIntervals.Count > 0 ? new TimeInterval?(busyIntervals.First()) : null;
// Инициализируем текущий свободный интервал
var currentFreeInterval = new TimeInterval(minDate, firstInterval?.StartDate ?? maxDate);
yield return currentFreeInterval;

// Если первого интервала нет, значит busyIntervals пустой и там только один слот
if (firstInterval is null)
   yield break;

// Перебираем занятые интервалы
for (var i = 0; i < busyIntervals.Count - 1; i++)
{
   var currentInterval = busyIntervals[i];
   var nextInterval = busyIntervals[i + 1];
   if (currentInterval.Overlaps(nextInterval))
   {
       busyIntervals[i + 1] = currentInterval.Combine(nextInterval);
       continue;
   }

   currentFreeInterval = new TimeInterval(currentInterval.EndDate, nextInterval.StartDate);
   yield return currentFreeInterval;
}

// Добавляем последний свободный интервал
currentFreeInterval = new TimeInterval(busyIntervals[^1].EndDate, maxDate);
yield return currentFreeInterval;
Таким образом для θ(n) встреч всех участников получаем θ(n * log(n)) времени и θ(n) памяти, что почти не отличимо от линейной обработки по типу обычного маппинга.А дальше делим полученные промежутки на интервалы заданной длины и возвращаем фронтенду.Небольшая хитрость с рабочим временемНапомню, что слоты должны искаться с 10:00 до 18:00 в часовом поясе автора запроса. Было бы неэффективно сначала искать кучу свободных слотов лишние 16 часов в каждых сутках, особенно если учитывать, что встреч там почти ни у кого нет, а потом фильтровать их. Поэтому ко встречам, полученным из запроса к базе, мы добавляем фейковую занятость на нерабочее время.var chillTime = filter.CreateFakeMeetings(holidaysDates);


var intervals = await FindAsync(filter.Emails, filter.StartDate, filter.EndDate, filter.Duration, chillTime);А вот так она создаётся:  var workDays = filter.WorkingDaysSource.HasFlag(WorkingDaysSourceType.WorkDaysOfWeek)
   ? filter.DaysOfWeek
   : _allWeek;

var startDate = filter.StartDate.Date;
var endDate = filter.EndDate.Date;
var result = new List<TimeInterval>
{
   new(startDate.AddDays(-2), filter.StartDate),
   new(filter.EndDate, endDate.AddDays(3))
};

if (filter.TimeZoneSpan == TimeSpan.Zero)
   for (var currentDate = startDate;
        currentDate <= endDate;
        currentDate = currentDate.AddDays(1))
   {
       if (holidaysDates.Contains(currentDate.Date) || !workDays.Contains(currentDate.DayOfWeek))
           result.Add(new TimeInterval(currentDate, currentDate.AddDays(1)));
       else
       {
           result.Add(new TimeInterval(currentDate, currentDate.AddHours(filter.WorkStartHour)));
           result.Add(new TimeInterval(currentDate.AddHours(filter.WorkEndHour), currentDate.AddDays(1)));
       }
   }
Кстати, рекомендация слотов учитывает производственный календарь, и поэтому может рекомендовать встречи на рабочие субботы (например, 1 ноября), но при этом не предлагать их на новогодние каникулы.  Пересечение слотов со свободным временем переговорок  Реализовано тоже не очень сложно, но чуть хитрее. Подходящие интервалы для списка обязательных участников возьмём из прошлой части.Также первым запросом получаем все встречи в выбранном промежутке у переговорок в офисе, которые можно бронировать всем. А дальше надо пересечь их отдельно для каждой переговорки со слотами для участников.public async Task<Dictionary<string, TimeInterval[]>> FilterSlotsForRoomsAsync(List<TimeInterval> freeIntervals,
   IEnumerable<string> roomsEmails, DateTime startDate, DateTime endDate)
{
   var calendarsItems = await calendarService.GetEvents(roomsEmails, startDate, endDate);


   return calendarsItems.Select(item =>
           (roomEmail: item.Key, Intervals: FilterIntervalsByMeetings(freeIntervals, item.Value)))
       .Where(val => val.Intervals.Length > 0)
       .ToDictionary(item => item.roomEmail, item => item.Intervals);
}В этом нам поможет алгоритм двух указателей. Свободные слоты уже отсортированы, объединяем соседние встречи переговорки и сортируем их. А затем проходимся по слотам и параллельно по коллекции встреч, и добавляем в результат только слоты, которые подходят и для переговорки.  private static TimeInterval[] FilterIntervalsByMeetings(List<TimeInterval> timeIntervals, CalendarItem[] calendarItems)
{
   var result = new List<TimeInterval>();
   // Объединяет пересекающиеся интервалы (если они есть) и сортирует по возрастанию времени начала
   using var roomMeetings = CombineIntervals(calendarItems
       .Select(item => new TimeInterval(item.StartDate, item.EndDate))
       .OrderBy(item => item.StartDate))
       .GetEnumerator();
   roomMeetings.MoveNext();
  
   // Аналог 2-х указателей, только чуть красивее с Enumerator-ом вместо индексов
   foreach (var timeInterval in timeIntervals)
   {
       // Устанавливает в roomMeetings.Current максимально близкий (не заканчивающийся раньше) интервал к timeInterval
       // Если перенести MoveNext внутрь будет бесконечный цикл
       while (roomMeetings.Current is not null
              && roomMeetings.Current.Value.EndDate <= timeInterval.StartDate
              && roomMeetings.MoveNext())
           ;

       if (roomMeetings.Current is null || !timeInterval.StrongOverlaps(roomMeetings.Current.Value))
           result.Add(timeInterval);
   }

   return result.ToArray();
}Для n свободных ранее найденных слотов у обязательных участников, m переговорок и k встреч в среднем у каждой их них получается θ (m  (k  log(k) + n)) по времени и θ (m * k + n) по памяти.  Значительно лучше очевидного решения с асимптотикой θ (m  (n  k)) по времени. Имею в виду перебор для каждой переговорки всех слотов и проверку каждого на пересечение с её встречами.  Если говорить о точных числах, то для двухнедельного интервала k ~= 100, m~=30, n~=160 (при запросе на 2 недели по офису с интервалом в 30 минут). Разница времени обработки ответов от БД нашего решения и примитивного - в 16000 / 820 ~= 19 раз, и, соответственно, такая же разница в затрачиваемых ресурсах CPU.А потом результаты по переговоркам сортируются (если искать их в своем офисе) по удалённости от своего этажа, так что важно указывать своё местоположение в офисе.if (userFloor == null)
   return roomsIntervals;


return rooms.Where(room => roomsIntervals.ContainsKey(room.Email))
   .OrderBy(room => Math.Abs(userFloor.Value - room.GetFloorNumber()))
   .ToDictionary(room => room.Email, room => roomsIntervals[room.Email]);Готовы ответить на вопросы, поэтому задавайте их в комментариях!  Теги:бэкендалгоритмыпереговоркапланировщикХабы:Блог компании КонтурC#IT-компанииАлгоритмы",216,0,0,7 мин,https://habr.com/ru/companies/skbkontur/articles/965280/,11410,1343,4
3D-карта вместо инстинктов: как робот учится ползать и прыгать,Cloud4Y,2025-11-12T14:34:21.000Z,"['Блог компании Cloud4Y', 'Big Data *', 'Читальный зал', 'Робототехника', 'Научно-популярное']","Cloud4Y 20 часов назад3D-карта вместо инстинктов: как робот учится ползать и прыгатьВремя на прочтение4 минКоличество просмотров586Блог компании Cloud4YBig Data * Читальный залРобототехникаНаучно-популярноеПереводАвтор оригинала: Michelle HampsonВ Гонконге разработали технологию для передвижения четвероногих роботов. Теперь они почти как настоящие животные способны автономно преодолевать экстремально сложные препятствия. Роботы находят обходные пути там, где кажется, что пройти невозможно. Как это стало возможно и какие возможности открывает новая технология?Робот вместо человека в опасных условияхМеханические животные словно перенимают навыки своих проворных двуногих и четвероногих собратьев. Они, как по инстинкту, адаптируются к самым разнообразным типам местности. Робот проползает под скамейкой и перелезает через бордюр  Это принципиально меняет возможности человека. Только вдумайтесь: таких роботов можно будет производить для выполнения смертельно опасных задач. Яркий пример — мониторинг и оценка состояния нестабильных завалов после землетрясения, где каждое движение может спровоцировать новый обвал. До сих пор главной проблемой оставалась неспособность роботов не только физически преодолевать препятствия, но и распознавать и анализировать их.Многослойная карта местностиРешение пришло из Гонконгского университета. Команда под руководством доцента Пэна Лу и аспиранта Йеке Чэнь разработала революционную модель многослойного картографирования. В чем её суть? Вместо плоской карты выстраивается детальная 3D-модель рельефа. Это значит, что для принятия решений складывается картинка из множества различных слоёв препятствий — будь то пропасть, которую нужно перепрыгнуть, или узкий лаз с выступающими элементами, под которым необходимо проползти. Новая технология даёт машине не просто «зрение», а структурное понимание пространства, что позволяет отличать преодолимое от непреодолимого.Как лидар помогает роботам «видеть» мирВ основе способности робота ориентироваться в сложной местности лежит его «цифровое зрение» — технология под названием лидар (LiDAR). Аббревиатура расшифровывается как Light Detection and Ranging, что дословно переводится как «обнаружение и определение дальности с помощью света».Как устроен лидарПринцип работы лидара напоминает эхолокацию, только вместо звука он использует лазер. Устройство испускает в окружающее пространство тысячи невидимых лазерных импульсов каждую секунду и с высочайшей точностью замеряет время, за которое они, отразившись от объектов, возвращаются обратно. Это позволяет молниеносно вычислять дистанцию до каждого объекта и создавать детальную трёхмерную карту окружающего пространства в реальном времени.Именно эта способность «ощупывать» мир лазерным лучом делает лидар незаменимым для робота, пробирающегося через завалы. В отличие от камеры, которая просто видит плоское изображение, лидар предоставляет точные данные о глубине, форме и рельефе каждого препятствия.Обучение в симулятореПрежде чем столкнуться с реальными препятствиями, робот прошёл суровую школу в виртуальной реальности. Исследователи использовали симуляции, чтобы обучить его распознавать и реагировать на бесчисленное множество сценариев. В цифровой среде он освоил ключевые навыки передвижения: ползание, прыжки и карабканье.«Благодаря освоению различных навыков в процессе симуляции и обобщению знаний, робот способен переключаться между ними, чтобы преодолевать различные препятствия», — поясняет доцент Гонконгского университета Пэн Лу.Эта виртуальная подготовка наделила робота и ещё одной важной способностью: когда в реальном мире данных с датчиков оказывается недостаточно, он может восполнить пробелы, опираясь на оценки, сформированные в процессе обучения.От симуляции к реальному мируТеорию проверили на практике. В серии экспериментов, проведенных как в помещениях, так и на открытом воздухе, робот Unitree Go1 с лёгкостью демонстрировал приобретённые навыки. Он автономно прокладывал путь, самостоятельно принимая решение, когда проползти под преградой, а когда перепрыгнуть через нее.Но самым удивительным открытием стала непреднамеренная способность робота к планированию пути. Инженеры не закладывали в него этот алгоритм напрямую. Однако, сталкиваясь с препятствием, которое было слишком высоким для прыжка, робот не останавливался, а начинал искать обходной путь, методом проб и ошибок находя решение. Это доказало, что система способна не только следовать инструкциям, но и проявлять зачатки автономного принятия решений.Коммерция и существующие ограниченияНесмотря на впечатляющие успехи, у технологии есть и ограничения. Главное из них, как отмечает Лу, — это зависимость робота от данных симуляционного обучения. Он пока не может учиться «на ходу», непосредственно в реальном мире, а полагается лишь на тот опыт, что получил виртуально.Однако будущее проекта выглядит светлым. Учёные уже планируют коммерциализировать разработку, например, для автоматизированной инспекции на строительных площадках. Следующая важная цель — интегрировать в процесс обучения данные из реального мира, чтобы окончательно «стереть грань» между симуляцией и реальностью и создать робота, способного покорить абсолютно любую местность.Примечание редакцииТехнологии, описанные в статье, требуют значительных вычислительных мощностей для обучения и надёжного хранения больших данных. Решением таких задач занимается, в частности, российский облачный провайдер Cloud4Y, предоставляющий услуги для ML-расчётов и работы с данными.Теги:роботы3dкартытехникаробототехнкиаХабы:Блог компании Cloud4YBig DataЧитальный залРобототехникаНаучно-популярное",586,0,0,4 мин,https://habr.com/ru/companies/cloud4y/articles/965758/,5592,679,5
Вас скоро заменит ИИ. Опять. Или как искусственный интеллект меняет профессию разработчика,USSC,2025-11-13T08:43:16.000Z,['Блог компании Уральский центр систем безопасности'],"USSC 2 часа назадВас скоро заменит ИИ. Опять. Или как искусственный интеллект меняет профессию разработчикаУровень сложностиПростойВремя на прочтение9 минКоличество просмотров300Блог компании Уральский центр систем безопасностиМнениеНедавно OpenAI объявила о новой возможности ChatGPT — открывать через интерфейс чата сторонние приложения. Теперь через нейронку можно послушать музыку в Spotify или сделать презентацию в Canva. Больше не нужно самому изучать интерфейс, выбирать паттерны или песни — достаточно составить промпт, а нейросеть разберется в сервисе сама.Вокруг новой функции тут же возникло много слухов. Например, что фронтенд умирает. Ведь зачем нужен интерфейс, если пользователи взаимодействуют с сервисами через ИИ? Даже UX теперь не так важен, как раньше.Звучит как задел на начало новой эры в разработке. Так ли это и что на самом деле происходит с ИТ под влиянием вездесущих нейросетей — давайте разбираться. Спойлер: рынок и правда меняется, но до упадка ему еще очень далеко.Фронтенд превращается в сквозной интерфейс  Новая фишка ChatGPT пока что пилотная и работает только с единичными приложениями. Разработчики OpenAI сами интегрировали в нейросеть несколько сервисов, в частности Canva, Booking и другие. Подключить свой сервис к ChatGPT самостоятельно пока что нельзя — сейчас это прерогатива больших компаний, с которыми в OpenAI договариваются напрямую. На демовидео от OpenAI новая фича выглядит очень стильно.А вот как эта фича будет развиваться в будущем, пока сложно предсказать. Сами OpenAI, похоже, претендуют на то, чтобы стать новым универсальным интерфейсом для любых сервисов. Об этом явно говорит тот же App SDK — набор инструментов, буквально созданный, чтобы разработчики могли подключать свои приложения к ChatGPT. Пока что App SDK позволяет только тестировать и собирать приложения. Загрузку приложений в ChatGPT обещают реализовать в этом же году, но как скоро это пойдет в широкие массы — пока неизвестно. Источник Если предположить, что ChatGPT как интерфейс действительно взлетит — роль фронтенда и вправду может сместиться. Раньше пользователь должен был сам изучать UI, находить нужные функции, переходить по экранам. А в новой реальности ChatGPT будет делать все за него. Человеку не придется пользоваться интерфейсом напрямую. Соответственно, ему станет менее важно, насколько удобно расположены кнопки и легко ли найти детали оформления заказа. Как пример — Figma. Вместо того чтобы вручную создавать элементы, перетаскивать их, менять размер и добавлять текст, пользователь просто вводит промпт. ChatGPT в ответ выдает готовый дизайн. Источник Основной вопрос: значит ли это, что интерфейсы станут не нужны и их полностью заменит ChatGPT?Вряд ли. Судя по тому, что мы видим сейчас, интерфейс приложений все-таки отображается в окне чата. Они точно так же рендерятся, но уже не внутри браузера, а в ChatGPT. Как это выглядит, уже можно посмотреть на примере той же Figma или Spotify.Интерфейс Spotify со списком треков отображается в ChatGPT. Главное отличие: он проще, чем привычный UI плеера. ИсточникРаньше интерфейсы создавали с расчетом на то, что человек будет работать с ними вручную. С участием нейросети нужда в сложном UI понемногу отпадает — можно предположить, что UX/UI и дальше будет двигаться в сторону упрощения визуала. Тем не менее вряд ли универсальный интерфейс ChatGPT покроет 100% приложений во всем мире. И вряд ли нужда в UI «для людей» полностью отпадет. Причин несколько:Не все люди пользуются ChatGPT в принципе. Как и в целом нейросетями. Да, с каждым днем таких людей меньше, но они есть и никуда не денутся.Есть сервисы, которые принципиально не стоит открывать через ChatGPT. Например, корпоративные системы, которые в целях безопасности не выпускают за пределы внутренней рабочей сети.К ChatGPT вряд ли согласятся подключиться финтех и государственные сервисы. Это крайне небезопасно. Обмен информацией с нейросетью, которая имеет доступ в открытый интернет, — риск утечки данных и юридических проблем. ChatGPT продолжает учиться на данных пользователей, и отдавать ей чувствительные данные запрещено во многих компаниях.Не факт, что сами пользователи будут готовы отдавать ChatGPT свои учетные данные от сервисов. Особенно если к аккаунту, например, привязана банковская карта. Это опять-таки риск утечки персональных и платежных данных.Так что фронтенд никуда не исчезает. Его роль может сместиться, но не за день и не за два. В истории веб-приложений такое уже случалось — как с теми же мобильными устройствами. Когда-то верстка мобильных версий была редкостью, а теперь mobile first — золотой стандарт. Бэкенд взаимодействует с серверами OpenAI  В ситуации, когда ChatGPT берет на себя «пользовательскую» часть, меняется и роль бэкенда. Когда SDK пойдет в массы, сервер приложения будет взаимодействовать не только с собственным фронтендом, но и с серверами OpenAI. Так, чтобы пользователь мог получить доступ к сервису через ChatGPT. Технически это те же самые запросы, что и обычно. Бэкенду все равно, кто его вызывает: свой фронтенд или сервер OpenAI. Изменится только получатель и структура взаимодействий. К классической паре «клиент — сервер» добавится новая: «сервер нашего приложения и сервер OpenAI». На этом фоне важность бэкенда не просто не снижается — она растет. Бэк, в отличие от фронта, не так просто потеснить за счет нейросетей. Мало того что для него нужно больше ресурсов — он еще и реализует уникальную бизнес-логику, которую не сможет повторить AI. Ни сейчас, ни в обозримом будущем, если архитектура нейросетей принципиально не изменится.Условный пример. ChatGPT может забронировать отель в Booking на имя пользователя. Но механизм бронирования, связь с отелем, хранение данных заказчика и многое другое — все это он не реализует. Максимум — предложит фрагменты кода для построения такого сервиса. И то не факт, что они будут работать. Дело даже не в мощностях. Так происходит в первую очередь потому, что нейросети не умеют думать в привычном нам понимании. Это мощные шаблонизаторы, которые ищут закономерности в больших массивах данных и повторяют их. Инженерное мышление, креативность, логика взаимодействий — все это для них недоступно.Если человека научить складывать 2 + 2, он по похожему принципу научится считать и 2 + 3, и 3 + 5, и любые другие числа. Нейросеть — не научится. Она знает, что 2 + 2 = 4, потому что видела множество примеров с этими цифрами. Но логически вывести правила сложения других чисел она не сможет. А в бэкенде без логических выводов никуда.Главный вопрос — что может человек, а не насколько продвинутыми стали машины. Источник ИТ в целом: автоматизация не заменит человеческую логикуПриложения внутри ChatGPT — это частный случай влияния нейросетей на ИТ в целом. Если посмотреть шире, видно, что AI меняет не только пользовательский опыт. Он влияет и на сам процесс разработки.Самый очевидный пример — AI-ассистенты, которые есть в новых версиях практически всех популярных IDE и редакторах кода. Они не просто автодополняют написанные строки, а пишут за разработчика целые блоки кода. В том же GitHub Copilot достаточно описать, чего разработчик хочет от функции, и тот сгенерирует ее сам.Конечно, у таких ассистентов немало преимуществ:Ускорение отладки. Уже почти можно забыть о безуспешном поиске багов по 12 часов подряд — AI-ассистент найдет их гораздо быстрее и с минимальным вмешательством человека.Ускорение кодинга в целом. Особенно если речь идет о повторяющихся блоках кода. Бесконечные Ctrl + C и Ctrl + V превращаются в пару строк промпта.Упрощение прототипирования. Работающий прототип можно выпустить заметно быстрее за счет того, что часть кода сгенерирует AI.Но у повсеместного AI в разработке есть один подвох. Уже встречаются разработчики, которые отдают нейросети слишком многое и превращаются в «операторов кнопки Tab». Нейросеть генерирует для них код, а они даже не задумываются, что именно та написала.Из-за подобной тенденции и появляются мысли, что нейросети готовы заменить разработчиков. Но фактически люди заменяют себя сами. Они добровольно отдают нейросети слишком большое количество решений. И не всегда это приводит к хорошим результатам. Нередко на выходе появляются неочевидные баги, неоптимальные решения и дыры в безопасности.Приведем пример. Вот ответ нейросети на запрос: «Как сделать так, чтобы сервер на Go принимал запрос, делал поход в базу данных и возвращал список товаров? Не добавляй ничего лишнего, дай сразу готовый код. Нужно в продакшен».func productsHandler(w http.ResponseWriter, r *http.Request) {
  if r.Method != http.MethodGet {
    http.Error(w, ""Method not allowed"", http.StatusMethodNotAllowed)
    return
  }

  db, err := sql.Open(""postgres"", os.Getenv(""DATABASE_URL""))
  if err != nil {
    http.Error(w, ""Database connection failed"", http.StatusInternalServerError)
    return
  }
  defer db.Close()

  db.SetMaxOpenConns(25)
  db.SetMaxIdleConns(25)
  db.SetConnMaxLifetime(5 * time.Minute)

  rows, err := db.Query(""SELECT id, name, price FROM products"")
  if err != nil {
    http.Error(w, ""Database query failed"", http.StatusInternalServerError)
    return
  }
  defer rows.Close()

  var products []Product
  for rows.Next() {
    var p Product
    if err := rows.Scan(&p.ID, &p.Name, &p.Price); err != nil {
      http.Error(w, ""Data scan failed"", http.StatusInternalServerError)
      return
    }
    products = append(products, p)
  }

  w.Header().Set(""Content-Type"", ""application/json"")
  json.NewEncoder(w).Encode(products)
}С первого взгляда код, может, и выглядит рабочим. Но если присмотреться — в нем очень много проблем:в хендлере каждый раз устанавливается новое соединение с базой и конфигурация пула;нет проверки, действительно ли установлено соединение;нет разделения на слои, хотя Dependency Injection — фактически стандарт;нет отдельной конфигурации, переменная окружения загружается прямо в хендлере без проверок;нигде не прокидывается контекст, нет логирования и много чего еще.Технически нейросеть способна написать код и без этих ошибок. Но для этого нужно сделать к ней запрос, где будут описаны все перечисленные нюансы, причем одновременно. А на такое способен только разработчик, который понимает, что он делает.Почему отдавать все AI невыгодно для рынкаС первого взгляда код, сгенерированный нейросетью, — это выгодно для всех сторон. Молодые разработчики могут быстрее влиться в профессию без должного обучения. Бизнес быстрее выпускает работающие приложения и получает прибыль. Но если заглянуть чуть дальше, становится ясно: этот путь тупиковый.Нейросеть не человек и тем более не инженер. Она не способна на мышление и не может продумать что-то наперед. Поэтому при генерации кода не учитывает многие критически важные для бизнеса факторы:Не закладывает будущее масштабирование. Если бизнес вырастет, код придется переписывать с нуля, иначе его технически нельзя будет масштабировать.Не продумывает поддержку legacy-кода. Поддерживать будет сложно: нейросеть не думает об оптимизации зависимостей, обратной совместимости и долговременной поддержке. Пропускает ошибки и уязвимости. Нейросети учатся на уже существующем коде, в том числе с уязвимостями и неэффективными решениями. А затем воспроизводят их, создают дыры в безопасности или утечки памяти.Примеров, когда код от нейросетей приводил к ошибкам и сбоям, уже достаточно. Буквально в июле 2025 года известный AI-сервис для вайб-кодинга удалил ключевую базу данных в приложении, которое создали через него же. А до этого неоднократно подменял настоящие данные фальшивыми.Да, написанный с помощью нейросетей код может работать здесь и сейчас. Но в перспективе такой подход небезопасен для бизнеса. Риск потери данных и неочевидные ошибки не стоят того, чтобы выпустить приложение быстрее.Работать с опытными специалистами выгоднее, особенно в долговременной перспективе. Поэтому потребность в квалифицированных разработчиках в ближайшее десятилетие никуда не денется.Рынок ждет трансформация, но не упадок  Недавно Сэм Альтман заявил, что в ближайшем будущем AI заменят специалистов поддержки, врачей и разработчиков. Но такие заявления — скорее маркетинг, чем серьезные прогнозы. Мы, в свою очередь, предполагаем, что ситуация будет выглядеть примерно так:Сильные разработчики создают умный код. Умный код порождает слабых разработчиков. Слабые разработчики создают плохой код. Плохой код порождает нужду в сильных разработчиках. Это, конечно, шутка. А если строить прогнозы всерьез — можно выделить несколько трендов, которые прослеживаются уже сейчас. Они действительно меняют сферу, но потребность в разработчиках не снижают. Вот какие прогнозы можно сделать на ближайшие 3–5 лет.Разработка и отладка ускорятся за счет AI. Код быстрее пишут, тестируют и отлаживают — нейросети берут на себя шаблонные задачи. Конечно, разработчикам все еще нужно проверять написанное и составлять промпты так, чтобы закрыть большинство распространенных ошибок. Но это все равно быстрее, чем писать вручную.Разработчики будут больше думать о бизнес-логике. Если отдать рутину нейросетям — у разработчиков освободится время и энергия, чтобы уделить больше внимания потребностям бизнеса. Они не просто будут писать код — они будут решать задачи компании, в которой работают.Чтобы влиться в ИТ, по-прежнему нужно будет учиться. Чтобы написать качественный код, нужно хорошо разбираться в теме. Это справедливо, даже если разработчик пользуется AI-инструментами. А те, кто компенсирует нейросетями нехватку знаний, вряд ли будут востребованы на рынке труда.Часть фронтендеров перейдет в бэкенд. Это уже более смелое предположение — не факт, что все будет именно так. Но есть вероятность, что компании смогут разрабатывать интерфейсы усилиями меньшего количества людей. Во фронтенде много повторяющегося кода, а с такими задачами нейросети справляются особенно успешно.Одно понятно точно. Рынок осознает, что без человеческого участия нейросети не принесут долгосрочной выгоды. А значит, квалифицированные ИТ-специалисты будут цениться еще сильнее. Нейросети — это инструмент, а не замена человека. Если, конечно, человек не решит заменить себя сам. А что вы думаете по этому поводу?Автор: Федор Якушков @OyminiRole1776Теги:ииии помощникразработкамнениенейросетиopenaiпрофессияХабы:Блог компании Уральский центр систем безопасности",300,0,0,9 мин,https://habr.com/ru/companies/ussc/articles/965966/,14218,1963,1
Как на Хабре каждый раз хоронят Flutter,lil_master,2025-11-12T19:39:43.000Z,"['Dart *', 'Разработка мобильных приложений *', 'Flutter *', 'Программирование *']","lil_master 15 часов назадКак на Хабре каждый раз хоронят FlutterУровень сложностиПростойВремя на прочтение10 минКоличество просмотров1.2KDart * Разработка мобильных приложений * Flutter * Программирование * Recovery ModeПредставьте, что вы Flutter-разработчик. Вы заходите на Хабр. В комменты статьи про Flutter. И там опять: «Flutter уже не развивается давно, нет 3д», «Dart — зачем нужен этот новый странный язык», «Google закроет проект, там в Индии всех разрабов уволили, они уже на кладбище». И это каждый. Божий. День.Вот что интересно: пока на Хабре хоронят Flutter, 30% всех новых iOS-приложений пишутся на нём. BMW делает на Flutter приложение для своих машин. Google Pay работает на Flutter. Credit Agricole Bank переписал на него банковский апп на миллион пользователей — и поднялся с 10-го на 3-е место в рейтинге. Но на Хабре Flutter, конечно, мёртв. Как и в прошлом году. И позапрошлом.Конкретный процент Android-приложений на Flutter:По состоянию на 2023 год, в Google Play Store опубликовано около 500,000 Flutter-приложений, при этом общее количество приложений в Google Play Store составляет примерно 1.58-3.5 миллиона AppinventivOwebest (в зависимости от методики подсчета).Это означает примерно 14-32% от всех приложений в Google Play Store, в зависимости от того, как считать общее количество. И, естественно, в основном новых приложений больше.Важный нюанс: Данные от Apptopia показывают, что в App Store (iOS) Flutter вырос с 10% всех отслеживаемых бесплатных приложений в 2021 году до почти 30% в 2024 году Google DevelopersMedium. К сожалению, аналогичные точные данные именно для Android отдельно найти сложнее.Почему так происходит? Потому что критиковать — это дофига проще, чем разобраться. Потому что мемы про «Google кладбище» работают лучше, чем скучная статистика. И потому что у каждого есть знакомый, который слышал от знакомого, что Flutter тормозит. Проверено.Сейчас я покажу, как выглядит типичная «похоронная» дискуссия на Хабре, разберу по косточкам главные обвинения, и объясню, почему Flutter не просто жив, а чувствует себя охренительно. Слабонервным критикам лучше отойти от экранов.Главный миф: «Google убьёт Flutter, как убивал всё остальное»Это самая популярная страшилка. И она работает, потому что у Google действительно есть кладбище проектов. Google Reader, Google+, ещё тонна сервисов — всё это Google закрыл. И теперь каждый раз, когда в Google что-то увольняют, на Хабре начинается: «Всё, Flutter помер».А теперь факты. Flutter — это 3-й по популярности проект Google на GitHub. Больше только у TensorFlow и Kubernetes. Сейчас у Flutter ~174k★, у TensorFlow ~192k★, у Go ~130k★. То есть Flutter, скорее, №2 среди крупнейших Google-проектов по звёздам (если не считать Kubernetes как CNCF). Google переписывает на Flutter собственные приложения — NotebookLM (AI-ассистент от Google Labs, 2025), Google Pay, Google Ads. Если завтра Google решит закрыть Flutter, им придётся выкинуть в помойку половину своих мобильных продуктов. Это не Reader, который был экспериментом. Это core infrastructure.Ещё момент. Даже если — чисто гипотетически — Google завтра возьмёт и уйдёт в отпуск на год, Flutter продолжит работать. Open source, детка. Samsung, Microsoft, Canonical и куча других контрибьюторов уже в проекте. Бывший PM Flutter-команды Google Лиа Джаретт прямо сказала: если Google забросит, крупные компании с радостью возьмут проект под крыло. Её команда FlutterFlow готова дальше развивать Flutter.А что по цифрам? Stack Overflow Developer Survey 2024 (65 тысяч разработчиков): Flutter используют 9.40%, React Native — 8.40%. Admired: Flutter 60.60% vs React Native 56.50%. GitHub stars: Flutter 170k, React Native 121k. JetBrains Survey: Flutter — самый используемый мультиплатформенный фреймворк с 2021 года. Statista: 30% разработчиков используют Flutter — это самый популярный кросс-платформенный фреймворк.Короче, если Flutter мёртв, то это какой-то очень живой труп, который бегает быстрее всех остальных.«Dart — это Java образца 2005 года»Это прям классика жанра. Цитата с Хабра: «По сути, сейчас Dart — это ООП язык с довольно низким порогом входа, который, по большей части, позволяет делать плюс-минус то же самое, что и все остальные аналогичные языки. Только, по ощущениям, в среднем хуже. Очень похоже на Java образца года этак 2005».Окей, давайте по пунктам.Null safety. Да, Dart добавил sound null safety только в 2021 году. Это поздно. Но знаете что? Dart стал единственным языком, который успешно добавил полноценный sound null safety в уже существующий язык. Не в новую версию с нуля, не форкнув всё к чёрту, а реально мигрировав живую экосистему. Kotlin, Swift — у них null safety был с первой версии. У Dart этого не было, и команда проделала гигантскую работу, чтобы это внедрить. Весь pub.dev мигрировал. 55 тысяч пакетов. Это не «провал проектирования», это эволюция.Records и patterns. Dart 3 (май 2023) добавил records — возможность возвращать несколько значений из функции и destructuring их в переменные. Добавил patterns — полноценный pattern matching, algebraic data types, более функциональный стиль. Это современные языковые возможности, которых в Java до сих пор нормально нет.Extension types, class modifiers, async/await из коробки. Dart не стоит на месте. Он оптимизирован для UI, имеет AOT и JIT compilation, быстрый как чёрт, и учится за пару дней, если вы знаете любой C-подобный язык.«Где киллер-фича Dart?» А вот она: Dart — это единственный язык, который идеально подходит для Flutter. Весь его синтаксис, вся архитектура — всё заточено под декларативный UI. Hot reload работает, потому что Dart поддерживает JIT. Компиляция в нативный код — потому что Dart поддерживает AOT. Null safety, async/await из коробки, isolates для многопоточности — всё это работает вместе как швейцарские часы. Можно было бы Flutter сделать на другом языке? Теоретически. Но на практике — нет, потому что ни один другой язык не даёт такую комбинацию возможностей.«Flutter тормозит и жрёт память»Окей, это серьёзное обвинение. И оно, в отличие от предыдущих, имеет под собой некую почву. Проблемы с производительностью у Flutter иногда есть. Но тут важны два момента.Момент первый: в 99% случаев вы этих проблем не заметите чаще чем у других технологий. Те приложения, которые торомозят у меня - написаны не на Dart/Flutter, а, в основном, на Java, Python, JS. Те приложения, которые я пишу на Flutter - не тормозят. Цитата от Ильи Вирника из Яндекс Про (2023): «Недостатки есть у всего. Flutter ни в коем случае не серебряная пуля, он не избавлен от проблем. На мой взгляд, именно производительность для Flutter всё ещё остаётся основной бедой. Вот мы разрабатываем очень большое приложение, суперапп, которое использует огромное количество платформенных возможностей. Основные его сценарии завязаны вокруг карт, что само по себе уже крайне тяжеловесно. При этом очень важная оговорка: таких Flutter-приложений, в которых проблемы с производительностью заметны, можно по пальцам пересчитать.»Вы делаете типичный B2C или B2B апп? Никаких проблем. Вы делаете суперапп с картами и миллионами объектов? Тогда да, будет сложнее. Но таких приложений — единицы. Момент второй: Flutter в большинстве бенчмарков быстрее React Native. Исследование nateshmbhat (2024): Flutter 3.19.5 vs React Native 0.74.1 — Flutter показал меньше APK size, меньше CPU usage, меньше memory на Android; на iOS Flutter вообще уделал React Native по всем метрикам. Исследование marcosouz4 (декабрь 2024): Flutter 3.24.5 vs React Native New Architecture — Flutter outperforms across all metrics. Почему? Потому что у Flutter нет JavaScript bridge. Нет overhead на сериализацию и десериализацию данных между JS и нативом. Flutter компилируется в machine code. Нативный код. Это принципиально другая архитектура. И с каждым новым выпуском производительность растет. Impeller. Flutter 3.10 (май 2023) сделал Impeller (новый rendering engine) дефолтным для iOS. Это устранило shader jank — когда UI подтормаживал при первом рендере сложных шейдеров. Flutter 3.x поддерживает стабильные 120 FPS на ProMotion-дисплеях. Память сократили на 50% для сложных экранов. Frame build times улучшили на 20%.Размер приложений. Да, Flutter добавляет к приложению свой движок. Это 4-9 МБ «стартовых». Но реальные бенчмарки показывают, что итоговый размер Flutter APK часто меньше, чем React Native, потому что Flutter делает aggressive tree-shaking и выкидывает весь неиспользуемый код. А главное — это не 2019 год. В 2025 году 9 МБ — это вообще не проблема. Средний размер приложения в App Store — 40-50 МБ. Flutter добавляет 9 МБ к этому. Ну и что?«Экосистема слабая, библиотек нет, всё заброшено»Это моё любимое. Потому что это критика образца 2019 года, которую люди повторяют в 2025-м, не удосужившись проверить.Цифры: pub.dev — 55 тысяч пакетов (2024). Рост ~10 тысяч пакетов за год. Для сравнения: в 2019 году было 350 Flutter-зависимых пакетов. Тысячи. Сейчас — 55 тысяч. Stack Overflow: у Flutter 160 тысяч вопросов, у React Native — 130 тысяч. Это говорит не о проблемах, а о большей вовлечённости сообщества.Да, некоторые пакеты заброшены. Как и в npm. Как и в любой экосистеме. Но популярные пакеты активно поддерживаются: riverpod, bloc, dio, get, drift, freezed, go_router (официальный пакет Google для роутинга). Firebase имеет first-class integration с Flutter — 62% Flutter-разработчиков используют Firebase. И да, в проблемах Firebase винят тоже Flutter. «Нет официальной поддержки аудио, SVG, пуш-уведомлений». Это правда. И это — философия Flutter. Flutter не пытается запихнуть всё в core framework. Вместо этого есть огромная экосистема сторонних пакетов. Нужен аудио? Вот вам just_audio, audioplayers. Нужен SVG? Вот flutter_svg. Пуши? firebase_messaging, flutter_local_notifications. Всё это есть, работает, поддерживается. Да, это сторонние пакеты. Но они качественные.«State management — это боль, никто не понимает, что выбрать»Окей, это частично правда. У Flutter действительно дофига вариантов state management: Provider, Riverpod, Bloc, Cubit, GetX, Redux, MobX, и ещё тонна других. И да, это запутывает новичков.Но вот в чём прикол: это не баг, это фича. Flutter не навязывает вам единственный правильный способ. Вы выбираете то, что подходит вашей команде и вашему проекту. Делаете маленький апп? GetX или Provider. Делаете энтерпрайз-монстра? Bloc или Riverpod. Нужна строгая архитектура? Bloc с event'ами и state'ами. Нужна простота? Riverpod.Да, это создаёт когнитивную нагрузку. Да, новичку сложно выбрать. Но это взрослая разработка. В React Native тоже миллион способов управления состоянием. В нативной iOS-разработке — MVC, MVVM, VIPER, Clean Architecture, и каждый сеньор считает, что его подход единственно верный.Официальная рекомендация Google: начинайте с Provider или Riverpod. Если проект растёт и усложняется — переходите на Bloc. Всё. Это не ракетостроение.«Инструменты тормозят, IDE падает, кодогенерация медленная»Это реальная проблема на больших проектах. build_runner может тупить на 50 тысячах строк кода. Android Studio плагин может жрать память и подвисать. Hot reload иногда не подхватывает изменения, и приходится делать full restart.Но:Во-первых, это проблемы больших проектов. Если у вас 5-10 тысяч строк — всё летает. Проблемы начинаются на энтерпрайзных монстрах. Но это справедливо для любой технологии. Большие Android-проекты на Kotlin тоже тормозят. Большие iOS-проекты на Swift тоже собираются по 10 минут.Во-вторых, это активно улучшается. Flutter 3.35 (август 2025) стабилизировал hot reload для web. DevTools постоянно обновляется. Анализатор улучшается. Да, это не Kotlin с KSP, который шустрее. Но это и не катастрофа.В-третьих, сравните с альтернативами. React Native Metro bundler тоже не идеал. Xcode для нативной iOS-разработки — это вообще отдельный вид ада, если честно. Flutter DevTools хотя бы работают стабильно и кроссплатформенно.«Нативный UI лучше, Flutter выглядит не по-платформенному»Это философский спор. Flutter рендерит свой UI через собственный движок (Skia, теперь Impeller). Это значит, что Flutter-приложение на iOS выглядит так, как вы его запрограммируете. Хотите Material Design? Пожалуйста. Хотите Cupertino виджеты (iOS-стиль)? Вот они. Хотите полностью кастомный дизайн, который не похож ни на iOS, ни на Android? Легко.Проблема: если вы ленивый разработчик и используете Material виджеты на iOS, оно будет выглядеть как Android. И пользователи заметят. Но это не проблема Flutter. Это проблема разработчика.Преимущество: вы можете сделать UI, который выглядит абсолютно одинаково на iOS и Android. Единый brand identity. Единая дизайн-система. Дизайнеры рисуют один макет — вы реализуете его один раз. Это экономит дофига времени и денег.Примеры: realtor.com написан на Flutter. Вы не отличите его от нативного приложения. BMW MyBMW app — Flutter. Google Pay — Flutter. Credit Agricole Bank — Flutter с миллионом пользователей. Они все выглядят как нативные приложения, потому что разработчики не поленились и сделали правильный UI.«Сложно найти разработчиков, никто не знает Dart»Статистика: Stack Overflow 2024 — 5.9% всех разработчиков используют Dart. Звучит мало? Это миллионы человек. Flutter используют 9.4% разработчиков в мире. Statista 2023: 30% разработчиков используют Flutter — это больше, чем React Native.Порог входа низкий. Если вы знаете Java, Kotlin, Swift, C#, или любой другой C-подобный язык — вы выучите Dart за неделю. Личный опыт разработчика с Хабра: «за несколько месяцев без опыта меня взяли на проект. Спустя 2 недели макет первого экрана был осилен». Другой разработчик: «MVP разработан за месяц без предварительных знаний».Рынок труда: да, Flutter-разработчиков меньше, чем iOS или Android. Но спрос растёт. 35.5% iOS-разработчиков готовы перейти на Flutter (рост с 24.2%). 51.9% Android-разработчиков готовы перейти (рост с 42.3%). Это не проблема найма. Это возможность для разработчиков.Реальные истории успеха с ХабраДавайте я процитирую реальных людей, которые используют Flutter в production и делятся опытом на Хабре.ATI.SU: «Выбор Flutter полностью себя оправдывает. Flutter позволяет не только описывать общую бизнес-логику, но и реализовывать общий UI. Экономит ресурсы команд проектирования UX, разработки и тестирования».Digital Design: «Команда из 3 разработчиков переписала сложное приложение за 6 месяцев» (60+ экранов, пуши, deeplinks, offline-режим). «Разработка на Flutter для обоих платформ составляет примерно ¾ от разработки 2-х нативных приложений».Finam: Три начинающих разработчика успешно прошли стажировку и влились в штат. «Успехом является сам факт участия в разработке чего-то крупного».RSHB.цифра: «Быстро выпустить новое приложение — выбрали Flutter. React Native и Xamarin отбросили после негативного опыта. Доверие к экосистеме Google».Переход с React Native на Flutter (реальная команда): «После очередного поднятия версии React Native полностью перестала отображаться вью с картой. Пожалуй, для меня это был последний гвоздь в крышку гроба этой технологии. Переписали на Flutter за 6 месяцев».Это не теория. Это реальные компании, реальные разработчики, реальный production опыт.Так почему же Flutter хоронят каждый день?Потому что это приносит хайп. Статья «Flutter мёртв» соберёт в 10 раз больше просмотров, чем «Flutter неплохой фреймворк с некоторыми компромиссами». Потому что критиковать — проще, чем разбираться. Потому что у людей есть любимые технологии, и они защищают их эмоционально.Я не говорю, что Flutter идеален. Производительность на сложных приложениях — проблема. State management запутывает новичков. Инструменты на больших проектах тормозят. Dart действительно используется в основном для Flutter. Это всё правда.Но вот что тоже правда: Flutter — самый популярный кросс-платформенный фреймворк с 2021 года. 170 тысяч звёзд на GitHub. 500 тысяч приложений в продакшене. 30% новых iOS-приложений. 92.5% пользователей довольны (опрос 7000+ разработчиков). BMW, Google, Alibaba, Credit Agricole, eBay — все они используют Flutter в production.Flutter жив. Более того, он чувствует себя лучше, чем когда-либо. Регулярные релизы в 2025: 3.29, 3.32, 3.35. Новые возможности, улучшения производительности, стабилизация инструментов. Экосистема растёт на 10 тысяч пакетов в год. Сообщество активно.Так что в следующий раз, когда вы увидите на Хабре очередную статью «Flutter помер», просто вспомните эти цифры. И улыбнитесь. Потому что где-то в это время разработчик на Flutter делает Hot Reload, видит изменения за секунду, и думает: «Хм, а ведь это действительно удобно».P.S. Если вы всё ещё не верите — просто попробуйте. Установите Flutter, создайте первый проект, сделайте Hot Reload. Почувствуйте эту магию. А потом решайте.P.P.S. Спасибо всем, кто делится реальным production-опытом на Хабре. Особенно тем, кто пишет не только свою философию по пьянке в комментах к каждому новому выпуску перевода об обновлении фреймворка, но и полезные статьи по Flutter. Вы делаете сообщество лучше.P.P.P.S. А критикам Flutter я желаю того же, что и всем остальным: пишите код, который работает, и будьте счастливы. На чём угодно. Потому что в конце концов технологии — это просто инструменты. А инструменты выбирают под задачу, а не под хайп. Dart - отличный язык, Flutter - замечательный фреймворк. А мозги - не брови, если нет - не нарисуешь, даже на канвасе.Теги:flutter appdartdartlangflutter app developmentflutterХабы:DartРазработка мобильных приложенийFlutterПрограммирование",1200,0,0,10 мин,https://habr.com/ru/articles/965848/,17380,2466,4
CI/CD-инфраструктура на базе Puppet и GitLab для управления флотом из 9000 хостов,K0SHiK,2025-11-13T10:00:38.000Z,"['Блог компании SM Lab', 'Puppet *']","K0SHiK 1 час назадCI/CD-инфраструктура на базе Puppet и GitLab для управления флотом из 9000 хостовУровень сложностиСреднийВремя на прочтение13 минКоличество просмотров115Блог компании SM LabPuppet * КейсВведениеКогда инфраструктура насчитывает тысячи хостов, ручное управление превращается в источник нестабильности. Становится невозможным гарантировать единое состояние систем, своевременно обновлять конфигурации и быстро реагировать на инциденты. Особенно остро это ощущается в масштабах 9000+ рабочих станций, где даже малейшая ошибка масштабируется мгновенно.Для централизованного и безопасного управления такой инфраструктурой используется связка Puppet и GitLab CI/CD. Эта комбинация позволяет автоматизировать весь цикл: от написания конфигурации до её контролируемого применения на каждом узле.В основе подхода — принцип постепенного внедрения изменений: сначала изменения тестируются в безопасной среде, где можно экспериментировать без риска, потом — раскатываются на небольшую группу хостов (канареечное тестирование), чтобы посмотреть, как они поведут себя в реальных условиях. И только если всё прошло гладко — обновление отправляется на весь флот.В этой статье я описал:Архитектуру кластера Puppet и способы масштабирования без единых точек отказа;Конвейер доставки изменений через GitLab CI/CD;Организацию окружений и продвижение между ними (test → pilot → prod);Управление Puppet-модулями и зависимостями через Puppetfile;Практики контроля качества, мониторинга конфигурационного дрейфа и безопасного отката изменений.Материал будет полезен инженерам, которые работают с инфраструктурой в масштабе, и хотят строить надёжные процессы, способные масштабироваться вместе с бизнесом.Ключевые элементы и как это работаетИнфраструктура управления конфигурациями — это не просто один Puppet‑сервер где‑то в углу. Это разнесённый, отказоустойчивый кластер, состоящий из нескольких взаимосвязанных ролей. Каждая выполняет свою задачу, а вместе они обеспечивают стабильность, масштабируемость и прозрачность.Puppet-agent — исполнитель на местахВсе конфигурационные изменения применяются на хостах с помощью puppet-agent — фонового демона, установленного на каждом управляемом устройстве. Он выполняет роль «исполнителя»: подключается к кластеру, получает чёткий набор инструкций — так называемый каталог конфигурации, описывающий, как должна выглядеть система.Как это работает:Сначала агент удостоверяется, есть ли у него действующий сертификат, и если нет, запрашивает его у Puppet CA.Далее через HAProxy он отправляет запрос на одну из компиляционных нод.Нода собирает индивидуальный каталог для этого хоста и отправляет его обратно.Агент получает каталог и начинает применять: создаёт файлы, настраивает сервисы, правит конфиги, меняет права и владельцев.Агент можно запустить вручную (puppet agent -t), но чаще он работает в фоне — раз в 30 минут сверяя систему с эталоном. И если что-то не так — исправляет. Молча, надёжно и по правилам.Есть и режим просмотра — --noop: агент покажет, что бы он сделал, но ничего не тронет. Удобно, если хочется проверить эффект изменений до реального применения.HAProxy — балансировщик на входе в кластерКогда в системе тысячи агентов, каждый регулярно запрашивает конфигурации или сертификаты. Чтобы они не штурмовали один сервер, используется балансировка нагрузки. Именно этим занимается HAProxy — он направляет запросы к нужным нодам кластера и распределяет нагрузку между ними.В нашей инфраструктуре работают два HAProxy-сервера, которые объединены с помощью Keepalived и доступны через единый виртуальный IP (VIP). Это означает, что даже если один из них выйдет из строя, второй тут же подхватит трафик.Как это работает:Запрос от puppet-agent (по портам 8140 или 8141) поступает на VIP.HAProxy перенаправляет его на один из backend-серверов: сервер сертификации или компиляционную ноду.Здоровье backend'ов отслеживается автоматически — если один из них упал, HAProxy исключит его из пула.Такое решение даёт системе гибкость, отказоустойчивость и возможность масштабироваться без остановки сервисов и ручных переключений.Сервер сертификации (Puppet CA)Каждый puppet-agent, прежде чем получить конфигурацию, сначала должен доказать, что он — доверенное лицо. Для этого существует Puppet CA — отдельный компонент кластера, отвечающий за выдачу, подпись и проверку TLS-сертификатов.В архитектуре он выделен в самостоятельную роль: CA работает независимо от компилятора и базы данных. Это упрощает обслуживание и снижает риски — даже если какой-то компонент выйдет из строя, авторизация хостов продолжит работать.Чтобы избежать единой точки отказа, в системе предусмотрены две CA-ноды с репликацией. Основная обрабатывает запросы, резервная синхронизирует данные и автоматически подхватывает работу при сбое. Как это работает:puppet-agent подключается к VIP по порту 8141.HAProxy перенаправляет запрос на активную CA-ноду.Если у хоста уже есть сертификат — CA его проверяет.Если нет — создаёт и подписывает новый, при необходимости с ручным одобрением.Готовый сертификат синхронизируется на вторую CA-ноду, чтобы обе были в актуальном состоянии.Такое устройство обеспечивает надёжную и масштабируемую систему аутентификации:агенты проходят проверку быстро и безопасно;сбой одного сервера не останавливает всю инфраструктуру;зоны ответственности чётко разграничены: CA занимается только сертификацией и делает это стабильно.Компиляционные ноды — фабрика конфигурацийЕсли CA пускает агента в систему, то именно компиляционные ноды говорят ему, что конкретно делать. Эти ноды — рабочие лошадки Puppet-кластера. Они получают запрос от агента, смотрят на факты о системе, и на основе кода окружения собирают индивидуальный каталог конфигурации.Можно думать о них как о сборочном конвейере: каждому хосту — свой комплект инструкций.Как это работает:Puppet-agent обращается к кластеру через HAProxy (порт 8140).Балансировщик направляет запрос по кругу: каждый следующий запрос отправляется на следующую свободную компиляционную ноду. Такой метод (round-robin) обеспечивает равномерную загрузку сервера.Нода собирает все необходимые инструкции: загружает код конфигурации (манифесты), подключает нужные модули, подтягивает переменные из системы управления данными в Puppet, и на основе этих данных формирует индивидуальный план действий для конкретного хостаГотовый каталог конфигурации возвращается агенту, и тот приступает к работе.Каталог учитывает:все известные факты о хосте (OS, IP, hostname, кастомные параметры);выбранное окружение (test, pilot, prod);активные Puppet-модули и их зависимости.Что это даёт:Горизонтальное масштабирование  Хватает мощности — отлично. Не хватает — просто добавляем ещё одну компиляционную ноду в пул. Сейчас четыре ноды обрабатывают более 20 000 запросов в час.Изоляция и отказоустойчивость  Если одна из компиляционных нод падает, остальные продолжают обрабатывать запросы. Отказ одного элемента не влияет на всю систему.Чёткое разделение окружений  Поскольку запросы учитывают, из какой ветки пришёл код (test, pilot, prod), можно безопасно разносить конфигурации по группам хостов. Это позволяет разворачивать изменения поэтапно без риска задеть все хосты сразу.PuppetDB — аналитическое ядро кластераПрименить конфигурацию — это только половина дела. Важно понимать, что именно произошло, на каких узлах, и почему. Для этого в экосистеме Puppet есть ключевой компонент — PuppetDB.Это центральное хранилище, куда стекается вся «телеметрия» от агентов:факты об узлах (ОС, IP, архитектура, кастомные параметры),каталоги конфигураций (что именно должно быть применено),отчёты о выполнении (что реально произошло и с каким результатом).Как это работает:После выполнения манифестов агент отправляет отчёт на одну из компиляционных нод.Та пересылает данные в PuppetDB — сервис, работающий на отказоустойчивом кластере PostgreSQL.Данные индексируются и становятся доступны через API, а также через визуальный интерфейс Puppetboard.Что это даёт:Можно в любой момент узнать: какие узлы применили изменения, у кого были ошибки, а кто «выпал» из цикла.Можно делать выборки: «покажи все хосты с CentOS 7», «кто не запускался последние 2 дня», «где установлен nginx».Никаких внешних агентов, скриптов или обходов вручную — все данные уже есть в системе, в одной точке.PuppetDB — это не просто журнал. Это инструмент понимания: он помогает анализировать инфраструктуру в динамике, находить отклонения, строить отчёты и принимать технически обоснованные решения. Puppetboard — простая визуализация для сложных системКогда у тебя сотни или тысячи узлов, JSON-ответы из API быстро перестают радовать. В такие моменты особенно ценится Puppetboard — веб-интерфейс поверх PuppetDB, который позволяет видеть состояние всей инфраструктуры вживую и сразу.Это не «декоративная панель», а по-настоящему полезный инструмент. Мы открываем его каждый день, чтобы быстро понять, где что пошло не так, какие агенты выпали из цикла, и какие узлы вообще живы.Что показывает Puppetboard:Общее состояние всех узлов: успешно, с изменениями, с ошибками, недоступен;Дату и время последнего запуска агента;Количество обработанных и изменённых ресурсов;Ошибки и предупреждения;Информацию о фактах узла: ОС, IP, hostname, uptime и так далее.Почему это удобно:Всё собирается с PuppetDB, то есть без лишней нагрузки на серверы или агентов;Интерфейс отзывчивый и быстрый, работает даже при большом объёме данных;Есть поиск, фильтры, и можно быстро провалиться на конкретный узел.Что это даёт в итогеБыстрая диагностика — если что-то пошло не так после выката, Puppetboard покажет это в первую очередь;Контроль отклонений — легко заметить, если какие-то машины «выпали»: не получили нужные настройки или не применили их до конца;Наблюдение за обновлениями — можно увидеть, как ведёт себя пилотная группа во время раскатки обновлений: применились ли изменения, есть ли ошибки, всё ли прошло гладко;Живая картина всей инфраструктуры — и не надо лезть в консоль или писать API-запросы.Puppetboard — это тот случай, когда простое средство делает работу с масштабной системой в разы удобнее.Результаты архитектурного подходаЛюбая система, работающая в масштабе, должна быть не просто производительной, а спокойно переживающей сбои. Архитектура изначально строилась с расчётом на отказоустойчивость и масштабируемость — не как опцию, а как базовое требование.Эта архитектура успешно работает в реальных условиях — с более чем 9000 хостами, тысячами запусков агентов каждый день и постоянными изменениями конфигураций. И что важно — система продолжает расти без потери стабильности.Что мы получили на выходе:Доступность: сбой одного сервера не влияет на остальную систему.Масштабируемость: линейное расширение через HAProxy и PostgreSQL-кластер.Производительность: никаких очередей агентов или таймаутов при генерации каталогов конфигураций.Наблюдаемость: всё прозрачно — от уровня ресурсов до статистики отклонений.Безопасность: централизованная выдача сертификатов, изолированные среды, защищённые ветки и доступы.И самое главное — уверенность в каждом запуске. Мы точно знаем, что произойдёт, где, и как это откатить, если что-то пойдёт не так.Такая архитектура стала надёжной основой для CI/CD — от постепенной раскатки обновлений на часть хостов до полностью автоматизированных деплоев с проверками. Как именно это устроено — расскажу дальше.GitLab CI: Интеллектуальный дирижёр Puppet-инфраструктурыВ нашей экосистеме управления хостами GitLab CI/CD выступает не просто инструментом автоматизации, а центральной нервной системой, которая координирует доставку изменений, обеспечивает контроль качества и гарантирует предсказуемость. Это тот самый механизм, который превращает строки кода в реальные настройки на тысячах машин.Почему GitLab CI — идеальный дирижёр для Puppet?Из всех возможных вариантов CI/CD GitLab отлично вписался в связку с Puppet. Он обеспечивает и прозрачность, и контроль, и полный цикл автоматизации.Единый источник истиныВся инфраструктура хранится в GitLab: манифесты, модули, параметры — всё под контролем версий.Каждый коммит — это след: кто, когда и зачем внёс изменения.Все изменения сначала отправляются на проверку — через merge request, где их просматривают и тестируют, прежде чем они попадут в основное окружение.Автоматизация от начала до концаCI/CD-процесс построен так, что человеку не нужно заходить на серверы вручную. Всё работает по цепочке:Администратор делает коммит;GitLab сам запускает проверки — чтобы убедиться, что всё работает как надо;Если всё ОК — изменения автоматически доставляются на компиляционные ноды;Система мониторит результат и сообщает, как всё прошло.Не нужно вручную копировать файлы или подключаться к серверам. Все шаги заранее описаны в конфигурации — и каждый запуск проходит одинаково, без сюрпризов.Этот подход делает инфраструктурные изменения такими же управляемыми и предсказуемыми, как обновления обычного кода. И это именно то, что нужно в больших системах.Механизм доставки изменений: r10k — надежный курьерКогда в Git вносятся изменения в конфигурацию, их нужно как-то быстро и безопасно доставить на компиляционные ноды. Этим занимается r10k — надёжный механизм, который синхронизирует содержимое репозитория.Как работает доставка через CI/CD:Изменение попадает в Git. Кто-то делает коммит или отправляет merge request в одну из рабочих веток:test — код уходит в тестовое окружение;pilot — начинается раскатка на небольшую группу хостов;prod — после всех проверок изменения доходят до основной среды.GitLab запускает пайплайн. CI (Continuous Integration) определяет, в какую ветку пришёл коммит, и запускает соответствующие шаги. Один из таких шагов — вызов r10k.r10k синхронизирует окружение, запуская команду: r10k deploy environment ${CI_COMMIT_BRANCH} -p -v info.  Что делает эта команда:Определяет, какие окружения нужно обновить;Забирает свежий код из Git;Подтягивает нужные версии всех Puppet-модулей из Puppetfile;Обновляет соответствующую директорию с конфигурацией на компиляционных нодах.Процесс полностью повторяемый и одинаковый для всех окружений — нет «особых правил для основной среды». Всё, что попадает в test, проходит тот же путь, что и prod, только с разным масштабом. Это снижает риск ошибок и делает доставку конфигураций стабильной и предсказуемой.Пример CI/CD-конфигурации в .gitlab-ci.yml (с пояснениями):stages:
  - check    # Этап проверки: проверяем синтаксис и стиль Puppet-кода
  - deploy   # Этап деплоя: развертывание кода на серверах

puppet_check:
  stage: check
  script:
    # Проверяем каждый Puppet-манифест (*.pp) на корректность синтаксиса
    - find manifests/ -name '*.pp' -exec puppet parser validate {} +
    # Проверяем стиль кода Puppet с помощью puppet-lint
    - find manifests/ -name '*.pp' -exec puppet-lint {} +
  tags:
    - puppet-runner  # Запускать этот job только на runner'ах с тегом puppet-runner

deploy_to_puppet_servers:
  stage: deploy
  tags:
    - puppet-runner  # Используем GitLab Runner с тегом puppet-runner для запуска деплоя
  script:
    # Запускаем r10k для развертывания окружения с именем ветки CI_COMMIT_BRANCH
    # Ключ -p означает обработку Puppetfile (зависимостей модулей)
    # -v info включает подробный лог
    # Путь к r10k может отличаться, в данном случае — стандартный путь Puppet Labs
    - /opt/puppetlabs/puppet/bin/r10k deploy environment ${CI_COMMIT_BRANCH} -p -v info
  parallel:
    matrix:
      # Параллельно запускаем деплой на 4 серверах
      # Для каждого сервера будет создана отдельная копия job, что ускоряет процесс
      - SERVERS: ['puppet-node01', 'puppet-node02', 'puppet-node03', 'puppet-node04']Как устроена Puppet-среда: три ветки, один поток измененийЗа основу мы взяли простой, но проверенный подход: все конфигурации описаны в центральном Git‑репозитории. Но магия начинается не с самих файлов, а с того, как они продвигаются от идеи до продуктивной среды.У нас — три ветки. Каждая играет свою роль в CI/CD‑цикле:test — лаборатория. Здесь появляются первые строки кода, тестируются идеи, даже рисковые. Эта ветка изолирована, и любой «эксперимент» безопасен.pilot — буфер между тестом и боем. Сюда попадают только прошедшие проверку изменения. Они применяются к ограниченному числу хостов, где мы наблюдаем поведение вживую.prod — финальная остановка. Только надёжный и проверенный код попадает сюда. Это стабильная ветка, которая управляет всей продуктивной инфраструктурой.Как всё устроено на практикеТестовое внедрение  Всё начинается в test. Это песочница, где допускаются даже спорные решения — главное, чтобы они не дошли до боевых машин без проверки.Синхронизация с реальностью  Ветка test регулярно подтягивает изменения из prod, чтобы не застрять в прошлом. Мы всегда тестируем на фоне актуального состояния инфраструктуры.Переход в pilot  После прохождения тестов и проверки изменения вручную переходят в pilot. Здесь они выкатываются на небольшую часть флота. Если система не подаёт тревожных сигналов, а количество ошибок не лезет в гору, продолжаем путь.Релиз в prod  Всё стабильно? Значит, можно сливать pilot в prod. После этого обновления автоматически распространяются на все узлы.Почему мы выбрали именно такой подход?Автоматическое слияние напрямую из test в prod — слишком опасно, особенно при работе с тысячами машин. Наш подход даёт время на осознание, тестирование, наблюдение. Мы уверены, что каждое изменение контролируемо, прозрачно и готово к жизни.Puppetfile и управление модулями: гибкость с контролемОдин из ключевых элементов Puppet-инфраструктуры — это модули. Именно через них осуществляется вся конфигурация хостов: от установки нужных приложений и настройки пользовательских доступов до приведения рабочих станций к корпоративному стандарту. Управление всеми этими модулями централизовано через Puppetfile.Файл Puppetfile играет роль локального пакетного менеджера: он фиксирует, какие модули используются, откуда их брать, и, что особенно важно, какие именно версии или ветки подключать.Как всё устроено у насКаждый модуль живёт в своём Git‑репозитории и имеет как минимум две ветки:main — стабильная, продуктивная;pilot — для проверки новых изменений.Когда нужно протестировать новую функциональность модуля, его pilot‑ветка явно прописывается в Puppetfile, который подключён к окружению pilot. Таким образом только небольшая группа хостов начнёт использовать новую версию.Чтобы не допустить случайного слияния pilot в main, сам Puppetfile защищён:его изменения проходят ручную ревизию;файл включён в .gitattributes как защищённый от merge‑конфликтов по умолчанию;автослияние отключено — только ручной контроль.Когда изменения проходят тесты и обкатку, происходит финальное оформление:ветка pilot модуля сливается в main;в Puppetfile обновляется ссылка с pilot обратно на main — теперь стабильная версия доступна всем окружениям.Такой подход позволяет:изолировать и управлять рисками на уровне отдельных модулей;контролировать, где и когда применяется новая логика;избежать ситуаций, когда неподготовленный код «утекает» в продуктовую среду.И, что не менее важно, он прекрасно масштабируется. Даже если у вас десятки или сотни модулей, всё по-прежнему управляется централизованно и прозрачно.Проверяем качество и держим всё под контролемКогда управляем тысячами машин, самое важное — надёжность. Обновление должно устанавливаться одинаково на каждом хосте, ничего не сломать и быть незаметным для пользователей. Поэтому весь наш CI/CD‑конвейер вокруг Puppet построен на нескольких уровнях проверок и быстрых откатов.Быстрые проверки кодаКаждое изменение Puppet-кода начинает путь с базовых проверок:puppet-lint — ловит опечатки и «битый» синтаксис ещё до сборки.puppet parser validate — следит за стилем: отступы, имена классов, порядок атрибутов.Обе команды запускаются автоматически при каждом коммите, чтобы остановить ошибку на раннем этапе.Интеграционные тесты и ручная проверкаПолноценное тестирование мы пока только планируем внедрить. Речь о acceptance‑тестах — когда модуль проверяется на реальной виртуалке, как будто это настоящий хост.Для этого подойдут инструменты вроде Litmus или Beaker — они позволяют поднять тестовую машину, применить к ней конфигурацию и посмотреть, всё ли работает как нужно.Это важно для модулей, которые затрагивают сетевые настройки или меняют политики безопасности на рабочих станциях.Puppetboard и мониторинг дрейфаЧтобы понимать, как применяются конфигурации, мы используем связку PuppetDB + Puppetboard. Это даёт нам удобный способ следить за состоянием всей инфраструктуры прямо из браузера - без логов, консоли и API-запросов.Что можно увидеть:какие хосты запускались недавно, и всё ли у них прошло успешно;где возникли ошибки или предупреждения;у каких узлов конфигурация не совпадает с заданной — это называется дрейф;статистику по применённым ресурсам — помогает выявить нестабильные модули.Если что-то пошло не так, мы узнаем об этом не через жалобу пользователя, а заранее — через дашборд.Откаты и контроль измененийЕсли после обновления что-то пошло не так, важно уметь быстро вернуть систему в стабильное состояние.Для этого:каждое изменение, попадающее в рабочее окружение, фиксируется с помощью тега в Git;при необходимости возврата — используется команда git revert для отмены конкретного изменения, после чего запускается пайплайн повторно;система автоматически разворачивает предыдущую версию конфигурации на всех нужных узлах.Откат происходит быстро, без ручного вмешательства и риска забыть важные шаги. Всё воспроизводимо и документировано.PS.Вся эта система — результат постепенной эволюции. Она началась не с полной автоматизации или масштабируемого контроля, а шла к этому шаг за шагом: от наблюдения — к предотвращению.CI/CD-инфраструктура на базе Puppet и GitLab позволяет нам управлять флотом из тысяч машин, сохраняя при этом контроль, прозрачность и предсказуемость. И самое главное — не бояться изменений, потому что теперь у нас есть инструменты, чтобы их проверять, отслеживать и откатывать.Теги:puppetgitlabcicdавтоматизацияинфраструктура как кодХабы:Блог компании SM LabPuppet",115,0,0,13 мин,https://habr.com/ru/companies/sportmaster_lab/articles/965238/,21914,2823,2
Как полученные патенты повышали рыночную капитализацию публичных компаний,letsweb,2025-11-13T05:33:26.000Z,"['Блог компании Online patent', 'Патентование *', 'Финансы в IT', 'Развитие стартапа', 'IT-компании']","letsweb 5 часов назадКак полученные патенты повышали рыночную капитализацию публичных компанийУровень сложностиПростойВремя на прочтение4 минКоличество просмотров120Блог компании Online patentПатентование * Финансы в ITРазвитие стартапаIT-компанииОбзорПатенты — это не просто защита от конкурентов, а мощный финансовый актив, который может значительно увеличить рыночную стоимость компании. Будучи форматом нематериальных активов, они напрямую влияют на балансовую стоимость, привлекают инвестиции и открывают новые монетарные потоки через лицензирование. В современной экономике знаний именно патентный портфель зачастую становится ключевым драйвером роста капитализации инновационных предприятий.Что отделяет стартап с многообещающей идеей от дорогостоящего технологического гиганта? Часто ответ кроется не в продукте как таковом, а в патентах, которые его защищают. Инвесторы всё чаще оценивают предприятия через призму их нематериальных активов, где патенты играют первостепенную роль. Они трансформируют рискованные проекты в защищённые, ликвидные бизнес-модели, что напрямую конвертируется в более высокую рыночную стоимость.Иногда всего один одобренный патент увеличивал рыночную капитализацию фирмы на десятки и даже сотни процентов. Об этих компаниях сегодня мы и расскажем. Из недавних примеров:Beam Global получила защитный документ на технологию терморегулирования батарей Smart PCC™. «Онлайн Патент» ранее рассказывал об этом. Главное — акции американской компании выросли на 12,5% после того, как у них в активе появилась данная интеллектуальная собственность;Wearable Devices Ltd. увеличила стоимость своих ценных бумаг сразу на 609%(!), так как запатентовала «Устройство интерфейса с управлением жестами и голосом». Как отмечают профильные порталы, данная технология соединяет распознавание жестов и голосовое управление с биометрической аутентификацией. Все это делает серьезный шаг вперед в сфере бесконтактных носимых устройств, снабженных искусственным интеллектом;Affle получила патент на систему переключения и передачи данных между одним или несколькими чат-агентами. Это достижение было позитивно воспринято инвесторами: в феврале 2025 акции корпорации на фоне столь значимой интеллектуальной победы выросли на 7%.Microbot Medical Inc. запатентовала одно из направлений эндоваскулярной роботизированной системы LIBERTY. В основе оформленного — модульная конструкция, использующая сменные блоки приемников инструментов, что, как подчеркивали исследователи, обеспечивает гибкость продукта с возможностью адаптации его к спектру соответствующих процедур. После публикации данных новостей — акции компании выросли на 7% к закрытию торгов предыдущего дня. При этом — с начала года стоимость фирмы на бирже увеличилась на 195,5% (для сравнения: отрасль в целом просела на 7%).А до 2025 года были какие-то подобные случаи, когда патенты увеличивали стоимость корпорации?Конечно. В 2023 году акции SciSparc Ltd. поднялись на 57% после того, как стало известно, что власти Австралии запатентовали ее уникальную технологию лечения боли.Еще один случай, но теперь уже из Индии. Фирма Gufic Biosciences получила патент на изобретение, которое называлось так: «Лиофилизированная парентеральная композиция омадациклина тозилата и способ ее получения». Стоимость ее акций выросла на 5% после этой новости. Инвестиционные порталы при этом указывали, что за три года цена акций корпорации, придерживающейся правильной политики в сфере интеллектуальной собственности, увеличились с 82,65 до 310 рупий (то есть доходность составила около 275%): «Таким образом, если бы инвестор вложил 100 000 рупий в акции компании три года назад (2020), стоимость его акций сегодня (2023) составила бы 375 000 рупий!»Рост финансовой привлекательности активов происходит и после выигранного патентного спора…Catalyst Pharmaceuticals — одна из крупнейших биотехнологических компаний. Она занимается выпуском препаратов для лечения редких заболеваний нервной системы, влияющих на работу мышц. После того, как фирма победила в патентном споре с Teva Pharmaceuticals (конкуренту запретили выводить на рынок дженерик препарата Catalyst LEMS) — ее акции поднялись на 15%.Ценные бумаги еще одной корпорации — Exelixis — в 2024 году были оценены экспертами позитивно: они посчитали возможным увеличение стоимости акции на 4 доллара сразу. Причина — суд подтвердил патентную защиту, например, препарата против рака Cabometyx (Cabo).В 2023 году компания Netlist одержала убедительную победу над Samsung: суды посчитали, что южнокорейский гигант умышленно нарушил запатентованные технологии, связанные с высокоскоростной памятью. Результат: компенсация ущерба в размере 303 млн долларов и подтверждение юридической действительности ее патентов.Как видите, очевидна прямая корреляция между получением ключевых патентов и ростом котировок акций публичных компаний. О сервисе Онлайн Патент:Онлайн Патент — цифровая система № 1 в рейтинге Роспатента. С 2013 года мы создаем уникальные LegalTech‑решения для защиты и управления интеллектуальной собственностью. Зарегистрируйтесь в сервисе Онлайн‑Патент и получите доступ к следующим услугам: Онлайн‑регистрация программ, патентов на изобретение, товарных знаков, промышленного дизайна;Подача заявки на внесение в реестр отечественного ПО;Поиск по программам;Регистрация программы в Роспатенте;Регистрация товарных знаков;Опции ускоренного оформления услуг;Бесплатный поиск по базам патентов, программ, товарных знаков;Мониторинги новых заявок по критериям;Онлайн‑поддержку специалистов.Теги:патентыпатентный портфельфинансы в itинвестицииХабы:Блог компании Online patentПатентованиеФинансы в ITРазвитие стартапаIT-компании",120,0,0,4 мин,https://habr.com/ru/companies/onlinepatent/articles/965408/,5645,695,5
Пустая трата времени или путь к лишним  итерациям? Как работают исследования в продукте и нужны ли они разработчикам,Yana_Rachkovski_1,2025-11-12T13:20:05.000Z,"['Блог компании МТС', 'Дизайн', 'Исследования и прогнозы в IT *', 'Управление продуктом *', 'Интерфейсы *']","Yana_Rachkovski_1 22 часа назадПустая трата времени или путь к лишним  итерациям? Как работают исследования в продукте и нужны ли они разработчикамУровень сложностиПростойВремя на прочтение8 минКоличество просмотров461Блог компании МТСДизайнИсследования и прогнозы в IT * Управление продуктом * Интерфейсы * Обзор«Да все равно эти исследования в итоге не пригодятся», —– именно такими словами однажды на созвоне прервал мое выступление коллега. Я замерла с открытым ртом. Хорошо, что камера была выключена. За секунду до этого я рассказывала о результатах проведенного исследования. Перед ноутбуком у меня лежал листок с заметками, которые я старательно писала от руки. Мне казалось, все идет хорошо. А тут такое!Уже позднее, на другом проекте и другой встрече, ситуация повторилась. Исследование было уже проведено, я сделала анализ и рассказывала о результатах и гипотезах. И снова вопрос «Зачем это нужно?». Б — боль.Меня зовут Яна, я дизайнер внутренних продуктов в MWS. Одна из моих задач — проводить исследования, чтобы лучше понимать потребности аудитории и определять, какие функции приоритетны. Но после кейсов, описанных выше, мне стало казаться, что исследования интересны только дизайнеру, аналитику и продакту — но никак не разработчикам и другим членам команды. Спойлер: это тот случай, когда «кажется» — действительно кажется. Как обстоят дела в реальной жизни, предлагаю обсудить сегодня.Ситуации, которые я описала выше, заставили меня пересмотреть мою стратегию. На очередном созвоне с большой командой я сразу обозначила, что буду говорить про исследования и если кому-то это направление не интересно, можно отключиться. К моему удивлению, техлид, фронт- и бэк-разработчики не только остались на встрече и внимательно слушали. Каждый проявил инициативу и поделился идеями, как можно решить проблему на его стороне.Командная работа сказалась на задаче самым благоприятным образом — нам удалось в минимальные сроки и более точечно закрыть боли пользователей.Подробнее об этом еще скажу ниже, но сначала немного базы.Какие бывают исследования Сейчас может стать душно, но обещаю не мучать вас долго. В интернете достаточно информации об исследованиях — только успевай гуглить, поэтому не буду останавливаться тут подробно. Но чтобы в тексте было проще ориентироваться, приведу несколько определений.Скрытый текстИтак, какими бывают исследования:Качественные — отвечают на вопросы «как» и «почему». Сюда относятся подходы, которые собирают не числовые данные. Например, это интервью с пользователями (custdev), юзабилити-тестирования и наблюдение в среде. Такие исследования обычно применяются на этапе макетов еще до того, как задача ушла в разработку. Они помогают выявить первичные данные, которые мы можем использовать, чтобы избежать ошибок на более поздних этапах.Количественные методы — отвечают на вопросы «сколько» и «как часто». Они нацелены на сбор числовых статистических данных, которые можно обобщить для большой группы пользователей. На них мы подробно останавливаться не будем.Теперь — о двух методиках качественных исследований:Интервью, или Custdev, помогают определить боль пользователя и понять, как он закрывает ее сейчас, когда еще нет интерфейса. Понимание текущих «костылей» и обходных путей, которыми пользователь закрывает интерфейсные потребности, — это благодатная почва для получения инсайтов. Под «инсайтами» тут я подразумеваю неожиданные открытия, которые касаются истинных или скрытых потребностей аудитории. Они помогают создать практическую и эмоциональную ценность для пользователя.Изучая, как люди пытаются решить свою проблему сейчас (даже неуклюже), мы понимаем, каким должно быть идеальное решение.В моей практике в роли интервьюеров обычно выступают дизайнер и продакт или дизайнер и аналитик. Но у меня был опыт проведения интервью вместе с техлидом и разработчиком, и результат был отличным. Ребята задавали технические вопросы, которые мне никогда не пришли бы в голову, при этом они были полезны и нам удалось получить те самые инсайты.Юзабилити-тесты на макетах — своего рода «контроль качества» принятых интерфейсных решений. Здесь мы, например, выясняем, насколько понятна кнопка «далее», достижима ли цель сценария, какие эмоции испытывает респондент, когда взаимодействует с прототипом. В юзабилити-тестах есть несколько этапов: Подготовка макетов и кликабельного прототипа.Организация тестирования с помощью специальных онлайн-сервисов вроде useberry или через личную встречу с фокус-группой.Проведение тестирования.Анализ результатов, генерация гипотез и решений.Обычно разработчки полноценно подключаются именно к последнему этапу. Тут нужно понять, возникает ли у пользователя проблема и когда именно.Зачем проводить исследования на макетахПредставим ситуацию: от заказчика поступает запрос на доработку интерфейса. Нужно вывести на странице сложную таблицу, отчет по КПЭ сотрудников. Она потребует значительных ресурсов на всех этапах работы — от подготовки бэка до отрисовки на фронте. Задачу берут в работу. Через месяц выходит релиз новой страницы с гибким и настраиваемым отчетом по КПЭ. Но метрика удовлетворенности падает, сыпется негативная обратная связь. Команда анализирует ситуацию и решает провести кастдев (от англ. Customer Development, сокращенно custdev, «кастдев»). Выясняется, что у конечного пользователя абсолютно другое представление об интерфейсе, работать ему неудобно и непривычно, что влияет на метрику NSAT.Тут важно понимать, что прежде всего пользователи делятся на сегменты: сами сотрудники и их руководители. В работе с отчетом по КПЭ у них разные задачи и цели. Например, специалист хочет знать, получит ли он премию по итогам месяца. Ему не подходит сложная и гибко настраиваемая таблица с перечнем всех возможных показателей и всех сотрудников. А руководитель хочет направить данные по бюрократической цепочке, чтобы ему и его группе подчиненных вовремя были назначены премии. Ему, в целом, подошла бы и кнопка скачивания документа в Excel, ведь он все равно отправляет данные по почте файлом.Зная это заранее, команда разработки предложила бы совсем другое решение — менее ресурсозатратное и более подходящее для пользователей. Например, для сотрудников можно было бы реализовать небольшой дашборд с данными в разрезе месяца, а для руководителей — предложить кнопку скачивания Excel-файла. Отсюда вытекают минимум три причины, почему исследования на этапе макетов можно и нужно проводить:Чтобы быстрее выпускать качественный продукт, избегая трудозатратных переделок на поздних этапах. Например, юзабилити-тест показал, что в сценарии формирования какого-нибудь отчета у пользователей возникают проблемы на одном из шагов. Еще на этапе макетов появляется возможность разобрать причины и спроектировать более удобный вариант.Чтобы команда могла принять обоснованные решения касаемо UX/UI и функциональности в целом. Если сотрудники спорят, как реализовать ту или иную фичу, исследования помогают разрешить конфликт. Верно то решение, которое отвечает потребностям пользователей и которое подтвердило метрики. Чтобы сэкономить ресурсы. Да, на старте исследования занимают время, но на поздних этапах они, наоборот, экономят его, ведь к разработчикам попадают уже проверенные решения.Когда исследования не нужныИсследования полезны, но это не маст-хэв. Иногда есть более эффективные и быстрые пути. Давайте рассмотрим ситуации, когда исследования проводить не стоит:У команды уже достаточно информации для принятия решения. Это могут быть данные из предыдущих кастдевов, аналитики и обратной связи от пользователей. Если они однозначно указывают на проблему и ее причины — зачем нам исследование? Например, аналитика показывает, что у 80% пользователей возникает сбой при попытке скачать документ с сайта. А из обратной связи ясно, что проблема связана с расположением кнопки «скачать» — например, она незаметная или до нее сложно «дойти». Тогда у нас достаточно информации, чтобы оперативно предложить и реализовать решение, — переместить кнопку в более очевидное место.Когда дешевле сделать и проверить, чем исследовать. Если речь о небольших изменениях или функциональности, проще и быстрее будет сразу сделать эту доработку и посмотреть, как ее приняли пользователи. Например, можно использовать A/B-тестирование или просто проанализировать, как обновление повлияло на метрики. Ситуации, где можно это применить:Небольшие визуальные изменения — новый цвет кнопки, выравнивание текста.Проверка гипотез с низким уровнем влияния на основной флоу.Если новый дизайн отличается от старого незначительно, и нет явных рисков.Когда можно опереться на проверенный опыт. Бывает, что целый продукт или отдельно взятая функциональность используют стратегию копирования. Тогда тебе нужно просто внедрить хорошо проверенное решение, адаптировав под свой продукт. Такой подход применим, когда у тебя стандартные и понятные задачами, нет уникальных пользовательских сценариев.Самый что ни на есть классический пример — интернет-магазин. Уже сейчас есть масса готовых решений, например, функциональности корзины и избранного. Тут не нужно изобретать велосипед. Достаточно посмотреть, как это сделано в других популярных магазинах — и повторить.Исследования нужны только дизайнерам. Или нет?Сразу возникает вопрос: если исследования проводятся на этапе макетов, не должны ли за них отвечать только дизайнеры или продакты? На первый взгляд, логично. Но практика показывает, что так работает не всегда.Я опросила своих коллег-фронтендеров, чтобы понять, как они смотрят на исследования. Результаты такие: когда разработчик понимает, кто конечный пользователь продукта, какие у него потребности и цели, что ему нужно в интерфейсе, это напрямую влияет на его мотивацию. «Пишешь ты этот код-код-код и не понимаешь для чего», — прокомментировала разработчица в продуктовой команде, где исследования на этапе макетов не используются. Получается, она видит лишь поток задач и регулярные переделки. Как это улучшает опыт пользователя и зачем это вообще нужно — понимания нет. Конечно, это убивает желание работать.Разработчик из другого продукта, где исследования проводятся регулярно, четко прослеживает их позитивное влияние на работу. Они помогают понять, где можно заложить более адаптивные моменты для пользователя. Осознать, что конкретно нужно людям и какие еще мелкие детали можно добавить, чтобы улучшить пользовательский опыт. Продукт для него — это не просто код, а инструмент, который реально помогает людям. Вот несколько примеров из практики, какие действия совершил разработчик после анализа исследования:1. В дропдауне не просто выдавать список всех доступных пунктов для выбора, но и дать возможность поиска, чтобы из тысяч вариантов было удобнее найти что-то конкретное.2. Заложить допсостояния там, где это «пропустил» дизайнер. Например, при ошибке в таблице данные не выводятся — а значит, нужно дать пользователю пояснение, почему так произошло. 3. Прокачать удобство пользования — например, настроить работу скроллов, подогнать страницу под ширину экрана и так далее.Если подвести итог вышесказанному, исследования на этапе макетов помогают всей команде, включая разработчиков, увидеть конечную цель: зачем мы все вообще сегодня здесь собрались и что и для кого мы делаем.Команда мечтыОбычно отношение технических специалистов к исследованиям ложится на эти три сценария:Я не вижу пользы от исследований и считаю их тратой времени.Я вижу пользу от исследований, но не работаю с ними. Получаю только итоговые данные в виде задач и макетов.Я вижу пользу от исследований и активно участвую в их проведении и генерации гипотез.Последний сценарий самый выигрышный для команды и продукта в целом. Мой опыт показывает: когда разработчик не просто получает установку «что делать», а участвует в формулировании тезисов, «кому», «для чего» и «почему» это нужно, потенциал команды многократно возрастает. Вернемся к нашей ситуации со страницей КПЭ. Допустим, команда пошла по пути исследований, проанализировала аудиторию и выяснила, что у разных сегментов разные цели. На общей встрече дизайнер и, например, продакт представляют результаты. Начинается генерация гипотез: что мы можем предложить, какие сценарии использования стоит предусмотреть? При этом каждый член команды генерирует решение в пределах своей компетенции.Дизайнер фокусируется на визуале и UX, продакт — на метриках. Разработчики же могут предложить практические решения, которые учитывают ограничения и возможности реализации. Зная технические нюансы, они могут подсказать более эффективные способы решения задач и сделать это еще на этапе макетов, до начала кодинга.К тому же вовлеченность в исследования значительно повышает мотивацию, ведь разработчик видит прямую связь между своим трудом и пользой для конечного пользователя. Он не просто выполняет абстрактные задачи, которые нельзя «пощупать». Это кстати работает не только на разработчиков, а на всех членов продуктовой команды. А что по этой теме думаете вы? Делитесь мнением в комментариях — мне будет интересно почитать. Теги:ux-исследованияпродуктовые исследованияcustdevюзабилити-тестированиеразработка продуктапользовательский опытпродуктовый дизайнгипотезы продуктаХабы:Блог компании МТСДизайнИсследования и прогнозы в ITУправление продуктомИнтерфейсы",461,0,3,8 мин,https://habr.com/ru/companies/ru_mts/articles/965606/,13175,1765,5
Сила оттенков серого: компьютерное зрение с нуля,PatientZero,2025-11-13T08:27:59.000Z,"['Обработка изображений *', 'Машинное обучение *', 'Программирование *', 'C *', 'Искусственный интеллект']","PatientZero 2 часа назадСила оттенков серого: компьютерное зрение с нуляУровень сложностиПростойВремя на прочтение16 минКоличество просмотров502Обработка изображений * Машинное обучение * Программирование * C * Искусственный интеллектПереводАвтор оригинала: zsergeВ обсуждениях компьютерного зрения обычно речь идёт об OpenCV или нейронных сетях глубокого обучения наподобие YOLO. Однако в большинстве случаев для работы с компьютерным зрением требуется понимание базовых алгоритмов, чтобы можно было адаптировать их под свои нужды.Мне захотелось понять, насколько далеко я смогу зайти, оставив в computer vision только самый минимум: одни лишь 8-битные изображения в градациях серого; никаких сложных структур данных, старый добрый C, немного байтовых массивов и единственный файл заголовка. В конце концов, изображение — это ведь просто прямоугольник из чисел, не так ли?Этот пост — экскурсия по алгоритмам, лежащим в основе Grayskull — минималистичной библиотеки компьютерного зрения, спроектированной для устройств с ограниченными ресурсами.ПикселиПиксель в градациях серого обычно представлен одним байтом: 0 означает чёрный цвет, 255 — белый, а промежуточные значения представляют различные оттенки серого.По сути, изображение в градациях серого — это 2D-массив таких пикселей, задаваемый шириной и высотой, однако на языках с более простой структурой памяти наподобие C он часто представляется в виде 1D-массива размером width * height:// Изображение размером WxH пикселей, хранящееся как плоский массив байтов
struct gs_image { unsigned w, h; uint8_t *data; };

// Вспомогательные функции для считывания/записи значений пикселей с учётом границ
uint8_t gs_get(struct gs_image img, unsigned x, unsigned y) {
  return (x < img.w && y < img.h) ? img.data[y * img.w + x] : 0;
}
void gs_set(struct gs_image img, unsigned x, unsigned y, uint8_t value) {
  if (x < img.w && y < img.h) img.data[y * img.w + x] = value;
}

// Удобный макрос для итеративного обхода всех пикселей
#define gs_for(img, x, y)                \
  for (unsigned y = 0; y < (img).h; y++) \
    for (unsigned x = 0; x < (img).w; x++)Этот скромный фундамент уже позволяет нам выполнять определённые трюки, например, инвертировать или отзеркаливать изображения:// Инвертирование изображения (создание негатива): px[x,y] = 255 - px[x,y]
gs_for(img, x, y) gs_set(img, x, y, 255 - gs_get(img, x, y));

// Отзеркаливание изображения: заменяем px[x,y] на px[w-x-1,y]
gs_for(img, x, y) {
for (unsigned y = 0; y < img.h; y++) {
  for (unsigned x = 0; x < img.w/2; x++) { // итеративно обходим только первую половину
    uint8_t tmp = gs_get(img, x, y);
    gs_set(img, x, y, gs_get(img, img.w - x - 1, y));
    gs_set(img, img.w - x - 1, y, tmp);
  }
}struct gs_rect { unsigned x, y, w, h; }; // Для интересующих нас областей (region of interest, ROI)

// Обрезаем src изображения в dst на основании roi
gs_for(roi, x, y) gs_set(dst, x, y, gs_get(src, roi.x + x, roi.y + y));

// Уменьшаем размер в два раза: усредняем значение пикселя по четырём соседям (2x2)
gs_for(dst, x, y) {
    unsigned sum = 0;
    for (unsigned j = 0; j < 2; j++)
      for (unsigned i = 0; i < 2; i++)
        sum += gs_get(src, x * 2 + i, y * 2 + j);
    gs_set(dst, x, y, sum / 4);
}Можно выполнять наивное изменение размера по ближайшим соседям, результат будет быстрым, но слишком грубым, или же выполнять билинейную интерполяцию, что медленнее и требует операций с плавающей запятой, зато часто выглядит красивее:// Изменение размера по ближайшим соседям
void gs_resize_nn(struct gs_image dst, struct gs_image src) {
    gs_for(dst, x, y) {
        unsigned sx = x * src.w / dst.w, sy = y * src.h / dst.h;
        gs_set(dst, x, y, gs_get(src, sx, sy));
    }
}

// Билинейное изменение размера
GS_API void gs_resize(struct gs_image dst, struct gs_image src) {
  gs_for(dst, x, y) {
    float sx = ((float)x + 0.5f) * src.w / dst.w, sy = ((float)y + 0.5f) * src.h / dst.h;
    sx = GS_MAX(0.0f, GS_MIN(sx, src.w - 1.0f)), sy = GS_MAX(0.0f, GS_MIN(sy, src.h - 1.0f));
    unsigned sx_int = (unsigned)sx, sy_int = (unsigned)sy;
    unsigned sx1 = GS_MIN(sx_int + 1, src.w - 1), sy1 = GS_MIN(sy_int + 1, src.h - 1);
    float dx = sx - sx_int, dy = sy - sy_int;
    uint8_t c00 = gs_get(src, sx_int, sy_int), c01 = gs_get(src, sx1, sy_int),
            c10 = gs_get(src, sx_int, sy1), c11 = gs_get(src, sx1, sy1);
    uint8_t p = (c00 * (1 - dx) * (1 - dy)) + (c01 * dx * (1 - dy)) + (c10 * (1 - dx) * dy) + (c11 * dx * dy);
    gs_set(dst, x, y, p);
  }
}Так исходное изображение (слева) выглядит после билинейного изменения размера (посередине) и изменения размера по ближайшим соседям (справа).Обработка изображенийТеперь, когда мы можем манипулировать отдельными пикселями, можно приступать к более серьёзной обработке изображений.Полезным инструментом здесь будут свёрточные фильтры. Фильтр — это маленький 2D-массив (ядро), применяемый к каждому пикселю изображения. Новое значение пикселя вычисляется как взвешенная сумма соседних пикселей, где веса определяются ядром.void gs_filter(struct gs_image dst, struct gs_image src, struct gs_image kernel, unsigned norm) {
    gs_for(dst, x, y) {
        int sum = 0;
        gs_for(kernel, i, j) {
            sum += gs_get(src, x + i - kernel.w / 2, y + j - kernel.h / 2) * (int8_t)gs_get(kernel, i, j);
        }
        gs_set(dst, x, y, sum / norm);
    }
}Эту методику можно использовать для размытия, повышения резкости, распознавания краёв и многих других эффектов. Вот примеры одних из самых распространённых ядер. Учтите, что они определяются, как 8-битные integer со знаком:// box blur 3x3, все пиксели имеют одинаковый вес
struct gs_image gs_blur_box = {3, 3, (uint8_t *)(int8_t[]){1, 1, 1, 1, 1, 1, 1, 1, 1}};
// гауссово размытие 3x3, центральные пиксели имеют больший вес
struct gs_image gs_blur_gaussian = {3, 3, (uint8_t *)(int8_t[]){1, 2, 1, 2, 4, 2, 1, 2, 1}};
// повышение резкости и усиление краёв
struct gs_image gs_sharpen = {3, 3, (uint8_t *)(int8_t[]){0, -1, 0, -1, 5, -1, 0, -1, 0}};
// тиснение, изображение становится более ""3D""
struct gs_image gs_emboss = {3, 3, (uint8_t *)(int8_t[]){-2, -1, 0, -1, 1, 1, 0, 1, 2}};Аналогично, можно применить фильтры Собеля, которые полезны, если мы хотим распознавать на изображениях края:struct gs_image gs_sobel_x = {3, 3, (uint8_t *)(int8_t[]){-1, 0, 1, -2, 0, 2, -1, 0, 1}};
struct gs_image gs_sobel_y = {3, 3, (uint8_t *)(int8_t[]){1, 2, 1, 0, 0, 0, -1, -2, -1}};

void gs_sobel(struct gs_image dst, struct gs_image src) {
    struct gs_image gx = {src.w, src.h, malloc(src.w * src.h)};
    struct gs_image gy = {src.w, src.h, malloc(src.w * src.h)};
    gs_filter(gx, src, gs_sobel_x, 1);
    gs_filter(gy, src, gs_sobel_y, 1);
    gs_for(dst, x, y) {
        int mag = sqrt(gs_get(gx, x, y) * gs_get(gx, x, y) + gs_get(gy, x, y) * gs_get(gy, x, y));
        gs_set(dst, x, y, GS_MIN(mag, 255));
    }
    free(gx.data);
    free(gy.data);
}Вот примеры таких фильтров; обратите внимание, как некоторые из них устраняют шум или выделяют края:Первое изображение — это оригинал, далее идут box filter и фильтр гауссова размытия. Затем идёт фильтр повышения резкости, фильтр тиснения и, наконец, фильтр Собеля, выделяющий края.Пороговые значенияЧтобы «видеть» объекты на изображении, нам нужно разделить его на передний и задний планы, а затем работать с передними сегментами, пытаясь найти интересующие нас объекты.Гораздо проще это делать, если каждый пиксель полностью чёрный или полностью белый. Такое преобразование из градаций серого в чёрно-белые данные называется thresholding (бинаризацией).В простейшем случае мы можем считать все пиксели со значениями больше 127 белыми, а ниже — чёрными. Это thresholding с фиксированным уровнем. Разумеется, если изображение тёмное, то такое значение thresholding окажется слишком высоким и многие значимые детали будут утеряны. Как же выбрать пороговое значение точнее?// Применяем фиксированное пороговое значение и бинаризируем изображение
GS_API void gs_threshold(struct gs_image img, uint8_t thresh) {
  for (unsigned i = 0; i < img.w * img.h; i++) img.data[i] = (img.data[i] > thresh) ? 255 : 0;
}Одно из решений заключается в вычислении распределения яркости как гистограммы. Мы знаем, что на изображении в градациях серого есть 255 уникальных значений, поэтому можем итеративно обойти все пиксели и посчитать, какие из них имеют то или иное значение. Проанализировав получившуюся гистограмму, мы получим представление, какое значение пикселя лучше всего подойдёт в качестве порогового.Это можно сделать при помощи способа Оцу. Он автоматически определяет оптимальное пороговое значение, проверяя каждое возможное (от 0 до 255). Для каждого значения он разбивает пиксели изображения на два класса (фон и передний план), а затем вычисляет их межклассовую дисперсию. Пороговым значением, максимизирующим эту дисперсию, будет то, которое создаёт наилучшее разделение между двумя классами. Такой способ довольно неплохо работает для изображений с хорошей контрастностью:// пытаемся найти наилучшее пороговое значение для разделения ""переднего"" и ""заднего"" плана
uint8_t gs_otsu_threshold(struct gs_image img) {
  unsigned hist[256] = {0}, wb = 0, wf = 0, threshold = 0;
  // вычисляем количество пикселей с каждым значением яркости
  for (unsigned i = 0; i < img.w * img.h; i++) hist[img.data[i]]++;
  float sum = 0, sumB = 0, varMax = -1.0;
  for (unsigned i = 0; i < 256; i++) sum += (float)i * hist[i];
  // пытаемся найти пороговое значение, максимизирующее межклассовую дисперсию
  for (unsigned t = 0; t < 256; t++) {
    wb += hist[t];
    if (wb == 0) continue;
    wf = (img.w * img.h) - wb;
    if (wf == 0) break;
    sumB += (float)t * hist[t];
    float mB = (float)sumB / wb;
    float mF = (float)(sum - sumB) / wf;
    float varBetween = (float)wb * (float)wf * (mB - mF) * (mB - mF);
    if (varBetween > varMax) varMax = varBetween, threshold = t;
  }
  return threshold;
}Однако в реальной жизни условия освещения часто неравномерны. В таких случаях единое глобальное пороговое значение может не подойти, и ни одно из потенциальных 255 пороговых значений не обеспечит хорошего результата.Для решения этой проблемы можно использовать адаптивный thresholding. Мы не будем использовать одно пороговое значение для всего изображения, а станем вычислять локальный порог для каждого пикселя на основании средней яркости соседних пикселей. Благодаря этому можно лучше учитывать разные условия освещения в изображении:gs_for(src, x, y) {
  unsigned sum = 0, count = 0;
  for (int dy = -radius; dy <= (int)radius; dy++) {
    for (int dx = -radius; dx <= (int)radius; dx++) {
      int sy = (int)y + dy, sx = (int)x + dx;
      if (sy >= 0 && sy < (int)src.h && sx >= 0 && sx < (int)src.w) {
        sum += gs_get(src, sx, sy);
        count++;
      }
    }
  }
  int threshold = sum / count - c;
  gs_set(dst, x, y, (gs_get(src, x, y) > threshold) ? 255 : 0);
}Посмотрим, как разные способы вычисления thresholding выглядят на одном и том же изображении:Первое изображение — оригинал, за ним идут thresholding с фиксированным уровнем (80), способ Оцу и адаптивный thresholding. Обратите внимание, что адаптивный thresholding сохраняет больше деталей и в ярких, и в тёмных областях изображения, а у способа Оцу возникают проблемы с неравномерным освещением.Морфологические операцииИз-за особенностей работы датчиков изображения в камерах снимки часто содержат шум.Это означает, что после thresholding в изображении будут встречаться случайные отдельные пиксели, не относящиеся к какому-то объекту, небольшие отверстия в объектах или небольшие разрывы между частями объектов. Всё это сбивает с толку алгоритмы распознавания объектов, однако очистке двоичного изображения могут способствовать морфологические операции.Две наиболее распространённые операции: это erosion (морфологическое сужение) и dilation (расширение). Сужение убирает пиксели на границах объектов (уменьшая их размеры), а расширение добавляет пиксели к их границам (увеличивая объекты).Также существует opening (erosion с последующим dilation) и closing (dilation с последующим erosion). Opening полезно для удаления маленьких объектов или шума, а closing — для заполнения небольших отверстий в объектах.void gs_erode(struct gs_image dst, struct gs_image src, unsigned radius) {
  gs_for(dst, x, y) {
    uint8_t min_val = 255;
    for (int dy = -radius; dy <= (int)radius; dy++) {
      for (int dx = -radius; dx <= (int)radius; dx++) {
        int sy = (int)y + dy, sx = (int)x + dx;
        if (sy >= 0 && sy < (int)src.h && sx >= 0 && sx < (int)src.w) {
          uint8_t val = gs_get(src, sx, sy);
          if (val < min_val) min_val = val;
        }
      }
    }
    gs_set(dst, x, y, min_val);
  }
}
void gs_dilate(struct gs_image dst, struct gs_image src, unsigned radius) {
  gs_for(dst, x, y) {
    uint8_t max_val = 0;
    for (int dy = -radius; dy <= (int)radius; dy++) {
      for (int dx = -radius; dx <= (int)radius; dx++) {
        int sy = (int)y + dy, sx = (int)x + dx;
        if (sy >= 0 && sy < (int)src.h && sx >= 0 && sx < (int)src.w) {
          uint8_t val = gs_get(src, sx, sy);
          if (val > max_val) max_val = val;
        }
      }
    }
    gs_set(dst, x, y, max_val);
  }
}Вот пример того, как морфологические операции могут подчистить довольно непонятное изображение с метками ArUco:Первое изображение — оригинал, за которым следует изображение с thresholding (способ Оцу). Далее идут erosion с последующим dilation. Маркеры чёрные, поэтому это может показаться странным, но так как морфологические операции работают с белыми пикселями, мы, по сути, выполняем closing, но для чёрных пикселей. Также можно предварительно инвертировать изображение, а затем выполнять opening и обратное инвертирование.В конце все маркеры уменьшаются до их исходных размеров и все их легко можно распознать по форме и размеру.Пятна и контурыПолучив чистое двоичное изображение, можно приступать к распознаванию объектов на нём. Классический способ решения этой задачи — нахождение соединённых компонентов (пятен, blob).Blob — группа соединённых белых пикселей (255), образующих объект. Для разметки связанных пикселей проще всего находить blob при помощи алгоритма заливки (flood-fill) или поиска в глубину (depth-first search, DFS):void gs_flood_fill(struct gs_image img, unsigned x, unsigned y, uint8_t target, uint8_t replacement) {
  if (x >= img.w || y >= img.h) return;
  if (gs_get(img, x, y) != target || gs_get(img, x, y) == replacement) return;
  gs_set(img, x, y, replacement);
  gs_flood_fill(img, x + 1, y, target, replacement);
  gs_flood_fill(img, x - 1, y, target, replacement);
  gs_flood_fill(img, x, y + 1, target, replacement);
  gs_flood_fill(img, x, y - 1, target, replacement);
}Разумеется, при обработке большинства реальных изображений это мгновенно увеличит стек до огромных величин. Поэтому предпочтителен итеративный подход с очередью или стеком.Однако это всё равно не самый оптимальный способ поиска blob. Более эффективный способ — это алгоритм, состоящий из двух проходов, сканирующий изображение дважды: сначала он присваивает предварительные метки и записывает соответствия между ними, а затем выполняет второе сканирование для ресолвинга этих соответствий и присвоения каждому blob окончательной уникальной метки.Будет здорово, если мы сможем использовать один и тот же тип gs_image для меток, но в большинстве случаев для этого потребуется больше, чем 256 меток (особенно для временных промежуточных меток). Поэтому для хранения меток нужен отдельный массив integer большего размера:typedef uint16_t gs_label; // наверно, 64 тысяч будет достаточно?

struct gs_blob {
  gs_label label; // какую метку имеет blob?
  unsigned area; // сколько белых пикселей находится в blob?
  struct gs_rect box; // ограничивающий прямоугольник
  struct gs_point centroid; // центр ""масс""
};
unsigned gs_blobs(struct gs_image img, gs_label *labels, struct gs_blob *blobs, unsigned nblobs) { ... }Прежде чем двигаться дальше, давайте поговорим о связанности. Существует два распространённых типа связанности пикселей: 4-connectivity и 8-connectivity. В 4-connectivity пиксель соединён с четырьмя непосредственными соседями (сверху, снизу, слева, справа). В 8-connectivity пиксель связан со всеми восемью окружающими его пикселями (те же, плюс по диагонали). То есть, в случае 8-connectivity показанный ниже пример будет одним blob, а в случае 4-connectivity — двумя отдельными blob:......
.#..#.
.##.#.
.###..
......В нашей реализации для простоты будет использоваться 4-connectivity. Рассмотрим следующее изображение:.........
.###..#..
.###.##..
.#####..#
.......##Начнём сканировать его строка за строкой. Если найден белый пиксель, проверяем соседей слева и справа:Если оба чёрные (0), то присваиваем пикселю новую метку, считая, что это будет новый blob.Если один из них белый, присваиваем его метку текущему пикселю, так как это продолжение уже имеющегося известного blob.Если оба белые, но имеют разные метки, то присваиваем наименьшую из меток текущему пикселю и записываем соответствие между двумя метками в особую структуру данных (как объединение-поиск).После первого прохода массив меток будет выглядеть так:.........
.111..2..
.111.32..
.11111..4
.......55Таблица соответствия будет выглядеть так: 1 <-> 3, 3 <-> 2, 4 <-> 5.Во втором проходе мы разрешаем соответствия и присваиваем каждому пикселю окончательные метки. Массив окончательных меток будет выглядеть так:.........
.111..1..
.111.11..
.11111..4
.......44На втором проходе также можно вычислять такие свойства blob, как площадь, ограничивающий прямоугольник и центр масс. Площадь самого большого blob составляет 14 пикселей, его ограничивающий прямоугольник имеет координаты от (1,1) до (6,3). Центр масс вычисляется как среднее координат всех пикселей blob, в нашем случае по оси X это будет (3*1+3*2+3*3+4+2*5+2*6)/14, или округлённо 4. По оси Y это будет (4*1+5*2+5*3)/14, или округлённо 2. То есть центр масс находится в координате (4,2), не относящейся к «телу» blob.Эти геометрические свойства уже дают нам довольно много информации о blob. Например, мы можем фильтровать blob по площади, чтобы удалять небольшие пятна шума. Также можно вычислять соотношение сторон (ширина/высота) ограничивающего прямоугольника, чтобы отфильтровывать очень тонкие или очень широкие blob. Соотношение между реальной площадью и ограничивающим прямоугольником blob позволяет отфильтровывать недостаточно компактные blob. У прямоугольников соотношение близко к 1.0, у кругов — к pi/4 = 0.785, а у линий приближается к нулю.Другими подсказками будут позиция центра масс, ориентация с использованием моментов или форма контура. Существует достаточно простой способ трассировки контура blob при помощи алгоритма трассировки окрестности Мура. Он начинает с известного пикселя границы и следует по контуру по часовой стрелке, проверяя соседние пиксели, пока не вернётся в начальный:struct gs_contour { struct gs_rect box; struct gs_point start; unsigned length; };
void gs_trace_contour(struct gs_image img, struct gs_image visited, struct gs_contour *c) {
  static const int dx[] = {1, 1, 0, -1, -1, -1, 0, 1};
  static const int dy[] = {0, 1, 1, 1, 0, -1, -1, -1};
  c->length = 0;
  c->box = (struct gs_rect){c->start.x, c->start.y, 1, 1};
  struct gs_point p = c->start;
  unsigned dir = 7, seenstart = 0;
  for (;;) {
    if (!visited.data[p.y * visited.w + p.x]) c->length++;
    visited.data[p.y * visited.w + p.x] = 255;
    int ndir = (dir + 1) % 8, found = 0;
    for (int i = 0; i < 8; i++) {
      int d = (ndir + i) % 8, nx = p.x + dx[d], ny = p.y + dy[d];
      if (nx >= 0 && nx < (int)img.w && ny >= 0 && ny < (int)img.h &&
          img.data[ny * img.w + nx] > 128) {
        p = (struct gs_point){nx, ny};
        dir = (d + 6) % 8;
        found = 1;
        break;
      }
    }
    if (!found) break;  // открытый контур
    c->box.x = GS_MIN(c->box.x, p.x);
    c->box.y = GS_MIN(c->box.y, p.y);
    c->box.w = GS_MAX(c->box.w, p.x - c->box.x + 1);
    c->box.h = GS_MAX(c->box.h, p.y - c->box.y + 1);
    if (p.x == c->start.x && p.y == c->start.y) {
      if (seenstart) break;
      seenstart = 1;
    }
  }
}Таким образом можно получать контуры всех blob и анализировать их формы или сравнивать длины контуров (периметр) с площадью blob. Также можно аппроксимировать контуры прямыми отрезками при помощи алгоритма Рамера — Дугласа — Пекера, заменяющего кривую последовательностью отрезков прямых с сохранением общей формы.Всё это здорово, если мы хотим распознавать простые объекты на статическом изображении. Но что если нам нужно распознавать или отслеживать более сложные объекты, такие как лица, автомобили или пешеходы?Ключевые точки и дескрипторыКлючевая точка (keypoint) — это определённый участок изображения, уникальный и надёжно распознаваемый вне зависимости от масштаба, поворота и освещения объекта.На практике, ключевые точки часто оказываются углами (также называемыми «признаками», feature). Один из самых интуитивно понятных алгоритмов распознавания признаков — это FAST (Features from Accelerated Segment Test). Он исследует окружность из 16 пикселей вокруг пикселя-кандидата (в 4 пикселях от него). Если как минимум 9 смежных пикселей в этой окружности r=4px вокруг пикселя P одновременно ярче или темнее, то пиксель-кандидат считается углом:..............
......012.....
.....F...3....
....E.....4...
....D..P..5...
....C.....6...
.....B...7....
......A98.....
..............Этот алгоритм прост, однако на реальных изображениях находит слишком много признаков. Решить эту проблему можно, записывая «оценку» (score) каждого признака и сохраняя только точки с наибольшей оценкой. Оценку можно вычислить как сумму абсолютных разностей между пикселем-кандидатом и смежными пикселями в окружности или как минимальную разность между центральным пикселем и пикселями на окружности.  gs_assert(gs_valid(img) && kps && nkps > 0);
  static const int dx[16] = {0, 1, 2, 3, 3, 3, 2, 1, 0, -1, -2, -3, -3, -3, -2, -1};
  static const int dy[16] = {-3, -3, -2, -1, 0, 1, 2, 3, 3, 3, 2, 1, 0, -1, -2, -3};
  unsigned n = 0;
  // первый проход: вычисляем карту оценок
  for (unsigned y = 3; y < img.h - 3; y++) {
    for (unsigned x = 3; x < img.w - 3; x++) {
      uint8_t p = img.data[y * img.w + x];
      int run = 0, score = 0;
      for (int i = 0; i < 16 + 9; i++) {
        int idx = (i % 16);
        uint8_t v = img.data[(y + dy[idx]) * img.w + (x + dx[idx])];
        if (v > p + threshold) {
          run = (run > 0) ? run + 1 : 1;
        } else if (v < p - threshold) {
          run = (run < 0) ? run - 1 : -1;
        } else {
          run = 0;
        }
        if (run >= 9 || run <= -9) {
          score = 255;
          for (int j = 0; j < 16; j++) {
            int d = gs_get(img, x + dx[j], y + dy[j]) - p;
            if (d < 0) d = -d;
            if (d < score) score = d;
          }
          break;
        }
      }
      scoremap.data[y * img.w + x] = score;
    }
  }
  // второй проход: отбрасывание немаксимальных значений
  for (unsigned y = 3; y < img.h - 3; y++) {
    for (unsigned x = 3; x < img.w - 3; x++) {
      int s = scoremap.data[y * img.w + x], is_max = 1;
      if (s == 0) continue;
      for (int yy = -1; yy <= 1 && is_max; yy++) {
        for (int xx = -1; xx <= 1; xx++) {
          if (xx == 0 && yy == 0) continue;
          if (scoremap.data[(y + yy) * img.w + (x + xx)] > s) {
            is_max = 0;
            break;
          }
        }
      }
      if (is_max && n < nkps) kps[n++] = (struct gs_keypoint){{x, y}, (unsigned)s, 0, {0}};
    }
  }
  return n;
}Обратите внимание, как на фотографии кота распознаются ключевые точки в углах и таких уникальных признаках, как глаза, нос, усы и так далее. Но как использовать ключевые точки для распознавания объектов?Здесь на помощь приходит ORB. Он надстраивается поверх того же распознавателя углов FAST, пытается находить резкие изменения яркости, но также добавляет ещё два компонента: ориентацию и дескриптор.После нахождения углов ORB определяет их ориентацию, вычисляя моменты изображения на небольшом участке вокруг каждой ключевой точки. Благодаря этому у каждой ключевой точки появляется угол, по сути, сообщающий, где у неё «верх».Дескриптор — это компактное описание локального участка изображения вокруг ключевой точки, инвариантное относительно масштаба и поворота. В ORB используется модифицированная версия дескриптора BRIEF (Binary Robust Independent Elementary Features), позволяющая хитрым образом кодировать участок изображения в виде небольшой битовой строки.Дескриптор просто сравнивает яркости битов, и если один пиксель светлее другого, то соответствующий бит получает значение 1, а в противном случае 0. Выполнив серию таких сравнений, можно создать двоичную строку, описывающую локальный участок изображения.Дескриптор имеет длину 256 бит; для каждого из 256 «сэмплов» мы при помощи псевдослучайной таблицы поиска выбираем две точки сэмплирования внутри площади участка и кодируем следующий бит, как 0 или 1 в зависимости от относительных значений пикселей.Сравнение ключевых точек становится тривиальной задачей, достаточно просто выполнить XOR двух битовых строк и посчитать биты.Так как ключевые точки не зависят от поворота и условий освещения, можно использовать их для распознавания объектов в различных сценариях.Последним дополнением этого алгоритма станет многократное изменение размера/масштаба изображения и распознавание ключевых точек в разных масштабах. Благодаря этому можно распознавать объекты, которые ближе или дальше от камеры, повёрнуты на любой угол или частично скрыты.LBP-каскадыКлючевые точки и дескрипторы отлично подходят для распознавания произвольных объектов, но иногда нам нужен более специализированное решение для определённых типов объектов, например, лиц, транспорта или жестов. Тут нам приходят на помощь каскадные классификаторы, применяемые, например, в методе Виолы — Джонса.Вместо сложных признаков Хаара, применяемых в алгоритме Виолы — Джонса, можно использовать нечто более простое: локальные бинарные шаблоны (Local Binary Patterns, LBP). LBP — это мощный дескриптор текстур. Он исследует 8 соседей каждого пикселя, и если сосед ярче центрального пикселя, записывает 1, а в противном случае 0. Так мы получаем 8-битное число, описывающее локальную текстуру.«Каскад» — это последовательность простых классификаторов, или «этапов». На каждом из этапов исследуется окно изображения и применяется несколько признаков LBP для определения того, может ли это окно содержать интересующий объект (например, лицо).Если окно не проходит проверку на любом из этапов, оно немедленно отбрасывается. Это очень быстрый процесс.Окно классифицируется, как положительное распознавание, только если оно прошло все этапы.Такая структура позволяет классификатору быстро отбрасывать подавляющую часть площади изображения и сосредоточить вычислительные ресурсы только на областях с высоким потенциалом. Перемещая это окно распознавания по всему изображению (и во множестве разных масштабов), можно находить объекты конкретного заранее обученного класса. В Grayskull есть предварительно обученный детектор лиц анфас, использующий именно эту методику.Выше показан результат работы LBP-каскадного классификатора, успешно распознавшего лицо сэра Гэри Олдмена во всём разнообразии его ролей. Слева выбрано минимальное количество соседей, равное 4, а справа — 14. Благодаря этому слева распознаётся больше лиц, но и возникает больше ложноположительных срабатываний, а справа выполняется более консервативное распознавание, обнаруживающее только полностью видимые лица анфас.ЗаключениеМы проделали путь от скромного пикселя до сложного распознавания объектов, использовав для всего этого лишь простые структуры C и фундаментальные алгоритмы. Мы попробовали манипулировать пикселями, применять фильтры для обработки изображений, сегментировать объекты при помощи thresholding и подчищать их. Также мы научились находить и анализировать blob, распознавать устойчивые ключевые точки при помощи FAST и ORB, и, наконец, использовали LBP-каскады для распознавания специализированных объектов.В этом и заключается фундаментальная философия Grayskull: срывание покрова таинственности с компьютерного зрения созданием минимального, свободного от зависимостей и понятного набора инструментов. Она доказывает, что для получения приемлемых результатов не всегда нужны тяжеловесные библиотеки или фреймворки глубокого обучения, особенно в системах с ограниченными ресурсами.По сути, изображения — это просто прямоугольники из чисел, поэтому благодаря основам знаний алгоритмов можно научить компьютер видеть их. Как всегда, я рекомендую вам изучить репозиторий, экспериментировать с кодом и, может быть, даже попробовать собрать собственный простой проект CV!Теги:компьютерное зрениеcomputer visionorbfastклассификаторраспознавание изображенийХабы:Обработка изображенийМашинное обучениеПрограммированиеCИскусственный интеллект",502,0,0,16 мин,https://habr.com/ru/articles/965706/,28874,4174,5
Ожидания типа IO как необходимое и достаточное условие отсутствия индекса. Проверка гипотезы с помощью PG_EXPECTO,pg_expecto,2025-11-13T07:54:56.000Z,['PostgreSQL *'],"pg_expecto 3 часа назадОжидания типа IO как необходимое и достаточное условие отсутствия индекса. Проверка гипотезы с помощью PG_EXPECTOУровень сложностиСложныйВремя на прочтение6 минКоличество просмотров62PostgreSQL * МнениеRecovery ModeВ мире баз данных очевидное часто обманчиво, а простое объяснение — скорее исключение, чем правило.  ПредисловиеВ эпоху, когда нейросети становятся первым источником знаний для многих разработчиков, особенно важно проверять их утверждения на практике. Один из таких вопросов — прямая связь между типами ожиданий в PostgreSQL и отсутствием индексов. AI-помощники часто дают логичные, но упрощённые ответы, которые могут ввести в заблуждение при решении реальных задач оптимизации. В этой статье мы экспериментально, с помощью инструмента pg_expecto, проверим , насколько обоснованно распространённое мнение о том, что IO-ожидания однозначно указывают на проблемы с индексацией.ℹ️Новый инструмент с открытым исходным кодом для статистического анализа, нагрузочного тестирования и построения отчетов доступен в репозитории GitFlic и GitHubkznalp/PG_EXPECTO: Комплекс статистического анализа производительности СУБД PostgreSQL pg-expecto/pg_expecto: Комплекс pg_expecto для статистического анализа производительности и нагрузочного тестирования СУБД PostgreSQL ВопросЯвляются ли ожидания типа IO необходимым и достаточным условием отсутствия индекса для таблицы или материализованного представления в СУБД PostgreSQL ?ℹ️Ответ нейросети Ask PostgresВ PostgreSQL ожидание типа IO (ввод-вывод) не является ни необходимым, ни достаточным условием для отсутствия индекса на таблице или материализованном представлении.Пояснение:1. Что такое ожидание IO?Ожидание IO — это состояние, при котором процесс СУБД ожидает завершения операций чтения/записи на диске (например, чтение страницы данных с диска в буферный пул). Это метрика производительности, отражающая задержки, связанные с физическим доступом к данным, а не структурную особенность таблицы.2. Связь с индексамиНаличие или отсутствие индекса — это логическая структура, определяемая при создании таблицы или явно через CREATE INDEX.Ожидание IO возникает, когда PostgreSQL не может найти нужные данные в оперативной памяти (буферном пуле) и должен прочитать их с диска — независимо от наличия индекса.Без индекса: PostgreSQL может выполнять полное сканирование таблицы (Seq Scan), что приводит к большим IO-ожиданиям.С индексом: PostgreSQL может выполнять индексный сканирование (Index Scan), но если индекс не покрывает запрос, он всё равно может обращаться к таблице (Index + Heap Scan), что также вызывает IO.3. Необходимость и достаточностьНе является необходимым: Таблица без индекса может не вызывать IO-ожиданий, если все данные уже в буферном пуле (например, после нескольких повторных запросов).Не является достаточным: Наличие IO-ожиданий не означает отсутствие индекса — индексированные запросы тоже могут вызывать IO, если индекс или данные не кэшированы.4. ВыводОжидание IO — это результат выполнения запроса, а не причина или признак отсутствия индекса.Отсутствие индекса может способствовать увеличению IO-ожиданий, но не является единственной причиной, и наличие IO-ожиданий не доказывает отсутствие индекса.Итог: Ожидание типа IO — независимый параметр производительности, не связанный логически с наличием индекса как с необходимым или достаточным условием.См. документацию: [PostgreSQL Performance Monitoring] — раздел о pg_stat_activity и pg_stat_statements.🕵️‍♂️Экспериментальная проверка необходимого и достаточного условия отсутствия индексаИсходные данные: инцидент производительности СУБДИспользование PG_EXPECTO для выявления проблемных SQL запросов при анализе инцидента производительности СУБД PostgreSQL | Postgres DBA | Дзен ℹ️SQL запросы, вызывающие 80% ожиданий типа IO - 43 запросаPG_EXPECTO : Диаграмма Парето по ожиданиям SQL запросов | Postgres DBA | Дзен Вопрос по ожиданиям IO❓Как среди списка SQL запросов - определить запросы для которых в таблицах не хватает индексов ?ГипотезаИспользуя, результаты ранее сделанных экспериментов :PG_HAZEL : ожидания СУБД PostgreSQL при отсутствии индексов. | Postgres DBA | Дзен Использование метода доступа Seq Scan | Parallel Seq Scan совместно с использованием параллельных процессов( Workers Planned ,Workers Launched ) в плане выполнения запросов, обнаруженных в ходе корреляционного анализа....ℹ️При возникновении данных условий могут возникнуть массовые ожидания IPC/BgWorkerShutdown.Можно сделать предположение:👉Подтверждающим признаком, сужающим круг поиска SQL запросов, для оптимизации которых, необходимо создать индексы в таблицах - является ожидание BgWorkerShutdown.Проверка гипотезыℹ️SQL запросы, вызывающие 80% ожиданий типа IPCPG_EXPECTO : Диаграмма Парето по ожиданиям SQL запросов | Postgres DBA | Дзен ⚠️SQL запросы, вызывающие ожидания типа IO и IPС :👉Проведенный анализ таблиц, участвующих в запросах, вызывающих ожидания BgWorkerShutdown и DSMFillZeroWrite, показал отсутствие индексов по столбцам, используемым в условиях запросов.ВЫВОД💥Одновременная корреляция ожидания IPC/BgWorkerShutdown и IO/DSMFillZeroWrite может служить надежным признаком необходимости добавления индексов для таблиц, участвующих в запросах, выявленных в ходе анализа инцидента производительности СУБД, значительно сужая область оптимизации по ожиданиям IO.Экспериментальное подтверждение гипотезыожидания типа IO не являются ни необходимым, ни достаточным условием отсутствия индекса.1. Почему это не необходимое условие?Необходимое условие означает: ""Если индекса нет, то мы обязательно увидим IO ожидания"".Это неверно. Отсутствие индекса может проявляться другими типами ожиданий, или вообще не проявляться в мониторинге ожиданий в конкретный момент времени.Единственный запрос, вызывающий ожидания IPC, но не вызывающий ожидания IO: queryid = 3449463017331132112Текст SQL запросаSELECT * FROM ""Table1"" WHERE ""Table1"".""Col2"" IN (X1)""Table1"" является материализованным представлениемMaterialized view ""public.Table1""
Column | Type   | Collation | Nullable | Default
-------+--------+-----------+----------+--------
Col1   | bigint |           |          |
Col2   | bigint |           |          |
Col3   | bigint |           |          |
Indexes:
""Table1_Col1_idx"" btree (""Col1"")
""Table1_Col1_uniq_idx"" UNIQUE, btree (""Col1"")⚠️Индекс по столбцу Col2 отсутствует , но ожиданий IO - нет. 2. Почему это не достаточное условие?Достаточное условие означает: ""Если мы видим IO ожидания, то это гарантированно означает, что не хватает индекса"".Это тоже неверно. IO-ожидания — это очень общий симптом, который указывает на то, что СУБД много читает с диска. Причин для этого может быть множество, и отсутствие индекса — лишь одна из них.SQL запрос, вызывающий ожидания IOТекст SQL запросаSELECT
""Table1"".""Col1"",
""Table1"".""Col2"",
""Table1"".""Col3"",
""Table1"".""Col4"",
""Table1"".""Col5"",
""Table1"".""Col6"",
""Table1"".""Col7"",
""Table1"".""Col8"",
""Table1"".""Col9"",
""Table2"".""Col1"" AS ""Table2.Col1"",
""Table2"".""Col10"" AS ""Table2.Col10"",
""Table2"".""Col11"" AS ""Table2.Col11"",
""Table2"".""Col12"" AS ""Table2.Col12"",
""Table3"".""Col13"" AS ""Table3.Col13"",
""Table3"".""Col14"" AS ""Table3.Col14"",
""Table3"".""Col15"" AS ""Table3.Col15"",
""Table3"".""Col16"" AS ""Table3.Col16"",
""Table3"".""Col17"" AS ""Table3.Col17"",
""Table3"".""Col18"" AS ""Table3.Col_18"",
""Table3"".""Col19"" AS ""Table3.Col_19"",
""Table3"".""Col20"" AS ""Table3.Col_20"",
""Table3"".""Col21"" AS ""Table3.Col_21"",
""Table4"".""Col1"" AS ""Table4.Col1"", ""Table4->Table5"".""Col1"" AS ""Table4.Table5.Col1"",
""Table4->Table5"".""Col22"" AS ""Table4.Table5.Col22"",
""Table4->Table5"".""Col23"" AS ""Table4.Table5.Col23""
FROM
""public"".""Table1"" AS ""Table1""
LEFT OUTER JOIN ""public"".""Table2"" AS ""Table2"" ON ""Table1"".""Table2Col1"" = ""Table2"".""Col1""
LEFT OUTER JOIN ""public"".""Table3"" AS ""Table3"" ON ""Table1"".""Col1"" = ""Table3"".""Col14""
INNER JOIN ""public"".""Table4"" AS ""Table4"" ON ""Table1"".""Table4Col1"" = ""Table4"".""Col1""
LEFT OUTER JOIN ""public"".""Table5"" AS ""Table4->Table5"" ON ""Table4"".""Table5Col1"" = ""Table4->Table5"".""Col1""
WHERE
""Table1"".""Col24"" IS NULL AND
""Table1"".""Col1"" > X1
ORDER BY ""Table1"".""Col1"" ASC LIMIT L1План выполненияLimit (cost=2.17..70.52 rows=L1 wCol1th=173)
-> Nested Loop Left Join (cost=2.17..856461.37 rows=2505886 wCol1th=173)
-> Nested Loop (cost=1.73..528761.08 rows=2505886 wCol1th=160)
-> Merge Left Join (cost=1.29..304507.29 rows=2505886 wCol1th=152)
Merge Cond: (""Table1"".Col1 = ""Table3"".Col14)
-> Nested Loop Left Join (cost=0.86..208037.72 rows=2505886 wCol1th=80)
-> Index Scan using ""Table1_pkey"" on ""Table1"" ""Table1"" (cost=0.43..139011.91 rows=2505886 wCol1th=68)
Index Cond: (Col1 > X1)
Filter: (""Col24"" IS NULL)
-> Memoize (cost=0.43..0.45 rows=1 wCol1th=20)
Cache Key: ""Table1"".""Table2Col1""
Cache Mode: logical
-> Index Scan using ""Table2_pkey"" on ""Table2"" (cost=0.42..0.44 rows=1 wCol1th=20)
Index Cond: (Col1 = ""Table1"".""Table2Col1"")
-> Index Scan using ""Col1x_N_Col14"" on Table3 ""Table3"" (cost=0.43..70410.89 rows=2375831 wCol1th=72)
-> Memoize (cost=0.43..0.47 rows=1 wCol1th=16)
Cache Key: ""Table1"".""Table4Col1""
Cache Mode: logical
-> Index Scan using ""Table4_pkey"" on ""Table4"" Table4 (cost=0.42..0.46 rows=1 wCol1th=16)
Index Cond: (Col1 = ""Table1"".""Table4Col1"")
-> Memoize (cost=0.44..0.55 rows=1 wCol1th=21)
Cache Key: Table4.""Table5Col1""
Cache Mode: logical
-> Index Scan using ""Table5_pkey"" on ""Table5"" ""Table4->Table5"" (cost=0.43..0.54 rows=1 wCol1th=21)
Index Cond: (Col1 = Table4.""Table5Col1"")
(25 rows)⚠️Метод доступа - только Index Scan, но  при выполнении запроса возникают ожидания IO. Выводℹ️Наличие ожиданий ввода-вывода (IO) при выполнении SQL-запросов не служит однозначным индикатором потребности в создании индексов для задействованных в запросе таблиц.Теги:postgresqlпроизводительностьожиданиястатистический анализнагрузочное тестированиеХабы:PostgreSQL",62,0,0,6 мин,https://habr.com/ru/articles/965960/,9745,1131,1
Как понимать разработчиков через простые аналогии,lukyan73,2025-11-13T04:22:12.000Z,"['Офисы IT-компаний', 'Лайфхаки для гиков']","lukyan73 7 часов назадКак понимать разработчиков через простые аналогииУровень сложностиПростойВремя на прочтение4 минКоличество просмотров614Офисы IT-компанийЛайфхаки для гиковИз песочницыКогда я перешел в IT из абсолютно нетехнической среды, на первом же совещании услышал: «Нужно пофиксить баг в продакшене, перед этим замержить пул-реквест, и если что-то пойдет не так — будем экстренно откатываться». Я улыбался и кивал, не понимая ровным счетом ничего.Оказалось, что язык IT — это не набор заклинаний, а логичная система. И ее можно понять, не становясь программистом. Просто нужно найти правильные аналогии. Этот опыт стал для меня настолько ценным, что я систематизировал его в книге «Птичий язык: как говорить на языке разработчиков не написав ни строчки кода». А здесь делюсь ключевыми находками.Вот как я начал понимать команду разработки, проводя параллели с привычными вещами.1. «Прод», «деплой» и экстренные ситуации — это работа с версиямиПредставьте, что ваше приложение — это книга в библиотеке.Прод — это стеллаж с готовыми книгами, которые читатели берут в руки. Всё должно быть идеально.Деплой — это процесс, когда издатель привозит новую партию исправленных книг и ставит их на стеллаж вместо старых.Экстренный возврат к предыдущей версии — это когда оказалось, что в новой партии критические опечатки, и издатель срочно возвращает на стеллаж предыдущее, стабильное издание.Что вы теперь понимаете?Когда разработчик говорит: «Вчера задеплоили новую фичу, но пришлось срочно вернуть старую версию», это значит: «Мы выложили обновление, но там был грубый баг, поэтому мы экстренно вернули старую, рабочую версию приложения».2. «Бэкенд», «фронтенд» и «API» — это ресторанЭто классика, но она работает безупречно.Фронтенд — это зал ресторана: интерьер, меню, удобство столов. Всё, что видит и с чем взаимодействует гость.Бэкенд — это кухня ресторана: повара, плиты, холодильники. Гость этого не видит, но именно здесь готовится еда.API — это официанты, которые принимают заказ в зале, передают его на кухню и приносят готовое блюдо обратно.Что вы теперь понимаете?Фраза «Фронтенд отправил запрос к API, но бэкенд вернул ошибку» превращается в: «Гость в зале сделал заказ официанту, но кухня сказала, что такого блюда нет».3. «Баг», «фича» и «хотфикс» — это поломки и улучшения в автомобилеБаг — это поломка в машине. Например, не работает кондиционер или скрипит тормоз.Фича — это новая функция или опция. Например, вы купили машину с подогревом сидений или камерой заднего вида.Хотфикс — это срочный ремонт того, что сломалось прямо в пути. Например, на трассе спустило колесо, и вы его меняете на запаску, чтобы доехать до сервиса. Это не плановое ТО, а экстренное исправление.Что вы теперь понимаете?«В последнем обновлении мы завезли кучу багов, пришлось выпускать хотфикс» = «В новой комплектации машины оказалось много недоработок, и дилер выпустил срочное исправление».4. «Стек технологий» — это не груда непонятного, а набор инструментовСтек — это просто набор инструментов для работы.JavaScript + React + Node.js — это как универсальный швейцарский нож и электродрель. Ими можно много что сделать, они популярны и подходят для разных задач.Python + Django — это как точная лазерная рулетка и набор для черчения. Идеально для сложных расчетов и систем, где важна точность (например, для анализа данных).Java + Spring — это как тяжелая строительная техника: бульдозер и кран. Мощно, надежно, но для постройки сарая это будет избыточно. Идеально для больших корпоративных систем (банки, госструктуры).Что вы теперь понимаете?На вопрос «Каким стеком вы пользуетесь?» вы больше не смотрите в потолок. Вы понимаете, что это просто вопрос выбора правильных инструментов под задачу.5. «Кэш» и «база данных» — это система храненияБаза данных — это главный склад компании на окраине города. Там есть всё, но чтобы что-то получить, нужно время на дорогу и поиск.Кэш — это небольшой склад-быстросклад в центре города. На нем лежат только самые ходовые товары (популярные данные), чтобы можно было быстро их отдать.Что вы теперь понимаете?Фраза «Нужно закэшировать ответы API» означает: «Давайте самые частые запросы пользователей будем хранить на быстром складе, а не ходить каждый раз на главный».РезюмеЯзык IT — это не магия, а отражение логичных процессов. Как только вы находите бытовую аналогию для термина, он перестает быть страшным заклинанием и становится понятным инструментом. Именно этот принцип — объяснение сложного через простое — я положил в основу своей книги «Птичий язык: как говорить на языке разработчиков, не написав ни строчки кода», где подобные аналогии применяются ко всем аспектам IT — от архитектуры до методологий разработки.С чего начать?В следующем разговоре с разработчиком мысленно подставляйте аналогии.Не стесняйтесь переспрашивать: «Правильно ли я понимаю, что это как...?». Разработчики только рады будут помочь и уточнить, ведь это показывает вашу вовлеченность.Когда вы перестаете бояться терминов, вы начинаете не просто «слышать», а понимать команду. А это — первый шаг к по-настоящему эффективной работе вместе.P.S. Если у вас есть свои примеры удачных (или неудачных) аналогий для IT-терминов — делитесь в комментариях, будет интересно обсудить! И если тема показалась полезной — в моем профиле есть ссылка на более подробные материалы.Теги:менеджерhrмаркетингбизнесуправлениепереговорыХабы:Офисы IT-компанийЛайфхаки для гиков",614,0,0,4 мин,https://habr.com/ru/articles/965886/,5383,765,2
"Дерево против Wi-Fi, «проклятый» стол, погодные катаклизмы и другие причины и истории о неочевидных, порой нелепых багах",beeline_cloud,2025-11-13T08:09:11.000Z,"['Блог компании Beeline Cloud', 'Системное администрирование *', 'Компьютерное железо', 'Тестирование IT-систем *', 'Софт']","beeline_cloud 3 часа назадДерево против Wi-Fi, «проклятый» стол, погодные катаклизмы и другие причины и истории о неочевидных, порой нелепых багахВремя на прочтение7 минКоличество просмотров655Блог компании Beeline CloudСистемное администрирование * Компьютерное железоТестирование IT-систем * СофтОбзорСегодня мы в Beeline Cloud решили взглянуть на нестандартные и порой совершенно нелепые баги из мира ИТ — например, когда причиной сбоя стало чрезмерно разросшееся растение или криво уложенная плитка. Рассказываем о подобных ситуациях.Изображение: wirestock (freepik free license)Не баг, а [действительно] фичаКомпания Rogue Amoeba, специализирующаяся на разработке ПО для работы со звуком, недавно поделилась историей из 2002 года, когда ошибка в коде привела фирму к неожиданному успеху. Тогда разработчики выпустили свой первый продукт — приложение для аудиозаписи Audio Hijack. Они предложили пользователям 15-дневный пробный период, после которого требовалось купить лицензию. Первое время продажи шли вяло, но команда продолжала дорабатывать решение.И в какой-то момент спрос на приложение начал резко расти — причем без видимой на то причины. Компания не меняла ценовую политику, не проводила дополнительных рекламных кампаний. Проверив код, инженеры обнаружили источник «удачи» — в последнее обновление закрался баг, который серьезно сократил пробный период... до 15 минут!Так, совершенно случайно разработчики реализовали крайне эффективную стратегию продаж. Судя по всему, за две недели пользователи успевали решить все свои задачи и забывали о продукте. Но если кому-то требовалось записать или отредактировать длинную аудиодорожку «здесь и сейчас» — приходилось покупать лицензию. Глава компании признался, что эта ошибка, вероятно, спасла фирму: «Если бы не эта счастливая случайность, мы бы давно сдались и прекратили разработку. Я не преувеличиваю, когда говорю, что этот баг спас и приложение, и нашу компанию». Спустя почти четверть века Rogue Amoeba все еще успешно применяет эту модель продвижения и даже распространила ее на другие свои продукты.Подобные случаи, когда баг превращается в «удачную фичу» с финансовой точки зрения, встречаются довольно часто — даже у независимых разработчиков. Один специалист на профильном форуме рассказал, что случайно отключил недельный пробный период для платных функций своего приложения, и продажи резко выросли. Однако нужно понимать, что продвижение ИТ-продуктов всегда связано с рисками, и такая ошибка может как помочь, так и «потопить» проект. Так, один из резидентов Hacker News поделился противоположным опытом: его команда внедрила бесплатный тариф в своем SaaS-сервисе, рассчитывая, что он даст пользователям возможность ближе познакомиться с функциональностью продукта и принять решение о покупке. В итоге пользователи оставались на бесплатной версии, а продажи платных пакетов упали.Wi-Fi дождяЭту историю рассказал open source-разработчик Предраг Груевский [он пишет движок запросов Trustfall]. Много лет назад, будучи студентом, он приехал на каникулы к родителям. Отец пожаловался, что с домашним интернетом происходят странные вещи: большую часть времени потери пакетов достигали 98%, но стоило пойти дождю, как Wi-Fi начинал работать стабильно. Инженер дождался непогоды (долго ждать не пришлось), чтобы убедиться в проблеме, а затем — отбросив «магическое мышление» — решил установить причину. Она оказалась как минимум интересной.Ранее Груевский-старший настроил Wi-Fi-мост с направленными антеннами между домом и своим офисом, находившимся в двух кварталах (но в прямой видимости). Так он обеспечивал семью высокоскоростным корпоративным интернетом.Система проработала безупречно почти десять лет, пока на соседнем участке не выросло большое дерево с раскидистыми ветвями. Когда дерево стало достаточно высоким, оно начало блокировать радиосигнал.Однако во время дождя капли тянули вниз ветки и листву, и сигнал снова проходил беспрепятственно. Нет, Груевский не вырубил дерево, но поменял антенны, поддерживающие стандарт 802.11g, на новые — 802.11n. Технология формирования луча (beamforming), в том числе позволила фокусировать Wi-Fi-сигнал в нужном направлении, и связь стала стабильной.Читатели, обсуждавшие эту историю, нашли интересным тот факт, что инженер первым делом не проверил возможное наличие препятствий на пути сигнала, учтивая, что в его сети имелся LOS-компонент. Впрочем, как часто бывает, очевидное решение приходит в голову последним. Также участники обсуждения поделились похожими «магическими» историями. Один из комментаторов рассказал, как связь между офисом его компании и ретрансляционной вышкой каждую ночь прерывалась ровно на десять минут. Причиной неполадки были новые натриевые газоразрядные лампы наружного освещения — во время включения они генерировали радиочастотные помехи в диапазоне 5 ГГц. Проблему решили заменой освещения на менее «шумное».Электричество — не игрушкаПрограммист Алекс Йорк рассказал историю, которая произошла с ним в 2022 году. Он писал программу для распаковки GZIP-файлов и столкнулся с неожиданной ошибкой, указывающей на повреждение архива. Это было странно — ранее ничего подобного не случалось, но теперь ошибка появлялась каждые несколько минут и не исчезала даже после перезагрузки компьютера. Йорк начал искать и устранять возможные причины неполадки одну за другой. Сперва он решил, что проблема возникала из-за состояния гонки, но GZIP-компрессор не был многопоточным. Затем исключил варианты с ошибкой кодирования и повреждением данных. Однако ошибки возникали без всякой логики: то появлялись, то исчезали. Алекс стал искать менее очевидные объяснения.Изображение: freepik (freepik free license)Йорк даже заметил, что сбои начались с приходом жары, и попытался связать нестабильное поведение ноутбука с работой кондиционера. Возможно, воздух делался слишком сухим или, наоборот, слишком влажным, и это как-то отражалось на работе его ноутбука?Позже к программным сбоям добавились проблемы с операционной системой, а приложения стали загружаться невероятно долго — но только если ноутбук был подключен к сети. И тогда программист предположил, что проблема должна быть связана с перепадами напряжения и электропроводкой в квартире — и точно, розетка, к которой он подключал устройство, не имела заземления. Йорк вызвал электрика, который установил новую розетку. После этого проблемы исчезли, а программа заработала без сбоев.Стол-убийца XboxРазработчик Аллен Пайк поделился историей коллеги — она про Xbox, который отказывался работать... на конкретном столе.В прошлом его коллега был членом команды, которая разрабатывала игру для еще не вышедшей на тот момент консоли Microsoft. Разработка близилась к завершению, и QA-инженеры установили три пре-релизных Xbox, чтобы проводить на них ночные тесты. Если утром игра все еще работала, билд считался стабильным, однако одна из консолей «крашнулась». Ошибка указывала на сбой, связанный с GPU, а отладка такого бага — нетривиальная задача. Что интересно, проблема повторилась и на вторую ночь, и на третью… QA-инженеры довольно быстро нашли закономерность: сбой возникал не на конкретной приставке, а на любой, которую ставили на определенный стол. Чтобы разгадать загадку, ведущий инженер решил провести ночь в офисе.И под утро он понял, в чем дело: первые лучи солнца пробивались через окно, падали на консоль и нагревали корпус. Этого оказалось достаточно, чтобы устройство отключалось из-за перегрева. Так, всему виной было неудачное расположение рабочего стола. Смотрите под ногиИнженер Патрик Томсон рассказал историю из профессиональной практики своего отца, который в 1980-х работал в Storage Technology, занимавшейся разработкой аппаратного обеспечения для хранения данных [ленточных накопителей в частности]. Эта история старая, но до сих пор известна в интернете под заголовком: «Лучшая история отладки, которую я когда-либо слышал». Одна крупная компания, использовавшая ленточные накопители Storage Technology, столкнулась с загадочной проблемой во время обработки и печати данных. Оборудование сбоило во время многочасовых сессий, и техники не могли повторить ошибку в тестовых условиях. Специалисты компании даже заменили часть компонентов, но это не помогло решить проблему. Тогда они пригласили человека, которого автор называет «экспертом с большой буквы» (The Expert).Он остался в комнате наблюдать за процессом работы. Несколько тестов подряд закончились сбоем, но источник неисправности по-прежнему ускользал. Наконец, Эксперт заметил закономерность: ошибка возникала ровно в тот момент, когда один из сотрудников проходил по определенной алюминиевой плитке на полу, под которым проходили коммуникации.Оказалось, что панель была слегка деформирована. Когда на нее наступали, она терлась о соседние плитки, вызывая радиочастотные помехи, которые приводили к сбоям в чувствительной оперативной памяти компьютера. Плитку заменили, и проблема больше не проявлялась.На фото: IBM 7030 Stretch, CC BY-SA 3.0Вообще, сбои в работе «старого железа» часто были вызваны именно физическими факторами. Так, ученые из Лос-Аламосской национальной лаборатории в США в 50–60-х годах столкнулись с необычной проблемой в работе компьютера IBM 7030 Stretch. Время от времени в памяти на магнитных сердечниках возникали ошибки, а потом так же внезапно пропадали. Инженеры сетовали на программные неполадки, но за несколько дней так и не нашли источник. А причина была следующей — память IBM 7030 Stretch для температурной стабилизации погружалась в масло. Это масло постоянно циркулировало, и в нем плавал отколовшийся кусочек припоя. Именно он время от времени вызывал кратковременный сбой памяти. Решение — замена масла.Чужое решение непростой проблемыРазработчик из команды Google Docs Джейкоб Войтко вспомнил, как он с коллегами устранял критический сбой, который не позволял пользователям редактировать свои документы в Google Docs без обновления странички. Причем проблема повторялась только в Chrome определенной версии. Несколько дней специалисты потратили на поиск причины, но серьезных зацепок получить не удавалось — ошибку видели, но не понимали, что с ней делать и почему она появляется. Но затем случился маленький прорыв — благодаря помощи коллеги команда убедилась, что неполадка связана с методом Math.abs(). Он должен возвращать неотрицательное число, но почему-то возвращал отрицательные значения для отрицательных входных данных.Разработчик вышел в интернет с этим вопросом обратился к команде Google Chrome, а те перенаправили его к специалистам, занимающимся V8-движком. Те отрапортовали Войтко, что в одной из последних версий был небольшой баг в применяемых методах оптимизации, который они уже исправили — и обновление скоро выйдет. Так что Войтко с коллегами два дня безуспешно искали решение проблемы, которую уже исправили за них. Грандиозного фикса не было, был добавлен лишь временный костыль для проверки версий браузера. Как иронизирует сам автор, единственный урок, который он усвоил из ситуации: «Такова жизнь». А читатели блога отметили, что контроль качества не только сложен, но бывает чрезмерно утомительным и стрессовым — и с неясными перспективами. Иногда QA-специалисту просто не предоставляют нужной информации или грамотной обратной связи — ему приходится почти вслепую разбираться с неполадками.Beeline Cloud — secure cloud provider. Разрабатываем облачные решения, чтобы вы предоставляли клиентам лучшие сервисы.Другие подборки об интересном и необычном в нашем блоге:Музей тех. провалов, «ошибка медитации гуру» и другие странностиВозможно ли возродить ретроигры и старые консоли на FPGA?Визитка в 2025? Светодиоды, микроконтроллер и руки из плечТеги:beeline cloudсофтнеобычные багиwi-fiдебаггингзанимательные задачиХабы:Блог компании Beeline CloudСистемное администрированиеКомпьютерное железоТестирование IT-системСофт",655,0,0,7 мин,https://habr.com/ru/companies/beeline_cloud/articles/965850/,11765,1570,5
Зачем Тьюринг изобрёл Redux?,AlekseiVolkov,2025-11-13T08:15:06.000Z,['JavaScript *'],"AlekseiVolkov 3 часа назадЗачем Тьюринг изобрёл Redux?Уровень сложностиПростойВремя на прочтение4 минКоличество просмотров305JavaScript * МнениеКонечно, он его не изобретал. Но если бы посмотрел сегодня сливы собесов, точно увидел бы в них свои идеи. Ведь есть же фундаментальные идеи за всей этой шелухой про синтаксис mapStateToProps!Тьюринг думает надо логотипом Redux Императивное программированиеДопустим, вам поручили сделать web проигрыватель. Бизнес ставит задачи, сроки горят. Что делаем? Правильно, берём родной <audio>.<audio src=""https://cdn.pixabay.com/audio/2025/03/18/audio_7d5c12b31a.mp3""></audio>Нужно будет лишь подписаться на его события. // Обработчики событий элемента <audio>

audio.addEventListener(""play"", () => {
  // ...
});

audio.addEventListener(""pause"", () => {
  // ...
});

audio.addEventListener(""ended"", () => {
  // ...
});

// Обновление прогресса
audio.addEventListener(""timeupdate"", () => {
  // ...
});Самое интуитивное желание — определить четыре флага для каждого возможного состояния проигрывателя. // Множество флагов состояния
var isPlaying = false;
var isPaused = false;
var isStopped = false;
var isEnded = false;И на каждое событие проверить все флаги и выключить логически ненужные. В конце - перерисовать интерфейс. audio.addEventListener(""play"", () => {
  isPlaying = true;
  isPaused = false;
  isStopped = false;
  isEnded = false;

  updateUI();
});Всё работает! Что ещё можно желать?Ссылка на рабочий пример: https://jsbin.com/huyijutoyi/edit?js,output. Автоматное программированиеПроблема с флагами в их необязательности. Может ли проигрыватель быть одновременно в isPlaying и isEnded? По логике — нет, но в коде ничто не мешает этому случиться. В командной разработке нужно решение, которое сделает такие ошибки невозможными. Решение, которое облегчит жизнь не только программистам, но и бизнесу. И оно было придумано задолго до JavaScript — это конечные автоматы. Заметный вклад в объединение этой темы с программированием внёс Алан Тьюринг. Вернёмся в воображаемую контору. Вот планы команды на ближайшее будущее:Записывать действия пользователейПоддерживать состояния: volumechange, waiting, emptied Прислушаться к советам мудрого сторожилы конторы и использовать «design patterns» для разбиения на файлыИ всего через три итерации уже ясно, что следующая задача — документирование количества флагов и мест, где они могут меняться. Нужно решение! Чтобы большая команда делала меньше ошибок. Да и чтобы бизнес понимал какие состояния бывают и что умеет приложение.Немного поискав информацию, можно найти прекрасные доклады и целую книгу:Кирилл Мокевнин, Конечные автоматы как способ значительно упростить логику и понимание кода""Автоматное программирование"" Поликарпова Н. И., Шалыто А. А.Александр Усков — Автоматное программирование и его применение в видеостримингеКонечный автомат на React Hooks и Typescript — Сергей ВолодинРешено! Будем использовать наследие Алана Тьюринга и его современников - использовать конечные автоматы. Конечные автоматыПосмотрим на схему работы, может что-то можно переписать иначе?Три флага состоянийНужно не улучшить текущий код, а переписать его заново (да, тоже нужно будет протолкнуть на планировании)!Буквально, поменять эти флаги на состояние. Написать простую схему переходов между ними и запрограммировать с функцией-редуктором. Граф состоянийБлаго, в воображаемом примере не всё так плохо. Можно легко переписать флаги на множество управляющих состояний. // Множество управляющих состояний
var STATES = {
  STOPPED: ""⏹️ Остановлен"",
  PLAYING: ""🎶 Играет"",
  PAUSED: ""⏸️ Пауза"",
  ENDED: ""✅ Завершено"",
};А бизнес-логику приложения перенести из каждого события в функцию переходов (она же редуктор). // Функция переходов
function handleAction(action) {
  switch (action) {
    case ""PLAY"":
      if (currentState === STATES.STOPPED || currentState === STATES.PAUSED) {
        audio.play();
        currentState = STATES.PLAYING;
      }
      break;

    case ""PAUSE"":
      // ...
      break;

    case ""STOP"":
      // ...
      break;
  }

  updateUI();
}Обновление интерфейса теперь простая задача: есть состояние - покажи сообщение. И так мы переехали на автоматное программирование!У нас есть единое управляющее состояниеЛогика переходов единственно, полно и декларативно описана в функции переходовЭтот switch-case уже можно вынести в отдельную библиотеку!Ссылка на рабочий пример: https://jsbin.com/kelukazovu/edit?js,output.ReduxТак и сделаем! А потом, а потом…Сделаем независимую от бизнес-логики библиотеку,чтобы и в реакте работало,чтобы и обновление частей состояния не вызывало перерисовки!!Чтобы были плагины в браузере для истории переходов!Постойте... нам что, нужен Redux?!Конечно, для нашего такого проигрывателя — это стрельба из базуки по воробьям. Но главное – это фундаментальный принцип, на котором он построен. Redux — это не просто магический и новомодный синтаксис с экосистемой. Это, в первую очередь, реализация архитектурного шаблона конечного автомата, дополненная удобными инструментами (middleware, dev-tools).Ссылка на рабочий пример с Redux: https://jsbin.com/wazimaziga/edit?js,output. ВыводТак я показал проблему управления состоянием и её решения, начав аж с самого Тьюринга!Я жалею, что никто не рассказывал мне про redux именно так. Большинство блогеров сразу переходят к синтаксису, не отвечая на вопрос: “оно вообще зачем нужно?”.Так же я ранее рассмотрел и тему замыканий, начав с проблемы фунарга. И ещё много говорил про концепции React. Если вы есть, будьте первыми!Теги:конечный автоматмашина тьюрингаstate managementХабы:JavaScript",305,0,0,4 мин,https://habr.com/ru/articles/962876/,5579,698,1
