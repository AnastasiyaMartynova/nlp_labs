title,author,publish_date,hubs,text,views,comments,rating,reading_time,url,text_length,word_count,hubs_count
"Один в поле не воин: как собрать команду для мероприятия, чтобы всё прошло гладко (и вы не сошли с ума)",kseniaevent,2025-11-13T06:04:31.000Z,"['Developer Relations *', 'Конференции', 'Управление персоналом *', 'Управление проектами *']","kseniaevent 5 часов назадОдин в поле не воин: как собрать команду для мероприятия, чтобы всё прошло гладко (и вы не сошли с ума)Уровень сложностиПростойВремя на прочтение8 минКоличество просмотров100Developer Relations * КонференцииУправление персоналом * Управление проектами * ТуториалПривет! Продолжаем говорить про организацию мероприятий для непрофессиональных ивентеров. HR, ассистенты, руководители проектов, маркетологи, пиарщики и все те, на чью долю выпало в нагрузку к основным рабочим обязанностям сделать мероприятие, эта статья для вас.Мы уже обсудили с чего начать подготовку к мероприятию, его упаковку, CJM участников, выбор площадки, организацию питания, как собрать программу, фото-/видеосъемку, организацию трансляции и спонсорские интеграции. Пришло время поговорить про команду. Как мы уже говорили ранее, в случае мероприятия один в поле - не воин. Это не только довольно тяжело физически и морально, но и опасно, так как если организатор делает все сам, то он представляет из себя страшный bus-фактор и если с ним что-то случится, то мероприятие может сорваться. Поэтому в этом разделе мы рассмотрим основные роли, которые могут быть в команде. В зависимости от масштаба мероприятия некоторые из этих ролей могут не пригождаться. Но я верю в то, что данный список поможет вам охватить контролем все зоны мероприятия и не взять на себя лишнего. Ключевые роли и их функцииКаждый член команды играет важную роль, и от их профессионализма, ответственности и способности работать вместе зависит общий успех. При формировании команды важно учитывать компетенции участников, четко распределять обязанности и зоны ответственности, корректно и понятно ставить задачи и контролировать их выполнение.Ключевые роли и их функцииМенеджер проекта - руководитель команды и основной координатор всех процессов. В его обязанности входит:Планирование всех этапов мероприятия.Контроль сроков, бюджета и ресурсов.Управление рисками и решение непредвиденных ситуаций.Он несет ответственность за все мероприятие в целом.Менеджер по продвижению - отвечает за привлечение участников и создание информационного поля вокруг мероприятия. На этой роли могут быть маркетологи, smm-менеджеры, контент-менеджеры и даже менеджеры по внутренним коммуникациям в зависимости от специфики вашего мероприятия и маркетинговой стратегииОсновные задачи:Разработка стратегии продвижения.Подготовка и размещение рекламных материалов.Работа с соцсетями и аналитикой.Дизайнер - тот человек, который сделает вам красиво. Задачи дизайнера минимально включают:Создание визуальной айдентики мероприятияРазработку логотипов, баннеров, презентаций и иных брендированных материалов.Подготовку макетов для печати.Веб-разработчик или команда - обеспечивают создание и техническую поддержку сайта мероприятия. Копирайтер - создает все тексты, которые будут использованы в рамках мероприятия, за исключением сценария разве что, хотя и там копирайтер может приложить свою руку к отдельным частям. Может также быть представлен в роли smm-менеджера. Задачи:Написание пресс-релизов, постов, анонсов.Подготовка текстов для ведущих и спикеров.Вычитка текстов от СМИРежиссер-сценарист - это может быть как один человек, который совмещает в себе обе роли, так и отдельные люди. Суть от этого не меняется, задача сформировать и реализовать программу. Обычно речь все же идет про шоу-программу. Кстати, режиссер мероприятия и режиссер трансляции - это точно 2 разных человека, если у вас гибридное мероприятие.Задачи: Составление сценария.Координация тайминга шоу. Постановка номеров.Организация репетиций.Программный директор и/или комитет - тоже работают над программой, но уже деловых событий и конференций, там, где встречаются доклады. Задачи: Наполнение программы в соответствии с тематикой мероприятия.Выбор подходящих докладов и их доработка.Репетиции с выступающими. Иногда они также занимаются модерацией и контролем за проведением сессий и выступлений.Юрист - проверяет все договоры, подписываемые с подрядчиками, партнерами и спонсорами, консультирует по вопросам соблюдения прав гостей (например, согласие на обработку персональных данных или съемку), разрабатывает договоры с сотрудниками и временным персоналом.Бухгалтер - отвечает за все финансовые операции: расчеты с подрядчиками, оплата услуг и закупок, организует прием оплаты от участников, если мероприятие платное. Иногда, если планируется продажа чего-либо в день мероприятия, то помогает с организацией приема оплаты на площадке. Банкетный менеджер - в некоторых случаях вам придется нанять и его, если площадка сама не справляется (см. подробнее в главе «Питание»). Отвечает за весь сервис связанные с питанием гостей и персонала, помогает составить меню, следит за его реализацией, контролирует выносы и объемы потребления. Хорошо бы, чтобы он имел доступ на кухню на случай кризисных ситуаций, а для этого нужна мед.книжка и предварительное согласование с кухней.Персонал на площадке в день мероприятияДля оперативной работы на площадке важно заранее определить и заказать или найти среди сотрудников необходимый персонал. Ниже очень важные люди, про которых очень часто забываютПерсонал на площадке в день мероприятияРегистраторы - встречают гостей, выдают бейджи и материалы. Мы говорили о них в блоке про организацию площадки и работу с гостямиХостесс - помогают с навигацией и отвечают на вопросы участников. Обычно приглашаются на более статичные мероприятия. На мероприятиях попроще могут быть заменены на волонтеров.Монтажно-демонтажная группа (АХО) - отвечает за установку оборудования и декораций.Охрана - обеспечивает безопасность гостей и защищает имущество. Подробнее в блоке про охрануГардеробщики - заботятся о хранении верхней одежды гостей.Клинеры - поддерживают чистоту и порядок на протяжении мероприятия.Информационная стойка - предоставляет информацию и оперативно решает вопросы. Расположена на площадке мероприятияКонсьерж - помогает гостям с заказом такси, билетов, навигацией по городу. Обычно встречается на ВИП-мероприятихКолл-центр или саппорт - отвечают на вопросы гостей в чате / боте / по телефонуВолонтеры — незаменимые помощники, особенно на больших мероприятиях.  Их основные задачи встреча гостей и помощь с навигацией, помощь в функциональных зонах (нетворкинг, фуд-корт, мастер-классы), быстрое реагирование на запросы организаторов (дай-подай-принеси). Супервайзер волонтеров - обучает и координирует работу команды волонтеров, а также контролирует их в день мероприятия, решает возникающие проблемы и следит за соблюдением расписания.Хелперы - диапазон их задач довольно велик, я для себя делю их на технические - помощь с застройкой, перетаскиванием и прочими физическими работами, которые не связаны с взаимодействием с гостями, и административные - это своего рода платные волонтеры, возможно, несколько более опытные, которые представляют собой запасной человеческий ресурс. Ими можно усилить регистрацию, потом перекинуть на навигацию, при необходимости попросить встретить спикера и т.д.Курьеры - их тоже часто забывают заложить в бюджет, поэтому напомню об этой статье тоже - доставляют материалы, оборудование и еду. Если в процессе организации вы можете пользоваться приложениями, то в день мероприятия, если оно напряженное и вы понимаете, что вам нужно оказывать сервис превосходящий ожидания, то хорошо бы иметь дежурного курьера во время всего мероприятия. Вообще это просто может быть водитель с машиной, который сможет съездить по вашему поручению. Сотрудники для функциональных зон - координируют работу зон нетворкинга, фотозон, интерактивных площадок, залов и проч. Они следят за выполнением запланированных активностей и решают возникающие вопросы. В зависимости от формата мероприятия важно предусмотреть персонал для контроля всех важных зон. На небольших мероприятиях допускается совмещение. Подбор временного персонала и его обучениеЧасто для проведения мероприятий нанимается временный персонал, это нормальная практика. Его правильная подготовка и управление являются залогом успеха и хорошего клиентского опыта ваших гостей. Рассмотрим основные этапы подготовкиПодборСейчас есть специализированные агентства, которые предоставляют временные персонал для мероприятия. Обычно это самый безопасный и надежный способ поиска. Выбирайте активных, коммуникабельных и ответственных людей. Просите отзывы и опыт работы на других мероприятиях. Проводите интервью с теми, кто будет много коммуницировать с гостями не по стандартному протоколу.Согласование условий работыЗаранее согласуйте график работы и обязанности временного персонала.Согласуйте оплату и форму одежды персонала в день мероприятия. Не стесняйтесь запрашивать фото, если персонал будет в своей одежде. Если вы предполагаете бронированную одежду, то соберите размеры и закажите ее. Мой совет, берите что-то в стиле one size, так как часть персонала может «отвалиться» в процессе подготовки и будет заменена другими людьми.Обеспечьте удобное место для отдыха между сменами, а также питание в случае длительных смен.Предусмотрите технические перерывы и человека, который будет на замене на время перерываНазначьте супервайзера для контроля временного персонала. Если у вас много временного персонала, то берите супервайзеров для каждой категории. Супервайзер должен быть связующим звеном между персоналом и организаторами, обеспечивать своевременную ротацию людей, чтобы избежать их перегрузки и быстро реагировать на вопросы и проблемы.Обучение Проведите вводный инструктаж на площадке за несколько дней до мероприятия, если есть такая возможность. Хотя бы покажите заранее площадку супервайзерам, чтобы в день мероприятия они смогли сами расставить подконтрольный им персонал и проинструктировать. Перед инструктажем на площадке рекомендую выслать письменную инструкцию со всеми правилами, чтобы во время очной встречи ответить на вопросы. Также эта инструкция будет шпаргалкой для вас, чтобы ничего не забыть. Разъясняйте обязанности, моделируйте ситуации и смотрите за реакцией. Обучите заранее работе с оборудованием и навигацией.Распределение зон ответственности Назначайте задачи в зависимости от интересов и компетенций персонала. Особенно это касается волонтеров и хелперов, которые в целом многофункциональны, но обычно имеют сильные стороны и предпочтения.Контроль в день мероприятияВ процессе мероприятия вы должны контролировать работу временного персонала. Если у вас есть супервайзер, то это происходит через него. Ег задача присылать вам периодически фото того, что персонал в нужном количестве находится в своем месте и выполняет обязанности. Также супервайзер следит за общением с гостями, если оно предусмотрено и корректирует его при необходимости. Благодарность после мероприятияНе забудьте после мероприятия поблагодарить весь персонал. Если не успеваете лично, то хотя бы передайте благодарности через супервайзера. При необходимости будьте готовы написать благодарственные письма. Если у вас работают волонтеры на безвозмездной основе, то подарите им фирменный мерч или иным способом порадуйте ребят. Кстати, у меня было несколько кейсов, когда встречались настолько хорошие волонтеры, что мы обменивались контактами и продолжали работать на других мероприятиях уже на платных условиях. Некоторые даже работали в штате компании какое-то время. Так что, не упускайте хорошие кадры.  Организация работы внутренней командыДопустим, вы добились того, что ваши коллеги будут вам помогать в процессе организации мероприятия и у них даже выделены время и ресурс на это. Для эффективной работы команды необходимо выстроить четкую систему внутренней коммуникации. Минимальный чек-листНазначьте ответственных за различные направления, чтобы минимизировать хаос в коммуникациях и понимать, с кого спрашивать результат.Создайте общий чат в мессенджере для оперативного решения вопросов. Возможно, будет даже несколько чатов по разным направлениям мероприятия. Проводите регулярные встречи для обсуждения задач и решения проблем. Будьте на связи и выстраивайте открытую коммуникацию, чтобы сотрудники не боялись задавать вам уточняющие вопросы и просить помощи в решении проблем. Ничего не замалчивайте.Организуйте перерывы для отдыха и питания, особенно во время интенсивных дней подготовки и самого мероприятия.Контролируйте работу в день мероприятия и просите регулярно (периодичность установите сами) присылать вам уведомления, все ли идет хорошо в их зоне ответственностиПредусмотрите бонусы или подарки по итогам мероприятия.Мой принцип в коммуникации с командой ивента: демократия в процессе подготовки и деспотизм на площадке. В процессе подготовки мы всегда могли обсудить разные варианты решения задач и я всегда рада выслушать мнение команды, но на площадке на демократию часто не остается времени, особенно в ситуации форс-мажора, я ответственна за весь процесс и поэтому придется принимать мои решения, даже если они кажутся неверными.  Ксения КазаковаHR-проекты / Employer Brand / DevRelТеги:мероприятияконференцииуправление людьмиуправление проектамиуправление командойХабы:Developer RelationsКонференцииУправление персоналомУправление проектами",100,0,0,8 мин,https://habr.com/ru/articles/964316/,13025,1685,4
Очереди сообщений в Postgres Pro: отказ от внешних брокеров ради транзакционной надёжности,slonik_pg,2025-11-12T15:32:34.000Z,"['Блог компании Postgres Professional', 'SQL *', 'PostgreSQL *', 'Базы данных *', 'Серверное администрирование *']","slonik_pg 19 часов назадОчереди сообщений в Postgres Pro: отказ от внешних брокеров ради транзакционной надёжностиУровень сложностиПростойВремя на прочтение7 минКоличество просмотров3.3KБлог компании Postgres ProfessionalSQL * PostgreSQL * Базы данных * Серверное администрирование * ОбзорВ эпоху распределённых систем, где каждая компонента должна быть не только эффективной, но и предсказуемой, вопрос надёжности обмена данными становится критическим. Представьте: пользователь нажимает кнопку «Сгенерировать отчёт», и в этот момент должны синхронизироваться десятки процессов — от создания документа до его отправки по email. Но что если почтовый сервер временно недоступен? Или обработчик задач падает, не завершив операцию? Именно здесь на сцену выходят очередь сообщений — механизм, который превращает хаотичные запросы в управляемый поток, гарантируя, что ни одна задача не потеряется в пути.История создания расширения встроенных очередей в PostgreSQL началась с простой боли: внешние брокеры вроде RabbitMQ или Kafka, хотя и мощные, добавляли слои сложности. Управление ими требует выделенных ресурсов: отдельные серверы, настройка кластеров, мониторинг доступности. В enterprise-среде, где системы развертываются тысячами экземпляров, каждый новый компонент увеличивает риски и административную нагрузку.Зачем подключать отдельный брокер, если очередь уже встроена в базу данных и работает «из коробки»? Это не только экономит время, но и исключает проблемы согласованности: если транзакция откатывается, сообщение автоматически возвращается в очередь для повторной попытки, без единой строчки дополнительного кода.Два подхода к очередям: Log-based и AMQP/JMSВ мире распределенных систем сформировались два подхода к обработке сообщений:Log-based-очереди (например, Kafka). Работают как непрерывный журнал событий. Данные записываются строго последовательно, и потребители читают их в том же порядке. Это идеально для синхронизации данных между микросервисами или репликации баз данных. Однако их сила — в линейности — становится слабостью, когда нужна гибкость: выборка сообщений по приоритету или фильтрам здесь невозможна.AMQP/JMS-брокеры (например, RabbitMQ). Позволяют не просто передавать сообщения, но и управлять их жизненным циклом: устанавливать приоритеты, фильтровать по условиям, обрабатывать ошибки. Такие очереди позволяют имитировать асинхронное RPC, где задача может быть повторена при сбоях. Но их главная слабость — фундаментальная проблема, общая для любого внешнего брокера, включая Kafka: невозможно гарантировать транзакционную целостность с базой данных.Проблемы внешних брокеров: транзакционность и сложностьИспользование внешних брокеров, таких как Kafka или RabbitMQ, в enterprise-средах часто сопряжено со скрытыми сложностями:Рассогласование данных. Представьте сценарий: приложение отправляет сообщение в RabbitMQ и начинает транзакцию в базе данных. Сообщение уже ушло в брокер, но в момент коммита транзакции в БД происходит сбой. Результат — рассогласование: задача будет обработана, но данные для неё не сохранены. Попытки гарантировать атомарность через двухфазный коммит (2PC) усложняют архитектуру, требуя дополнительного координатора транзакций, а также заметно снижают производительность.Административная нагрузка. Внешние брокеры требуют отдельной инсталляции, настройки, мониторинга и резервного копирования. Это создаёт дополнительные точки отказа (например, сетевые сбои между сервером приложений и брокером) и усложняет поддержку, особенно если за СУБД и брокер отвечают разные команды.Postgres Pro Enterprise Queues: транзакционность и автоматизация с pgpro_queueНовое расширение pgpro_queue — это ответ на «боль» разработчиков, уставших бороться с рассогласованием данных. Интеграция очередей непосредственно в СУБД устраняет необходимость во внешних компонентах. Сообщения хранятся в обычных таблицах, реплицируются через стандартные механизмы PostgreSQL и участвуют в тех же транзакциях, что и бизнес-логика приложения.Установка и настройкаИнтеграция pgpro_queue начинается с простых шагов, знакомых любому DBA:Добавление в shared_preload_libraries. В файле postgresql.conf необходимо добавить расширение в список предварительно загружаемых библиотек:shared_preload_libraries = 'pgpro_queue'Создание расширения. В нужной базе данных выполняется команда:CREATE EXTENSION pgpro_queue;Инициализация. Для хранения служебных объектов (таблиц метаданных, очередей) создаётся отдельная схема pgpro_queue_data. Это делается с помощью функции:SELECT queue_initialize();Такое разделение обеспечивает корректную работу pg_dump и репликации.Ключевые возможности и их реализация1. Retry-on-rollback (автоматический повтор при откате).Это главная особенность pgpro_queue. Если транзакция, в которой было прочитано сообщение, откатывается (например, из-за недоступности внешнего сервиса), сообщение не теряется, а автоматически возвращается в очередь для повторной обработки.Управление. Поведение повторов настраивается как на уровне очереди, так и для каждого сообщения. При создании очереди с помощью CREATE_QUEUE можно задать параметры q_retries (максимальное количество попыток) и q_retrydelay (задержка в секундах перед следующей попыткой).Важный параметр. Для работы механизма повторов необходимо явно указать базу данных в конфигурационном файле postgresql.conf:pgpro_queue.database_with_managed_retry = 'имя_вашей_бд'Без этой настройки сообщения при откате транзакции будут удаляться, а не ставиться на повтор.2. Фильтрация и приоритеты.pgpro_queue позволяет гибко управлять потоком сообщений:Приоритеты. При вставке сообщения с помощью INSERT_MESSAGE можно указать q_msg_priority. Чем ниже значение, тем выше приоритет. Сообщения с более высоким приоритетом обрабатываются в первую очередь.Фильтрация. Функции чтения READ_MESSAGE и READ_MESSAGE_XML принимают параметры q_msg_hfilter и q_msg_pfilter, позволяя извлекать сообщения по содержимому их заголовков или свойств.3. Поддержка форматов JSON и XML.Расширение предоставляет отдельные функции для работы с разными форматами данных, что упрощает интеграцию с различными системами:INSERT_MESSAGE и READ_MESSAGE для JSONB;INSERT_MESSAGE_XML и READ_MESSAGE_XML для XML;READ_MESSAGE_ANY для чтения сообщений любого формата из одной очереди.4. Отложенная обработка.При вставке сообщения можно указать параметр q_msg_enable_time, чтобы задача стала доступна для обработки только при наступлении указанного времени.Что мы положили под капот?В основе расширения pgpro_queue лежит простой и надёжный принцип: сообщения хранятся в обычных таблицах PostgreSQL. Такой подход позволяет использовать всю мощь стандартных механизмов СУБД, включая репликацию и восстановление через WAL-файлы. Каждое сообщение, попавшее в очередь, записывается в WAL, что гарантирует его восстановление даже после аварийного отключения сервера. Ключевая особенность pgpro_queue — это глубокая интеграция с транзакционной моделью PostgreSQL, реализующая механизм retry-on-rollback. Он работает как страховка: если транзакция, обработавшая сообщение, откатывается из-за любой ошибки, pgpro_queue автоматически возвращает это сообщение в очередь для повторной попытки. Это гарантирует, что задача будет считаться выполненной только тогда, когда вся связанная с ней работа успешно зафиксирована в базе данных.Ещё одна сильная сторона pgpro_queue — синергия со встроенным в Postgres Pro Enterprise планировщиком фоновых задач (Scheduler). Этот компонент, работающий как cron внутри базы данных, может запускать периодические родительские задачи, которые, в свою очередь, наполняют очередь подзадачами для асинхронного выполнения. Например, массовую генерацию отчётов для клиентов из разных часовых поясов можно разбить на этапы: основная задача в планировщике создаёт подзадачи для каждого региона, а те, в свою очередь, помещают сообщения в очередь с учётом локального времени.Внешние брокеры, такие как Kafka, справедливо славятся своей «непотопляемостью»: они действительно спроектированы выдерживать серьёзные сбои. Однако эта надёжность относится к самому брокеру в изоляции. В реальной системе слабое звено появляется в точках интеграции, и общая стабильность оказывается зависима от внешних факторов: сети, конфигурации, транзакционной согласованности с базой данных. Но pgpro_queue исключает эти риски: очереди живут в той же экосистеме, что и данные приложения. Сообщения не теряются при сбоях, так как их сохранность гарантируется механизмами репликации и восстановления СУБД. Администраторам больше не нужно настраивать отдельные системы мониторинга для очередей: всё управляется через знакомые инструменты PostgreSQL.Важные технические особенностиДля корректной работы с pgpro_queue специалистам следует учитывать несколько моментов:Уровень изоляции. Расширение может использоваться только в транзакциях с уровнем изоляции READ COMMITTED.Подготовленные транзакции. При использовании двухфазного коммита (2PC) есть нюансы. Если транзакция с READ_MESSAGE() подготавливается через PREPARE TRANSACTION, сообщение блокируется до выполнения COMMIT PREPARED или ROLLBACK PREPARED. При этом ROLLBACK PREPARED только разблокирует сообщение, но не активирует логику повторных попыток.Что ждет встроенные очереди: дорожная картаРазвитие pgpro_queue сфокусировано на расширении сценариев использования. Уже в ближайших версиях появятся две ключевые возможности:Система подписок (Pub/Sub). Будет реализован механизм, аналогичный Exchange в RabbitMQ. Продюсер сможет отправлять сообщение в «тему», а не в конкретную очередь. Все потребители, подписанные на эту тему, получат свою копию сообщения. Это открывает путь к созданию полноценных событийно-ориентированных архитектур.Callback-уведомления. Появится возможность настроить «обратный вызов» — HTTP-запрос к внешнему сервису при появлении сообщения в очереди. Это позволит СУБД самой инициировать обработку, а не ждать, пока приложение опросит очередь.Функциональность Dead Letter Queue (DLQ) для «отравленных» сообщений также остаётся в планах, но на более долгосрочную перспективу.ЗаключениеДля корпоративных клиентов ключевое преимущество — предсказуемость. Каждое сообщение обрабатывается в рамках транзакций СУБД, что исключает рассогласование данных. Инфраструктура становится проще: вместо набора разнородных сервисов — единая база данных, где очереди, бизнес-логика и планировщик работают как части одного организма.Такие решения идеальны для проектов, где цена ошибки высока: банковские транзакции, медицинские системы, государственные реестры. Если вашему приложению критически важны атомарность операций и минимизация ручного вмешательства, встроенные очереди PostgreSQL — не выбор, а необходимость. Они не просто обрабатывают сообщения, а становятся страховкой от хаоса в мире распределённых систем.Теги:postgresqlkafkakafka apacheброкер сообщенийброкеры сообщенийбазы данныхpostgres propostgres pro enterpriseХабы:Блог компании Postgres ProfessionalSQLPostgreSQLБазы данныхСерверное администрирование",3300,0,0,7 мин,https://habr.com/ru/companies/postgrespro/articles/965632/,10931,1281,5
"Культура «AI-First»: как перестроить мышление команды, чтобы не отстать от рынка",Michael_Jerlis,2025-11-12T22:37:26.000Z,"['Искусственный интеллект', 'Читальный зал', 'Управление проектами *', 'Машинное обучение *', 'Управление персоналом *']","Michael_Jerlis 12 часов назадКультура «AI-First»: как перестроить мышление команды, чтобы не отстать от рынкаУровень сложностиСреднийВремя на прочтение4 минКоличество просмотров683Искусственный интеллектЧитальный залУправление проектами * Машинное обучение * Управление персоналом * МнениеПока одни компании разочаровываются в искусственном интеллекте, другие строят на его основе бизнес-империи. В чем их секрет? Не в деньгах и не в доступе к технологиям, а в особой культуре.Разберемся, как перестать просто использовать ИИ в работе и перейти к мышлению в стиле AI-First, и почему это единственный способ не превратиться в динозавра.Сколько стоит промедлениеВ прошлой статье я рассказал, почему многие компании, которые внедрили ИИ, разочаровались. Это не кликбейт, а суровая реальность: по некоторым оценкам, до 80% ИИ-проектов проваливаются — это вдвое чаще, чем обычные IT-проекты.Проблема не в том, что ИИ не работает. Но если встроить двигатель от гиперкара в телегу, она точно не поедет. Технология требует не просто инвестиций, а ментального сдвига всей команды, от CEO до стажера.Чтобы избежать участи «динозавров» рынка, которые выбывают из гонки из-за медленного внедрения, нужно перестать воспринимать ИИ как инструмент и сделать его фундаментом всех бизнес-процессов.От инструмента к фундаменту: что значит быть AI-FirstТермин «AI-First» ввел в оборот CEO Google Сундар Пичаи в 2016 году, когда объявил о стратегическом развороте компании. Быть AI-First — значит не просто «прикручивать» ИИ к рабочим процессам, а проектировать продукты, операции и принимать решения вокруг возможностей, которые дает искусственный интеллект.Вместо вопроса «как нам использовать ИИ в этом проекте?», стоит задуматься «как ИИ может полностью изменить способ решения этой задачи?». Этот подход помогает перестроить мышление.Исследование RAND выявило 5 ключевых причин провала ИИ-проектов, и почти все они — культурные, а не технические:Непонимание бизнес-проблемы;Нет качественных данных;Фокус на технологии, а не на решении;Слабая инфраструктура;Попытка решить слишком сложные для ИИ задачи.Культура AI-First решает эти проблемы, выстраивая организацию на трех столпах.Три Столпа Культуры AI-FirstСтолп 1: Данные как главный активСтрогие стандарты управления данными (data governance) — это не бюрократия, а необходимое условие для выживания.Посмотри на Netflix: их система рекомендаций, подбор обложек для фильмов и даже решения о съемках новых сериалов — результат глубокого анализа данных. Сервис построил бизнес-модель на данных, и остается на вершине.Компании, которые пренебрегли данными, столкнулись с проблемой «мусор на входе — мусор на выходе», когда ИИ-модели, обученные на некачественных данных, начали принимать ошибочные решения, стоившие миллионы.Столп 2: Поощрение экспериментовПередовые компании создают «ИИ-песочницы» — изолированные среды, где команды могут тестировать гипотезы и прототипы без риска «сломать» основные бизнес-процессы. Так можно быстро проверять идеи, отбрасывать нерабочие и масштабировать успешные. Подход не только ускоряет инновации, но и повышает мотивацию сотрудников, которые видят, что их идеи не уходят «в стол». Если же попытаться внедрить ИИ-решения сразу в боевую среду, это может привести к дорогостоящим провалам и демотивации команды.Столп 3: ИИ-грамотность для всехСамая большая ошибка — считать, что в ИИ должны разбираться только инженеры. В AI-First компании базовая грамотность должна быть у всех: менеджеров, юристов, маркетологов, HR.Крупные компании уже осознали это и запускают массовые программы обучения:КомпанияИнициативаОхватIKEAПрограмма ИИ-грамотности, включая этику ИИ30,000+ сотрудниковJPMorgan ChaseОбязательное обучение prompt engineering для всех новых сотрудниковВсе новые сотрудникиMasterCard8-часовой курс по принципам ответственного ИИ (справедливость, прозрачность)Все сотрудникиВ EMCD каждый новый сотрудник проходит вводный ИИ-тренинг, а раз в неделю можно посещать лекции и воркшопы, чтобы освоить конкретные инструменты, облегчить рутину, автоматизировать отчеты и упростить коммуникацию.Когда вся команда говорит на одном ИИ-языке, рождаются самые сильные идеи. Маркетолог может предложить новый способ сегментации аудитории, юрист — вовремя заметить риски в использовании данных.Рынок труда и AI-FirstAI-First культура — это не только про эффективность, но и про найм самых талантливых кадров. Зарплаты в сфере ИИ на 67% выше, чем у традиционных разработчиков, а дефицит огромный — до 68% компаний говорят, что им не хватает кадров.Лучшие специалисты не пойдут в компанию, где им придется месяцами выбивать доступ к данным, где боятся экспериментов, а руководство не понимает, чем они занимаются. Они ищут среду, где можно реализоваться. Они ищут AI-First культуру.И здесь — важный нюанс. Мы в EMCD ищем не просто кодеров, а людей, которые умеют решать конкретные бизнес-задачи с помощью ИИ. Поделюсь случаем.Один из наших сервисов постоянно проседал под нагрузкой, и команда долго пыталась найти причину. Стандартные методы не давали результата.Новый разработчик попробовал вместо ручного анализа скормить данные ИИ-модели. Она выявила повторяющиеся паттерны, которые всегда предшествовали сбоям. После этого осталось добавить механизм предиктивного распределения нагрузки, и проблема исчезла полностью.Парень не был «сильнее» как программист, но мыслил в парадигме AI-First.Не стань динозавромЕсли не в этом, то в следующем году компании разделятся на два типа: те, кто смог перестроить культуру и сделал ИИ частью ДНК, и все остальные.Культура AI-First про здравый смысл: учить людей, пересобирать процессы, поощрять эксперименты и фиксировать лучший опыт внутри команды. Чем раньше начать, тем больше шансов не остаться вне игры через несколько лет.Поделись, как внедряешь ИИ в своей команде, и какие еще есть способы перейти на AI-First мышление?Теги:ai-firstии-стартапии в бизнесецифровая трансформацияцифровая трансформация бизнесаавтоматизация процессовИИ-песочницыобучение ииdata governanceИИ в бизнес-процессахХабы:Искусственный интеллектЧитальный залУправление проектамиМашинное обучениеУправление персоналом",683,0,0,4 мин,https://habr.com/ru/articles/965874/,6098,780,5
Эластик и проблемы хранения ленты операций,shoshana_kv,2025-11-12T15:35:02.000Z,"['Блог компании ОТП Банк', 'Финансы в IT']","shoshana_kv 19 часов назадЭластик и проблемы хранения ленты операцийУровень сложностиСреднийВремя на прочтение11 минКоличество просмотров475Блог компании ОТП БанкФинансы в ITКейсПривет, меня зовут Екатерина, я работаю в ОТП Банке на позиции Senior-разработчика в одном из трайбов. В продолжение предыдущей статьи мы вместе с Александром, главным solution-архитектором, расскажем о вызовах, с которыми столкнулись при внедрении нереляционного хранилища в наше ДБО.В ОТП в первую очередь думают об удобстве клиента. Использование современных отказоустойчивых систем, таких как Elasticsearch, — один из важных трейд-оффов: с одной стороны, мы повышаем скорость и качество клиентского опыта, с другой — усложняем архитектуру и сталкиваемся с проблемами, которых не было бы при более простом подходе. Все это работает на эту цель.В этой статье мы поделимся частью таких кейсов и расскажем, как наша команда их решала.Задача: современная лента операцийПеред нами стояла задача создать ленту операций, которая соответствовала бы трем ключевым требованиям:  Высокая производительность: Быстрый доступ к клиентским транзакциям при больших объемах данных.Гибкий поиск: Поддержка полнотекстового поиска и фильтрации.Масштабируемость по глубине данных: Возможность работы с разной глубиной истории операций.Мы рассмотрели несколько архитектурных подходов и проанализировали подходящие для них технологии.Вариант 1: Реляционная база данных с кешем Суть подхода: Классическая связка базы данных (например, PostgreSQL) и горячего кеша в памяти для оперативных данных.Почему отказались: Главная проблема — глубина поиска данных. Глубина поиска для выписок может составлять от нескольких дней до нескольких лет. Хранить многолетний кеш для всех клиентов крайне ресурсоёмко с точки зрения оперативной памяти. При горизонтальном масштабировании пришлось бы либо использовать отдельное распределенное кеш-хранилище (например, Redis), что усложняет архитектуру, либо механизм sticky sessions, от которого мы хотели уйти. Этот подход хорошо справляется с недавними данными, но не подходит для эффективного поиска по полной истории.Кроме того, по результатам множества бенчмарков, системы вроде Elasticsearch показывают значительно более высокую скорость выполнения сложных поисковых запросов по большим datasets по сравнению с реляционными СУБД.Вариант 2: ClickHouse Суть подхода: Использование колон��чной аналитической СУБД, оптимизированной для быстрого чтения.Почему отказались: ClickHouse отлично подходит для аналитических отчетов, но оказался неудобен для нашей онлайн-системы по нескольким причинам:Сложность эксплуатации: Требует глубоких знаний для поддержки и тонкой настройки.Ограничения на обновления: Отсутствие привычной поддержки точечных UPDATE. Клиентские операции часто обновляются (например, баланс карты может меняться многократно за день), что в ClickHouse приводит к необходимости замены всей записи. Это ведет к высокой фрагментации данных и снижению производительности.Вариант 3: ElasticsearchСуть подхода: Использование поискового движка, заточенного под работу с большими объемами неструктурированных данных.Почему выбрали: Этот вариант наилучшим образом соответствовал нашим требованиям:Скорость поискаВстроенная морфология и поддержка нечеткого (fuzzy) поискаПроизводительность на больших данных: Система стабильно работает с миллиардами документов, в то время как запросы аналогичной сложности в SQL-базах могут выполняться неприемлемо долго.Гибкость схемы: Данные хранятся в виде JSON-документов, возможность создавать динамический маппингМасштабируемость: Поддержка шардирования и репликации реализована на уровне платформы, что избавляет от необходимости разработки кастомных решений.Итог: После анализа мы остановились на Elasticsearch как на наиболее сбалансированном решении. Стоит отметить, что на момент принятия решения у команды не было практического опыта работы с этой технологией. Поэтому процесс внедрения сопровождался как успешными находками, так и преодолением трудностей, о которых мы подробнее расскажем в следующих разделах.Проблема 1. Производительность Elasticsearch и количество индексовВ Elasticsearch данные организованы в индексы, которые, в свою очередь, делятся на шарды. Изначально мы выбрали простую и логичную схему: выделить отдельный индекс для каждой организации (клиента). На старте, при небольшом количестве клиентов, система работала идеально, обеспечивая мгновенный отклик.Однако с ростом числа клиентов мы столкнулись с фундаментальным ограничением Elasticsearch: производительность критически падает при большом количестве индексов и шард. Согласно официальной документации, для одного узла не рекомендуется превышать лимит в 1000 шард [источник]. Хотя этот лимит можно увеличить, сама компания Elastic предупреждает, что это временное решение, и стабильная работа не гарантируется [источник].На практике деградация проявилась быстро: время операций (например, реиндексации документов) выросло до ~200 мс. Для высоконагруженной системы с SLA в десятки миллисекунд такие показатели стали неприемлемыми.Расчет допустимого количества индексовЧтобы найти решение, мы рассчитали верхнюю границу количества индексов для нашей инфраструктуры. Исходили из следующей логики:Конфигурация одного индекса: 8 шард (для горизонтального масштабирования) × 4 реплики (для отказоустойчивости) = 32 шарда на индекс.Общий лимит для нашего кластера из 4 узлов: ~4000 шард.С учетом запаса на миграции и техническое обслуживание мы задействовали только треть от общего лимита: ≈1300 шард.Поделив 1300 на 32, мы получили максимум ~40 индексов. Стало очевидно, что первоначальная схема «один индекс на организацию» нежизнеспособна на долгосрочную перспективу.Поиск оптимальной стратегииМы рассмотрели несколько путей оптимизации структуры индексов:Объединение мелких организаций в общие индексы.Почему отказались: Невозможно надежно прогнозировать объем транзакций компании. Маленький клиент сегодня мог стать крупным завтра, создавая дисбаланс нагрузки в общем индексе.Разбивка данных по временным интервалам (например, по месяцам).Почему отказались: Этот подход усложняет архитектуру, так как поиск по истории операций потребовал бы отправки множественных запросов (multisearch) к нескольким индексам. Это сделало бы время отклика непредсказуемым и увеличило бы нагрузку на кластер.Разбивка по регионам.Почему отказались: Деление оказалось слишком крупным и неравномерным, что не решало проблему эффективного распределения нагрузки.В итоге мы выбрали стратегию равномерного распределения организаций по фиксированному пулу индексов. Этот подход позволил нам жестко контролировать общее количество индексов (в пределах рассчитанного лимита в 40 штук), обеспечивая предсказуемую производительность, отказоустойчивость и простоту масштабирования.Скрытый текстpublic void init() {
       final var indexes = indexRepository.findAll().stream()
           .collect(Collectors.toMap(OrganizationIndex::getOrganizationId, Function.identity()));
       INDEX_BY_ORGANIZATION_ID_MAP.putAll(indexes);
   }    
 
    /**
    * При запросе на регистрацию присваиваем организации индекс (по очереди)
    */
   @Transactional
   public void initializeOrganization(final UUID organizationId) {
       init();
       // горячий кеш с данными индексов и организаций
       if (INDEX_BY_ORGANIZATION_ID_MAP.containsKey(organizationId)) {
           return;
       }
       final int organizationsCount = (int) indexRepository.count();
       // номер индекса - остаток от деления на количество организаций
       final var indexName = getIndexName(organizationsCount % indexCount + 1);
       final var index = indexRepository.save(OrganizationIndex.of(organizationId, indexName, OrganizationIndexStatus.READY));
       INDEX_BY_ORGANIZATION_ID_MAP.put(organizationId, index);
       log.info(""Add organization to index {}"", index);
   }Проблема 2. Дублирование данных и конкурентные обновленияКлиентские операции в нашей системе могут создаваться через несколько независимых каналов:Обращение в офис;Новое ДБО (дистанционное банковское обслуживание);Старое ДБО;Внутренние операции.После создания событие проходит через цепочку промежуточных сервисов и финализируется в АБС. Критически важным моментом стало то, что обновление одной и той же операции могло быть инициировано разными системами параллельно. На практике это вылилось в ситуацию, когда обновления для одного документа в Elasticsearch приходили одновременно из разных источников — через REST-API и несколько Kafka-топиков.Почему стандартные механизмы Elasticsearch не подходятПопытка обновлять данные «в лоб» привела к потере изменений. Причина в том, что Elasticsearch не гарантирует строгой консистентности (immediate consistency) на уровне отдельных документов. Даже принудительное обновление индекса после записи с помощью withRefreshPolicy(RefreshPolicy.IMMEDIATE) не защищает от race condition (гонки состояний) между параллельными запросами.В отличие от реляционных СУБД, в Elasticsearch отсутствуют механизмы строгой блокировки на уровне записи (row-level locking) в рамках транзакции. В результате конкурентные операции update могут выполняться на устаревшей версии документа, затирая изменения друг друга.Этап 1: Оптимистическая блокировка (Optimistic Concurrency Control)В качестве первого решения мы реализовали оптимистическую блокировку средствами самого Elasticsearch, используя механизм версий документов (_version). Однако этот метод не сработал в наших условиях. Из-за высокой задержки индексации, вызванной проблемой с большим количеством индексов (см. предыдущий раздел), обновления приходили быстрее, чем успевала обновляться версия документа в индексе. Это приводило к лавине ошибок конфликта, и большая часть обновлений не применялась.Этап 2: Специализированный сервис-агрегатор и синхронизация через RedisМы пришли к выводу, что нужен централизованный механизм синхронизации, вынесенный за пределы Elasticsearch. Для этого был разработан отдельный сервис-агрегатор, который выполняет следующие функции:Принимает события из всех источников (REST, Kafka-топики).Приводит их к единому формату данных.Определяет приоритет обработки для событий из разных каналов.Вводит небольшие искусственные задержки для выравнивания нагрузки.Таким образом, разнородные операции превращались в упорядоченный поток событий с единым идентификатором.Для синхронизации параллельных обращений к одному и тому же идентификатору операции и сохранения stateless-архитектуры самого агрегатора мы использовали Redis в качестве распределенного кэша и легковесного координатора:Ключ: Идентификатор операции.Значение: Временная метка последнего принятого обновления.Скрытый текстpublic void handle(final TransactionUpdate transactionUpdate) {
    final var startTime = System.currentTimeMillis();
    // transactionUpdate - это класс wrapper, который оперирует идентификаторами операций
    transactionUpdate.message().getUpdateIds().forEach(updateId -> {
        try {
            // делаем лок в redis c максимальным временем обработки  - 1 минута
            redisLockRegistry.executeLocked(
                updateId,
                Duration.ofMillis(lockTimeoutMs),
                () -> {
                     // в связи с тем, что нужно дать время elasticsearch обновить индексы, то вычисляем время ожидания для операций, которые одновременно обновляются из разных источников
                    final long timeToWait = getTimeToWaitAndUpdateCache(updateId);
                    if (timeToWait <= 0) {
                        process(transactionUpdate, updateId);
                        return;
                    }
                    Thread.sleep(timeToWait);
                    process(transactionUpdate, updateId);
                });
        } catch (Exception e) {
            log.error(""Error while processing update transaction {}, in: {}"", updateId, timeDurationFrom(startTime), e);
            throw new UnexpectedException(""Error while processing update transaction"");
        }
    });
    log.info(""Transaction message was processed in: {}"", timeDurationFrom(startTime));
}
 
public long getTimeToWaitAndUpdateCache(final String updateId) {
    // ищем в редисе последнее время обновления операции
    final var savedLastUpdateTime = transactionLastUpdateRepository.findById(updateId)
        .map(TransactionLastUpdate::getLastUpdateTime)
        .orElse(null);
    // получаем время для начала обновления
    final var newLastUpdateTime = calculateUpdateTime(savedLastUpdateTime);
    // обновляем данные в кеше redis
    transactionLastUpdateRepository.save(TransactionLastUpdate.builder()
        .updateId(updateId)
        .lastUpdateTime(newLastUpdateTime)
        .ttl(lastUpdateCacheTtlSec)
        .build());
    long timeToWait = newLastUpdateTime - System.currentTimeMillis();
    log.debug(""Calculate time to wait: id {}, new update time {}, time to wait {}"",
        updateId, newLastUpdateTime, timeToWait);
    return timeToWait;
}
 
private long calculateUpdateTime(@Nullable final Long lastUpdateTime) {
    // операцию еще не обновляли или обновляли в прошлом (раньше, чем delayMilliSeconds назад), значит можно обновлять сразу
    if (isReadyForProcess(lastUpdateTime)) {
        return System.currentTimeMillis();
    }
    // в ином случае надо ожидать
    return lastUpdateTime + delayMilliSeconds;
}
 
private boolean isReadyForProcess(@Nullable final Long lastUpdateTime) {
    return lastUpdateTime == null || lastUpdateTime < System.currentTimeMillis() - delayMilliSeconds;
}
 
 
 private void process(final TransactionUpdate transactionUpdate, final String updateId) {
    // по типу источника информации понимаем, какой пришел формат данных, и применяем необходимые операции к нему
    switch (transactionUpdate.type()) {
        case TYPE_1 -> service1.process(transactionUpdate);
        case TYPE_2 - > service1.process(transactionUpdate, updateId);
        .....
        default -> throw new UnsupportedOperationException(""Unknown update type "" + transactionUpdate.type());
    }
}
Это решение позволило эффективно справляться с параллельными обновлениями и стало временным решением проблемы.Этап 3: Гарантия порядка с помощью Kafka и унификация форматаРешение с сервисом-агрегатором и Redis было работоспособным, но добавляло сложность: необходимость поддерживать отдельный сервис и инфраструктуру кэша. Мы решили пересмотреть архитектуру потоков данных, чтобы найти более фундаментальное и простое решение.Проблемы предыдущего подхода:Разнородность форматов: Источники продолжали генерировать события в разных структурах данных.Нарушение последовательности: Не было гарантии, что обновления одной операции придут в агрегатор в правильном хронологическом порядке.Архитектурные измененияМы провели ряд ключевых изменений, переведя взаимодействие на Apache Kafka:Kafka с ключом сообщенияВ качестве ключа каждого сообщения стал использоваться идентификатор операции. Это фундаментальное изменение, так как Kafka гарантирует порядок доставки сообщений с одним и тем же ключом в рамках одной партиции. Это решило проблему конкурентных обновлений на транспортном уровне, устранив саму возможность нарушения последовательности.Единый унифицированный форматМы разработали и внедрили единый контракт для событий об операциях. Все системы-источники стали публиковать сообщения только в этом формате. Это позволило упростить логику агрегатора, который теперь мог напрямую десериализовать и обрабатывать входящие события без дополнительных преобразований.Консолидация топиковВместо множества специализированных топиков для каждого сервиса мы ввели единый топик для событий, связанных с операциями. Это сместило фокус: теперь не каждый сервис диктовал свой формат, а агрегатор определял единый контракт для всего потока данных.РезультатБлагодаря этим изменениям нам удалось полностью отказаться от механизма синхронизации через Redis. Новый подход обеспечил:Предсказуемость: Порядок обработки гарантирован средствами Kafka.Упрощение архитектуры: Исчезла необходимость в поддержке отдельного кэша и логики разрешения конфликтов.Повышение надежности: Система стала менее зависимой от дополнительных компонентов.Минимальная компонентная архитектура разверткиВ качестве итога приведем минимальную рабочую, по нашему мнению, архитектуру кластера Elasticsearch, разработанную для следующих условий:Инфраструктура: 2 основных ЦОДа с кластерами Kubernetes и микросервисами.Ограничение: 1 дополнительный ЦОД для кворума с ограниченными ресурсами.Схематичное изображение финальной архитектуры представлено ниже.Для начала кратко обозначим роли нод, которые мы использовали:Мастер-нода (Master Node): Формирует кворум, управляет состоянием кластера (метаданные, шардирование), обеспечивает консистентность и защиту от split-brain. Не хранит данные и не обрабатывает пользовательские запросы.Координирующая нода (Coordinating Node): Принимает запросы от микросервисов на чтение и запись, маршрутизирует их к нужным data-нодам и агрегирует результаты. Является входной точкой в кластер.Data-нода (Data Node): Хранит данные, выполняет поиск, агрегацию и другие операции непосредственно с индексами.Выбранная конфигурацияИсходя из требований отказоустойчивости и производительности, мы развернули кластер следующей конфигурации:Координирующие ноды: По 1 ноде в каждом из 2 основных ЦОДов. Это обеспечивает минимальные задержки для микросервисов, которые обращаются к координатору в своем ЦОДе.Мастер-ноды: 3 ноды, распределенные по трем ЦОДам. Это гарантирует работоспособность кворума (требуется большинство, т.е. 2 из 3) даже при полном отказе одного из основных ЦОДов. В случае сетевого разрыва кластер запретит запись на изолированном участке, но сохранит доступность данных для чтения.Data-ноды: 4 ноды в основных ЦОДах. Это минимальное количество для эффективного шардирования, которое можно легко увеличить для горизонтального масштабирования.ИтогПредставленная архитектура позволила нам построить отказоустойчивую и надежную систему, которая соответствует строгим требованиям бизнеса к доступности и производительности. Она устойчива к сбоям на уровне ЦОДа и предоставляет четкий путь для дальнейшего масштабирования хранилища данных.Теги:elasticsearchjavaarchitectureфинтехинтеграцииХабы:Блог компании ОТП БанкФинансы в IT",475,0,8,11 мин,https://habr.com/ru/companies/otpbank/articles/965420/,18221,1996,2
"Нечёткий поиск при пересечении множеств, или Как выжать все соки из Хэширования по сигнатуре",VGoren,2025-11-13T07:16:06.000Z,"['SQL *', '.NET *', 'C# *', 'Microsoft SQL Server *', 'Алгоритмы *']","VGoren 4 часа назадНечёткий поиск при пересечении множеств, или Как выжать все соки из Хэширования по сигнатуреУровень сложностиСреднийВремя на прочтение23 минКоличество просмотров265SQL * .NET * C# * Microsoft SQL Server * Алгоритмы * Из песочницыСлияние рек Солимоэнс (верхняя Амазонка) и Риу-Негру в БразилииНа просторах интернета легко можно найти материалы по реализации нечёткого поиска, в которых предполагается поиск одной строки в множестве строк M. Но что если возникнет необходимость реализовать нечёткое сравнение множества M₁ с множеством M₂? При классическом подходе нам придется выполнить  сравнений - при линейном росте этих множеств, сложность задачи будет расти экспоненциально, в плане производительности это решение никуда не годиться!Предложенное ниже решение для БД SQL реализовано с помощью хэширования строк по сигнатуре. Оно максимально эффективно выполняет данный поиск в пределах одной ошибки (по расстоянию Левенштейна), но его можно адаптировать и под поиск в пределах какого угодно количества ошибок, но увеличение допуска экспоненциально усложняет алгоритм.Размер строк также не ограничен, но нечёткий поиск очень большого текста в пределах одной, двух, трёх ошибок можно сравнить со ""сферический конём в вакууме"" (не могу вообразить, зачем это может понадобится), в этом случае гораздо эффективнее воспользоваться другими методами хэширования - например, SimHash(SimilarityHash)[1][2][3], MinHash[1][2]. Поэтому предложенный алгоритм наиболее уместен для поиска средних, малых строк - ФИ, ФИО, VIN, маркировка, спецификация техники, серийные номера, штрихкоды, хэшсуммы и т.д.Цели:Ознакомить с концепциейДать конкретный пример интеграции в БД SQL(MSSQL)Ознакомить с возможностями на базе практической реализацииПример результатов при тестировании пересечения 5000 customers с 5000 employees по условию нечёткого поиска в пределах 2-ух ошибок по расстоянию ЛевенштейнаОсновная концепцияХэш по сигнатуре В целях упрощения восприятия, здесь и далее будем разбираться на примере 32-битного хэша, хотя его размер может быть любым. Кратко описываю алгоритм получения хэша(в практической реализации я буду делать так же):  №  Операция                                                                                                                                                                                                               Результат                                           1  Берем за вводные хэша ASCII код каждой буквы                                                                                                                         значения в диапазоне байта - (1-255)                2  Пропускаем значения через рандомайзер (в моей реализации на C# это Алгоритм генератор случайных чисел D.E. Knuth)  значения в диапазоне int'а - (0-4294967295)  3  Применяем простейший остаток от деления на размер хэша(x32)                                                                                                                                                     значения в диапазоне - (1-32)                       4  В пустой битовой(x32) маске ""включаем"" биты по индексу полученных значений                                                                                                                                             значения в диапазоне байта - (1-255)               Правило 2-ух ошибокСреди всех нечётких хэшей он обладает уникальной особенностью, на которой будет строиться вся дальнейшая работа - он подчиняется ""правилу 2-ух ошибок""(для удобства изложения пришлось придумать название самостоятельно, в статье Бойцова Л.М. это свойство описано, но названия не получило). Формулировка: Если расстояние Левенштейна между строками А и B = 1, то расстояние Хэмминга между хэшами от А и Б меньше или равно 2, если это была замена, либо меньше или равно 1, если это была вставка/удалениеИными словами:    1 вставка/удаление     не может изменить хэш более чем на 1 ошибку         1 замена               не может изменить хэш более чем на 2 ошибки         ↓                      ↓                                               1 ошибка не может изменить хэш более чем на 2 ошибки Например, получим сигнатурные хэши от 2-ух одинаковых строк с одной ошибкой: СтрокаХэшAЧумаков Максим Глебович00011011 00001100 00101011 01100110BЧубаков Максим Глебович00011011 00001101 00001011 01100110В строке A была вводная м, которая, очевидно, включала бит №19 В строке B пропала вводная м, и появилась вводная б, которая, очевидно, включила бит №16Правило 2-ух ошибок позволяет сделать поиск детерменированным - мы можем быть уверены, что точно не пропустим строки с заданным количеством ошибок. Например, очень популярный и действительно неплохо работающий даже на таких небольших строках, как ФИО, SimHash все равно остается вероятностным - 1 ошибка скорее всего поменяет мáлую часть хэша, но всегда остается вероятность того, что хэш изменится кардинально. MinHash тоже вероятностный. Они лучше подходят для больших текстов, например, при сравнении на антиплагиат - особенно, если предварительно обработать их, исключив связующие предлоги, союзы, артикли и т.д.HEngineHEngine[1][2] является алгоритмом ускорения поиска запрашиваемой бинарной строки в множестве таких строк в пределах заданного расстояния Хэмминга. Сейчас попробуем применить этот алгоритм для поиска в пределах 2-ух ошибок на нашем примере, попутно раскрывая его суть:Если разделить 32-битные хэши A и B на 4 равные части(chunk'и) по 8 бит с сохранением информации об их порядковом номере, то как минимум по двум из них A и B совпадут:ichunkichunkmatchA000011011B000011011✓A100001100B100001101✗A200101011B200001011✗A301100110B301100110✓То есть необязательно проверять сплошняком каждый бит. Если мы ищем похожие на А строки в множестве М, мы можем таким же образом дробить хэши на 4, 8, 16 частей и искать match'и соответственно по 2/4, 6/8, 14/16, 30/32(собственно расстояние Хэмминга). Постоянно сужая выборку на текущем этапе ""предрасчёта"" расстояния Хэмминга, мы будем компенсировать возрастающую стоимость поиска match'ей на следующем этапе (очевидно, что проверить 2/4 легче, чем 30/32).Более того, если мы ищем строки из M₁, похожие на строки M₂ с расстоянием Левенштейна = 1, после первого самого дешевого этапа (2/4), мы уже получаем не просто сокращенные множества от M₁ и M₂, а суженный набор пар M₁.id-M₂.id, которые теоретически могут быть искомыми. Таким образом уже после первого этапа предрасчёта расстояния Хэмминга перед нами стоит задача не экспоненциальной сложности, а линейной.Специфика SQLВот и весь алгоритм - хэшируем по сигнатуре множества M₁, M₂ и ищем совпадения по HEngine (перебирая 2 массива вложенными циклами). Теоретической новизны в проекте практически нет. Осталось написать пару примеров реализации, и на этом можно было бы и закончить, если бы мы программировали на обычном императивном C-подобном языке программирования. Но, как правило, на практике для хранения/выборки данных используются БД SQL. Внедрение этого поиска в SQL оказалось достаточно сложной задачей, в ходе которой сам алгоритм претерпел много изменений, хотя в сущности концепция осталась прежней.Предоставление конкретной реализации нечёткого поиска является одной из целей этой статьи. Реализовано на примере MSSQL. Подобное решение можно портировать на любую СУБД, имеющую возможность интеграции стороннего кода(SQLCLR, PL/Java, PL/Python, MySQL Connector/C++ и др.), без каких-либо ограничений.Необходимость интеграции стороннего кодаСледующие функции очень проблематично реализовать силами процедурного SQL (T-SQL) - производительность будет на плачевном уровне. Поэтому они реализованы через интеграцию стороннего кода (C# SQLCLR)ФункцияНазначениеLevenshteinDistanceString()Принимает 2 строки,       возвращает расстояние Левенштейна между нимиHammingDistanceX32()Принимает 2 int'а по x32, возвращает расстояние Хэмминга между ними, прерывает подсчет расстояния, если оно превысит лимитGetSignatureHash()Возвращает сигнатурный хэш от строкиSplitSignatureHash()Разбивает сигнатурный хэш на chunk'и по x8В SQL нельзя эффективно реализовать алгоритм HEngine полностьюПосле первого самого дешёвого этапа 2/4, перед нами будет множество пар из M₁.id-M₂.id. Если мы начнем дальше ""копать"" согласно этому алгоритму (проходить этапы 6/8, 14/16), то в каждой итерации на каждую пару, нужно будет вызывать дважды(на M₁.id и M₂.id) функцию разбивки хэша SplitSignatureHash(), затем полученные множества придется join'ить. Операционная стоимость даже одной SplitSignatureHash() в любом случае больше, чем одной HammingDistanceX32(), потому что она примитивна и работает через битовый XOR. Очевидно, что проходить этапы 6/8, 14/16 в SQL для дальнейшего сужения выборки бессмысленно - эффективнее сразу вызывать HammingDistanceX32().Базово описываем алгоритм: №ОперацияРезультат1На этапе 2/4 вызываем SplitSignatureHash() для M₁ и M₂линейно растущие множества 2MEGRE/HASH JOIN'им с помощью стандартного SQLэкспоненциально растущее множество 3Агрегируем значения (GROUP BY M₁.Id, M₂.Id HAVING COUNT() >= 2)сокращенное множество комбинаций 4На остаток пар вызываем HammingDistanceX32()сокращенное множество комбинаций 5На остаток пар вызываем LevenshteinDistanceString()искомые комбинации Ускорение с помощью k-комбинаций chunk'ов.6 k-комбинаций 2/4При классическом исполнении алгоритма HEngine, БД тратит огромный ресурс на сложное агрегирование данных(3 этап). Эта операция настолько затратна, что оптимизатор запросов SQLServer'а предпочитает сначала выполнять 4-ый этап, производя много лишних запусков HammingDistanceX32() на несгруппированном множестве, но сокращая множество комбинаций, а затем - 3-ий, потому что так дешевле!Вернемся к нашему примеру. Если A находится в множестве M₁, а B - в M₂. То на 2-ом этапе  с комбинации А-Б в множестве комбинаций M₁.id-M₂.id мы получили 2 строки по 2-ум совпадающим парам chunk'ов по индексам 1 и 4. Но помимо интересной нам комбинации, в полученном несгруппированном множестве находятся как интересные нам комбинаций, которые заjoin'ились более чем по 2-ум chunk'ам, так и НЕинтересные, которые заjoin'ились только лишь по 1-му chunk'у - вот поэтому нам необходим 3-ий этап.Для избежания затратного агрегирования введем функцию стороннего кода SplitSignatureHashKComb() - возвращает все возможные k-комбинации 2/4 chunk'ов по 8 бит:Во-первых, будем возвращать не 2 поля (i, chunk), а сконкатенированное 2-байтовое (i_chunk). Да, для диапазона значений 0-3 достаточно 2 бит, но байт(8 бит) – это минимальная ячейка адресации. На примере A-B это будет выглядеть следующим образом.ii_chunkii_chunkmatchA000000000 00011011B000000000 00011011✓A100000001 00001100B100000001 00001101✗A200000010 00101011B200000010 00001011✗A300000011 01100110B300000011 01100110✓А теперь будем возвращать все возможные k-комбинации i_chunk'ов:k_combi_chunk_k_combii_chunk_k_combmatchA01__00000000 00011011        00000001 00001100B01__00000000 00011011        00000001 00001101✗A0_2_00000000 00011011        00000010 00101011B0_2_00000000 00011011        00000010 00001011✗A0__300000000 00011011        00000011 01100110B0__300000000 00011011        00000011 01100110✓A_12_00000001 00001100 00000010 00101011B_12_00000001 00001101 00000010 00001011✗A_1_300000001 00001100 00000011 01100110B_1_300000001 00001101 00000011 01100110✗A__2300000010 00101011 00000011 01100110B__2300000010 00001011 00000011 01100110✗-: Увеличение веса и количества возвращаемых строк. Раньше на каждый хэш приходилось по  байт, теперь  байт. +: Не нужно агрегировать комбинации, это дает несопоставимо бóльший выигрыш в производительностиОписываем ускоренный окончательный алгоритм, которым будем пользоваться (теперь можно оценить, как далеко мы ушли от первоначального HEngine): №ОперацияРезультат1На этапе 2/4 вызываем SplitSignatureHashKComb() для M₁ и M₂линейно растущие множества 2merge/hash join'им с помощью стандартного SQL, с чем он неплохо справляетсяэкспоненциально растущее множество  исключительно интересных нам комбинаций M₁.Id-M₂.Id3Агрегируем значения (GROUP BY M₁.Id, M₂.Id или DISTINCT)сокращенное множество комбинаций 4На остаток пар вызываем HammingDistanceX32()сокращенное множество комбинаций 5На остаток пар вызываем LevenshteinDistanceString()искомые комбинации Стоит отметить, что:На практике оптимизатор запросов SQLServer'а всё равно предпочитает менять 4-ый и - 3-ий этап местами, но сама агрегация гораздо дешевлеДо 5-го этапа алгоритм обладает свойством резистивности к перестановке слов в строке - порядок в строках не важен(ФИО!=ИФО). Чтобы сохранить это свойство, нам придётся разбивать 5-ый шаг на несколько однотипных расчётов расстояния Левенштейна для каждой комбинации слов в шаблоне(в случае ФИО - это факториал  ). Учитывая, что к 5-му этапу выборка уже максимально сужена, это не несёт слишком больших затрат, хотя, конечно, количество слов в шаблоне усложняет 5-ый этап экспоненциально.DDL vs расчёт на местеРасчёт на местеSQLServer на этапе 2/4 сортирует chunk'и и использует MERGE JOIN, т.е. план адекватныйDDLфактически экономим время на расчет хэшей и сортировку(индексацию)меньше дергаем tempDB(больше свободной ОЗУ)Более предсказуемый план выполнения, т.к. оптимизатор SQLServer'а неадекватно оценивает стоимость SQLCLR-функций - они для него словно ""закрытый ящик"". Например, более дешёвый HammingDistanceX32() имеет бóльший Estimated Operator Cost, чем у LevenshteinDistanceBytes(), потому что имеет больше параметров, несмотря на то, что он примитивнее:HammingDistanceX32() ""дороже"" LevenshteinDistanceBytes()Код запроса:SELECT *
FROM       (SELECT * FROM dbo.employees WHERE dbo.HammingDistanceX32      (id, 25, 32) <= 2) AS t1
INNER JOIN (SELECT * FROM dbo.employees WHERE dbo.LevenshteinDistanceBytes(id, 25)     <= 2) AS t2 ON t1.id = t2.id
Специфика SQL(необязательно к прочтению)Индексные таблицы для 2, 3, 4... этапов HEngineВыше я утверждал, что стоимость SplitSignatureHash() больше одной HammingDistanceX32(), чтобы доказать бессмысленность промежуточных этапов HEngine, но что если мы дополнительно создадим индексированные 6/8, 14/16 таблицы для M₁ и M₂?Если мы будем использовать базовый алгоритм, то всё равно на каждом этапе нам придется выполнять дорогую агрегациюЕсли мы будем использовать ускоренный алгоритм, то количество и размер этих k-комбинаций будет расти экспоненциально, как и затраты на обслуживание индекса.ЭтапКоличество строкВес строки(байт)Вес индекса на строку(байт)2/4646/8281214/1612028В подтверждение словSELECT COUNT(*) FROM SplitSignatureHashKComb(123456, 8, 2)
SELECT COUNT(*) FROM SplitSignatureHashKComb(123456, 4, 6)
SELECT COUNT(*) FROM SplitSignatureHashKComb(123456, 2, 14)
Как минимум, по расходу памяти это неэффективно - алгоритм из п.3 окончательный.super_function()Если мы положим абсолютно всю логику по сравнению хэшей в одну воображаемую super_function() из стороннего кода, то сравнивая M₁.id-M₂.id приемлемой производительности достигнуть не получится, потому что, как уже было сказано, для SQLServer'а эта функция - ""закрытый ящик"". Оптимизатор никак не сможет построить оптимальный план - он будет вызывать super_function()  раз.Если бы мы искали, к примеру, одинаковую длину строк через стандартный LEN(), SQLServer скорее всего:№Операция1посчитает все M₁.id.LEN(), M₂.id.LEN()2отсортирует полученные множества3заmerge join'ит их булевой сортировкой, или накрайняк(если их млрд) заhash join'ит4↓5экспоненциальную задачу  он сведет к линейной Здесь простейший запрос, который невозможно свести к линейной сложности, на котором можно ""прикинуть"" длительность выполнения при использовании super_function(): xor32() - возвращает побитовый XOR двух чисел, это простейшая функция. Она отрабатывает медленнее встроенного ABS() - видимо, дополнительные затраты идут на приведение SQLServer типов к SQLCLR C# типамЗапросDECLARE @start int = 0
DECLARE @end   int = 2000
DECLARE @step  int = 1

;WITH x AS
(
   SELECT n FROM (VALUES (0),(1),(2),(3),(4),(5),(6),(7),(8),(9)) v(n)
),
series AS
(
    SELECT TOP (ROUND((@end- @start) / @step, 0) + 1)
           @start - @step + ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) * @step AS n
    FROM
       x as x1, --1           - 10
       x as x2, --11          - 100
       x as x3, --101         - 1000
       x as x4, --1001        - 10 000
       x as x5, --10 001      - 100 000
       x as x6, --100 001     - 1 000 000
       x as x7, --1 000 001   - 10 000 000
       x as x8, --10 000 001  - 100 000 000
       x as x9  --100 000 001 - 1 000 000 000
)
SELECT *
FROM       series s1
CROSS JOIN series s2
WHERE --ABS(s1.n*s2.n - 305)  = 4056/*random int*/
      dbo.xor32(s1.n, s2.n) = 4056/*random int*/
То есть фактически бóльшая часть экономии стоимости при практической реализации алгоритма достигается за счёт того, что количество вызовов функций SplitSignatureHash() линейно, а задача экспоненциальной сложности - первичный (2/4) предрасчёт расстояния Хэмминга - ложится на плечи SQL, с чем он неплохо справляется с помощью своих инструментов - индексов, merge/hash join'ов и т.д.Почему мы не можем передать внутрь SQLCLR M₁ и M₂ целиком?Таблицы в SQLServer не передаются, придется передавать через XMLСчитаем предположительный вес этой переменной: M₁.N = 100 0001 строка содержит Вес переменной XML как минимум Далее её нужно распарсить и превратить в массивы struct'ур, пригодных для циклической обработки.С M₂.N нужно проделать то же самоеОперирование такими большими переменными будет вынуждать пользоваться кучей и следить за стекомSQLServer сам включает многопоток, в C# придется прописывать вручнуюMEGRE/HASH JOIN не используется, нужно будет внутри сортироватьВ общем слишком много трудозатрат для решения с очень сомнительной производительностью, реализовывать не пробовалСолениеСолениеВ этом разделе возможно есть немного теоретической новизны(по крайней мере для русскоязычного интернета). При классическом хэшировании по сигнатуре, если строки разные, но имеют одинаковый набор уникальных букв, случится коллизия.Например Строка                          Уникальные вводные  Чумаков Максим Глебович  Ч,у,м,...м,...б     Чубаков Максим Глебович  Ч,у,б,...м,...б    Хэш должен быть качественным - обладать свойствами устойчивости к коллизиям, которые гарантируются нормальным распределением битов в битовой маске. Иначе на этапе 2/4 будет много лишних join'ов, что негативно скажется на производительности. Чтобы повысить его качество были предприняты попытки его дополнительно посолить(добавить в хэш больше уникальной информации о строке). Пробовались следующие соли, которые применялись к каждой вводной(букве) хэша:Нарушающие правило 2-ух ошибокКоличество повторений буквы во всей строкеЗамена повторяющейся буквы не может изменить хэш более чем на 4 ошибкиНапример Строка                          Уникальные вводные      Чумаков Максим Глебович  Ч*1,у*1,м*2,...б*1,...  Чубаков Максим Глебович  Ч*1,у*1,б*2,...м*1,... Количество повторений буквы в словеЗамена повторяющейся буквы в пределах слова, если нет такой же буквы с таким же кол-вом повторений в другом слове(нет такой же вводной) не может изменить хэш более чем на 4 ошибкиНапример Строка                               Уникальные вводные  Колокольцев Максим Глебович   ...л*2.........     Корокольцев Максим Глебович   ...р*1...л*1...     Колокольцев Максим Гарриевич  ...л*2...р*2...     Корокольцев Максим Гарриевич  ...р*1...р*2...    N-gramm'ыЗамена повторяющейся буквы не может изменить хэш более чем на  ошибки.Например Строка                          Уникальные вводные по 2-gramm'ам  Чумаков Максим Глебович  ...ум, ма...                      Чубаков Максим Глебович  ...уб, ба...                     Вывод:Эти соли усиливают селективность хэша, но увеличение предела допустимых ошибок в хэше экспоненциально негативно влияет на производительность:Количество ошибокЭтапКоличество строкВес строки(байт)Вес индекса на строку(байт)12/4642416/82812336114/1612028336024/8708560212/161820244368032/8284112310/16800820160160В подтверждение слов          SELECT '1' AS lev_dist,'2/4' AS stage, COUNT(*) AS cnt, MAX(LEN(iChunkKComb)) AS kComb_size, COUNT(*) * MAX(LEN(iChunkKComb)) AS row_size FROM dbo.SplitSignatureHashKComb(123456, 8, 2)  
UNION ALL SELECT '1',            '6/8',          COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 4, 6)  
UNION ALL SELECT '1',            '14/16',        COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 2, 14) 
UNION ALL SELECT '2',            '4/8',          COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 4, 4)  
UNION ALL SELECT '2',            '12/16',        COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 2, 12) 
UNION ALL SELECT '3',            '2/8',          COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 4, 2)  
UNION ALL SELECT '3',            '10/16',        COUNT(*),        MAX(LEN(iChunkKComb)),               COUNT(*) * MAX(LEN(iChunkKComb))             FROM dbo.SplitSignatureHashKComb(123456, 2, 10) 
Но, тем не менее, они могут пригодиться, если мы хотим осуществить ""поверхностный/быстрый"" поиск. Эти соли значительно ускоряют алгоритм, но становится возможным пропуск искомых совпадений. Появляется уязвимость - можно целенаправленно изменить строку так, чтобы получить ложноотрицательный результат совпадения.Сохраняющие правило 2-ух ошибокОтбирать буквы определенного количества повторений.Замена повторяющейся буквы не может изменить хэш более чем на 2 ошибки, но теряет информацию.Например Строка                              Уникальные вводные с кол-вом 2        Колокольцев Максим Глебович  ...л*3...к*2...е*2, в*2...и*2  Корокольцев Максим Глебович  ...   л*2    ...к*2...е*2, в*2...и*2  Колокольцев Максим Глебович  ...л*3...   к*2                Кококольцев Максим Глебович  ...   л*2    ...к*3 Есть идея использовать как двойной хэш, например, хэш от букв с кол-вом 1 + хэш от букв с кол-вом 2. Они вместе дают неплохую селективность, но затраты на поиск по двойному хэшу тоже значительны - придётся выборку от хэша №1 intersect'ить с выборкой от хэша №2.Индекс словаЗамена повторяющейся буквы не может изменить хэш более чем на 2 ошибки, но теряется свойство резистивности к перестановке слов в строке.Например Строка                              Уникальные вводные с кол-вом 2  Колокольцев Максим Глебович  ...л*1...л*1...и*2...и*3        Корокольцев Максим Глебович  ...р*1...л*1...и*2...и*3       Колокольцев Максим Глебович  ...К*1...                      Сококольцев Максим Глебович  ...С*1...                      Вывод: Если нам НЕ нужна резистивность к перестановке слов в строке, то соль по индексу слова считаю наиболее перспективной.Погружение в кодКак вы уже поняли из всего вышеизложенного материала, я достаточно много ""игрался"" с размером хэша, k-комбинаций, солями. Объём DDL, который мне пришлось бы писать вручную параллельно дорабатывая ""шаблон"" создания индекса изначально меня насторожил. Поэтому я сразу прибег к динамическому SQL, который в конечном счёте получился достаточно громоздким, страшным, но работающим. В конце концов, для меня главное - дать пример реализации и представление о производительности интегрированного в SQL алгоритма. Шаблон не умеет реализовывать:резистивность к перестановке слов в строке - это перегрузило бы и без того перегруженный шаблонизатор DDL'а, это можно дописать вручную при необходимости на готовый шаблон. Все нижележащие тесты проводятся без реализации этого свойства, но это не помешает сравнительному анализу хэшей.двойной/тройной хэш - это перегрузило бы и без того перегруженный шаблонизатор DDL'а, это можно дописать вручную при необходимости на готовый шаблон. Вручную дописанный в тестах показал себя неудачно.Шаблон умеет:создавать индекс с несколькими однотипными солями:classic - без солиsalt_cnt - количество повторений буквы во всей строкеsalt_cnt_per_word - количество повторений буквы в словеsalt_i_word - индекс словасоздавать индекс с несколькими хэшами с разными фильтрами вводных - для соли с отбором букв определенного количества повторенийЯ не буду здесь объяснять, как именно работает инструмент - не вижу смысла лезть в дебри генератора SQL'а - я базово опишу, что он делает и покажу как им пользоваться:Допустим, нам нужно осуществить нечёткий поиск между двумя однотипными таблицами customers и employees по шаблону first_name + '%' + patronomyc_name + '%' + last_namecustomers, employeesCREATE TABLE dbo.customers
(
    id              int IDENTITY(1, 1) PRIMARY KEY,
    first_name      nvarchar(100),
    patronomyc_name nvarchar(100),
    last_name       nvarchar(100),
    gender          nchar(1)
)
CREATE TABLE dbo.employees
(
    id              int IDENTITY(1, 1) PRIMARY KEY,
    first_name      nvarchar(100),
    patronomyc_name nvarchar(100),
    last_name       nvarchar(100),
    gender          nchar(1)
)
Для этого нам понадобятся 2 процедуры - create_SH_fuzzy_search_index и create_SH_fuzzy_search_joincreate_SH_fuzzy_search_indexПредназначена для создания индекса на конкретную таблицу. На примере customers 1-граммный 32-битный индекс этапа 2/4 (8-битные chunk'и) без солей(classic) состоит из:Таблицы для хэша с внешним ключом на customers[ix].[customers_full_name_H_1gram_x32]CREATE TABLE [ix].[customers_full_name_H_1gram_x32]
(
     [row_num] int NOT NULL
    ,[classic] varbinary(4)
    ,CONSTRAINT [PK_customers_full_name_H_1gram_x32]                      PRIMARY KEY ([row_num])
    ,CONSTRAINT [FK_customers_full_name_H_1gram_x32.row_num-customers.id] FOREIGN KEY ([row_num]) REFERENCES [dbo].[customers]([id]) ON DELETE CASCADE
)
Таблицы для k-комбинаций chunk'ов от хэша с внешним ключом на таблицу для хэша и индексами - это основная таблица, по которой будет идти первичный поиск этапа 2/4[ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4]CREATE TABLE [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4]
(
     [row_num] int NOT NULL
    ,[classic] varbinary(4)
    ,CONSTRAINT [FK_customers_full_name_H_1gram_x32_C_x8_K_2/4.row_num-customers_full_name_H_1gram_x32.row_num] FOREIGN KEY ([row_num]) REFERENCES [ix].[customers_full_name_H_1gram_x32]([row_num]) ON DELETE CASCADE
)
CREATE CLUSTERED INDEX [customers_full_name_H_1gram_x32_C_x8_K_2/4.row_num] ON [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4] ([row_num])
CREATE INDEX [customers_full_name_H_1gram_x32_C_x8_K_2/4.classic] ON [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4] ([classic])
3.Триггер на customers для заполнения таблицы для хэша. Стоит отметить, что он достаточно умный:не триггериться на строки, у которых template не изменился, чтобы не замедлять update'ы в таблице, если они не меняют templateне включает в индекс строки, у которых пустой template - множество таких строк замедляет работу алгоритма, т.к. хэши от таких template'ов одинаковы -> появляются неуникальные значения -> селективность падает[dbo].[TR_customers_full_name_H_1gram_x32]-- generated in dbo.p_create_SH_fuzzy_search_index
ALTER TRIGGER [dbo].[TR_customers_full_name_H_1gram_x32]
   ON [dbo].[customers]
   AFTER INSERT, UPDATE
AS
BEGIN
    SET NOCOUNT ON;

    DELETE FROM [ix].[customers_full_name_H_1gram_x32]
    WHERE row_num IN
    (
    SELECT I.id
    FROM      (SELECT id, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS template FROM Inserted) I
    LEFT JOIN (SELECT id, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS template FROM Deleted)  D ON D.id        = I.id
                                                                                                                                               AND D.template <> I.template
    WHERE D.id IS NOT NULL -- changed rows
    )

    INSERT INTO [ix].[customers_full_name_H_1gram_x32] ([row_num], [classic])
    SELECT I.id
          ,dbo.GetSignatureHash(I.template, 1, '%', 65001, 0/*classic*/, 0, 32)
    FROM      (SELECT id, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS template FROM Inserted) AS I
    LEFT JOIN (SELECT id, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS template FROM Deleted)  AS D ON D.id        = I.id
                                                                                                                                                  AND D.template <> I.template
    LEFT JOIN [ix].[customers_full_name_H_1gram_x32]                                                                                          AS H ON H.row_num   = I.id
    WHERE (   D.id       IS NOT NULL  -- changed rows
           OR H.row_num  IS     NULL) -- haven't been inserted yet
      AND REPLACE(I.template, '%', '') <> ''
END
4.Триггер на таблицу для хэша для заполнения таблицы k-комбинаций хэша[ix].[TR_customers_full_name_H_1gram_x32_C_x8_K_2/4]-- generated in dbo.p_create_SH_fuzzy_search_index
ALTER TRIGGER [ix].[TR_customers_full_name_H_1gram_x32_C_x8_K_2/4]
   ON [ix].[customers_full_name_H_1gram_x32]
   AFTER INSERT, UPDATE
AS
BEGIN
    SET NOCOUNT ON;

    DELETE FROM [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4] WHERE row_num IN (SELECT row_num FROM Inserted)

    INSERT INTO [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4] ([row_num], [classic])
    SELECT [row_num], [classic].iChunkKComb
    FROM Inserted
    CROSS APPLY dbo.SplitSignatureHashKComb([classic], 8, 2) AS [classic] WHERE  [classic].i = [classic].i 
END
После всех манипуляций нам остается лишь ""дёрнуть"" за любое поле customers следующим образом UPDATE dbo.customers SET Gender = Gender WHERE 1 = 1 и индекс заполнится по цепочке: 1.UPDATE 2.Триггер на customers  заполнит таблицу для хэша 3.Триггер таблицы для хэша заполнит таблицу k-комбинаций хэшаТо же самое динамическим SQL:create_SH_fuzzy_search_indexEXEC dbo.create_SH_fuzzy_search_index
    @schema_name            = 'dbo',
    @table_name             = 'customers', @table_id_field_name = 'id',
    --@table_name             = 'employees', @table_id_field_name = 'id',
    @table_mock_field_name  = 'Gender',
    @postfix                = 'full_name',
    @template               = 'ISNULL(first_name, '''') + ''%'' + ISNULL(patronomyc_name, '''') + ''%'' + ISNULL(last_name, '''')',
    @delimiter              = '%',
                            
    @h_table                = '',
    @hc_name                = '',
    @hc_table               = '',
    @h_table_join           = '',
    @hc_table_join          = '',
    @hc_table_case_col      = '',
    @hc_table_case_col_name = '',
    @row_size               = NULL,
                            
    @h_schema_name          = 'ix',
    @codepage               = '65001',
    @n                      = '1',
    @hashSize               = '32',
    @hashChunkSize          = '8',
    @kCombCount             = '2',
    @nGramHashModes         = '0',
    @salt_filter            = '0',
                            
    @is_del                 = 1,
    @top                    = 999999,
    @DEBUG                  = 0
create_SH_fuzzy_search_joinСоздаёт полноценный связочный индекс между двумя таблицами и процедуры, функции для работы с ним. Возвращаясь к нашей задаче:Запускает create_SH_fuzzy_search_index для customersЗапускает create_SH_fuzzy_search_index для employeesСоздаёт основную TVF, в которой хранится весь алгоритм поиска[ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4]/*
Generated in create_SH_fuzzy_search_join
-- usage
SELECT * FROM [ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4](1, 1) AS SH_search
*/
ALTER FUNCTION [ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4]
(
    @col_num    int = 1,
    @only_fuzzy int = 1
)
RETURNS table AS RETURN
(
 	--DECLARE @col_num int = 1, @only_fuzzy int = 1
    WITH
    residual_chunks AS
    (
        SELECT hc1.row_num AS rn1,
               hc2.row_num AS rn2
        FROM       [ix].[customers_full_name_H_1gram_x32_C_x8_K_2/4] AS hc1
        INNER JOIN [ix].[employees_full_name_H_1gram_x32_C_x8_K_2/4] AS hc2 ON (@col_num = 1 AND hc1.classic = hc2.classic)
    ),
    residual_ham_dist AS
    (
        SELECT DISTINCT
               rn1, rn2
        FROM             residual_chunks AS residual
        INNER JOIN [ix].[customers_full_name_H_1gram_x32] AS h1 ON residual.rn1 = h1.row_num
        INNER JOIN [ix].[employees_full_name_H_1gram_x32] AS h2 ON residual.rn2 = h2.row_num
        WHERE (@col_num = 1 AND dbo.HammingDistanceX32(h1.classic, h2.classic, 2) <= 2)
    ),
    residual_lev_dist AS -- query optimizer leaves this CTE for last, because it's necessary to JOIN the templates, which is what we want
    (
        SELECT residual.rn1, residual.rn2, t1.templ1, t2.templ2
        FROM       residual_ham_dist AS residual
        INNER JOIN (SELECT id AS rn1, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS templ1 FROM dbo.customers) AS t1 ON t1.rn1 = residual.rn1
        INNER JOIN (SELECT id AS rn2, ISNULL(first_name, '') + '%' + ISNULL(patronomyc_name, '') + '%' + ISNULL(last_name, '') AS templ2 FROM dbo.employees) AS t2 ON t2.rn2 = residual.rn2
        WHERE (@only_fuzzy = 0 AND dbo.LevenshteinDistanceString(t1.templ1, t2.templ2) <= 1)
           OR (@only_fuzzy = 1 AND dbo.LevenshteinDistanceString(t1.templ1, t2.templ2)  = 1)
    )
    SELECT * FROM residual_lev_dist
)

Создаёт вспомогательную процедуру [ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4(filter)] для удобной работы с индексом, позволяет join'ить другие таблицы и навешивать предикатыСоздаёт вспомогательную процедуру [ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4(index_size)] для анализа размера индексаСоздаёт вспомогательную процедуру [ix].[customers_full_name-employees_full_name_H_1gram_x32_C_x8_K_2/4(stat) для анализа селективности и заполненности хэшаТо же самое динамическим SQLcreate_SH_fuzzy_search_joinEXEC dbo.create_SH_fuzzy_search_join
    @schema_name_1           = 'dbo',
    @table_name_1            = 'customers', 
    @table_id_field_name_1   = 'id',
    @table_mock_field_name_1 = 'Gender',
    @postfix_1               = 'full_name',
    @template_1              = 'ISNULL(first_name, '''') + ''%'' + ISNULL(patronomyc_name, '''') + ''%'' + ISNULL(last_name, '''')',
    @delimiter_1             = '%',

    @schema_name_2           = 'dbo',
    @table_name_2            = 'employees', 
    @table_id_field_name_2   = 'id',
    @table_mock_field_name_2 = 'Gender',
    @postfix_2               = 'full_name',
    @template_2              = 'ISNULL(first_name, '''') + ''%'' + ISNULL(patronomyc_name, '''') + ''%'' + ISNULL(last_name, '''')',
    @delimiter_2             = '%',

    @h_schema_name           = 'ix',
    @codepage                = '65001',
    @n                       = '1',
    @hashSize                = '32',
    @hashChunkSize           = '8',
    @kCombCount              = '2',
    @nGramHashModes          = '0',
    @salt_filter             = '0',
    @lev_dist                = '1',
                 
    @is_del                  = 0,
    @top                     = 999999,
    @DEBUG                   = 0
ТестированиеКак пройти по моим стопам описано в github.com/VGoren/SHFuzzySearch/README.md. На моей машине с Intel Core i3 7gen Windows 11 x64 все тесты отрабатывают примерно за 1 час. Если у вас железо помощнее, то можно увеличить переменные @top_stat(для анализа индекса) и @top_srch(для замера производительности индекса). @top_stat меньше, т.к. вспомогательная аналитическая процедура (с постфиксом stat) достаточно затратная(не использует многопоток), в то же время данные однородны - увеличение выборки не влияет на аналитические показатели индекса.Итак, если всё сделали верно, то получите следующий результат:РезультатыОсновные поля таблицы результатов Поле                 описание                                                                                                   rn                   порядковый номер теста                                                                                     hash_name            название хэша                                                                                              salt_name            название соли                                                                                              lev_dist             расстояние Левенштейна                                                                                     data(KB)             размер данных индексных таблиц без учёта индекса(индекс весит гораздо меньше и его размер пропорционален)  data_calculated(KB)  расчётный размер данных индексных таблиц без учёта индекса                                                 time_create          время создания индекса                                                                                     combs                количество комбинаций при тестовом поиске                                                                  found                количество найденных совпадений                                                                            time_srch            время поиска                                                                                               stat_combs           количество выборочных комбинаций для анализа                                                               residual_chunks      количество комбинаций после 1, 2, 3-го этапа алгоритма  residual_chunks_%    в процентном выражении                                                                                     residual_ham_dist    количество комбинаций после 4-го этапа алгоритма  residual_ham_dist_%  в процентном выражении                                                                                     residual_lev_dist    количество комбинаций после 5-го этапа алгоритма  residual_lev_dist_%  в процентном выражении                                                                                     med_fullness         медианна заполненности хэша                                                                                med_fullness_%       в процентном выражении                                                                                     avg_fullness         среднее арифметическое заполненности хэша                                                                  avg_fullness_%       в процентном выражении                                                                                    Проанализируем результаты на примере выборки 5000 customers с 5000 employees:Анализ основных солейЕсли требуется реализовать нечёткий поиск БЕЗ резистивности к перестановке слов, то:salt_i_word - показывал себя наилучшим образом2gram + salt_i_word несплошной(2/4) - можно использовать в качестве беглого поискаЕсли требуется реализовать нечёткий поиск С резистивностью к перестановке слов, то salt_i_word использовать нельзя, поэтому практически все соли могут найти своё применение:salt_cnt_per_word - беглый поискsalt_cnt - более беглый поиск2gram несплошной(2/4) - наиболее беглый поиск2gram сплошной(4/8) - отрабатывает лучше classic'а, но весит большеДвойной хэш classic_1_2 везде проигрывает в производительности, хотя его селективность(residual_chunks_%)  слегка выше, чем у classic'а. Он теряет информацию о буквах с 3, 4, 5... повторениями. Поиск по тройному, четверному... хэшу потребует дополнительных intersect'ов, что повлечёт за собой увеличение операционной стоимости.У беглых поисков уменьшение селективности(residual_chunks_%) - не всегда обозначает   снижение истинной селективности, это может быть связано с уменьшением уязвимости к ложноотрицательным результатам - просто поиск становится более тщательным. Особенности анализа беглых поисковНа малых размерах индекса (размер хэша/количество строк в исходной таблице) прослеживается, что предельное увеличение размера хэша(hash_size) меньше предельного увеличения размера данных индексных таблиц(data(KB)) - SQLServer резервирует минимальное место под хранение метаданных, таких как информация о таблицах, столбцах, индексах и ограничениях Кривая размера индекса от размера хэшаРасчётный размер данных индексных таблиц data_calculated(KB) меньше, чем data(KB) - SQLServer оставляет свободные ""страницы"" с определённым fill-factor'ом таблицы (не путать с fill-factor'ом индекса) в файле данных для оптимизации операций с таблицей.Кривая скорости поиска от размера хэшаГрафик зависимости размера хэша(hash_size) от фактического времени поиска(time_srch) представляет из себя кривую параболического типа, на неё с двух сторон влияют следующие основные негативные факторы:При уменьшении размера хэша:Поиск по переполненному хэшу сопровождается множеством коллизионных join'ов переполненных chunk'овВероятность коллизии разных вводных внутри хэша - 1/hash_size(одна и та же буква может занять один и тот же бит). Чем сильнее селекивность(residual_chunks_%) хэша, тем губительнее для него этот фактор, поэтому максимальная скорость classic'а раскрывается на x512, а salt_i_word'а - на x1024.При увеличении размера хэша:Увеличение размера хэша, очевидно, увеличивает операционную стоимость join'ов. На слишком малых размерах хэшей проявляется слабо, потому что как SQLServer в частности, так и .NET в целом ""под капотом"" зачастую приводят значения к бóльшему типу данных - в конце концов, в современной 64-битной ОС процессор обрабатывает 64 бита за один такт.Поиск по слишком недозаполненному хэшу сопровождается множеством коллизионных join'ов вообще пустых chunk'ов. Проявляется на слишком недозаполненных хэшах.Кривая смещена по горизонтали влево. Влияние негативных факторов при уменьшении размера хэша усиливается сильнее, чем у негативных факторов при увеличении размера хэша - недозаполненный хэш лучше переполненного.Помимо основных факторов, на графике не отображены, но продолжают воздействовать дополнительные факторы: величина шаблона, уникальность данных, лингвистический аспект, количество букв в алфавите, рандом и др. Поэтому не стоит удивляться, что хэш x128 выбивается из общего ряда в худшую сторону. Возможно, на другом наборе данных он отработал бы лучше.Тестирование изменения размера текста не проводилось, но очевидно, что с его увеличением при заполнении сигнатурного хэша, мы неизбежно упрёмся в количество уникальных символов в алфавите, а при использовании солей - в статистическое сглаживание. В этом случае остается лишь одно - за вводные брать большие n-граммы, но они пропорционально увеличивают допуск ошибок в хэше. Для очень больших текстов этот поиск - не лучший выборИтого, увеличивать размера хэша(hash_size) стоит до тех пор, пока:Вас устраивает размер индексаНе перестанет увеличиваться селективность(уменьшаться residual_chunks_%)Не перестанет уменьшаться фактическое время поиска(time_srch)Да, инструмент узкоспециализированный... Надеюсь, кому-то пригодится. Спасибо за внимание!Теги:нечёткое сравнение строкрасстояние хэммингарасстояние левенштейнаhengineхэширование по сигнатуреsqlинтеграция стороннего кода в SQLmssqlsqlclrc#.netХабы:SQL.NETC#Microsoft SQL ServerАлгоритмы",265,0,0,23 мин,https://habr.com/ru/articles/965934/,43272,4656,5
GFS2 — файловая система для новой виртуализации: наш опыт интеграции в SpaceVM,SpaceVM,2025-11-13T08:00:18.000Z,"['Блог компании Space', 'Виртуализация *', 'Хранение данных *']","SpaceVM 3 часа назадGFS2 — файловая система для новой виртуализации: наш опыт интеграции в SpaceVMУровень сложностиСреднийВремя на прочтение13 минКоличество просмотров259Блог компании SpaceВиртуализация * Хранение данных * ТуториалПривет, Хабр! Меня зовут Сергей Алексанков, я технический директор Space. В этой статье расскажу о нашем опыте внедрения файловой системы GFS2 в платформу виртуализации SpaceVM.Облачные среды, отказоустойчивые кластеры и платформы виртуализации требуют от хранилищ не только надежности, но и поддержки одновременного доступа. В этих условиях традиционные файловые системы (EXT4, NTFS, XFS и др.) оказываются недостаточными — они не рассчитаны на работу с общими блочными устройствами между несколькими узлами. Одним из решений может стать кластерная файловая система, и одной из самых зрелых в этом классе является GFS2 (Global File System 2). Современные ИТ-инфраструктуры часто строятся вокруг виртуализации и облаков, где несколько серверов одновременно обращаются к одним и тем же данным. В таких системах ключевым становится не просто объем или скорость хранилища, а способ доступа к данным — общий или локальный, файловый или блочный. От того, как именно организовано взаимодействие с хранилищем, зависит архитектура всего решения: от производительности виртуальных машин до отказоустойчивости кластера.Локальные хранилища привычны для одиночных серверов: диск или массив принадлежит конкретному узлу, который управляет им напрямую. Общие (shared) хранилища, напротив, предоставляют единое пространство данных для нескольких серверов. Именно они лежат в основе высокодоступных кластеров и виртуализационных платформ, где важно, чтобы виртуальные машины могли мигрировать между узлами без потери доступа к своим дискам.Но общий доступ — это не только вопрос архитектуры, но и способа взаимодействия с данными. Файловые протоколы (NFS, SMB и др.) дают возможность работать с файлами на уровне операционной системы, но вносят дополнительные задержки и ограничения. Блочные протоколы (iSCSI, Fibre Channel) предоставляют более низкоуровневый доступ — сервер видит удаленное устройство как локальный диск. Однако при этом возникает другая проблема: как синхронизировать работу нескольких узлов с одним и тем же блочным устройством, не разрушив файловую систему?Ответ на этот вызов дают кластерные файловые системы, специально разработанные для совместного блочного доступа. Одна из самых зрелых и функциональных среди них — GFS2 (Global File System 2). В нашем опыте ее интеграция в собственный продукт - платформу виртуализации SpaceVM - позволила приблизиться к созданию устойчивой, масштабируемой и по-настоящему отказоустойчивой среды.SpaceVM — собственная разработка компании, платформа виртуализации на базе гипервизора KVM. Своего рода аналог vSphere от VMware, адаптированный под отечественные реалии и высокую степень автоматизации, это программный комплекс для управления вычислительными кластерами, автоматизации развертывания виртуальных машин и балансировки ресурсов между узлами. В его основе — идея унифицированного управления инфраструктурой: от гипервизора до сетевых и дисковых подсистем, с акцентом на прозрачность процессов и возможность интеграции с существующими ИТ-ландшафтами.С использованием GFS2 достигается эффект, сопоставимый по удобству и стабильности с использованием VMFS в продуктах VMware — с открытым исходным кодом и возможностью глубокой адаптации под нужды отечественных заказчиков.В архитектуре платформы стояла задача обеспечить общий доступ к блочному хранилищу между узлами кластера — без потери целостности данных и с возможностью одновременной работы виртуальных машин. При этом многие заказчики ожидали получить функциональность, сопоставимую с VMFS, используемой в продуктах VMware. Эти два требования сошлись в одной точке: для реализации такой модели доступа мы выбрали GFS2 — кластерную файловую систему, по принципам работы близкую к VMFS, но основанную на открытом коде и допускающую глубокую адаптацию под особенности отечественной инфраструктуры.GFS2: преимущества архитектуры и механизма блокировокGFS2 — это POSIX-совместимая кластерная файловая система, разработанная для работы с общими блочными устройствами, подключенными по Fibre Channel или iSCSI. Ее ключевая особенность — координация доступа к метаданным и содержимому файлов при помощи распределенного менеджера блокировок DLM (Distributed Lock Manager). Это позволяет гарантировать целостность данных даже при одновременном доступе с нескольких узлов. В отличие от NFS или SMB, где блокировки файлов координируются через сеть, GFS2 использует механизм локальных блокировок. Это принципиальное отличие: при сетевых файловых системах каждая операция блокировки требует удаленного вызова — будь то RPC-запрос или SMB-пакет — и ожидания ответа от сервера. Такие сетевые транзакции неизбежно добавляют задержку, особенно при высокой нагрузке или нестабильных сетевых условиях.В GFS2 же блокировки реализованы на уровне узлов кластера с использованием локальных ресурсов ОС и специализированных сервисов. Это значит, что большинство операций синхронизации выполняется без сетевых вызовов, напрямую внутри ядра или через быстрые механизмы обмена между узлами. Благодаря этому GFS2 обеспечивает более предсказуемое время отклика и лучшую масштабируемость при росте количества клиентов, чем файловые протоколы NFS или SMB.По сравнению с прямым пробросом блочных устройств в виртуальные машины, GFS2 предлагает более безопасный и управляемый способ совместного доступа к данным, не жертвуя скоростью и устойчивостью файловой системы.GFS2 демонстрирует сбалансированный и технологически обоснованный подход к организации доступа к общим данным, выгодно отличаясь как от сетевых решений вроде NFS или SMB, так и от схем прямого подключения блочных устройств без кластеризации.В сетевых файловых системах вся работа с данными происходит через удаленный сервер — каждая операция чтения, записи или блокировки требует сетевого запроса и подтверждения. Это упрощает администрирование, но создает точку отказа и ограничивает масштабирование: при росте нагрузки сервер становится узким местом, а задержки на уровне сети напрямую влияют на производительность.В противоположность этому, при прямом пробросе блочного устройства (или подход называемый SharedLVM) в несколько узлов вся логика управления данными перекладывается на хосты. Такой подход обеспечивает высокую скорость, но лишен механизмов согласования и защиты целостности: две машины могут записать разные данные в один и тот же блок, разрушив файловую систему.GFS2 решает обе проблемы одновременно. Она обеспечивает одновременный блочный доступ к устройству, но при этом использует встроенные кластерные механизмы — журналирование, распределенный менеджер блокировок (DLM) и систему метаданных, общую для всех узлов. За счет этого данные остаются согласованными, а операции синхронизации происходят быстрее, чем в сетевых файловых системах, поскольку выполняются на уровне ядра и не требуют посредничества сервера. Таким образом, GFS2 сочетает надежность и консистентность сетевых решений с низкими задержками и эффективностью блочного доступа.Второе важное преимущество — масштабируемость. Архитектура GFS2 допускает одновременный доступ к файловой системе с большого числа узлов, не требуя специальных клиентских или серверных реализаций. Это делает ее пригодной для использования как в компактных конфигурациях с 2–3 серверами, так и в полноценных кластерах с десятками узлов. При этом поддержка iSCSI и Fibre Channel дает гибкость в выборе СХД и не требует полной перестройки инфраструктуры. Третье — отказоустойчивость. В отличие от решений, где потеря одного узла может повлечь за собой повреждение данных или зависание всей файловой системы, GFS2 умеет корректно обрабатывать сбои. Механизмы fencing и quorum позволяют изолировать некорректно работающий узел, предотвращая split-brain и обеспечивая консистентность хранилища. Это особенно важно в условиях непрерывной работы платформ виртуализации, где потеря доступности даже части пула данных может повлечь каскадный отказ сервисов.Наконец, GFS2 упрощает администрирование за счет единого пространства имен и поддержки различных форматов данных. Это позволяет централизованно управлять пулами хранения и использовать привычные средства резервного копирования, миграции и диагностики. Для конечного пользователя это выражается в предсказуемом поведении системы и возможности разворачивать кластерные решения без глубокого погружения в особенности конкретных СХД или драйверов.На фоне альтернатив — GlusterFS, CEPH, Lustre — GFS2 занимает уникальное положение: это именно кластерная файловая система, работающая поверх одного общего устройства хранения, а не распределенная система с репликацией. Что делает ее особенно актуальной для сценариев, где важна экономия места, контроль над изоляцией данных и высокая предсказуемость поведения I/O.Для чего мы используем GFS2 в SpaceVMОдной из ключевых архитектурных задач в SpaceVM стало обеспечение полноценного доступа к общему хранилищу с возможностью запуска и миграции виртуальных машин на любых узлах кластера. Без использования внешней кластерной файловой системы это невозможно реализовать корректно. В виртуализированной инфраструктуре, где диски ВМ хранятся на сетевых LUN, только кластерная файловая система обеспечивает безопасный параллельный доступ с нескольких узлов, защиту от повреждения данных и поддержание высокой доступности. Именно в этом контексте в архитектуру SpaceVM была интегрирована GFS2 как штатный компонент хранилищ виртуальных машин.Благодаря GFS2, платформа получает следующие возможности, критически важные для промышленного использования:Хранение образов ВМ в виде файлов (формата qcow2) на общем устройстве, доступном всем серверам одновременно;Реализация высокой доступности (HA) за счет возможности мгновенного запуска виртуальной машины на любом из узлов без предварительной миграции её диска;Поддержка динамического перераспределения ресурсов (DRS), включая live migration, без необходимости вручную управлять блочными устройствами;Использование моментальных снимков, клонов и тонких дисков, с одновременной защитой от потерь данных при overcommit-сценариях — благодаря механизмам блокировок GFS2;Выполнение требований безопасности, предъявляемых заказчиками: например, возможность использовать атрибуты безопасности на уровне файлового слоя, чего невозможно достичь при прямом пробросе блочных устройств в ВМ.GFS2 в SpaceVM — не просто еще один способ монтировать хранилище, а полноценный архитектурный уровень, обеспечивающий устойчивость, управляемость и безопасность всей платформы.Однако простая интеграция GFS2 в SpaceVM оказалась невозможной. Нужно было добиться не только корректной работы файловой системы, но и органичной интеграции в архитектуру SpaceVM, где свои требования к кластерному транспорту, управлению ограждением узлов и мониторингу.Архитектура и технические сложностиФайловая система GFS2 не существует в изоляции — она требует работы в составе кластера и взаимодействует с рядом системных компонентов, формирующих так называемый кластерный транспорт. В этой архитектуре важны не только сама файловая система, но и сопутствующий ИТ-ландшафт, обеспечивающий согласованность операций, отказоустойчивость и синхронизацию между узлами.В первую очередь, GFS2 включает в себя менеджер DLM, который отвечает за координацию доступа к метаданным и содержимому файлов. Чтобы избежать ситуаций типа split-brain и гарантировать консистентность, применяется механизм ограждения (fencing), реализуемый через SBD (Storage-Based Death), чаще всего с использованием аппаратного watchdog-интерфейса IPMI. Файловая система GFS2 не работает автономно — она является частью кластерной инфраструктуры и требует взаимодействия с рядом системных сервисов, которые обеспечивают согласованность доступа и защиту данных. В отличие от обычных файловых систем, где единственный узел управляет диском, в кластере одновременно несколько машин читают и записывают данные на одно и то же блочное устройство. Это создает типичные распределенные проблемы — от гонок за блокировки до рассинхронизации метаданных при сбоях узлов.Чтобы избежать подобных ситуаций, GFS2 использует DLM. Он координирует доступ к метаданным и содержимому файлов, гарантируя, что ни один узел не изменит данные, пока другой с ними работает. Механизм блокировок распределенный, но выполняется внутри кластера, без участия центрального сервера, что снижает задержки и устраняет единую точку отказа.Однако даже с DLM остаются риски, связанные с частичными сбоями — например, когда узел теряет связь с сетью, но продолжает считать себя активным. Чтобы предотвратить разрушение данных в таких сценариях, применяется механизм fencing — изоляции или «отключения» проблемного узла. В GFS2 это реализуется через SBD (Storage-Based Death), который хранит управляющие метки на общем хранилище и при необходимости инициирует аппаратный перезапуск узла через интерфейс IPMI или watchdog. Таким образом, система сохраняет консистентность даже в условиях сетевых сбоев и частичных отказов, что критически важно для кластерной файловой системы.Кластерный транспорт GFS2 опирается на стек Corosync — это системный уровень, обеспечивающий коммуникацию между узлами и согласование их состояний. Важной частью надежной работы всей системы является синхронизация времени на всех узлах, обеспечиваемая через NTP: даже небольшие расхождения могут привести к рассогласованию в блокировках или ложным срабатываниям watchdog-механизма.Кроме того, для подключения LUN в инфраструктуре используются утилиты multipath-tools (в случае Fibre Channel) и open-iscsi (для iSCSI). Они обеспечивают корректное определение, маршрутизацию и управление путями к блочным устройствам, поверх которых и разворачивается файловая система GFS2.GFS2 — целостная кластерная подсистема, в которой управление доступом, синхронизацией, диагностикой и восстановлением после сбоев вынесено на уровень кластера. К системе/кластеру предъявляется ряд требований: строгое время синхронизации между узлами, устойчивость к потере сетевого трафика и возможность автоматического fencing'а при ошибках. Любые отклонения ведут к перезагрузкам узлов или развалу кластера.Трудности при развертывании GFS2: от теории к полевым реалиямХотя GFS2 — зрелая и мощная технология, ее внедрение в реальной инфраструктуре связано с рядом инженерных сложностей. Эти проблемы не всегда очевидны на этапе проектирования, но проявляются в момент масштабирования, нагрузки или интеграции в существующую платформу. Наш опыт развертывания GFS2 в составе SpaceVM показал: надежная работа кластерной файловой системы требует учета множества низкоуровневых факторов.Один из критических аспектов — синхронизация времени между узлами. GFS2 использует DLM и SBD, которые чувствительны даже к незначительным расхождениям во времени. Практика показала: если разница между узлами превышает 60 секунд, возможны некорректные решения по кворуму или ошибочные fencing-сценарии. Поэтому требуется жесткая настройка NTP-инфраструктуры с контролем точности синхронизации вплоть до миллисекунд.Следующая проблема — сетевые коллизии. При использовании iSCSI поверх общей инфраструктуры наблюдались случаи, когда трафик от виртуальных машин перекрывал каналы кластерного транспорта. Это приводило к задержкам, потере пакетов и, как следствие, рассинхронизации между узлами, развалу кворума и перезагрузкам. Особенно остро это проявлялось при объединении сетевых интерфейсов по LACP: реализация агрегации каналов у разных производителей оборудования иногда конфликтовала с работой iSCSI, вызывая нестабильные и трудноотлавливаемые ошибки. Мы отказались от LACP в пользу Active-Backup-режима, обеспечив тем самым предсказуемую работу с минимальной зависимостью от поведения сетевого оборудования.Отдельного внимания требует механизм ограждения через аппаратный watchdog. В нашем случае это IPMI-интерфейс, который должен реагировать на команды от SBD каждые 5 секунд. Однако в ряде инсталляций IPMI не успевал обработать запросы вовремя — из-за параллельных обращений от других сервисов или под нагрузкой. Это приводило к ложным fencing-сценариям: узел перезагружался без реальной причины. Решение — настройка возможности изменения таймаута watchdog’а и тщательное тестирование поведения IPMI на каждом типе оборудования.Также важно отметить трудности, возникающие при блокировке монтирования LUN. В случае некорректного завершения работы или проблем с доступом к СХД, файловая система могла зависать в состоянии ожидания, блокируя операции на уровне ядра. Требовались либо перезагрузка узла, либо использование низкоуровневых утилит для разблокировки ресурса. Подобные случаи невозможно игнорировать в продуктивной системе, и мы встроили в SpaceVM механизмы предиктивной диагностики, позволяющие заранее идентифицировать потенциальные блокировки по косвенным признакам.Интеграция GFS2 в платформу виртуализацииИнтеграция GFS2 в платформу виртуализации SpaceVM потребовала глубокого осмысления архитектуры кластерного транспорта и адаптации логики GFS2 под централизованную модель управления. В отличие от классического использования GFS2 в кластерах с ручной настройкой, здесь требовалось встроить файловую систему в оркестратор, который сам управляет вычислительными узлами, хранилищами и службами мониторинга через API и веб-интерфейс.Одна из ключевых проблем заключалась в несовместимости базовой логики GFS2 с концепцией SpaceVM. GFS2 ставит во главу угла сохранность данных: при любых ошибках, нарушениях кворума или сбоях с доступом к хранилищу система инициирует ограждение узла, то есть его принудительную перезагрузку. Напротив, SpaceVM ориентирован на непрерывную доступность ВМ и динамическую работу с узлами — даже при частичных деградациях инфраструктуры. В результате потребовалось реализовать механизм точной настройки поведения GFS2 через интерфейс платформы, чтобы у администратора была возможность балансировать между отказоустойчивостью хранения и живучестью вычислительного пула.Также было необходимо централизовать управление кластерным транспортом. В классическом варианте администратор вручную настраивает corosync, DLM, SBD, конфигурирует кольца, задает пороги кворума и поведение watchdog. В SpaceVM же вся эта конфигурация задается через API и визуальный мастер (wizard) создания GFS2-пула. Он позволяет в одном окне:выбрать диски,указать тип монтирования,настроить кластерный транспорт и fencing,развернуть файловую систему.Эта автоматизация особенно важна, поскольку GFS2 имеет высокий порог входа в плане требований к администратору. Ручная настройка — сложна и не масштабируется. Внедрение через интерфейс SpaceVM снимает барьер для широкого использования технологии.Мы внедрили централизованную валидацию настроек, автоматическое создание кластерного транспорта, подключение или отключение ограждений узлов, а также поддержку резервных сетей на базе протокола SCTP — они выбираются из UI и позволяют без прерывания работы переключаться между каналами связи.На уровне гипервизора также произошла серьезная интеграция. Контроллер SpaceVM передает вычислительным узлам команды через GRPC, синхронизируя настройки кластера, хранилищ и дисков. Все компоненты — от планировщика задач и очередей до сервисов синхронизации — взаимодействуют через собственную распределенную архитектуру. В этом окружении GFS2 стала частью общей системы: она управляется не вручную, а автоматически, через механизмы модулей Puppet, SSH-контроллеров и специализированных микросервисов.Особое внимание мы уделили типам монтирования и поведению ограждений. В GFS2 предусмотрены два основных сценария fencing’а:При потере кворума кластерного транспорта (split-brain).При ошибках записи или потере связи с блочным хранилищем.Мы реализовали возможность включать или отключать каждый из этих типов независимо, а также контролировать режим монтирования LUN (например, errors=panic или debug). В результате администратор получает не «чёрный ящик», а управляемую систему, где последствия сбоев можно смоделировать и задать заранее.Также были предприняты меры по валидации некорректных состояний:перед созданием нового транспорта проверяются конфигурации всех оставшихся узлов;при попытке монтирования система автоматически выявляет заблокированные LUN и предотвращает зависание задачи;по умолчанию отключена устаревшая опция создания дисков с полным выделением пространства (full), вместо неё используется falloc — это снижает нагрузку на GFS2 и исключает ошибки при записи.Таким образом, мы не просто подключили GFS2 как файловую систему — мы встроили её в архитектуру платформы виртуализации, превратив в управляемый, безопасный и масштабируемый компонент. Получилась не просто интеграция, а полноценная унификация двух миров: надёжного хранения и гибкой оркестрации вычислений.Оптимизация производительностиОднако даже после корректной настройки GFS2 может демонстрировать нестабильную производительность. В процессе внедрения мы протестировали и включили ряд оптимизаций:BFQ-алгоритм планирования ввода-вывода, сглаживающий пики нагрузки между ВМ;Обязательное использование virtio и hyper-v оптимизаций для улучшения взаимодействия между хостом и гостевыми ОС;Отказ от метода выделения диска full в пользу falloc — это устраняет ошибки записи и уменьшает нагрузку на подсистему хранения.Итоги: GFS2 — зрелое решение с инженерной спецификойGFS2 — мощный инструмент, который требует глубокого понимания. Его сила — в контроле над данными, гибкости и отказоустойчивости. Чтобы получить все преимущества его использования нужно глубокое понимание принципов его работы, особенно если разворачивать его «сбоку», не связывая с платформой. Сложность GFS2 — это не недостаток, а плата за мощь. Осознавая это, разработчики SpaceVM провели кропотливую работу по ее комплексной интеграции в свою платформу. В результате, то, что в изоляции воспринималось как слабость (чувствительность к среде, сложность настройки), было устранено. Пользователь получает всю силу GFS2 — контроль, гибкость, отказоустойчивость — через доступный интерфейс, автоматизацию и диагностику SpaceVM.Мы добились того, чего ждали от VMFS в отечественном исполнении: общего хранилища, совместимого с кластерами, виртуальными машинами и требованиями безопасности. А главное — доказали, что даже сложные opensource-компоненты могут органично встраиваться в коммерческие платформы, если подойти к этому как инженеры.Теги:gfs2gfsвиртуализациявиртуализация серверовхранилищахранение данныхинфраструктураvmware vspherevmfsХабы:Блог компании SpaceВиртуализацияХранение данных",259,0,0,13 мин,https://habr.com/ru/companies/spacevm/articles/965388/,22523,2805,3
Оптимизация налогообложения в игровой индустрии: как снизить расходы и защитить ключевые активы,DaniilKadyrov,2025-11-13T05:14:56.000Z,['Финансы в IT'],"DaniilKadyrov 6 часов назадОптимизация налогообложения в игровой индустрии: как снизить расходы и защитить ключевые активыУровень сложностиПростойВремя на прочтение6 минКоличество просмотров91Финансы в ITОбзорСовременная игровая индустрия давно перестала быть исключительно сферой развлечений и превратилась в мощный сегмент мировой экономики. Сегодня цифровые активы — будь то скины, внутриигровая валюта или даже целые виртуальные миры — оцениваются в миллионы долларов. Показательные примеры: продажа планеты Calypso в Entropia Universe за 6 млн долларов или виртуального Амстердама в Second Life за 50 тыс. долларов. Эти сделки демонстрируют финансовый потенциал GameDev. Но вместе с доходами неизбежно появляются и налоговые обязательства. Как разработчикам в России и за рубежом выстроить грамотную стратегию налогообложения и обеспечить защиту активов? Рассмотрим ключевые подходы.Важно отметить, сам термин «оптимизация налогообложения» многие налоговые консультанты используют с осторожностью. В отличие от классической оптимизации расходов, здесь речь идёт не о сокращении обязательств, а о выборе наиболее подходящей модели ведения бизнеса, использовании предусмотренных законом льгот и специальных режимов. Однако, не имея более точного определения, будем использовать привычный термин «оптимизация».Налогообложение внутриигровых транзакций в РоссииВ российской юрисдикции для игровой индустрии не предусмотрено отдельного налога на виртуальные активы, поэтому компании подчиняются общим нормам Налогового кодекса РФ.Основные налоговые обязательства включают:налог на прибыль — 25%;налог на добавленную стоимость (НДС) — 20%;налог на имущество — до 2,2% (ставка зависит от региона);страховые взносы — около 30% от выплат в пользу физических лиц;НДФЛ — от 13% с доходов граждан.Любая реализация внутриигровых предметов, включая проекты по модели free-to-play, рассматривается как доход, облагаемый налогом на прибыль и, в ряде случаев, НДС. С правовой точки зрения такие транзакции квалифицируются как передача прав на использование «неактивированных данных и команд» по лицензионному соглашению.Эта позиция получила официальное подтверждение в письме ФНС России от 2017 года № СД-4-3/988@, где было закреплено, что при корректном оформлении подобные операции могут освобождаться от уплаты НДС. Для разработчиков это стало важным инструментом снижения налоговой нагрузки.Кейс Mail.ru: уроки из практикиВ 2015 году налоговые органы оспорили освобождение от НДС по операциям, связанным с продажей внутриигровых предметов в проектах Mail.ru Group (Warface, «Аллоды Онлайн»). Инспекция квалифицировала такие сделки не как передачу прав на программное обеспечение, а как оказание услуг по организации игрового процесса, что влекло обязанность уплачивать НДС.Суды поддержали позицию налоговиков, что создало неблагоприятный прецедент для разработчиков. Однако в 2017 году ситуация изменилась, в письме ФНС № СД-4-3/988@ было закреплено право рассматривать подобные операции именно как передачу прав на использование программного обеспечения. Это дало студиям возможность корректно оформлять сделки и существенно снижать налоговую нагрузку.Налоговые льготы для игровой индустрииРазработчики игр могут значительно снизить фискальную нагрузку, используя предусмотренные законом льготные режимы. Рассмотрим ключевые инструменты, доступные как в России, так и за её пределами.1. IT-аккредитация в РоссииСтатус аккредитованной IT-компании, подтверждаемый Минцифры РФ, предоставляет целый ряд преимуществ:налог на прибыль снижается до 5%;страховые взносы уменьшаются до 7,6%;передача прав на ПО из реестра российского ПО освобождается от НДС.Для получения аккредитации требуется, чтобы более 70% дохода компании приходилось на IT-деятельность.Пример: студия Astrum Entertainment, разработчик проекта BLACK RUSSIA.2. Режим «Сколково»Резиденты инновационного центра «Сколково» могут рассчитывать на ещё более заметные преимущества:налог на прибыль — 0%;НДС — освобождение;страховые взносы — 14%.Ограничения: льготы действуют в течение 10 лет и прекращают применяться при выручке свыше 1 млрд рублей или прибыли более 300 млн рублей. При грамотном комбинировании с IT-аккредитацией возможно снижение страховых взносов до 7,6% при нулевом налоге на прибыль.3. Упрощённая система налогообложения (УСН)Для небольших студий эффективным инструментом становится УСН:ставка 6% от дохода — без учёта расходов;ставка 15% от дохода за вычетом расходов — с возможностью учитывать траты на разработку и оборудование.УСН освобождает от НДС при выручке до 60 млн рублей. При превышении этого порога применяются льготные ставки:5% НДС при доходе до 250 млн рублей;7% НДС при доходе до 450 млн рублей.Ограничения:годовой доход — до 450 млн рублей;численность сотрудников — до 130 человек;остаточная стоимость основных средств — не выше 150 млн рублей.4. Международные инструменты: IP Box и налоговая отсрочкаМногие игровые компании регистрируют бизнес за рубежом, чтобы воспользоваться специализированными налоговыми режимами. Наибольшей популярностью пользуются Кипр, Эстония и ОАЭ.IP Box — режим снижения налоговой нагрузки на доходы от интеллектуальной собственности (ИС):Кипр: 80% прибыли от ИС исключается из налогооблагаемой базы;ОАЭ: ставка по прибыли от ИС — 0%;Казахстан: налоговая база уменьшается на 100%.Льгота зависит от коэффициента связи (nexus ratio), который отражает долю квалифицированных затрат (например, зарплат разработчиков) в общих расходах на создание продукта.Пример: в ОАЭ при nexus ratio 80% и прибыли $1 млн налог 0% применяется к $800 тыс., а оставшиеся $200 тыс. облагаются по ставке 9%.Отсрочка уплаты налога на прибыль действует в Эстонии и Грузии, налог платится только при распределении дивидендов. Это позволяет без потерь реинвестировать доходы в новые проекты, активы или объекты ИС, что особенно выгодно для холдингов и венчурных фондов.R&D-вычетыВ Чехии и Дании расходы на разработку ПО можно списывать по повышенной ставке (например, 200%). Это снижает налогооблагаемую базу.Пример: при выручке $1 млн и расходах на R&D $300 тыс. с вычетом 200% налогооблагаемая прибыль сокращается с $700 тыс. до $400 тыс. В Чехии подобные вычеты можно переносить на будущие периоды (до трёх лет), что особенно выгодно для стартапов без текущей прибыли.Защита и сопровождение активовДля компаний игровой индустрии нематериальные активы являются фундаментом бизнеса. Исходный код, графика, игровые механики, бренд и торговые марки — всё это требует не только технической проработки, но и надёжной правовой защиты.Ключевыми инструментами в этой сфере выступают:грамотно составленные пользовательские соглашения;прозрачные политики конфиденциальности, соответствующие требованиям международных платформ и регламентов (включая GDPR);регистрация прав на объекты интеллектуальной собственности.Без этих документов невозможно выйти на международный рынок, легально монетизировать продукт или привлечь инвестиции.Особое внимание необходимо уделять формулировкам, особенно в проектах с внутриигровыми покупками, подписками или возможностью вывода средств. Некорректные условия могут привести не только к блокировке приложения в App Store или Google Play, но и к риску признания игры азартной с последующими правовыми последствиями.На практике защита активов — это не разовое оформление документов, а непрерывный процесс, включающий аудит, регулярное обновление соглашений и мониторинг регуляторных требований в различных юрисдикциях. Только комплексный подход позволяет минимизировать риски штрафов, санкций и судебных разбирательств, сохраняя при этом контроль над ключевыми элементами продукта.Риски и практические рекомендацииНалоговые спорыОколо 20% конфликтов в IT-сфере связано с вопросами налогообложения роялти и проблемой двойного налогообложения. Снижение рисков достигается за счёт продуманного структурирования бизнеса — например, регистрации компании в юрисдикциях с режимом IP Box (Кипр, Нидерланды), что позволяет оптимизировать налогооблагаемую базу.Санкции со стороны платформНесоблюдение правил App Store или Google Play — например, публикация контента, не соответствующего возрастным ограничениям, — может привести к предупреждениям, удалению или даже полной блокировке приложения. Регулярная проверка документации, аудит пользовательских соглашений и контроль контента позволяют минимизировать эти угрозы.Внутренние конфликтыОколо 30% споров в GameDev связано с разногласиями между основателями или ключевыми сотрудниками. Чётко прописанные трудовые контракты, партнёрские соглашения и документы о распределении долей существенно снижают вероятность подобных конфликтов.Выбор юрисдикцииПравильный выбор страны регистрации играет решающую роль. Кипр, Эстония и Гонконг востребованы благодаря выгодным налоговым режимам и надёжной защите интеллектуальной собственности. При этом стоит избегать юрисдикций с высокой налоговой нагрузкой (Франция, Италия) или слабой правовой защитой прав на ИС (например, Украина).Комплексное внимание к этим аспектам позволяет не только снизить риски, но и создать устойчивую основу для долгосрочного развития игровой компании.ЗаключениеСовременная разработка игр — это не только креативные идеи и технологические решения, но и выстроенная правовая устойчивость, прозрачная налоговая структура и стратегическое планирование. Успешный проект требует не только качественного кода и проработанной механики, но и корректно оформленных прав на интеллектуальную собственность, продуманной налоговой модели и строгого соблюдения требований платформ и регуляторов.В российской практике разработчики могут использовать такие инструменты, как IT-аккредитация, статус резидента «Сколково» или УСН, чтобы сократить налоговую нагрузку. Международные механизмы — от IP Box и отсрочки налога на прибыль до R&D-вычетов — дают возможность масштабировать бизнес и защищать доходы на глобальном рынке. Однако все эти инструменты эффективны лишь при условии наличия комплексной юридической инфраструктуры.Без детально проработанных пользовательских соглашений, надёжной политики конфиденциальности и зарегистрированных прав на ключевые активы даже самая популярная игра остаётся уязвимой. Поэтому юридическое сопровождение — это не второстепенный элемент, а обязательная часть стратегии выхода на рынок и устойчивого развития любой GameDev-компании.Теги:оптимизация налогообложения gamedevналоги в игровой индустрииналоговые льготы для it-компанийналоговая оптимизация it-бизнесаХабы:Финансы в IT",91,0,0,6 мин,https://habr.com/ru/articles/965894/,10422,1287,1
Архитектурный выбор: Монолит против микросервисов без технического диплома,lukyan73,2025-11-13T09:59:44.000Z,"['Анализ и проектирование систем *', 'Микросервисы *', 'Управление продуктом *', 'Управление проектами *', 'Управление разработкой *']","lukyan73 1 час назадАрхитектурный выбор: Монолит против микросервисов без технического дипломаУровень сложностиПростойВремя на прочтение3 минКоличество просмотров167Анализ и проектирование систем * Микросервисы * Управление продуктом * Управление проектами * Управление разработкой * МнениеRecovery ModeКак нетехническому специалисту участвовать в принятии решений, от которых зависят бюджет, сроки и масштабируемость продуктаАрхитектурные решения — это фундамент цифрового продукта. Выбор между монолитной и микросервисной архитектурой определяет, насколько быстро вы сможете выпускать новые функции, как будет масштабироваться бизнес и какие команды вам потребуются. Это не чисто технический вопрос, а стратегический, напрямую влияющий на финансовые и операционные показатели.Многие нетехнические специалисты — продуктологи, менеджеры, основатели стартапов — чувствуют себя исключенными из этого разговора. Их задача — не писать код, а понимать бизнес-последствия выбора и говорить с разработчиками на понятном им языке. Вот практический фреймворк, который поможет участвовать в этих обсуждениях на равных.Базовые концепции: два подхода к архитектуреМонолит — единая, неделимая система. Все компоненты (база данных, серверная и клиентская логика) тесно связаны и развертываются как одно целоеМикросервисы — набор независимых сервисов, каждый из которых отвечает за конкретную бизнес-возможность и общается с другими через четко определенные интерфейсы (API)Три критерия для принятия решения на языке бизнесаВместо споров о технологиях сосредоточьтесь на факторах, которые понятны любому руководителю.1. Структура команды: Масштабируемость против оперативной скоростиОрганизация вашей команды напрямую определяет оптимальный архитектурный выбор.Монолит эффективен для небольших сплоченных команд. Разработчики работают с единой кодовой базой, что позволяет быстро вносить изменения и оперативно решать задачи. Отсутствие необходимости согласовывать форматы взаимодействия между независимыми сервисами ускоряет разработку на ранних этапах.Микросервисы становятся оправданы при наличии нескольких автономных команд, каждая из которых фокусируется на своей зоне ответственности. Эта модель позволяет командам разрабатывать, тестировать и развертывать свои сервисы независимо. Однако попытка разрабатывать микросервисную архитектуру силами одной небольшой команды ведет к резкому росту сложности и непропорциональным временным затратам.2. Стабильность требований: Гибкость против предсказуемостиВыбирайте монолит для продуктов с быстро меняющимися требованиями — стартапы, MVP, экспериментальные направления. Это позволяет быстро итерировать и перенаправлять ресурсы, не разрывая контракты между сервисами.Микросервисы эффективны для стабильных продуктов с четкими границами доменов. Когда функциональность модуля определена на месяцы вперед, его можно выделить в отдельный сервис. Частые кросс-сервисные изменения в микросервисной архитектуре требуют значительных координационных усилий.3. Толерантность к отказам: Простота против отказоустойчивостиВ монолите отказ одного модуля часто означает остановку всей системы. Это приемлемо на ранних стадиях, когда кратковременные простои не так критичны для бизнеса.Микросервисы обеспечивают изоляцию сбоев — если один сервис недоступен, остальные продолжают работать. За эту отказоустойчивость вы платите: мониторинг десятков сервисов и обеспечение их стабильного взаимодействия требуют значительных операционных ресурсов.Ключевой вывод для бизнесаПравило простое: начинайте с монолита, эволюционируйте к микросервисам через осознанный переход.Стартапы и новые продукты: Ваша главная валюта — скорость выхода на рынок и проверка гипотез. Монолит дает вам максимальную гибкость при минимальных операционных затратах.Растущие продукты: Когда команды начинают мешать друг другу в монолите, а границы сервисов стали четкими и стабильными — это сигнал к постепенному, обоснованному переходу на микросервисы.Самые дорогостоящие архитектурные ошибки происходят, когда компания пытается построить распределенную систему без соответствующих командных структур и процессов.P.S. Если вы хотите не просто понимать такие решения, а уверенно участвовать в архитектурных обсуждениях, задавать правильные вопросы и оценивать риски — в моей книге «Птичий язык: Как говорить на языке разработчиков, не написав ни строчки кода» я подробно разбираю логику IT-архитектуры, процессы разработки и модели сотрудничества. Это поможет вам говорить с техническими специалистами на одном языке и принимать взвешенные совместные решения.Теги:монолитархитектурамикросервисыХабы:Анализ и проектирование системМикросервисыУправление продуктомУправление проектамиУправление разработкой",167,0,0,3 мин,https://habr.com/ru/articles/966028/,4695,544,5
Как поменять улыбку без масштабного лечения?,docdeti_docmed,2025-11-13T10:33:45.000Z,"['Блог компании Сеть клиник docmed и docdent', 'Здоровье', 'Научно-популярное']","docdeti_docmed 42 минуты назадКак поменять улыбку без масштабного лечения?Время на прочтение1 минКоличество просмотров175Блог компании Сеть клиник docmed и docdentЗдоровьеНаучно-популярноеКейсБывает так, что улыбка не нравится, и хочется всё изменить, но... прямо сейчас нет возможности провести большое лечение. Но это не повод ходить грустить и не улыбаться. Как можно поменять улыбку рассказывает и показывает стоматолог-ортопед Мария СпивакДевушка обратилась с жалобами на внешний вид передних зубов: ей не нравилась и форма и их состояниеФото ""до""На осмотре мы определили ряд проблем: старые несостоятельные реставрациикариозные полостивыраженная стираемость передних зубовОбсудили с пациенткой план лечения и других зубов — с возможными удалениями, установкой имплантов и коронок.Но пока девушка не готова к масштабному лечению, решили начать с малого и заняться передними зубами, которые её беспокоят. Так бывает, и задача врача — услышать пациента и подобрать адекватные альтернативы, которые улучшат жизнь человека прямо сейчасЯ предложила промежуточный этап, который устроил пациентку, и мы воплотили план в жизньЧто сделали? удалили все старые реставрации с передних зубовпролечили кариозные полостивосстановили цвет и эстетику композитными винирамиФорму и цвет подбирали вместе с пациенткойВ будущем планируем провести большую работу со всеми зубами: с удалениями, имплантами и коронками. Композитные реставрации придётся менять, и пациентка к этому готова. Впереди тотальная реабилитация, но несколько лет красивой улыбки мы ""выиграли"", и сейчас девушка улыбается открытоПолучить красивую улыбку можно уже сейчас, не откладывая зубные проблемы ""на потом"". А над более сложными этапами планомерно работать вместе с врачом Теги:улыбказубвинирстоматологияХабы:Блог компании Сеть клиник docmed и docdentЗдоровьеНаучно-популярное",175,0,0,1 мин,https://habr.com/ru/companies/docmed_docdent/articles/965800/,1837,233,3
"Почему простые фичи — самые сложные: история о пет-проекте, Дженге и маржинальной торговле",impatient,2025-11-13T05:16:22.000Z,"['Java *', 'Программирование *', 'Финансы в IT']","impatient 6 часов назадПочему простые фичи — самые сложные: история о пет-проекте, Дженге и маржинальной торговлеУровень сложностиСреднийВремя на прочтение9 минКоличество просмотров538Java * Программирование * Финансы в ITИз песочницыПривет, Хабр! Меня зовут Иван, и сегодня я хочу поделиться историей о своём пет-проекте A-Zero. Истории про провалы традиционно интереснее историй об успехах, и моя как раз такая (почти). Довольно бодроначинавшийся проект чуть было не свёл меня с ума из‑за одной единственной фичи, «просочившейся» в MVP, и сейчас я расскажу, как я из этого выкарабкался и чему научился по дороге.Дисклеймер: в тексте присутствует некоторое количество терминов, относящихся к трейдингу. Для удобства не столь искушённого читателя большинство из них снабжены всплывающими подсказками с пояснениями.Мир, где всё простоВсё началось со слегка безумной идеи написать с нуля фреймворк для алгоритмического крипто-трейдинга в качестве сольного пет-проекта. Я понимал, что единственный шанс для начинающего Java-разработчика (меня) преуспеть в таком нелёгком деле — чётко следовать принципу итеративной сложности. Нужно было начать с чего-то совсем элементарного и очень маленькими порциями достраивать функционал — как будто играешь в Дженгу и боишься, что от каждой следующей палочки всё рухнет.Поначалу всё шло довольно бодро, и мы с проектом дожили до релиза 0.1.0 — он состоял из CLI утилиты для выгрузки исторических данных, плюс я успел прикрутить небольшой CI пайплайн. Вскоре у меня уже были готовы основные интерфейсы и базовая реализация движка для бэктестинга, который умел моделировать спотовые трейды. Казалось, что фундамент моей башни в Дженге заложен, и теперь осталось только докладывать на него палочки-фичи, а на горизонте уже замаячил следующий релиз с полноценной утилитой для бэктестинга — нужно было только реализовать логику описания стратегий и написать поверх всего этого CLI-обёртку.Стоит сказать, что слово ""базовая"" в применении к реализации движка тут не было скромностью — он действительно был безумно простым и использовал очень базовую логику:Один счёт (double balance).Только одна открытая позиция в один момент времени.Простая логика: купил — баланс уменьшился, продал — увеличился.И вот тут предыдущий успех, видимо, заставил меня расслабиться, и...Кроличья нораЯ решил, что такой релиз получится уж слишком скучным и примитивным. А вот если добавить в него всего одну «палочку» — шорт‑трейды... Тут, каюсь, сыграли роль сразу два фактора: во‑первых, отсутствие у меня чёткого плана по MVP, во‑вторых — немного замутнившиеся воспоминания из моего прошлого трейдерского опыта о том, как трейдинг, собственно, устроен. Поэтому я практически машинально добавил в модель стратегии возможность шортить активы, и только потом уже задумался, рассчитан ли на это мой движок — но обо всём по порядку.Изначально мне казалось, что добавление шорт‑трейдов — фича чисто номинальная, и на логику программы в целом не повлияет. Первым звоночком стало нарушение модели баланса (double balance). Дело в том, что изначально она была призвана играть двоякую роль: с одной стороны — отражать покупательную способность в ходе симуляции (то есть какие трейды мы можем себе позволить), с другой — показывать состояние нашего капитала, то есть сколько мы заработали/потеряли.Но при открытии шорта мы не тратим, а получаем средства — и, наоборот, теряем при закрытии. Так модель double balance моментально перестала работать. «Ничего, просто чуть‑чуть усложним модель!» — оптимистично подумал я и взял в руки следующую палочку Дженги: полноценный Map<String, Double> кошелёк, отслеживающий баланс (в т.ч. отрицательный) каждого имеющегося актива.Теперь я мог учитывать, что при открытии шорта для базовой валюты баланс уменьшается, а для валюты котировки — увеличивается. Также пришлось добавить движку два режима симуляции: маржинальная торговля и спотовая торговля, а ещё поддерживать в симуляции неограниченное количество открытых позиций.И вот тут я ощутил, что начинаю падать в кроличью нору маржинальной торговли. Теперь для реалистичной симуляции мне нужно было рассчитывать ещё и такие вещи как:Залог: Просто так взять актив в долг нельзя — биржа рассчитывает требуемый объём обеспечения заёма имеющимися у трейдера средствами. В современных крипто-биржах существует концепт ""объединённого торгового аккаунта"", для которых это обеспечение рассчитывается исходя из балансов всех активов на аккаунте. Биржи предоставляют некоторую документацию касательно алгоритма этого расчёта, которую мне пришлось изучить, а затем практически полностью переписывать логику исполнения ордеров.// Рассчитываем необходимую начальную маржу для новой позиции
BigDecimal imr = calculateInitialMarginRate();
BigDecimal newMarginRequired = positionValue.multiply(imr);
BigDecimal totalEquity = calculateTotalEquity(this.currentPrices);
BigDecimal existingMargin = calculateTotalInitialMargin();

// Проверяем, достаточно ли у трейдера общей эквити для открытия
if (totalEquity.subtract(existingMargin).compareTo(newMarginRequired) < 0) {
    log.warn(
        ""MARGIN CHECK FAILED: Cannot open {} position for {}. Required: {}, Available: {}"",
        direction, symbol, newMarginRequired, totalEquity.subtract(existingMargin));
    return;
}Поддерживающая маржа: Помимо расчёта обеспечения в момент заёма актива, постоянно проверяется, что общая стоимость активов трейдера не упала ниже уровня поддержания маржи (зависит от общего объёма заёмных активов; обычно равен определённой доле от изначального обеспечения при заёме). Нужно было добавить дополнительную логику на каждом цикле симуляции.// Обновляем актуальную информацию о ценах активов
context.updateCurrentPrices(currentPrices);

// С помощью хелперов проверяем, нужно ли запускать ликвидацию
if (config.getAccountMode() == AccountMode.MARGIN) {
    if (context.isMarginCallTriggered()) {
        context.liquidateAllPositions();
    }
}Принудительная ликвидация: Самая страшная и самая важная часть. Если баланс активов падает ниже уровня поддерживающей маржи (см. предыдущий пункт), происходит margin call — биржа начинает принудительно продавать активы трейдера, чтобы покрыть недостаток обеспечения. Это необходимо было реализовать, чтобы симуляция была ""честной"".Сложнее всего было, конечно, остановиться: можно было провести ещё неопределённое количество времени в попытках сделать симуляцию всё более и более реалистичной, но в какой-то момент я сказал себе ""стоп"". Во-первых, это нарушало бы принцип итеративной сложности. Во-вторых, очень сильно замедлило бы меня. В-третьих, не было особенно осмысленным: бэктестер призван служить первой ""дешёвой"" ступенью анализа, а следующий этап — демо-трейдинг, реализация которого у меня есть дальше в дорожной карте, — в любом случае решил бы проблему с неточной симуляцией. В итоге я уговорил себя сделать следующие упрощения:Явную формулу для расчёта объёма обеспечения заёма найти не удалось (точнее, формула из документации банально расходилась с тем, что я видел на самой бирже), поэтому в движке я использовал заведомо более «агрессивную» формулу — это значило, что движок позволит стратегии взять в долг чуть меньше, чем, возможно, позволила бы биржа (что безопасно).Вместо каскадной ликвидации (продажи активов в определённом биржей порядке) при принудительной ликвидации движок моделирует закрытие всех позиций — на мой взгляд, это простительная неточность, так как принудительная ликвидация сама по себе означает «фейл» стратегии, и моделировать реальные последствия не так критично для реализма.На руинах APIРазобравшись с тем, что фактически стало полным рефакторингом движка для бэктестинга, я обнаружил, что, помимо изменений в логике, теперь у меня полностью разрушились контракты API. Вот пара примеров:Пример 1: Эволюция TradingContext. Изначально это был крайне минималистичный интерфейс, описывающий взаимодействие стратегии с ""биржей"". В начале стало очевидно, что примитивные методы executeBuy/Sell плохо смотрятся в контексте нескольких режимов торговли (спот и маржинальная) — вместо них появился более абстрактный и семантичный submitOrder. После этого, во многом в процессе тестирования, стало понятно, что интерфейс слишком закрытый — нужен гораздо более широкой read-only доступ к состоянию аккаунта. Здесь помогла ментальная модель ""TradingContext — абстракция над веб-интерфейсом криптобиржи"". Так появились методы для получения состояния кошелька, общей стоимости активов на аккаунте и т.д.Пример 2: Эволюция Strategy и рождение MarketEvent. В сценарии, где стратегия взаимодействовала с несколькими активами одновременно, простой метод onCandle(Candle c, ...) уже не работал — тип Candle (свеча) был намеренно сделан минималистичным и не давал контекста о том, по какому активу получена информация. Из этого родился новый тип MarketEvent — ""обёртка"" над свечой и символом актива, а интерфейс Strategy теперь содержал onMarketEvent(MarketEvent event, ...). Так стратегия получала всю нужную её внутренней логике информацию — а ещё это сделало интерфейс более гибким: при необходимости я мог бы расширить тип MarketEvent, не меняя контракты API.С одной стороны, можно с уверенностью сказать, что это были правильные и необходимые изменения. С другой — они происходили абсолютно не в том порядке, в котором я хотел бы их вносить. Вместо ""отлично, работает, давайте улучшать"" получилось ""оно сейчас развалится, если я ничего не сделаю"".TDD?Неожиданно для меня там, где в борьбе с потихоньку наступавшим в проекте хаосом паттерны проектирования мне уже не помогали, на помощь пришли тесты — но не совсем в привычном их понимании.Из-за внезапного бурного разрастания бизнес-логики стало сложно держать в голове не только то, как именно она работает, но даже то, что она вообще говоря должна делать. И здесь тесты оказались прекрасным инструментом — не для того, чтобы верифицировать поведение, а для того, чтобы его прояснять. Фактически, я пытался декларативно описывать желаемое поведение через тесты (почти как в Test-Driven Development), чтобы затем на падающих тестах смотреть, в чём именно ошибка — в бизнес-логике или в моих от неё ожиданиях.Например, при тестировании движка для бэктестинга неожиданно стал падать тест spot_shortAttempt_ShouldThrowException. Вместо исключения при попытке в режиме спот-торговли продать актив, которого нет на балансе аккаунта, система... Не делала ничего. Проверив, по какой ветке идёт бизнес-логика, я выяснил, что вместо того, чтобы обрабатывать такой ордер как шорт-трейд, движок воспринимал его как попытку продать актив в большем количестве, чем есть у трейдера — и просто игнорировала его, выводя предупреждающее сообщение.Поведение системы оказалось правильнее моего собственного понимания — там, где я ожидал, что невозможная операция приведёт к ошибке, движок просто корректно её игнорировал. В этот момент я понял, что тестами можно не просто ловить баги и отслеживать, где логика работает не так, как задумано — хорошо написанные тесты помогают выявить места, где логика изначально была задумана неправильно.Отдельным вызовом при тестировании стала дилемма ""инкапсуляция vs. тестируемость"". Поскольку многие компоненты были довольно сложными stateful объектами, тестировать их без верификации внутреннего состояния было практически бессмысленно — но как сделать внутреннее состояние доступным в тестах, не засоряя публичный API? Решением стали аккуратно подобранные package-private методы, создававшие специальное тестовое API с минимальной необходимой ""площадью покрытия"":// package-private метод для мониторинга завершённых трейдов в тестах
List<Trade> getExecutedTradesForTest() {
    return List.copyOf(this.executedTrades); // Возвращяем безопасную immutable копию
}Хэппи эндВ итоге, спустя пару недель рефакторинга и доработки, релиз 0.2.0 был готов. Помимо CLI-бэктестера и YAML формата для описания трейдинговых стратегий, в нём теперь было гораздо более надёжное, гибкое и близкое к реальности ядро в виде бэктест-движка и API-контрактов. Но самым ценным для меня, пожалуй, стали не фичи, а сам опыт, который я приобрёл в процессе разработки:Беспощадное ""M"" в MVP. Невероятно трудно в процессе написания кода не хвататься за каждую возможность что-нибудь ""улучшить"" и добавить в свою башню в Дженге ещё одну палочку. Но, как показал мой кейс, очень важно иметь дисциплину этого всё-таки не делать — иначе всю башню придётся бесконечно пересобирать заново. Правильным решением в моём случае было бы остановиться в MVP на реализации логики спотовой торговли, а потом итеративно усложнять уже функционирующий движок.Упрощай, прежде чем усложнять. В процессе добавления в движок логики маржинальной торговли в какой-то момент начало казаться, что каждая новая реализованная концепция тянет за собой ещё две нереализованных. Для продуктивной разработки гораздо ценнее иметь что-то простое и работающее, поэтому было важно ""провести черту"" в том, насколько точно я хочу симулировать реальность. Логику всегда можно усложнить впоследствии — по результатам тестов, которые покажут, где именно эти усложнения действительно необходимы. А для этого нужно сначала создать что-то, что уже можно будет тестировать.Тесты как инструмент прояснения, а не только проверки. Я не могу с чистой совестью назвать свой подход реальным TDD — всё-таки тесты писались уже после того, как была написана основная масса логики. И всё-таки в критический момент именно тесты как раз оказались таким островком предсказуемости и стабильности, благодаря которому проект не развалился. Можно сказать, что в ходе разработки у меня самопроизвольно зародился некий паттерн TDC — ""Test Driven Clarification"". И я уверен, что обязательно прибегну к этой практике в дальнейших этапах работы над моим проектом.Спасибо, что дочитали мою первую публикацию до конца :-) Буду рад услышать ваши мысли и критику в комментариях. Расскажите, какие «простые» фичи стопорили ваши проекты?Весь код ядра проекта A-Zero открыт и доступен на GitHub.Теги:трейдингпет-проектХабы:JavaПрограммированиеФинансы в IT",538,0,0,9 мин,https://habr.com/ru/articles/965896/,13957,1864,3
Go-to-Community вместо Go-to-Market,badcasedaily1,2025-11-12T13:12:31.000Z,"['Блог компании OTUS', 'Developer Relations *', 'Управление продуктом *']","badcasedaily1 22 часа назадGo-to-Community вместо Go-to-MarketУровень сложностиСреднийВремя на прочтение17 минКоличество просмотров181Блог компании OTUSDeveloper Relations * Управление продуктом * ОбзорПривет, Хабр! Сегодня поговорим про стратегию Go‑to‑Community вместо Go‑to‑Market. Звучит конечно круто, но суть простая: перестать видеть разработчиков только как лидов в воронке продаж и начать работать с ними как с сообществом на равных, с созданием ценности для всех. Go-to-Market vs Go-to-Community: в чем разница?Для начала небольшой ликбез. Go‑to‑Market (GTM) это традиционный подход вывода продукта на рынок. Маркетологи гонят рекламу, собирают лиды, ведут их по воронке (от узнавания — к интересу — к триалу — к покупке). Цель GTM — захватить максимальную ценность (value capture) из аудитории: сконвертировать как можно больше людей в клиентов и продажи. Вы наверняка такое видели.Go‑to‑Community (GTC) — альтернативный (и дополняющий) путь. Проще говоря, вместо того чтобы на каждом шаге пытаться выудить из аудитории пользу для себя, мы сначала создаем ценность вместе с сообществом и для сообщества. Мы привлекаем вокруг продукта широкое техническое коммьюнити, даже тех, кто прямо сейчас ничего не купит и не принесёт денег. Пусть люди учатся, обмениваются знаниями, придумывают интеграции, помогают друг другу, а там, глядишь, со временем часть из них созреет и до продаж. Да и не только продажи важны, лояльное сообщество будет поддерживать продукт, создавать контент, рекомендовать его коллегам. Маркетинг ловит лишь тех, кто готов купить, а комьюнити охватывает всех, кому интересна тема продукта, и вовлекает их на своих условиях.GTC не противоречит GTM, а дополняет его. Никто не отменяет воронку продаж, просто параллельно выстраивается воронка сообщества, и они должны работать синхронно. Идеальный сценарий: обе стратегии согласованы, и участники сообщества постепенно переходят в разряд лидов, когда придёт время. Пока же они остаются в комьюнити, получают там пользу и сами вносят вклад. Если же стратегии разрознены, то конечно будет перекос. Либо вы бескорыстно вкладываетесь в комьюнити без всякой выгоды для компании (отличная благотворительность, но бизнес это долго не выдержит), либо зациклены на продажах и игнорируете остальных фанатов продукта (упускаете кучу возможностей и сами того не зная отталкиваете людей). Нужно равновесие.При согласованной стратегии Go‑to‑Community подпитывает Go‑to‑Market воронку. Разрозненные стратегии ведут к перекосу, либо ценность создаётся только для сообщества, либо упор только на продажи без вовлечения комьюнити.Ставку на сообщество делают многие успешные технологические компании. В 2021-м сразу несколько «единорогов» с сообществом в ДНК вышли на IPO, вспомним хотя бы GitLab, HashiCorp, Duolingo. Они смогли превратить комьюнити вокруг своих продуктов в реальный драйвер роста и выручки. Например, у HashiCorp открытое сообщество не было чем‑то второстепенным, оно с первых дней определяло архитектуру продуктов, монетизацию и всю стратегию компании.Выходит, Go‑to‑Community не благотворительность, а инвестиция. Маркетинг приносит рост до определённого предела, дальше без сообщества не выехать. Разработчики больше доверяют друг другу, чем рекламе, любят сами пробовать и учиться. Поэтому стоит задача создать среду, где люди получают ценность: знания, поддержку, возможность влиять на продукт. Тогда сообщество само станет вашим маркетингом. Удовлетворённые участники начнут советовать решение коллегам, писать статьи и туториалы, расширять продукт под свои нужды, словом, делать ту работу, за которую маркетинг платит бюджетами.Роли в сообществе и их цепочки ценностиДопустим вы решились сместить фокус с агрессивного маркетинга на комьюнити. Возникает вопрос: «А что вообще из себя представляет мое сообщество? Кто эти люди и как с ними работать?» Ошибка в том, чтобы считать комьюнити однородной массой. На самом деле внутри есть несколько ролей. Люди по‑разному взаимодействуют с вашим продуктом и вносят разный вклад. В этой статье выделим три роли: мейнтейнеры, интеграторы и эдьютейтеры (расскажу, что имеется в виду). Для каждой опишем её мотивацию и цепочку ценности, то есть, какую ценность эта роль приносит проекту и что сама получает взамен. Мейнтейнеры (Maintainers) – хранители кодаЕсли ваш продукт связан с open‑source или имеет бесплатное ядро, наверняка есть люди, отвечающие за поддержание и развитие этого проекта, помимо вашей команды. Это и есть мейнтейнеры: разработчики, которые на постоянной основе вкладываются в код, ревьюят чужие контрибьюты, следят за качеством. Чаще всего это либо сотрудники компании, либо ключевые внешние энтузиасты, заслужившие доверие. Их главная цель сделать проект лучше для всех пользователей. Мотивация обычно техническая и идеологическая: мейнтейнеры хотят, чтобы продукт решал их (и не только их) задачи эффективно, был надежным, соответствовал видению. Часто они сами начинали этот проект или присоединились, потому что горят этой технологией.Ценность для компании: мейнтейнеры фактически ваши добровольные разработчики и архитекторы. Они фиксят баги, пилят фичи, держат все в порядке. По сути, без них проект бы загнулся от перегрузки. Хороший мейнтейнер экономит компании кучу ресурсов, обеспечивая качество продукта и доверие сообщества. Кроме того, мейнтейнеры мост между компанией и широким кругом контрибьюторов, они направляют новых участников, формируют культуру проекта. В идеале мейнтейнер из комьюнити становится настоящим лидером мнений по вашему продукту. Его одобрение или критика очень влияют на репутацию проекта среди разработчиков.Ценность для мейнтейнера: а что ему с этого? Разные люди находят разную выгоду. Кто‑то просто решает свои задачи. Кто‑то прокачивает навыки, строит карьеру в опенсорсе, статус мейнтейнера известного проекта дорогого стоит на рынке труда. Кто‑то получает моральное удовлетворение и уважение коллег. Иногда бывают и прямые выгоды: компания может спонсировать ключевых мейнтейнеров, донатить им, приглашать на конференции. HashiCorp, например, на заре развития своих OSS‑продуктов активно нанимала внешних контрибьюторов на работу и спонсировала их проекты. В итоге многие мейнтейнеры стали сотрудниками HashiCorp, классический win‑win, когда энтузиаст получает стабильную работу над любимым детищем, а компания лояльного эксперта.Как работать с мейнтейнерами: прежде всего, уважать и признавать их вклад. Ваша комьюнити‑стратегия должна явно давать мейнтейнерам место и голос. Простые шаги: регулярно благодарить публично, давать статус (например, звание Core Contributor, доступ в приватный Slack с командой). Прислушиваться, звать в совет проекта, собирать фидбэк перед релизами. Предоставить ресурсы: может быть, вы поможете им с инфраструктурой для тестирования, оплатите облако, пришлёте мерч. Сюда же со‑creation активности: проводите кодовые спринты вместе с мейнтейнерами, брейнштормьте фичи. Если есть возможность, выделите бюджет на гранты или part‑time контракты для особо ценных мейнтейнеров. Словом, встроите их в свою ценностную цепочку: мейнтейнеры дают проекту свой труд, а компания возвращает им поддержку и возможности. Тогда они будут еще больше заинтересованы развивать продукт, и вокруг них подтянутся другие контрибьюторы.Интеграторы (Integrators) – двигатели экосистемыПод интеграторами я понимаю всех, кто внедряет ваш продукт в реальных решениях и интегрирует его с другими системами. Это могут быть разработчики в сторонних компания, которые внедряют вашу библиотеку/платформу у себя в продакшене. Либо авторы плагинов, расширений, SDK для вашего продукта. Либо партнеры — консалтеры, системные интеграторы, делающие комплексные проекты на базе вашего решения. По сути, это продвинутые пользователи, которые не просто «пощупали» продукт, а глубоко его используют и зачастую расширяют функциональность под свои нужды.Ценность для компании: интеграторы — те самые люди, которые находят новым технологиям реальные применения. Они показывают, как продукт вписывается в разные use‑case, часто в связке с другим софтом. Например, кто‑то сделал открытый коннектор, чтобы ваша платформа работала с Kafka, и вот у вас целый новый сценарий использования для целой группы потенциальных клиентов. Интеграторы фактически расширяют рынок вашего продукта, создают вокруг него экосистему. Многие SaaS и платформы выстрелили именно благодаря сообществу, написавшему кучу плагинов и интеграций (взять ту же VS Code, тысячи расширений написаны внешними девелоперами). Кроме того, интеграторы часто становятся адвокатами продукта: раз они встроили его в своё решение, то будут убеждать и других в его ценности. Они же первыми ловят узкие места API, дают глубочайший фидбэк, могут помочь ответами на форумах. Ценность для интегратора: эти ребята обычно решают прикладные задачи. Их главный профит — рабочее решение проблемы. Если ваш продукт помог закрыть потребность — интегратор уже выиграл. Но сверх того: интегратор вкладывает время, чтобы проектировать архитектуру, писать код интеграции, и ему важно, чтобы усилия были оценены. Например, если он сделал плагин, видеть, что сообщество им пользуется, получить звезд на GitHub, благодарности, может даже клиентов. Многие интеграторы по совместительству партнеры в бизнес‑смысле. Компания‑разработчик может направлять клиентов к сертифицированным интеграторам для внедрения. Тогда интегратор зарабатывает как эксперт. Либо вы откроете свой Marketplace расширений и разрешите авторам монетизировать плагины. Либо хотя бы упомянете их кейс в блогах/на конференциях, что приносит признание. Короче, интегратору важно, чтобы его работа была востребована и приносила ему выгоду, будь то деньги, репутация или новые возможности.Как работать с интеграторами: в первую очередь, облегчить им жизнь технически. Документация, стабильные API, SDK — это база. Сделайте отличные примеры интеграции, опишите case studies, чтобы новым людям было проще повторить успех. Создайте каналы связи: технические каналы поддержки специально для интеграторов (Slack/форум с инженерами). Хороший ход запустить программу партнеров/интеграторов. Например, HashiCorp помимо сообщества юзеров имеет сеть системных интеграторов и облачных провайдеров, которые обучают и поддерживают новых клиентов. Их вовлекают: дают материалы, возможно, сертификации. Да, кстати, сертификация важный мотиватор. Если интегратор может получить статус «Certified Expert по продукту X», он с большим энтузиазмом погрузится (HashiCorp выдала уже 20k сертификатов через свою обучающую платформу). Не забудьте про витрину успехов: рассказывайте о решениях, которые делают интеграторы. Например, раздел на сайте «Built with OurProduct», чтобы все видели, какие крутые штуки делают люди. И, конечно, обратная связь: регулярно спрашивайте интеграторов, что улучшить. Может, проведите совместный co‑creation спринт: соберите самых активных внедренцев и ваших инженеров, и за пару дней допилите вместе интеграцию с популярным инструментом — они принесут экспертизу домена, вы — ресурсы разработки. Эдьютейтеры (Edutainers) – евангелисты и учителяТретья важнейшая группа — люди, которые обучают и вдохновляют остальных пользователей. Назовем их условно эдьютейтеры (от education + entertainer): авторы статей, туториалов, докладчики на митапах, ютуберы, создатели курсов. Про них часто говорят «Developer Advocates», «Evangelists», но мы сейчас имеем в виду внешних энтузиастов, не штатных деврелов компании. В каждом активном сообществе находятся личности, которые обожают рассказывать другим, как пользоваться технологией, делиться своим опытом, упрощать сложное.Ценность для компании: эти люди — настоящие мультипликаторы знаний. Благодаря им даже небольшой проект может получать непропорционально широкое внимание. Написал кто‑то толковый гайд на медиуме и сотни новых разработчиков узнали о вашем инструменте. Записал обзор на YouTube, тысячи посмотрели и заинтересовались. Контент от сообщества бьет все рекорды доверия: он независимый, «от такого же разработчика, как я». А если у вас ещё и своя площадка для контента… DigitalOcean выезжает на том, что тысячи авторов публикуют обучающие статьи на их платформе, кучу туториалов создали эту экосистему знаний вокруг продукта. Это работает лучше любой рекламы и SEO: люди приходят за решением проблемы и попутно узнают о вашем бренде. Помимо привлечения новых пользователей, эдьютейтеры очень помогают с onboarding, ускоряют активацию новичков. Хороший видеоурок или примеры от опытного пользователя сокращают время, за которое начинающий разработчик получит первый результат. А чем быстрее он увидит пользу, тем больше шанс, что останется с продуктом. В итоге эдьютейтеры снижают нагрузку на вашу команду и масштабируют охват аудитории.Ценность для эдьютейтера: многие делают это из страсти, нравится им делиться. Но обычно есть и расчёт: создание контента добавляет личного бренда. Стать известным спикером, набрать подписчиков, получить статус эксперта — всё это ценно для карьеры. Плюс банальное человеческое спасибо: когда твоя статья собирает апвоуты и благодарности, это мотивирует. Некоторые получают и материальное вознаграждение. Кто‑то монетизирует YouTube‑канал или платные курсы. В любом случае, эдьютейтеры ищут аудиторию и признание. И им гораздо приятнее сотрудничать с компанией, которая их ценит, чем делать все втуне.Как работать с эдьютейтерами: находить и вдохновлять их. Выявляйте активных авторов в сообществе: кто пишет блоги, отвечает на Stack Overflow, делает демо‑проекты. Начните с простого: репостните их статью в своих соцсетях, похвалите в рассылке. Дайте почувствовать, что компания видит их вклад. Далее можно формализовать. Многие запускают программы амбассадоров. Например, HashiCorp запустила HashiCorp Ambassador Program, сейчас там 100+ человек со всего мира. Отбор по критерию: делится знаниями, помогает другим, проявляет экспертизу. Амбассадоры получают официальный статус, мерч, а главное эксклюзивную инфу: брифинги о новых релизах, превью фич, закрытые сессии с командой. Это отличный стимул для эдьютейтеров, они чувствуют себя инсайдерами, первыми узнают новости и могут готовить контент заранее. Плюс им просто приятно быть в клубе причастных. Ваша задача — сделать так, чтобы создавать контент по вашему продукту было легко и выгодно. Предоставьте материалы: готовые презентации, демо‑проекты, библиотеку изображений. Запустите конкурс статей или хакатон по созданию туториалов, с призами и публикацией лучших работ. Организуйте мероприятия, где эдьютейтеры смогут выступить: митапы, вебинары. Поощряйте их рост: может, кто‑то из них созреет стать официальным Developer Advocate в вашей команде — прекрасный вариант рекрутинга из сообщества.Подытожим сегментацию: в сообществе есть разные роли, и у каждой своя цепочка ценности. Мейнтейнеры улучшают продукт и получают поддержку и признание. Интеграторы расширяют сферу применения продукта и получают решения для своих задач (плюс статус экспертов и, возможно, бизнес‑возможности). Эдьютейтеры распространяют знания и получают аудиторию и благодарность. Эти цепочки переплетены: обучающие статьи привлекают новых интеграторов, хорошие интеграции разгружают мейнтейнеров от просьб о фичах, мейнтейнеры дают материал для новых статей и так далее Наша задача как DevRel‑стратегов — поддерживать баланс, чтобы каждый тип участников чувствовал: вклад окупается, ему есть смысл дальше участвовать. Для этого пригодятся специальные методики, о которых далее.Методы DevRel 2.0: value chain mapping, persona-jobs, co-creationКогда мы поняли, кто наше сообщество и что ценно для разных людей, стоит применить несколько инструментов. Расскажу о трех: community value chain mapping, persona‑jobs и co‑creation спринты. Community Value Chain Mapping – карта ценности сообществаЗвучит мудрено, но идея простая: картирование цепочки ценности означает явным образом расписать, как каждая роль в сообществе создает ценность и получает её обратно. Фактически, мы частично это сделали выше в описании ролей. Зачем нужна такая карта? Чтобы убедиться, что нигде не образуется разрыв. Если обнаружим, что какая‑то группа дает ценности больше, чем получает (или наоборот — много получает, но мало отдает), можно внести коррективы в программу работы с комьюнити.Как это сделать на практике: возьмите роли (персоны) — мейнтейнер, интегратор, эдьютейтер. Для каждой нарисуйте две колонки: «Что он дает проекту» и «Что проект (компания) дает ему». Подробно перечислите пункты. Например:Мейнтейнер дает: время на разработку, ревью кода, отвечает на issues, направляет архитектуру. Получает: влияние на развитие продукта, благодарность сообщества, поддержку ресурсами (в идеале финансами), повышение статуса.Интегратор дает: новые кейсы использования, обратную связь, готовые интеграции/плагины для других пользователей, экспертные ответы новичкам. Получает: решение своих бизнес‑задач, улучшение продукта под свои нужды, признание как эксперта, возможно клиентов/доход через партнерство.Эдьютейтер дает: контент (статьи, доклады, примеры кода), обучает новых юзеров, увеличивает охват аудитории, снижает нагрузку на техподдержку. Получает: популярность в сообществе, прямую благодарность от аудитории, эксклюзивный доступ к информации, мерч/призы, карьерные возможности.Если выяснится, что интеграторы у нас очень ценны, а мы им почти ничего не предлагаем, надо думать, как увеличить для них отдачу. Или наоборот, мы всем дарим мерч и даём статус амбассадора, а толку от человека ноль, надо условия изменить, требовать минимальный вклад. Эта же карта ценности помогает обосновать руководству бюджет на комьюнити‑программы: вы показываете, какую работу выполняет каждая группа пользователей, и что нужно вложить, чтобы это продолжалось. По сути, value chain mapping переводит мягкое «строим отношения» в понятный бизнес‑язык «мы инвестируем X и получаем Y ценности». В open‑source мире такой подход уже применяется для оценки устойчивости проектов, упоминается создание «симбиотической цепочки ценности», где участники взаимно выгодно связаны.Рекомендую пересматривать карту ценности хотя бы раз в год. По мере роста сообщества могут появляться новые роли (например, отдельным сегментом выделятся дизайнеры или студенты), добавляйте их в карту. Так вы не упустите новую аудиторию. Кроме того, на основе этой карты удобно строить метрики: если знаете, что ценность дает, скажем, количество написанных комьюнити‑статей, можете отслеживать этот показатель и целенаправленно его растить инициативами.Persona-Jobs – объединяем портреты и задачиТеперь о методе persona‑jobs. Он объединяет классический подход персонажей (persona) с фреймворком Jobs‑to‑be‑Done (JTBD, «работы, которые хотят выполнить пользователи»). Идея пришла из продуктового маркетинга: чтобы лучше понять потребности, полезно описать не только «кто наш пользователь», но и «какую работу он нанимает наш продукт выполнить». В контексте DevRel и сообществ это означает: для каждой ключевой персоны из нашего комьюнити мы прописываем её конкретные задачи/цели, с которыми она к нам приходит, и проблемы, мешающие их достичь.Мы берем нашу персону — например, «Интегратор Игорь» (можно даже придать образ: архитектор 35 лет, в большой компании, отвечает за внедрение технологий). Выписываем, какие у него Jobs‑to‑be‑Done относительно нашего продукта. Допустим интегрировать библиотеку в существующую систему без простоев; убедиться в безопасности решения для продакшена; обучить команду пользоваться новым инструментом; и тому подобное Для каждой такой задачи укажем, что ему помогает или мешает. Возможно, у Игоря боль в нехватке документации по масштабированию, или бюрократия на согласование новых технологий. Проделав это, мы увидим, как лучше помочь интеграторам: например, сделать whitepaper «Как убедить менеджмент внедрить OurProduct» или добавить раздел доки про безопасность. Точно так же делаем для мейнтейнера Марины (job: привлечь новых контрибьюторов, автоматизировать релизы, и так далее) и эдьютейтера Евгения (job: быстро разбираться в новых фичах, получать благодарную аудиторию, иметь доступ к примерам из реальной практики, и так далее).Персона сама по себе фокусируется на кто наш пользователь, каков его контекст и мотивы. А Jobs‑to‑be‑Done фокусируется на что пользователь пытается сделать и почему. Вместе они позволяют выйти за рамки стереотипов. Например, если смотреть только на персону «DevOps инженер, 5 лет опыта, такого‑то возраста», мы можем упустить, что конкретно ему нужно от нашего сообщества. А если смотреть только на абстрактный «job: получить ответ на вопрос по настройке CI/CD», упустим контекст, новичок это или эксперт, как он предпочитает учиться (читать, смотреть видео, задавать в чате?). Persona‑JTBD гибрид учитывает и то, и другое.В итоге получаем очень интересные инсайты. Например, выясняется, что молодые разработчики (персона: Студент Саша) хотят прокачать навыки (job: найти pet‑проект и ментора). Тогда вам стоит запустить для них программу стажировок в опенсорс‑проекте сообщества или выделить «good first issues». А, скажем, Solution‑архитекторы в компаниях (персона: Архитектор Антон) хотят делиться экспертизой (job: выступать на конференциях, чтобы признали). Значит, их можно вовлекать модераторами вебинаров, авторами гостевых постов, то есть давать площадку для самореализации в рамках вашего комьюнити. Применять persona‑jobs можно на этапе планирования DevRel‑инициатив. Собираетесь сделать хакатон, подумайте, для каких персон и каких «jobs» он вообще нужен. Закрывает ли он задачу мейнтейнера (например, собрать новых контрибьюторов)? Или нацелен на эдьютейтеров (дать им показать свой проект)? Если не понимаете, для кого стараетесь, возможно, мероприятие получится пустым. А когда явно видишь: для такой‑то персоны решаем такую‑то задачу, успех измеряется легко. Co-creation спринты – совместное творчество с комьюнитиПоследний метод — co‑creation спринты, то бишь совместные короткие циклы разработки/творчества с участием сообщества. Идея навеяна дизайн‑спринтами и хакатонами, но с упором на совместную работу команды продукта и внешних участников. Если перевести дословно — спринты с со‑творчеством. Это могут быть разные форматы:Community Hackathon, классика: вы объявляете тему (например, расширения для вашего API), собираете команды из внешних разработчиков и своих менторов, и за выходные они пилят готовые проекты. В отличие от обычного хакатона, здесь важно участие ваших инженеров бок о бок с комьюнити — это ломает барьеры «разработчик vs компания». Все становятся коллегами на пару дней.Documentation Sprint, узконаправленный спринт, когда собираются техписатели, эдьютейтеры, разработчики и дружно улучшают документацию или обучающие материалы. Feedback/Design Sprint — это когда сообщество участвует в проектировании новых фич. Например, у вас назрел большой релиз, проведите двухдневный спринт с наиболее вовлеченными пользователями и мейнтейнерами. В первый день соберите боль и хотелки (что нужно улучшить, какие use‑case не покрыты), во второй совместно приоритизируйте и набросайте макеты решений. Можно даже прототипировать вместе. Content Sprint, похож на докатон, но шире по форматам. Например, объявляете «Writing Sprint» на неделю: каждый день даёте тему (пн — установка продукта, вт — кейсы интеграции, ср — разбор ошибок и тому подобное), участники пишут небольшие заметки или снимают скринкасты. В конце недели готов целый пакет контента от сообщества для сообщества.Ключевое в co‑creation спринтах это не соревнование (как часто бывает в хакатонах), а именно сотрудничество. Здесь уместно убрать соревновательность и делать упор на общий результат. Как в Open Source, все коммитят в один проект. Роли можно распределять: кто‑то кодит, кто‑то тестирует, кто‑то пишет документацию. Это очень сплочает. Люди чувствуют: «Мы вместе сделали что‑то крутое, и наш вклад встроен прямо в продукт/доку/базу знаний».Эффект от таких спринтов потрясающий. Во‑первых, куча полезных артефактов, новые фичи, улучшенные доки, примеры, плагины. Во‑вторых, вы выращиваете новых лидеров: тот, кто блеснул на спринте идеей или решением, потом наверняка станет еще активнее в сообществе. В‑третьих, это привлекает внимание более широкой аудитории, результаты спринта можно анонсить, хвастаться, мол вот, сообщество вместе с нами сделало релиз. Для участников co‑creation это тоже реклама и признание.Планируя co‑creation sprint, четко обозначьте цель и формат. Люди должны понимать, что на выходе. Желательно ограничить время (не более недели, а лучше 1–3 дня). И обязательно отпразднуйте результаты, финальный демо‑день, список всех авторов, сертификаты участникам, сувениры, словом, отметьте вклад каждого. Тогда в следующий раз желающих будет ещё больше.Конечно, полностью заменить классический маркетинг на одну только работу с сообществом не получится. Да и не нужно. Правильный шаг — интегрировать GTC и GTM. Например, KPI маркетинга и DevRel должны быть взаимосвязаны. Сообща решайте, как перевести рост активности в сообществе в бизнес‑метрики: «community members → leads → клиенты». Куда приятнее иметь дело с технологией, вокруг которой есть дружное сообщество, где твой вклад ценят, где можно учиться и расти вместе. Такие продукты мы выбираем сердцем, и остаёмся с ними надолго. Так что стройте сообщества, а не только воронки продаж. Если у вас есть опыт внедрения подобных стратегий, расскажите в комментариях.Чтобы превратить GTC из концепции в работающий процесс, присмотритесь к курсу OTUS «DevRel». В программе — EVP/EJM, стратегия HR-бренда, метрики комьюнити и отчётность, контент- и event-практики, работа с амбассадорами и внешними сообществами на реальных кейсах. Если хотите понять формат обучения — записывайтесь на бесплатные демо-уроки от преподавателей курса:20 ноября: «DevRel и HR на практике: формула успешных мероприятий для разработчиков». Записаться25 ноября: «DevRel и HR-метрики: какие показатели будут важны стейкхолдерам в 2026 году». ЗаписатьсяТеги:devrelgtm.gtcGo-to-CommunityGo-to-Marketсообщество разработчиковворонка сообществаметрики комьюнитиHR-брендХабы:Блог компании OTUSDeveloper RelationsУправление продуктом",181,0,0,17 мин,https://habr.com/ru/companies/otus/articles/961436/,26000,3438,3
Без дизассемблера: как предварительный анализ документа GOFFEE раскрыл всю цепочку заражения,remadev,2025-11-13T08:18:22.000Z,['Информационная безопасность *'],"remadev 3 часа назадБез дизассемблера: как предварительный анализ документа GOFFEE раскрыл всю цепочку зараженияУровень сложностиПростойВремя на прочтение5 минКоличество просмотров238Информационная безопасность * Из песочницыКиберугрозы постоянно эволюционируют, и для эффективного противодействия важно понимать тактики и инструменты злоумышленников. Группировка GOFFEE, также известная как Paper Werewolf, представляет собой яркий пример такой угрозы:    АспектОписаниеПериод активностиС начала 2022 года по настоящее время.Основные целиОрганизации на территории Российской Федерации, в частности: медиа- и   телекоммуникации, строительные, государственные структуры, энергетические   компании.Основной вектор атакЦелевой фишинг (Spear-phishing) с   вредоносными вложениями.Ключевые инструментыPowerModul, PowerTaskel, модифицированный файл explorer.exe, Owowa (вредоносный   модуль IIS).GOFFEE демонстрирует высокую адаптивность, постоянно обновляя свои схемы заражения. Исследователи отмечают, что во второй половине 2024 года группировка активно внедряла свой имплант PowerModul, написанный на PowerShell, который способен загружать и выполнять дополнительные скрипты с командного сервера. Наряду с этим, GOFFEE использует инструменты для похищения данных с USB-накопителей, такие как FlashFileGrabber, и даже распространяет свою нагрузку через зараженные съемные носители. Несколько атак с использованием Zero-day уязвимостей было замечено летом 2025 года.    В этом материале мы разберем один из вредоносных документов этой группировки без применения глубоких знаний в реверс-инжиниринге и убедимся, что большую часть цепочки заражения можно восстановить, не прибегая к сложным инструментам вроде дизассемблера или отладчика. Меня зовут Александр, я вирусный аналитик и реверс инженер.Приступим! ДисклеймерСразу хочу отметить, что все нижеперечисленные действия необходимо выполнять в изолированной среде.    Этап 1. Анализ входных данных    Всё начинается с письма с вложением. По легенде аналитики SOC обнаружили, как сотрудникам компании был направлен следующий документ:    Прежде, чем открывать его и смотреть на поведение ОС, можно поискать информацию в открытых источниках. Например, по вычисленной хэш-сумме файла можно проверить, встречался ли он в публичных онлайн-песочницах. Для этого отлично подходит утилита HashMyFiles. После открытия файла в утилите, можно увидеть следующую картину:    Полученный хэш SHA-256: 8f7c38804d63d89a83d11c5c112850febf6d5e302f63f367860e78ac72f09e4c    Поискав этот хэш в VirusTotal, мы выяснили, что документ уже числится во многих антивирусных базах:    Подсказка «Code Insights» помогает догадаться, что внутри документа зашит какой-то макрос. Но что делать, если информации о вредоносном документе ещё не появилось? Правильно, анализировать вручную! Для анализа метаданных документа можно воспользоваться утилитой ExifTool. В выводе видим много полезной информации:Следующим этапом с помощью скрипта oleid определим структуру вредоносного образца. Как видно из вывода работы скрипта, исследуемый файл содержит макросы VBA:    Далее с помощью утилиты olevba можно получить сам макрос с целью его дальнейшего глубокого изучения:    На данном этапе мы определили, что исследуемый файл представляет собой  документ (.docx) Misrosoft Word, содержащий в себе Macros VBA. Извлекли макрос с помощью утилиты olevba. Следующим шагом при глубоком анализе нам необходимо использовать инструменты для просмотра и отладки VBS-макросов (что в рамках текущей статьи мы делать не будем).Также не стоит забывать, что файл DOCX — это не монолитный файл, а, по сути, архив в формате ZIP, содержащий набор XML-файлов и других ресурсов, которые вместе описывают содержимое, стили, настройки и медиа-файлы документа. Его можно разархивировать обычным 7-Zip. Чуть более подробно про структуру .docx файла можно почитать здесь. Нам же интересен единственный файл – document.xml - ""cердце"" документа. Здесь хранится весь текстовый контент в виде XML-тегов. Абзацы, таблицы, пробелы и сам текст. В самом конце этого файла можно обнаружить странную сигнатуру, с закодированной в Base64 текст, разделенный ключевым словом «Checksum»:Декодировав строки, мы обнаружим следующие интересные зацепки: В первом случае код представляет собой обфусцированный вредоносный скрипт, создающий файлы UserCacheHelper.lnk.js и UserCache.ini в директории %USERPROFILE%. Также для создания скрытых процессов скрипт использует WMI.    В случае с декодированной последовательностью после «Checksum», можно наблюдать ещё одну Base64-закодированную строку, и, судя по переменной $code, в ней содержится какая-то исполняемая часть, и здесь мы попали прямо в точку:Судя по всему, этот код создает постоянное соединение с командным сервером для удаленного управления зараженной системой.Этап 2. Поведенческий анализ    Пришло время запустить вредоносный образец, чтобы посмотреть в динамике на его поведение. Для этого будем использовать утилиты Process Monitor, System Informer и Fakenet-NG. Запустим их. После запуска файла видим закодированный текст:Чтобы отобразить весь текст, MS Word предлагает нажать кнопку «Enable Content». System Informer отобразил появившийся процесс Winword.exe и его PID = 2264. Поставим фильтр в Process Monitor по этому PID и начнем запись событий. Далее отобразим весь контент:  Process Monitor сразу же отобразил следующие события:Видим создание двух файлов в директории %USERPROFILE% - UserCache.ini.hta и UserCache.ini. В эти файлы записана как раз та нагрузка, которую мы обнаружили в ходе анализа document.xml. Также в каталоге %USERPROFILE% мы можем наблюдать файл UserCacheHelper.lnk.js:    Этот js файл маскируется под ярлык (на это указывает расширение .lnk). Внутри можем наблюдать ещё один обфусцированный js, который, как несложно догадаться, выполняет скрытый запуск файла UserCache.ini через WMI.    Зачем WMI?С помощью WMI вредонос запускается вне текущего дерева процесса, что значительно усложняет поиск нужного процесса. Также опция ShowWindow = 0 запускает процесс в «скрытом» окне    Теперь проверим как вредонос закрепляется в системе с помощью AutorunsFakenet-NG также зафиксировал подозрительную активность на адресе 45.84.1[.]150 – правда подключиться к этому адресу не удастся:    По исследованиям специалистов из Лаборатории Касперского ответы с этого адреса содержали скрипты для управления зараженным устройством.Этап 3. Собираем всё вместеПроведя поэтапный анализ, мы смогли восстановить полную цепочку заражения, используя исключительно базовые методики предварительного анализа — без дизассемблера и отладчика. Давайте соберём все элементы пазла в единую картину атаки:Цепочка компрометации группировки GOFFEE:Фишинговое письмо → Документ maldoc.doc с социальной инженерией (имитация официального письма) T1566, T1589;Активация макроса → Пользователь нажимает ""Enable Content"" T1059.005; Извлечение нагрузки → Макрос декодирует Base64-строки из document.xml T1027 T1059.005;Создание файлов → В %USERPROFILE% появляются:UserCache.ini (PowerShell-лоадер) T1059.001;UserCache.ini.hta (исполнитель);UserCacheHelper.lnk.js (обфусцированный JavaScript) T1059.007Закрепление → HTA-файл регистрируется в автозагрузке реестра T1547.001;Запуск импланта → Через цепочку HTA → JS → WMI выполняется PowerModul T1059.003, T1047;Установление связи → подключение к C2: http[:]//45.84.1[.]150:80 T1071.001.Теги:goffeepowermodulmalware analysisХабы:Информационная безопасность",238,0,0,5 мин,https://habr.com/ru/articles/965970/,7422,898,1
Забудьте про print(): Современное и красивое логирование в Python с помощью Loguru,enamored_poc,2025-11-13T10:19:30.000Z,['Python *'],"enamored_poc 48 минут назадЗабудьте про print(): Современное и красивое логирование в Python с помощью LoguruУровень сложностиСреднийВремя на прочтение14 минКоличество просмотров330Python * ОбзорВведение: Боль и страдания от print() и стандартного loggingЕсли вы пишете на Python, скорее всего, ваша карьера разработчика начиналась с одной простой, но незаменимой команды — print(). Нужно проверить значение переменной? print(my_variable). Хотите убедиться, что функция вообще вызвалась? print(""Я внутри функции!""). Этот метод прост, интуитивно понятен и кажется верным другом в мире отладки.Но дружба эта длится ровно до первого серьезного проекта. Внезапно оказывается, что ваш терминал завален десятками отладочных сообщений, и вы уже не понимаете, какое из них к чему относится. Вы начинаете писать print(""--- HERE ---""), чтобы хоть как-то ориентироваться в этом хаосе. А когда приходит время выкатывать код в продакшен, вы судорожно ищете и комментируете все свои print(), надеясь не пропустить ни одного.В этот момент опытные коллеги (или статьи в интернете) говорят вам: ""Для этого есть стандартный модуль logging!"". И они правы. logging — это мощный, гибкий и правильный инструмент. Но давайте будем честны, его настройка часто напоминает бюрократическую процедуру. Чтобы просто начать писать логи в файл с указанием времени и уровня, нужно написать что-то вроде этого:import logging

# Настройка... снова и снова
logging.basicConfig(
    level=logging.INFO,
    format=""%(asctime)s - %(levelname)s - %(message)s"",
    handlers=[
        logging.FileHandler(""debug.log""),
        logging.StreamHandler()
    ]
)

logging.info(""Пользователь вошел в систему."")
logging.warning(""Не удалось найти файл конфигурации."")
Это работает, но это громоздко. Пять-шесть строк кода только для того, чтобы начать. А если вам понадобится ротация файлов, кастомные форматы или что-то посложнее? Конфигурация становится еще запутаннее.Именно в этот момент на сцену выходит Loguru. Эта библиотека была создана для того, чтобы избавить нас от страданий. Она берет лучшее из двух миров: феноменальную простоту print() и всю мощь взрослого логирования, но без лишнего ""шума"" и сложной настройки.Что, если я скажу вам, что цветное логирование в консоль, запись в файл с автоматической ротацией и сжатием, а также невероятно удобная отладка исключений могут быть настроены всего одной строкой кода?В этой статье мы раз и навсегда забудем про print() для отладки и посмотрим, как Loguru может сделать ваше логирование не только мощным, но и по-настоящему красивым и удобным.Часть 1: Первое знакомство с Loguru — магия ""из коробки""��так, мы оставили позади боль от print() и сложность logging. Давайте посмотрим, что предлагает Loguru и почему его называют ""логированием для удовольствия"".Установка: один шаг до магииКак и положено современному Python-пакету, установка предельно проста и выполняется одной командой в терминале:pip install loguru
Всё, библиотека готова к работе. Никаких зависимостей, никаких сложных настроек окружения.""Hello, Loguru!"": начинаем за 5 секундПомните, сколько кода нужно было для базовой настройки logging? Забудьте. С Loguru вы просто импортируете готовый к использованию объект logger и начинаете его использовать.from loguru import logger

logger.info(""Hello, Loguru!"")
logger.warning(""Это предупреждение, будьте внимательны."")
logger.debug(""Это сообщение не будет видно по умолчанию."")
Запустите этот код, и вы увидите в консоли что-то вроде этого:2025-11-13 13:00:00.000 | INFO     | __main__:__main__:3 - Hello, Loguru!
2025-11-13 13:00:00.000 | WARNING  | __main__:__main__:4 - Это предупреждение, будьте внимательны.
Обратите внимание: debug-сообщение не появилось. По умолчанию Loguru, как и любая серьезная система логирования, показывает сообщения уровня INFO и выше. Это правильное поведение, которое избавляет от лишнего шума в консоли.Разбираем стандартный вывод: всё, что нужно, и ничего лишнегоДавайте внимательно посмотрим на первую строку вывода:2025-11-13 13:00:00.000 | INFO | __main__:__main__:3 - Hello, Loguru!Даже без единой строчки конфигурации Loguru сразу предоставляет нам богатый контекст:Дата и время: 2025-11-13 13:00:00.000 — точная временная метка, когда произошло событие. Больше не нужно гадать, к какому моменту времени относится print.Уровень лога: INFO — показывает важность сообщения. WARNING говорит о потенциальной проблеме, ERROR — о серьезной ошибке.Местоположение: __main__:__main__:3 — это самая полезная часть. Loguru автоматически указывает имя файла, функцию и номер строки, откуда был сделан вызов. Это невероятно ускоряет отладку!Сообщение: Hello, Loguru! — непосредственно то, что мы хотели записать.Магия цвета: прощай, монотонная консоль!Если вы запустите приведенный выше код в терминале, который поддерживает цвета (а сегодня это делают почти все), вы увидите еще одну приятную особенность: вывод будет раскрашен!(Примечание: в статье можно вставить реальный скриншот)INFO обычным белым.WARNING — заметным желтым.ERROR и CRITICAL — тревожно-красным.Это не просто украшение. Цвета помогают мгновенно сканировать лог глазами и находить проблемы, не вчитываясь в каждое слово. Это та самая ""красота"" и ""удобство"", которых так не хватает стандартным инструментам.Всего один import — и мы уже получили цветное, информативное и правильно структурированное логирование. Это и есть магия Loguru ""из коробки"". А ведь мы еще даже не начали знакомиться с его основными возможностямиЧасть 2: Основные возможности на практических примерахМы убедились, что Loguru прекрасен ""из коробки"". Но его настоящая сила раскрывается, когда мы начинаем использовать его основ��ые функции. И здесь Loguru остается верен своему принципу: максимум пользы при минимуме кода.Уровни логирования: управляем потоком информацииКак мы уже видели, Loguru поддерживает стандартные уровни важности сообщений:TRACE (самый подробный)DEBUGINFOSUCCESS (приятное дополнение для успешных операций)WARNINGERRORCRITICALПо умолчанию в консоль выводятся сообщения от INFO и выше. Это легко изменить. Например, чтобы включить DEBUG-сообщения, нужно перенастроить стандартный обработчик (sink), указав новый уровень:from loguru import logger

# Удаляем стандартный обработчик и добавляем новый с уровнем DEBUG
logger.remove()
logger.add(sys.stderr, level=""DEBUG"") # sys.stderr - это стандартный поток ошибок (консоль)

logger.debug(""Теперь это сообщение будет видно!"")
logger.info(""И это, конечно, тоже."")
Запись логов в файл: одна строка, чтобы править всемиЭто одна из самых мощных и востребованных функций. Забудьте про FileHandler и его сложную настройку. С Loguru все делается одной командой — logger.add().Просто записать в файл:from loguru import logger

logger.add(""my_app.log"")

logger.info(""Это сообщение попадет и в консоль, и в файл my_app.log"")
Но настоящая магия начинается, когда мы добавляем параметры в эту команду.Автоматическая ротация файлов (Rotation)Ваш лог-файл не будет расти бесконечно, занимая все место на диске. Loguru может автоматически создавать новый файл, когда старый достигает определенного размера или проходит определенное время.# Создавать новый файл, как только текущий достигнет 500 MB
logger.add(""big_file.log"", rotation=""500 MB"")

# Создавать новый файл каждую неделю
logger.add(""weekly_log.log"", rotation=""1 week"")

# Создавать новый файл каждый день в полночь
logger.add(""daily_log.log"", rotation=""00:00"")
Автоматическая очистка старых логов (Retention)Чтобы старые логи не копились вечно, можно указать, как долго их хранить.# Хранить логи за последние 10 дней
logger.add(""cleaned_log.log"", retention=""10 days"")
Автоматическое сжатие (Compression)Loguru может даже самостоятельно архивировать старые лог-файлы, чтобы они занимали меньше места.# При ротации сжимать старые файлы в zip-архив
logger.add(""compressed_log.log"", rotation=""10 MB"", compression=""zip"")
Давайте соберем все вместе в один мощный пример для реального проекта:from loguru import logger

# Настраиваем логирование для продакшена:
# - Пишем в файл `prod.log`
# - Уровень - INFO и выше
# - Ротация при достижении 10 MB
# - Храним файлы 1 месяц
# - Сжимаем старые логи в .gz
logger.add(""prod.log"", level=""INFO"", rotation=""10 MB"", retention=""1 month"", compression=""gz"")

logger.info(""Система запущена и готова к работе!"")
Всего одна строка кода заменяет десятки строк конфигурации стандартного logging. Это невероятно удобно.Форматирование в стиле f-stringЕще одна мелочь, которая делает жизнь разработчика проще. Loguru использует для форматирования строк фигурные скобки, как в f-strings, что гораздо читабельнее и привычнее, чем старый %-стиль.user_id = 123
status = ""success""

# Привычный и читаемый синтаксис
logger.info(""Аутентификация для пользователя {id} прошла со статусом: {status}"", id=user_id, status=status)
Вывод будет таким: ... | INFO | ... - Аутентификация для пользователя 123 прошла со статусом: successМы рассмотрели базовые, но самые часто используемые возможности Loguru. Уже на этом этапе видно, насколько он упрощает и ускоряет разработку. Но это еще не все — в следующей части мы погрузимся в продвинутые техники, которые сделают вашу отладку по-настоящему волшебной.Часть 3: Продвинутые техники для настоящих профиМы освоили основы, которые уже делают работу с логами приятнее. Но Loguru был создан не только для удобства — он предоставляет мощнейшие инструменты для отладки, которые могут сэкономить вам часы поиска ошибок. Давайте рассмотрим функции, которые отличают новичка от профессионала.1. Идеальная отладка исключенийЭто, пожалуй, главная ""киллер-фича"" Loguru. Стандартный трейсбэк в Python информативен, но часто хочется видеть значения переменных, которые привели к ошибке.Логирование внутри try...exceptВо-первых, вы можете использовать logger.exception(), который автоматически захватит и красиво отформатирует информацию о последнем исключении.from loguru import logger

try:
    result = 10 / 0
except ZeroDivisionError:
    logger.exception(""Произошла ошибка при вычислении."")
Вывод будет содержать не только ваше сообщение, но и полный, аккуратно отформатированный трейсбэк ошибки.Декоратор @logger.catch — ваша секретная кнопка ""Найти ошибку""Но настоящий прорыв — это декоратор @logger.catch. Вы можете просто ""обернуть"" в него любую функцию, и Loguru автоматически поймает любое исключение, которое в ней произойдет, и выведет самый подробный отчет из возможных.Посмотрите на этот пример:from loguru import logger

@logger.catch
def calculate(a, b, c):
    return a / b + c

calculate(10, 0, 5) # Эта строка вызовет ошибку
Без @logger.catch вы бы получили стандартный трейсбэк. Но с декоратором Loguru выведет в консоль нечто гораздо более ценное:> 2025-11-13 13:00:00.000 | ERROR    | __main__:calculate:5 - An error has been caught in function 'calculate', process 'MainProcess' (1234), thread 'MainThread' (5678):
Traceback (most recent call last):
...
  File ""my_script.py"", line 6, in <module>
    calculate(10, 0, 5)
    │         │  │  └ c = 5
    │         │  └ b = 0
    │         └ a = 10
  File ""my_script.py"", line 5, in calculate
    return a / b + c
           ──┘ └───
            │   └ 0
            └ 10

ZeroDivisionError: division by zero
Посмотрите внимательно: Loguru не просто показал, где произошла ошибка, он показал значения всех аргументов (a = 10, b = 0, c = 5) в момент падения! Это бесценная информация для отладки, которую вы получаете, добавив всего одну строку кода (@logger.catch).2. Кастомизация формата логовСтандартный формат хорош, но иногда его нужно адаптировать под требования проекта. Это легко сделать с помощью параметра format в logger.add().Вы можете собрать свой формат из готовых ""кирпичиков"":from loguru import logger
import sys

# Удаляем стандартный обработчик, чтобы не было дублирования
logger.remove()

# Создаем свой, очень подробный формат
custom_format = (
    ""<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | ""
    ""<level>{level: <8}</level> | ""
    ""<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>""
)

logger.add(sys.stderr, format=custom_format)

logger.info(""Лог в кастомном формате."")
Здесь мы добавили теги для цвета (<green>, <level>) и указали новые поля, например, {function} и {name}. Полный список доступных полей есть в официальной документации Loguru.3. Структурированное логирование (JSON)В современных системах логи часто собираются и анализируются машинами (например, в ELK Stack, Graylog или Datadog). Для этого они должны быть в структурированном формате, чаще всего — JSON.С Loguru это делается элементарно — добавлением одного параметра: serialize=True.from loguru import logger

logger.add(""structured_data.log"", serialize=True)

user_data = {""id"": 123, ""name"": ""John Doe""}
logger.info(""Пользователь {user} обновил профиль"", user=user_data)
В файле structured_data.log появится запись в формате JSON, идеально подходящая для парсинга:{
    ""text"": ""Пользователь {'id': 123, 'name': 'John Doe'} обновил профиль\n"",
    ""record"": {
        ""elapsed"": {""repr"": ""0:00:00.001000"", ""seconds"": 0.001},
        ""exception"": null,
        ""extra"": {},
        ""file"": {""name"": ""my_script.py"", ""path"": ""/path/to/my_script.py""},
        ""function"": ""<module>"",
        ""level"": {""icon"": ""ℹ️"", ""name"": ""INFO"", ""no"": 20},
        ""line"": 6,
        ""message"": ""Пользователь {'id': 123, 'name': 'John Doe'} обновил профиль"",
        ""name"": ""__main__"",
        ""process"": {""id"": 1234, ""name"": ""MainProcess""},
        ""thread"": {""id"": 5678, ""name"": ""MainThread""},
        ""time"": {""repr"": ""2025-11-13T13:00:00.000000+03:00"", ""timestamp"": 1762989600.0}
    }
}
4. Добавление контекста с помощью bind()В сложных приложениях, например, в веб-сервисах, важно отслеживать цепочку событий, относящихся к одному запросу. Loguru позволяет ""привязать"" контекст к логгеру, и этот контекст будет добавляться во все последующие сообщения.from loguru import logger
import uuid

# Создаем логгер с привязанным ID запроса
request_id = str(uuid.uuid4())
context_logger = logger.bind(request_id=request_id)

context_logger.info(""Получен новый запрос."")
# ... какой-то код
context_logger.info(""Данные из базы успешно получены."")
# ... еще код
context_logger.warning(""Внешний API ответил с задержкой."")
В каждом из этих сообщений будет автоматически добавлено поле request_id, что позволит вам в системе сбора логов легко отфильтровать все события, связанные с одним конкретным запросом.Эти продвинутые техники превращают Loguru из простого инструмента для записи сообщений в мощный фреймворк для отладки и мониторинга, который остается таким же простым в использовании.Часть 4: Loguru в реальном проекте: Советы и лучшие практикиМы изучили мощные функции Loguru, но как грамотно внедрить его в полноценное приложение? Просто импортировать logger в каждом файле — это начало, но для создания надежной и масштабируемой системы стоит учесть несколько моментов.1. Создайте централизованную конфигурациюЧтобы не настраивать логгер в разных частях вашего проекта, лучше всего создать один модуль, отвечающий за всю конфигурацию логирования. Это сделает ваши настройки последовательными и легко изменяемыми.Создайте файл, например, app/logging_config.py:# app/logging_config.py
import sys
from loguru import logger

def setup_logging():
    """"""
    Настраивает логгер для всего приложения.
    """"""
    # Удаляем стандартный обработчик, чтобы избежать дублирования
    logger.remove()

    # Добавляем обработчик для вывода в консоль (для разработки)
    # Уровень DEBUG, цветной вывод
    logger.add(
        sys.stderr,
        level=""DEBUG"",
        format=""<white>{time:HH:mm:ss}</white> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>"",
        colorize=True
    )

    # Добавляем обработчик для записи в файл (для продакшена)
    # Уровень INFO, ротация, сжатие
    logger.add(
        ""logs/app.log"",
        level=""INFO"",
        rotation=""10 MB"",
        retention=""1 month"",
        compression=""zip"",
        serialize=False, # В данном примере используем текстовый формат
        format=""{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}""
    )

    logger.info(""Конфигурация логирования завершена."")
Теперь в главном файле вашего приложения (например, main.py) вам нужно просто импортировать и вызвать эту функцию один раз при старте:# main.py
from app.logging_config import setup_logging
from loguru import logger

# Вызываем настройку в самом начале работы приложения
setup_logging()

@logger.catch
def main_logic():
    logger.info(""Приложение начинает работу."")
    # ... ваш основной код ...
    a = 10
    b = 0
    result = a / b # Это вызовет ошибку, которую поймает @logger.catch

if __name__ == ""__main__"":
    main_logic()
Такой подход гарантирует, что логирование будет работать одинаково во всех частях вашего проекта.2. Интеграция с библиотеками, использующими стандартный loggingЧто делать, если вы используете библиотеки (например, requests, SQLAlchemy, uvicorn), которые пишут свои логи через стандартный модуль logging? Loguru может элегантно ""перехватить"" эти сообщения и направить их в свои обработчики.Для этого нужно создать специальный класс-обработчик и настроить logging на его использование. Этот код может показаться сложным, но вы можете просто скопировать его в свой конфигурационный файл — он работает как готовый рецепт.# Добавьте это в ваш app/logging_config.py

import logging

class InterceptHandler(logging.Handler):
    def emit(self, record):
        # Получаем соответствующий уровень loguru
        try:
            level = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno

        # Находим вызывающий код
        frame, depth = logging.currentframe(), 2
        while frame.f_code.co_filename == logging.__file__:
            frame = frame.f_back
            depth += 1

        logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())

# ... внутри функции setup_logging() добавьте эту строку:
def setup_logging():
    # ... предыдущие настройки ...

    # Настраиваем перехват логов из стандартного logging
    logging.basicConfig(handlers=[InterceptHandler()], level=0)
    logger.info(""Стандартный logging перехвачен."")
Теперь все сообщения от сторонних библиотек будут проходить через Loguru и записываться в ваши файлы с нужным форматом и правилами ротации.3. Безопасность прежде всего: осторожно с данными в продакшене!Функция @logger.catch и logger.exception() с параметром diagnose=True (который включен по умолчанию) — это мощнейший инструмент отладки. Но в продакшене он может стать источником утечки конфиденциальных данных! Он выводит значения всех переменных, а среди них могут оказаться пароли, ключи API, персональные данные пользователей.Правило для продакшена: всегда отключайте диагностику.# НЕ ДЕЛАЙТЕ ТАК В ПРОДАКШЕНЕ
@logger.catch
def process_user_data(user, password):
    # ...

# ДЕЛАЙТЕ ТАК В ПРОДАКШЕНЕ
@logger.catch(diagnose=False)
def process_user_data(user, password):
    # ...
То же самое касается и обработки исключений: logger.opt(exception=True, diagnose=False).error(""Произошла ошибка"").4. Потокобезопасность и асинхронностьХорошая новость: Loguru потокобезопасен ""из коробки"". Вам не нужно беспокоиться о блокировках при использовании логгера в многопоточных приложениях.Для высоконагруженных или асинхронных приложений, где операции ввода-вывода (запись на диск) могут блокировать основной поток или event loop, Loguru предлагает параметр enqueue=True.logger.add(""high_load_app.log"", enqueue=True)
С этой опцией сообщения сначала помещаются в очередь, а запись на диск происходит в отдельном процессе, не замедляя работу вашего основного приложения.Следуя этим простым правилам, вы сможете построить надежную, информативную и безопасную систему логирования для проектов любого масштаба.Домашнее задание: Закрепляем Loguru на практикеПрочитав статью, вы узнали, как сделать логирование в Python удобным и мощным. Теперь давайте применим эти знания на практике! Нажмите на каждую задачу, чтобы раскрыть её условие.Задача 1: Первая настройка и знакомствоЦель: Убедиться, что Loguru установлен, и научиться выводить базовые сообщения.Создайте новый Python-файл (например, task_1.py).Импортируйте logger из библиотеки loguru.Напишите код, который последовательно выводит в консоль сообщения следующих уровней: DEBUG: ""Это сообщение для отладки.""INFO: ""Приложение успешно запущено.""SUCCESS: ""Операция выполнена успешно!""WARNING: ""Внимание: используется устаревшая версия API.""ERROR: ""Не удалось подключиться к базе данных.""Запустите скрипт и посмотрите на вывод в консоли. Обратите внимание, какие сообщения были выведены, а какие нет, и почему (вспомните про уровень по умолчанию).Задача 2: Запись логов в файл с ротациейЦель: Научиться настраивать запись логов в файл, используя logger.add(), и настроить автоматическую ротацию.Создайте файл task_2.py.Настройте логгер так, чтобы он записывал сообщения в файл app_actions.log.Добавьте в настройку logger.add() следующие параметры: Уровень логирования для файла должен быть INFO и выше.Ротация: новый файл должен создаваться, как только текущий достигнет размера 1 KB (килобайт).Сжатие: старые файлы логов должны сжиматься в формат zip.Напишите цикл, который 100 раз выводит информационное сообщение (например, logger.info(f""Запись номер {i}"")).Запустите скрипт и проверьте папку с вашим проектом. Вы должны увидеть несколько файлов: текущий app_actions.log и несколько архивов .zip со старыми логами.Задача 3: Магия отладки с @logger.catchЦель: Практически применить самую мощную отладочную функцию Loguru для перехвата исключений.Создайте файл task_3.py.Напишите функцию divide_numbers(a, b), которая принимает два числа и возвращает результат их деления.Оберните эту функцию декоратором @logger.catch.Вызовите вашу функцию с параметрами, которые приведут к ошибке ZeroDivisionError (например, divide_numbers(10, 0)).Запустите скрипт и изучи��е вывод в консоли. Обратите внимание, как Loguru показал не только ошибку, но и значения переменных a и b в момент её возникновения.Задача 4: Создание собственного формата логовЦель: Научиться кастомизировать формат вывода сообщений для лучшей читаемости.Создайте файл task_4.py.Удалите стандартный обработчик с помощью logger.remove().Добавьте новый обработчик для вывода в консоль (sys.stderr), который будет использовать следующий кастомный формат: {time:HH:mm:ss} | {level.icon} | {message} {time:HH:mm:ss} — время в формате ""часы:минуты:секунды"".{level.icon} — иконка, соответствующая уровню лога (например, ℹ️ для INFO).{message} — само сообщение.Выведите несколько сообщений разных уровней (INFO, WARNING, ERROR) и убедитесь, что они отображаются в консоли в вашем новом, лаконичном формате.Задача 5: Сборка всего вместе — конфигурация для проектаЦель: Симулировать настройку логирования для реального проекта, вынеся конфигурацию в отдельную функцию.Создайте файл task_5_config.py. В нём определите функцию configure_logger().Внутри этой функции настройте два обработчика (sinks): Первый (для консоли): должен выводить сообщения уровня DEBUG и выше, быть цветным и использовать простой формат (например, {level} | {message}).Второй (для файла): должен записывать сообщения уровня WARNING и выше в файл project_warnings.log в формате JSON (serialize=True).Создайте главный файл task_5_main.py.В task_5_main.py импортируйте функцию configure_logger() и вызовите её в самом начале.После вызова конфигурации, напишите код, который генерирует несколько сообщений разных уровней (DEBUG, INFO, WARNING, ERROR).Запустите task_5_main.py. Убедитесь, что: В консоли отображаются все сообщения, начиная с DEBUG.В файле project_warnings.log появились только сообщения WARNING и ERROR, записанные в формате JSON.Анонс новых статей, полезные материалы, а так же если в процессе решения возникнут сложности, обсудить их или задать вопрос по статье можно в моём Telegram-сообществе.Уверен, у вас все получится. Вперед, к практике!Теги:logurupythonpython3python для начинающихлогированиеХабы:Python",330,0,0,14 мин,https://habr.com/ru/articles/966048/,23954,2913,1
CyBОК. Глава 3. Законы и регуляторные нормы. Часть 4,RRakhmetov,2025-11-13T10:14:00.000Z,"['Блог компании Security Vision', 'Информационная безопасность *', 'Учебный процесс в IT', 'Профессиональная литература *']","RRakhmetov 1 час назадCyBОК. Глава 3. Законы и регуляторные нормы. Часть 4Уровень сложностиПростойВремя на прочтение7 минКоличество просмотров60Блог компании Security VisionИнформационная безопасность * Учебный процесс в ITПрофессиональная литература * ОбзорМы продолжаем серию публикаций, посвященную своду знаний по кибербезопасности - Cybersecurity Body of Knowledge (CyBOK). В Главе 3 данного свода знаний описываются основные регуляторные нормы и принципы международного права, которые имеют отношение к кибербезопасности и могут применяться при оценке киберрисков, управлении ИБ, расследовании киберинцидентов. Сегодня – четвертая часть обзора Главы 3 CyBOK, в которой описываются различные типы киберпреступлений и особенности применения норм права в отношении кибератак.Руслан Рахметов, Security VisionПредыдущие главы и части:Глава 1Глава 2 часть 1, часть 2Глава 3 часть 1, часть 2, часть 33.5. Компьютерные преступления.Термин «киберпреступность» часто используется для трёх различных категорий криминальной активности:- «классические» преступления, в которых киберпространство используется как инструмент (например, финансовое мошенничество, кибермошенничество);- распространение противоправной информации в киберпространстве;- преступления, нацеленные непосредственно на инфраструктуру киберпространства (например, кибератаки, взломы).Данный раздел посвящен как раз третьей категории – компьютерным преступлениям, т.е. преступлениям, направленным против информационных систем, что представляет интерес для специалистов по ИБ. 3.5.1. Преступления в отношении информационных систем.По мере развития информационных технологий всё больше вредоносных действий стало совершаться в киберпространстве, однако законодательные изменения не всегда успевают за прогрессом. Ключевой проблемой остаётся международное применение согласованных мер по борьбе с трансграничной киберпреступностью. Одной из важных мер стала разработка Будапештской конвенции («Конвенция о преступности в сфере компьютерной информации»), которая была утверждена Советом Европы в 2001 году и к настоящему времени подписана и ратифицирована 66 странами. В 2013 году в ЕС была принята Директива 2013/40, которая обязала всех стран-членов актуализировать их уголовное законодательство для включения киберпреступлений в состав уголовно наказуемых деяний. При этом в конце 2024 года Генеральная Ассамблея ООН одобрила новую «Конвенцию против киберпреступности», принятую в целях укрепления международного сотрудничества для борьбы с компьютерными преступлениями и обмена доказательствами по таким преступлениям. Работа над данным документом была инициирована Россией и велась в течение 5 лет.Далее авторы документа приводят классификацию киберпреступлений в соответствии с положениями Будапештской конвенции. 3.5.1.1. Несанкционированный доступ к информационной системе.Несанкционированный (неавторизованный) доступ к информационной системе означает доступ к системе без разрешения её владельца, в нарушение правил и порядка доступа и обычно обозначается общим термином «взлом» (англ. «hacking»). При этом в разных странах действия, составляющие неавторизованный доступ, отличаются: в Великобритании попыткой взлома считается ввод пароля без согласования владельца системы, а в США киберпреступлением считается попытка установить несанкционированное сетевое соединение с системой. Отметим, что термин «неавторизованный доступ» до сих пор законодательно не определен с необходимой точностью во всех странах и зависит от решения должностных лиц в каждом конкретном случае. 3.5.1.2. Несанкционированное взаимодействие с данными.Несанкционированным взаимодействием с данными в соответствии с Будапештской конвенцией считается несогласованное удаление, повреждение, порча, изменение данных или нарушение доступности данных. Данные положения могут использоваться в отношении тех, кто разрабатывает или распространяет вирусы-шифровальщики. 3.5.1.3. Несанкционированное взаимодействие с системами.На заре киберпреступности атакующие взламывали системы и меняли данные в них, но с ростом других типов атак, прежде всего DoS/DDoS, в законодательство были внесены изменения, которые отражают новые типы вредоносной активности – теперь нарушением считаются в том числе действия, которые привели к снижению производительности систем. 3.5.1.4. Несанкционированный перехват коммуникаций.Следствием принятия различных законодательных норм по защите приватности стала криминализация действий по несанкционированному перехвату сетевого трафика, особенно в публичных сетях. 3.5.1.5. Несанкционированная разработка хакерских инструментов.Во многих странах правонарушением является разработка или распространение хакерских инструментов с целью их дальнейшего использования для взлома информационных систем. Подобные нормы могут создавать сложности для тех, кто создает решения для проведения тестирований на проникновение и выполнения иных легитимных ИБ-задач. 3.5.2. Исключения в силу незначительности нарушения.В некоторых случаях применение законодательных норм ограничено только действиями, которые можно признать значительными. Например, в Директиве ЕС 2013/40 говорится, что киберпреступлением может считаться только действие против информационных систем, являющихся значимыми, а уровень значимости зависит от относительной опасности создаваемого риска или ущерба, нанесенного несанкционированными действиями. Подобные исключения создают неопределенности в части расчета опасности последствий: в некоторых случаях ущерб очевиден, но в каких-то инцидентах оценить весь масштаб и последствия атаки будет сложно. 3.5.3. Меры принуждения и ответственность за киберпреступления.Каждая страна сама принимает решения относительно расследования киберпреступлений и возбуждения уголовных дел. Ответственность виновных суды также определяют самостоятельно, руководствуясь границами, предусмотренными уголовным законодательством. Например, в Великобритании типовые сроки заключения за киберпреступления составляют от 2 до 5 лет, но даже такие приговоры выносятся редко. Для сравнения, в США расследование дел о киберпреступлениях часто при��одит к срокам заключения в 20 и более лет.Вопрос адекватного наказания за киберпреступления остается открытым, особенно с учетом развития технологий – например, взлом широко применяемых IoT-устройств может привести к ущербу для жизни граждан или их частной собственности. В Директиве ЕС 2013/40 сказано, что сроки лишения свободы должны быть больше в случае кибератак на критическую национальную инфраструктуру или в случае причинения существенного ущерба. В США с 2015 года закон «Computer Misuse Act» предусматривает тюремный срок до 14 лет в случае причинения существенного ущерба, а в случае серьезного ущерба (или риска) здоровью и благополучию граждан или национальной безопасности может быть назначен пожизненный срок заключения. 3.5.4. Санкционированные государственные действия.В случае совершения действий, связанных с расследованием преступления или защитой национальной безопасности, выдаётся специальное разрешение – ордер на выполнение определенных действий. Лицо, выполняющее санкционированные в соответствии с данным ордером операции, не несет ответственности за выполненные действия, включая взлом систем. 3.5.5. Действия по исследованию и разработке, выполняемые негосударственными организациями.Негосударственные организации, которые исследуют вопросы кибербезопасности или разрабатывают ИБ-решения, могут столкнуться со сложностями, поскольку некоторые из их действий могут подпадать под определение киберпреступлений, например:- Несогласованный анализ мер защиты, реализованных на серверах третьих лиц;- Несогласованный удаленный анализ Wi-Fi оборудования третьих лиц;- Несогласованный анализ сетевой инфраструктуры третьих лиц;- Проведение согласованного нагрузочного тестирования (стресс-тестирования) оборудования, при котором деградирует производительность инфраструктуры третьих лиц, не осведомленных и не согласовывавших тестирование;- Анализ ВПО и тестирование методов защиты от ВПО;- Анализ компонентов и функционала ботнетов;- Создание и распространение инструментов для тестирования киберзащищенности;- Использование различных техник сбора разведданных.При рассмотрении случаев применения инструментов для проведения пентестов оценивается обычно не их технический функционал, а цели и намерения организации или лица, которое их создаёт или распространяет, а ответственность наступает в случае, если эти инструменты предполагалось использовать для нарушения закона. Исследователи ИБ, вендоры и профильные ИБ-компании могут столкнуться со сложностями при оценке рисков ведения тех или иных исследований или разработок – в некоторых обстоятельствах и условиях они могут столкнуться с обвинениями в нарушении законодательства; кроме того, важно оценивать все применимые законодательные нормы во всех юрисдикциях, где ведется потенциально рискованная деятельность. 3.5.6. Самозащита: блокировки ПО и ответный взлом.Под терминами «самопомощь», «самозащита» (англ. self-help) понимаются действия, которые лицо предпринимает для защиты своих прав без привлечения представителей государственной власти. В общем случае подобные действия, как правило, не приветствуются во многих странах, поскольку негосударственное лицо пытается выполнить функцию обеспечения законности, которая должна исполняться властью. Если же в некоторых странах разрешено выполнять определенные действия по самозащите, то существует множество сопутствующих ограничений и условий. Выполнение действий по самозащите может привести к обвинениям в нарушении законодательства и судебным искам. 3.5.6.1. Недекларированные блокировки ПО.Существует практика наложения разработчиками ограничений на использование ПО или сервисов: например, некоторое ПО не будет работать после истечения срока лицензии, а облачный провайдер вправе отключить доступ к сервисам при просрочке платежа. Однако проблемы у вендора или провайдера могут возникнуть в случае, когда подобный функционал блокировки не описан в договоре, лицензионном соглашении или инструкции к ПО. Например, с точки зрения законодательства будет нарушением блокировать работу ПО с применением недекларированного функционала даже в случае, если покупатель не оплатил продление лицензии или нарушил условия лицензионного договора. 3.5.6.2. Ответный взлом.Термин «ответный взлом», «ответный удар» (англ. hack-back) используется для описания действий по проведению ответной кибератаки против ИТ-инфраструктуры, из которой была проведена кибератака. Подобные действия обычно оцениваются в контексте кибератаки, которая была проведена с территории иностранного государства, при этом взаимодействие с ним по вопросу расследования данной кибератаки, скорее всего, не принесет результата. Подобный ответный взлом может включать в себя DDoS атакующей инфраструктуры, взлом или вывод из строя атакующей инфраструктуры и т.д. В подобном случае такой ответный взлом будет расценен как компьютерное преступление в стране, с территории которой он производится, и в целевой стране, а также в странах, чья инфраструктура будет задействована при данных действиях. Кроме того, страна, ставшая целью такого ответного взлома, может задействовать принципы международного права и механизмы защиты собственного суверенитета в отношении лиц, которые проводят такой hack-back, и в отношении инфраструктуры, используемой для ответного взлома.Теги:рекомендациистандартыучебникилучшие практикиХабы:Блог компании Security VisionИнформационная безопасностьУчебный процесс в ITПрофессиональная литература",60,0,0,7 мин,https://habr.com/ru/companies/securityvison/articles/966040/,11498,1344,4
Как нас четыре раза пытались купить,ntsaplin,2025-11-13T07:01:54.000Z,"['Блог компании RUVDS.com', 'Хостинг', 'IT-инфраструктура *']","ntsaplin 4 часа назадКак нас четыре раза пытались купитьУровень сложностиПростойВремя на прочтение10 минКоличество просмотров1.9KБлог компании RUVDS.comХостингIT-инфраструктура * ОбзорНашему VDS-хостингу — 10 лет. За это время нас серьёзно пытались купить четыре раза, ещё пару раз — несерьёзно.И каждый раз это была совершенно другая история, как будто из разных учебников по бизнесу. У нас было всё — от пособия для начинающих рейдеров до конкурентной разведки. Ну, знаете, как Яндекс любит приезжать, задавать вопросы про финансы, сообщать про то, что возможна сделка, а потом говорить спасибо за ценные данные.Мы не хотели продаваться, но пару раз были близки к этому, потому что наступали моменты, когда не хватало денег и мы их искали. Одним словом, бывали ситуации, когда мы действительно рассматривали интересные предложения.Начну с самой отбитой категории покупателей. Я даже не могу назвать их инвесторами. Схема у них примитивная и рассчитана на молодых и голодных, какими мы были в первые годы. Они никогда не покупают компанию за живые деньги. Они приходят и говорят:— Чуваки, вы хотите больше? Мы, естественно, такие: — Да, мы хотим! — Отлично! — говорят они. — Мы сейчас дадим вам денег. Однако для этого вы отдадите нам половину компании. Но на самом деле денег мы вам не дадим!«Благодетель»Процесс вы уже, возможно, поняли.Инвестор оценивает вас, скажем, в пять выручек последнего года (тогда был такой стандарт рынка для хостингов) и говорит, что даст их деньгами за 50% компании. Получается, что компания теперь стоит 10 выручек, у инвестора остаётся 50%, у исходного владельца — тоже 50%. Но реальных денег владелец не видит, потому что инвестор вкладывает их в компанию. Вы, основатель, остаётесь с нулём в кармане, но с партнёром, перед которым теперь должны отчитываться. По факту вы рискуете стать просто наёмным сотрудником в своей же фирме, отдав половину бизнеса даром.Эта схема имеет право на жизнь, когда суммы и условия немного другие и когда инвестор профильный или стратегический. В смысле он знает, что делать с вашим бизнесом, и его ему не хватает для какого-то комплекса, который через год он рассчитывает продать, например. То есть он разбирается в вопросе и предлагает реальную ценность. Если же он самоустраняется от управления, то ничем, кроме геморроя, это не закончится. И по принятию решений, и много по чему ещё.Почему я считаю, что сделка была плохой:Инвестор не разбирался в ИТ, а занимался стройкой.Кредитное предложение денег (тогда) выглядело примерно так же привлекательно, но не требовалось отдавать кому-то 50% бизнеса. По крайней мере, пока шли выплаты.Первый раз такие ребята нарисовались в самый паршивый для нас момент. Наш банк, где мы кредитовались, внезапно рухнул. Мы оказались в подвешенном состоянии, постоянно нужны были деньги, чтобы просто держаться на плаву, и мы искали хоть какие-то точки опоры.И тут появились они — представители одного московского инвестхолдинга. Ситуацию весело дополняло то, что я поспрашивал про них у коллег из финансов (а мы пришли в хостинг из финансов) и узнал, что они с рейдерской репутацией, мягко говоря.Они говорили правильные слова, называли нас «ценным активом», но суть предложения была та самая: мы отдаём им долю, они вливают деньги в развитие, и всем становится хорошо. Мы сели считать. Вот мы с их деньгами через год вырастаем в два раза. А наша доля прибыли в абсолютных цифрах остаётся ровно такой же, как сейчас.Однако есть одно огромное «но»: это ещё не факт. И главное — у нас появляется партнёр, который в любой момент может сказать: «Чуваки, а зачем вам дивиденды или зарплаты? Давайте лучше ещё вложим, ещё вырастем!»Если что, у нас с такими людьми фундаментально разные цели. Наша цель — заработать и на что-то жить всё время. Их цель — раздуть компанию, потом отжать менеджмент и продать её по-настоящему. Есть риски потерять контроль, не получив ни копейки.Жизнь становится более нервной, вы постоянно испуганы и параноидальны, потому что не знаете, чего ждать от человека из не самого прозрачного бизнеса.А если проект провалится? Ты, как соучредитель, отвечаешь по всем долгам.Именно поэтому я не доверяю таким инвесторам. Меня в этом сильно укрепила история моего друга и однокурсника. Он работал техническим директором в американском стартапе Trucker Path. Получил опцион, вкалывал. Компания выстрелила, стала «единорогом» с оценкой больше миллиарда долларов. А потом их всех, держателей опционов, кинули. Классическая практика: как только стартап становится успешным, инвесторы — а там был какой-то крупный китаец — проводят фиктивную сделку. Продают компанию за копейки подставной фирме, «высаживают» всех миноритариев, а потом уже перепродают по-настоящему. Мой друг остался ни с чем. Он звонил даже мне, искал юристов: хотел судиться. Эта история навсегда отпечаталась в моей памяти как пример того, почему нельзя продавать долю в компании без получения реальных денег на руки.Возможно, конечно, я динозавр и чего-то не понимаю, но лучше пускай я останусь динозавром с бизнесом, чем лохом с отличной историей для кухонных рассказов.«Вы, Никита, ценный актив»Второй заход был от зарубежного олигарха, которого на нас вывели знакомые. С нами общался его человек из Нью-Йорка. План был такой: купить нашу платформу и построить на ней международный хостинг, благо у нас уже было много зарубежных дата-центров. Но разговор очень быстро скатился к обсуждению затрат.Они относились к нашей команде, как к гастарбайтерам, которым можно платить три копейки. Когда они услышали, что наши инженеры хотят зарплату больше 100 тысяч рублей, у них глаза на лоб полезли. Началась эта песня: «Ох, так вы хотите зарабатывать и жить в комфорте прямо сейчас или вам нужно построить что-то крупное?»То есть, по их логике, строить великое можно только натощак.В целом это правда: в культурном коде стартаперов из Долины и Европы учредители действительно получают символические зарплаты. Основные деньги приходят с продажи компании или после объединороживания, когда можно позволить платить и себе, и Совету директоров. До этого надо жить на накопления.Они даже сделали оценку: около пяти годовых выручек за всю компанию. Но схема была мутная. Опять та же история: мы теряем контроль и не получаем денег.Мы подумали и отказались.«Вы неправильно работаете»Был и ещё один, уже третий подход от владельца крупного сервиса, который хорошо сочетался с хостингом. Профильный игрок, казалось бы. Но разговор — тот же: «Вот, давайте мы так хотим…» Я сидел, слушал и ощущал дежавю.Ещё до подписания чего-либо они уже начали учить нас жизни. Оказалось, что у себя они используют б/у железо. И нам говорят: «Нет, вот вы дорого покупаете, надо брать бэушное». Мы им: «Ребят, мы на таком работать не будем». Это был конец разговора.Если ещё до сделки начинается такое продавливание, то что будет после этого?Цивилизованный флиртМы надолго забили на все эти переговоры, решив для себя, что это не очень осмысленная трата времени. Но потом на рынке случилась движуха: государство ввело реестр хостинг-провайдеров и ужесточило требования по СОРМ. Один из крупных игроков рынка тут же подсуетился и начал рассылать письма всем, кто поменьше. Посыл был такой: «Вам сейчас будет пипец, вы разоритесь, давайте мы вас купим». По сути, они хотели скупить клиентские базы. Всем нужны клиентские базы. Бренд не нужен никому. Даже если мы сейчас будем брать региональный хостинг, то бренд не будет нас интересовать — только база.Так вот, нам тоже пришло такое письмо. Мы об этом написали на VC и Хабре. Покупатели после такой огласки сначала отморозились, но через пару недель мне позвонили: «Слушайте, то было экстренное реагирование, но всё равно мы хотели бы с вами поговорить». И вот тут начался совсем другой разговор. Они назвали ожидаемые нами цифры: оценка была в районе шести годовых выручек. Мы впервые подписали соглашение о неразглашении и вступили в процесс due diligence. Это когда покупатель проводит полную проверку вашего бизнеса, чтобы убедиться, что вы не продаёте ему кота в мешке.Они вели себя очень цивилизованно, задавали логичные вопросы без залезания в трусы.Но вскрылись две проблемы. Во-первых, технологический стек: у нас всё на одном гипервизоре, а у них — на другом. Они поняли, что перенос клиентов будет сложным и дорогим. Во-вторых, им не понравилось, что у нас есть зарубежные локации. В итоге предложения не последовало.Но мы не расстроились: это был первый опыт, когда с нами говорили как с равными, а не как с просителями.Высшая лигаИ вот совсем недавно на нас вышел игрок совсем другого масштаба. Назовём его N3. Они пришли не сами, а через профессиональных посредников — M&A-компанию. И вот тут мы увидели, как на самом деле выглядят серьёзные сделки.Там всё начинается не со звонка основателя, а с общения с этими M&A-специалистами, по сути — инвестбанкирами. Они задают очень правильные вопросы про качество актива: срок жизни клиента, динамику роста и так далее. Они не лезут в ноу-хау: все понимают границы. Если их всё устраивает, то следует звонок от самих покупателей. Они обсуждают сценарий сделки. И если стороны сходятся, то присылают Letter of Intent — официальное предложение с главными условиями.Только после его подписания начинается самый ад — due diligence, который может длиться месяцами. На этом уровне никто уже не считает по выручке. Всё считается по мультипликатору к EBITDA: это, грубо говоря, ваша прибыль до налогов, процентов и амортизации. Самый честный показатель. Нормальным для текущей рыночной ситуации считается мультипликатор от 5. Хочешь больше? Докажи, что у тебя есть супертехнология или ты растёшь как на дрожжах.И самое интересное — деньги. 100% суммы тебе никогда не заплатят сразу. Аудиторы посчитают все риски, которые сочтут вероятными для покупателя, и порекомендуют заморозить эту сумму на эскроу-счёте на три года. Вы получите её, только если за это время не будет претензий.Процесс этот дико дорогой. Покупатель тратит на аудиторов миллионов 30–50. Но и вы, продавец, если хотите, чтобы ваши интересы нормально представляли, должны нанять своих консультантов. А это ещё минимум 20–40 миллионов. Получается, что сама сделка обходится обеим сторонам под 80–100 миллионов. Поэтому покупать маленькую конторку в таком формате просто бессмысленно. Но у M&A-отделов этих гигантов есть своя пропускная способность: они не могут покупать по 10 компаний в год, т. к. ведут одну сделку по полгода. Поэтому все эти байки про «M&A-бюджет, который надо освоить до Нового года» — чушь: считают каждый рубль.По итогам due diligence определяются финальная сумма и структура сделки. И вот тут начинается самое интересное — игра в деньги и власть.Для вас, как для продавца, идеальный сценарий — это получить все деньги сразу. Чемодан с немечеными купюрами, как в кино. Ну или просто всю сумму на счёт за вычетом того самого эскроу на риски. Получил, попрощался — ушёл в закат.Но покупатель, особенно в текущей экономической ситуации, смотрит на это совсем иначе. Сейчас высокие ставки, дешёвых денег нет ни у кого, даже у гигантов. Поэтому платить всю сумму сразу им дико невыгодно — им гораздо интереснее растянуть сделку во времени. И они предлагают схему, которая на бумаге выглядит логично, а на деле это адская ловушка для продавца.Схема называется earn-out, или выплата по результатам. Выглядит она так: «Мы платим тебе часть суммы, скажем, 50%, прямо сейчас. А за это получаем контрольный пакет — 51% акций. Оставшиеся 50% ты получишь частями в течение следующих двух-трёх лет, если выполнишь определённые KPI, например, по росту той же EBITDA».Для покупателей это соломоново решение. Во-первых, им не нужно вываливать всю сумму сразу: по факту для них это бесплатная рассрочка. Во-вторых, они могут внедрить своих людей, всё проконтролировать и убедиться, что вы не увели клиентскую базу или не открыли такой же бизнес с другой буквой в названии. Им так комфортнее.А вот для вас, продавца, в этот момент всё меняется. В тот самый миг, когда вы подписываете договор и получаете первый транш, вы теряете контроль над своей компанией. Вы больше не хозяин.И тут же возникает очевидный и неразрешимый конфликт интересов. Покупателю теперь НЕВЫГОДНО, чтобы вы выполнили свои KPI. Ведь если вы их выполните, то ему придётся платить вам следующий большой транш. А зачем, если контроль над активом он уже получил? И он будет делать всё что угодно, чтобы вы не достигли этих показателей. Вплоть до умышленного вредительства.Как это выглядит на практике? Очень просто. Вы приходите и говорите: «Нам нужно поднять цены для клиентов, чтобы увеличить прибыль и выполнить KPI». А новый мажоритарный акционер отвечает: «Нет, мы не можем: это отпугнёт клиентов». Вы говорите: «Нужно вложиться в маркетинг, чтобы привлечь новых пользователей». Они: «Бюджета нет». Нужно нанять ключевого разработчика? «Давайте подождём». Любая инициатива, направленная на рост, будет тормозиться. Вам просто не дадут работать.Для продавца есть одно золотое правило, которое нужно высечь в граните: рассматривайте первый транш как финальную и единственную сумму сделки. Всё остальное — это бонус, которого вы, скорее всего, никогда не увидите. Нужно быть готовыми к тому, что покупатель всеми силами, опираясь на своих сильных юристов и новый статус хозяина, не захочет вам платить. И это его право: он играет по правилам, которые сам же и установил в договоре. Вы утратили контроль и теперь находитесь в его власти.С N3 мы в итоге не сошлись в оценках. Мы хотели мультипликатор 8 к EBITDA, а они — немного меньше (сколько — не позволяет сказать NDA) .Мы такие: «ОК, давайте поговорим через год. Мы не торопимся».ИтогоПервое. Если вы хотите идти на продажу, то готовиться к ней надо с первого дня. Если у вас бардак в бухгалтерии и какие-то мутные схемы, то потом будет мучительно больно: либо сделка сорвётся, либо вам насчитают такой эскроу, что от суммы продажи ничего не останется. Финансовая гигиена — это основа.Бизнес «для себя» оптимизирован для вашего личного комфорта и эффективности. Там могут найтись какие-то схемы, которые тебе посоветовал бухгалтер на аутсорсе, чтобы платить поменьше налогов. Юридическая структура может быть запутанной. Идеальная документация — в голове. И если на такое приходят аудиторы из «Большой четвёрки», то начинается ад. Всё это риски и красные флаги, нет документа — это юридическая проблема. Они пересчитают все эти риски в конкретные суммы и либо вычтут их из цены, либо заморозят на эскроу-счёте на три года, либо вообще уйдут из сделки. Нельзя десять лет строить сарай для себя, а потом за один день пытаться продать его как дворец.Второе. Ваш бренд, скорее всего, не стоит ничего. Покупателю нужны ваша клиентская база, а также EBITDA. После сделки ваш бренд убьют. Никто уже не помнит, что был Foodfox, а не «Яндекс Еда». Исключение — если какой-нибудь Wildberries решит выйти на рынок ЦОДов и купит компанию с именем, чтобы не стартовать с нуля.Третье. Не ждите щедрости. Покупатели — не дураки. Они найдут все ваши косяки, даже те, о которых вы не подозревали. Они будут торговаться за каждый рубль.Четвёртое. Все вокруг говорят про «экзит» как главную цель любого стартапа. Продать компанию гиганту, получить мешок денег и уехать на Бали. Звучит красиво, как в кино. Но в реальной жизни, если твой бизнес работает и приносит стабильный доход, продавать его — это экономически нелогично. Зачем резать курицу, которая несёт золотые яйца?На мой взгляд, есть только две по-настоящему веские причины для продажи. Первая — у вас на горизонте появилось что-то более выгодное, более маржинальное, и вам нужны деньги и время, чтобы туда вложиться. Это холодный рациональный расчёт. А вот вторая причина — вы просто устали. Это выгорание другого уровня. Это когда вы десять лет тащите на себе ответственность за всё: за людей и их зарплаты, за серверы, которые могут упасть ночью, за клиентов, которые могут уйти. Проблемы, которые раньше были интересными вызовами, превращаются в тупое нудное раздражение. И в этот момент продажа — это не про деньги. Это про то, чтобы вернуть себе свою жизнь.Если вам нужны деньги — рассмотрите кредит или другие варианты. Там можно продать только будущую выручку (или прибыль) без других частей компании.Мы не продались не потому, что мы такие гордые, а потому, что ни одно из предложений не было для нас по-настоящему выгодным. Но общаться с потенциальными покупателями полезно. Вы смотрите на свой бизнес, на своё детище не только своими любящими глазами, но и холодным взглядом рынка.И это очень отрезвляет и помогает понять, что на самом деле вы построили.© 2025 ООО «МТ ФИНАНС»Теги:хостингбизнесрыноксделкадью-диллиженсслияние и поглощениеотжимоценкамифический экзитХабы:Блог компании RUVDS.comХостингIT-инфраструктура",1900,0,0,10 мин,https://habr.com/ru/companies/ruvds/articles/965860/,16616,2520,3
"LLM vs. почерк: практическое сравнение GPT-5, Gemini и Claude в задачах OCR",Tehnologika,2025-11-13T09:18:26.000Z,"['Natural Language Processing *', 'Искусственный интеллект', 'Машинное обучение *', 'Обработка изображений *', 'Исследования и прогнозы в IT *']","Tehnologika 2 часа назадLLM vs. почерк: практическое сравнение GPT-5, Gemini и Claude в задачах OCRУровень сложностиПростойВремя на прочтение7 минКоличество просмотров436Natural Language Processing * Искусственный интеллектМашинное обучение * Обработка изображений * Исследования и прогнозы в IT * АналитикаРаспознавание рукописного текста — задача, которая остаётся болезненной даже в 2025 году. Именно это не позволяет оцифровать многие архивы и документы, а также является камнем преткновения в разной бизнес деятельности. OCR-движки вроде Azure Document Intelligence, Google Vision или ABBYY уже давно научились безошибочно читать печатные формы, но всё рушится, когда на сцену выходит человек с ручкой.Почерк — это хаос, в котором буквы скачут, строки уползают, а “5M” внезапно превращается в “5 PM”, если повезёт. И если обычный OCR видит только буквы и пиксели, LLM видит смысл.Производители заявляют, что модели вроде GPT-5, Gemini 2.5 Pro и Claude Sonnet 4.5 способны не просто распознать текст, а догадаться, что автор имел в виду: исправить пунктуацию, восстановить сокращения, даже понять, что стоит за пометками на полях.Звучит красиво. Но работает ли это на реальных документах, а не в демо-видео с идеально отсканированными формами?  Чтобы ответить, мы провели исследование и сравнили, как три топ-LLM обрабатывают рукописные и смешанные документы — с точки зрения точности, структурной консистентности и понимания контекста.СодержаниеМетодологияЧто мы тестировалиКак мы тестировалиПочему три модели?Как справились моделиФормуляр в Музей трамваев (Streetcar Museum Event Form)Анкета для фотооархива (“Johnny – King of Sausages” Photo Form)Форма для смены в больнице (Change of Shift Huddle)Кто победил?Себестоимость и скоростьА можно ли лучше?Пример на практике: Azure Document Intelligence + Gemini 2.5 ProЧто делать, когда облако – не вариант?Заключение и выводыМетодологияЧто мы тестировалиДокументы, которые мы отдавали в LLMЧтобы не скатиться в синтетические бенчмарки, мы взяли три реальных документа, типичных для корпоративных сценариев, где OCR-ошибки могут стоить времени и денег:Формуляр мероприятия в Музей трамваев (Event Form to Streetcar Museum)Анкета для фотооархива (Photo Submission Form “Johnny – King of Sausages”)Форма для смены в медицинском учреждении (Medical Change of Shift Huddle Form)Каждый документ имеет свои особенности и сложности для OCR и LLM:Формуляр в Музей трамваев – смешанные шрифты, наложения текста, пересекающиеся строки и неоднозначные цифрыАнкета для фотооархива – курcив, капс, подписи, апострофы и артефакты от скана старой бумагиФорма для смены в больнице – несколько почерков, медицинские сокращения, пересекающиеся сетки таблицКак мы тестировалиДля чистоты эксперимента:Все три документа поданы в неизменном виде, без предварительной очистки изображений.Каждая модель получала один и тот же ввод (скан) и задачу: извлечь данные в структурированном JSON.Оценка велась вручную и по метрикам:CA (Character Accuracy) – сколько символов распознано корректноFA (Field Accuracy) – сколько полей правильно извлеченоSA (Semantic Accuracy) – насколько правильно понят смысл и контекстC (Completeness) – доля извлечённых полей от ожидаемогоS (Schema Consistency) – единообразие JSON-структуры между документамиПочему три модели?Особенности каждой LLMДля эксперимента были выбраны три модели разных архитектурных подходов:Claude Sonnet 4.5 — фокус на интерпретации и языковом рассуждении;Gemini 2.5 Pro — структурная точность и стабильность вывода;GPT-5 — сильная контекстная и семантическая обработка.Это позволило оценить не только базовую точность OCR, но и то, как модели восстанавливают смысл, формат и структуру данных после распознавания.Как справились модели1. Формуляр в Музей трамваев (Streetcar Museum Event Form)Эта форма — старомодный бланк с полями вроде Sponsor, Date of Event, Category, Mailing Address. Часть напечатана, часть вписана вручную — и именно это делает задачу интересной.Основные сложности:строки налезают на границы таблиц;одни слова написаны капсом, другие — курсивом;рукописные цифры 2 и 5 путаются с буквами S и Z;некоторые поля («Tour» с обведённым кружком) требуют не просто OCR, а понимания логики формы.Как LLM справились с музейным формуляромClaude 4.5 справился посредственно: понял суть, но несколько полей спутал, а слова вроде “Cultural” превратил в мифических существ (“Scolyunile”). Он явно склонен к «творческой интерпретации».Gemini 2.5 Pro показал почти безупречный результат. Он не только правильно распознал все поля, но и нормализовал их формат: аккуратные даты, выровненные адреса, единый стиль JSON. Даже догадался, что “5M” — это “5 PM”, и восстановил недостающие запятые.GPT-5 занял второе место. Он хорошо понял смысл, корректно обработал сложные фразы вроде “Share the fun of a streetcar ride…”, но слегка потерял структурность — часть данных «сплющил» в одну строку.Итог: Gemini — образцовый инженер, GPT-5 — умный редактор, Claude — вдохновлённый поэт.2. Анкета для фотооархива (“Johnny – King of Sausages” Photo Form)Форма из городского архива, заполненная синей ручкой поверх выцветших полей. Здесь OCR сталкивается с типичными проблемами старых документов: кривые линии, неровный почерк, пересечение букв с рамками и случайные артефакты сканера.Как LLM справились с анкетойClaude 4.5 всё понял правильно, но внёс хаос в ключи — где-то snake_case, где-то CAPS, а где-то просто пропустил пустые поля.Gemini 2.5 Pro снова показал себя с лучшей стороны: идеально воспроизвёл текст, сохранил пунктуацию (даже кавычки в “King of Sausages”), выстроил вложенные секции “WHEN / WHERE / WHO” и сдал JSON, который можно прямо загружать в базу.GPT-5 чуть проиграл по структурной чистоте, но оказался лучшим по естественности текста. Он сгладил орфографические неровности и исправил регистр, делая результат ближе к «читаемому человеку описанию».Итог: Gemini — идеален для продакшена, GPT-5 — ближе к “human-readable”, Claude — опять решил, что JSON — это рекомендация, а не стандарт. 3. Форма для смены в больнице (Change of Shift Huddle)Форма с дежурства — таблица, где вписаны имена, числа, состояния пациентов и короткие заметки вроде “MRSA developing in a patient” или “Wiping down the surfaces”.Такие документы — кошмар для любого OCR: сетка таблицы ломает структуру, буквы сливаются, а почерки отличаются настолько, что кажется, будто форму заполняли трое разных людей.Как LLM справились с медицинской формойClaude 4.5 сработал лучше, чем ожидалось: правильно распознал числовые поля и основные секции (Team, Advocate, Motivate), но перепутал имена и местами дублировал значения.Gemini 2.5 Pro снова лидер: он сохранил структуру таблицы, корректно распределил имена по колонкам, а странное «Einnish» (OCR-ошибка) — единственный заметный промах. При этом общая точность полей — 99 %.GPT-5 показал сильное понимание контекста: фраза “Extremely busy but managed well” была не просто прочитана, а осмыслена как unit status. Но часть имён он перепутал (“Mac” вместо “Max”) — классическая жертва OCR.Итог: Gemini вновь впереди, GPT-5 почти догнал, Claude — стабильный, но не продакшен-уровень.Кто победил?После десятков прогонов трёх LLM по реальным рукописным формам результат оказался довольно однозначным — Gemini 2.5 Pro уверенно вырвался вперёд по всем формальным метрикам.МетрикаClaude Sonnet 4.5Gemini 2.5 ProGPT-5КомментарийCharacter Accuracy(сколько символов распознано корректно)93–97 %98–99 %97–98 %Gemini чище по символам, Claude чаще теряет пробелы и пунктуациюField Accuracy (сколько полей правильно извлечено)90–96 %99–100 %96–98 %Gemini безошибочно выстраивает ключи и значенияSemantic Accuracy (насколько правильно понят смысл и контекст)95–98 %99–100 %98–99 %GPT-5 чуть сильнее в логике, но слабее в формеCompleteness (доля извлечённых полей от ожидаемого)96–100 %100 %96–100 %Все трое неплохо, но Gemini стабильно полонSchema Consistency (единообразие JSON-структуры между документами)СредняяОчень высокаяВысокаяJSON у Gemini можно подавать в прод без пост-обработкиЕсли коротко:Gemini 2.5 Pro показал лучшую комбинацию точности (до 99 %) и структурной стабильности.GPT-5 чуть уступает в формализме, но сильнее в семантическом «понимании» текста.Claude 4.5 часто ошибается в полях, но умеет красиво пересказывать — что полезно для описательных задач.Себестоимость и скоростьМы замеряли не только качество, но и цену и время обработки, потому что в продакшене важно не просто «чтобы красиво», а чтобы дёшево и предсказуемо.МодельСтоимость на документ (USD)Среднее время обработкиПримечаниеClaude Sonnet 4.50.0165~ 75 сБыстрее, но дороже и менее точенGemini 2.5 Pro0.0080~ 90 сОптимальное соотношение цена / качествоGPT-50.0094~ 120 сМедленнее, но стабильно выдаёт контекстно-богатый результатЧто это значит:Обработка одного рукописного документа в Gemini стоит достаточно дёшево.Claude — самое «дорогое вдохновение», но не лучший вариант, если счёт идёт на тысячи страниц.GPT-5 — универсал: если нужно чуть больше глубины в понимании текста, разница в 30 секунд может быть оправдана.А можно ли лучше? Когда речь идёт о распознавании реальных документов, ни один инструмент в одиночку не идеален. OCR прекрасно видит буквы, но не понимает смысла. LLM — наоборот: понимает смысл, но не знает, где на картинке что написано.Поэтому оптимальная архитектура гибридная:OCR → LLM → Валидация.Оптимальная гибридная архитектураПример на практике: Azure Document Intelligence + Gemini 2.5 ProAzure DI делает то, что умеет лучше всего — распознаёт текст, возвращает bounding-box координаты и confidence-оценку для каждого слова.Gemini 2.5 Pro принимает этот результат и:исправляет очевидные OCR-ошибки («5M» → «5 PM», «SBO» → «SBO Card»);нормализует формат (даты, капитализацию, пунктуацию);собирает данные в чистый JSON;при необходимости восстанавливает контекст («Tour» отмечено кружком — значит, выбрано).Система валидации проверяет поля с низкой уверенностью Azure и подставляет исправления Gemini.После того как мы применили этот подход, мы получили лучшие результаты. Вместе эти инструменты формируют систему, где человек нужен только в спорных случаях, а всё остальное идёт автоматически. Вот результаты:точность полей выросла с ~84 % (Azure DI в одиночку) до ~99 %;ручная проверка сократилась на 90–95 %;JSON стал полностью пригоден для загрузки в базу без ручного редактирования.Что делать, когда облако – не вариант? Облачные OCR и LLM-платформы вроде Gemini, GPT-5 или Azure Document Intelligence уже обеспечивают почти идеальную точность и масштабируемость. Но не все компании могут позволить себе отправить документ в облако и дождаться JSON.  Иногда это просто запрещено политиками безопасности. Если данные нельзя выносить в облако, ту же логику можно воспроизвести локально:Внутренний OCR → приватная LLM.Локальная реализация гибридаЛокальный гибрид может быть идеален для некоторых компаний компаний, поскольку:Никакие данные не покидают инфраструктуру организации.Модели можно обучать и настраивать на образцах почерка, характерных для конкретной бизнес-темы.Работает в отключенных сетях или в средах с высоким уровнем безопасности.Единовременная стоимость установки и неограниченное использование.Возможность прямой подачи данных во внутренние озера данных или ERP-системы без внешних зависимостей.Заключение и выводыLLM действительно умеют читать почерк — не идеально, но с пониманием.И если раньше «распознать рукописную форму» означало час ручной чистки Excel, то теперь это вопрос одной модели и пары API-вызовов.После трёх десятков тестов, сотен строк JSON и пары нервных шуток про курсив можно сделать несколько уверенных выводов.LLM-OCR перестал быть экспериментом. Гибриды вроде Azure DI + Gemini 2.5 Pro уже сегодня обеспечивают до 99 % точности и сокращают ручную проверку на 90–95 %.Gemini 2.5 Pro — оптимальный выбор для продакшена. Высокая структурная точность, стабильный JSON и лучшая цена за страницу делают его рабочей лошадкой enterprise-уровня.GPT-5 — лидер в понимании контекста. Он «думает» о смысле текста, но требует строгих схем и валидации, если данные идут в базу.Claude 4.5 Sonnet — отличный интерпретатор и storyteller, но не всегда детерминирован. Хорош для описательных задач, не для бухгалтерии.On-prem OCR — не про экономию, а про суверенность данных. Его выбирают не те, кто хочет сэкономить, а те, кто не может позволить себе облако.Теги:llm-моделиchatgpt-5claude sonnetgemini proязыковые моделиобработка документовпочеркпочерк врачейgpt-5обработка изображенийХабы:Natural Language ProcessingИскусственный интеллектМашинное обучениеОбработка изображенийИсследования и прогнозы в IT",436,0,0,7 мин,https://habr.com/ru/articles/966002/,12541,1665,5
"Нефункциональные требования. Список, который вспоминают в последний день перед релизом. Часть 1",SiYa_renko,2025-11-12T17:33:30.000Z,"['Блог компании OTUS', 'Управление разработкой *', 'Анализ и проектирование систем *']","SiYa_renko 17 часов назадНефункциональные требования. Список, который вспоминают в последний день перед релизом. Часть 1Уровень сложностиПростойВремя на прочтение9 минКоличество просмотров911Блог компании OTUSУправление разработкой * Анализ и проектирование систем * ОбзорПредставьте, что вы покупаете мотоцикл. Чего вы от него ожидаете? Чтобы он мог разгоняться до 180км/час и при этом не разваливался? Чтобы к нему можно было прикрепить коляску? И не забудем про систему безопасности.Эти требования не описывают основную функцию мотоцикла — перемещать человека из точки А в точку Б — но они важны для удовлетворения ваших потребностей, как водителя.Точно так же, как у мотоциклов, да и любой другой техники, у программных продуктов есть свои нефункциональные требования. То есть атрибуты качества, которые будут удовлетворять потребности конечного пользователя.Сегодня я предлагаю рассмотреть те нефункциональные требования, которые влияют на деньги, но для начала...Что это вообще такое?Обратившись к терминологии мы поймем, что нефункциональные требования — это набор спецификаций, описывающих эксплуатационные характеристики системы и её ограничения. По сути, это требования, определяющие как хорошо система должна работать (скорость отклика, безопасность, надёжность, целостность данных и так далее).Однако это лишь один тип требований в разработке ПО, поэтому прежде чем углубляться в них, стоит сказать о другой группе — функциональных требованиях.И функциональные, и нефункциональные требования описывают характеристики, которые продукт должен иметь, чтобы удовлетворить потребности стейкхолдеров и бизнеса. Однако, как видно из названий, они фокусируются на разных вещах.Функциональные требования определяют, что система должна делать, какие функции и возможности иметь. Это может быть как возможность отправлять сообщения и редактировать их, так и оплачивать подписку.Нефункциональные требования определяют как система должна работать. Например, говорить о том, что измененное сообщение у участников чата должно обновляться не позднее, чем через 0.1 секунды, при условии, что все пользователи онлайн и имеют подключение уровня LTE или выше.Все требования к системе обычно фиксируются в спецификации требований к программному обеспечению и документе требований к продукту. Эти документы содержат описание функций и возможностей, которые продукт должен предоставлять, а также список ограничений и допущений. Погуглите материалы про SRS и PRD, если вам интересно.Теперь, когда общая картина требований понятна, рассмотрим нефункциональные требования подробнее.Типы нефункциональных требованийНаиболее распространенные группы нефункциональных требований включают:Производительность (Performance) — насколько быстро система возвращает результат.Масштабируемость (Scalability) — как изменяется производительность при росте нагрузки.Переносимость (Portability) — на каком оборудовании, операционных системах, браузерах и их версиях может работать система.Совместимость (Compatibility) — не конфликтует ли система с другими приложениями или процессами. Надёжность (Reliability) — как часто происходят критические сбои. Сопровождаемость (Maintainability) — сколько времени требуется для устранения проблемы при её возникновении. Доступность (Availability) — каков средний простой системы. Безопасность (Security) — насколько хорошо защищены система и данные от атак. Удобство использования (Usability) — насколько легко пользователю взаимодействовать с системой.Помимо мною перечисленных, некоторые выявляют и другие атрибуты качества, которые могут быть включены в список, это уже зависит от специфики проекта. Мы же рассмотрим подробнее именно эти группы.Требования к производительностиПроизводительность определяет, насколько быстро программная система (или её компонент) реагирует на действия пользователя при определённой нагрузке. Это один из ключевых типов нефункциональных требований — без него не обходится ни одна система.В большинстве случаев метрика производительности описывает, как долго пользователь должен ждать до выполнения целевой операции (отображение страницы, обработка транзакции и так далее) при текущем количестве активных пользователей.Однако это не единственный вариант. Требования к производительности могут также описывать фоновые процессы, которые пользователь не видит — например, резервное копирование или обработку данных.Но в рамках этого раздела мы сосредоточимся на показателях производительности, влияющих на взаимодействие пользователя с системой.Примеры требований к производительностиПосадочная страница, поддерживающая 5 000 пользователей в час, должна обеспечивать время отклика не более 6 секунд в браузере Chrome на десктопе, включая загрузку текста и изображений при подключении уровня LTE.Результаты поиска по товарам должны отображаться не более чем за 3 секунды в 90% запросов при нормальной нагрузке.Система должна быть способна принимать потоки данных со скоростью не менее 1 000 000 записей в минуту из различных источников без потери данных.Для обновления данных на дашборде в реальном времени, система должна обновлять и отображать новые аналитические данные в течение 10 секунд после получения новых данных.Как работать с требованиями к производительностиНачните с рекомендаций Google для обычных веб‑страниц. Google уделяет особое внимание скорости загрузки на десктопах и мобильных устройствах. Поэтому, если вам нужны базовые ориентиры производительности для публичных веб‑страниц, используйте сервис Google PageSpeed Insights. Он поможет оценить время полной загрузки, время до полного рендера, отклик интерфейса на действия пользователя.На основе совокупности факторов сервис формирует оценку производительности, по которой можно судить о скорости вашего сайта. Это особенно важно для посадочных страниц, поскольку низкая скорость загрузки может привести к снижению ранга страницы в поисковой выдаче.Базовые рекомендации по времени отклика появились еще в 1993 году, их выделил Якоб Нильсен и они представляют собой три ключевых порога времени реакции системы. Несмотря на то, что эти значения появились давно, они по‑прежнему актуальны, поскольку основаны на особенностях восприятия и внимания человека:Время реакцииКак воспринимается0.1 секундыРеакция системы ощущается мгновенной1 секундаЗадержка заметна, но мыслительный поток не прерывается10 секундВнимание пользователя полностью теряетсяКак правило, до 10 секунд доходить нельзя, потому что около 55% пользователей покидают сайт, если он грузится дольше 3 секунд.А согласно исследованию Portent:«Сайт, загружающийся за 1 секунду, имеет конверсию в 3 раза выше, чем сайт, загружающийся за 5 секунд».То есть время в прямом смысле деньги.Что нужно учестьУточняйте сценарий измерения производительности. При формулировании требований важно указать что именно измеряется и включает ли метрика только время доставки данных до браузера (время ответа сервера) или полное время рендера страницы?Если разные типы контента загружаются с разной скоростью, то для каждого из них нужны отдельные ограничения.Также уточняйте нагрузочный сценарий. Если, к примеру, днем на сайте 5000 активных пользователей, а ночью 1000, необходимо указать, к какому сценарию относится метрика (средняя нагрузка, пиковая или стресс‑тест). Иногда фиксируют оба варианта.Исключайте задержки, зависящие от сторонних систем. Если система зависит от внешнего API, не следует включать в требование время, которое уходит на ответ третьей стороны. Команда разработки не отвечает за производительность внешних сервисов.Требования к масштабируемостиМасштабируемость определяет, при какой максимальной нагрузке система всё ещё способна соответствовать требованиям к производительности и удобству использования. Важно понимать, что масштабируемость относится к способности системы справляться с ростом как объёма данных, так и нагрузки со стороны пользователей.Существует два основных способа масштабирования системы по мере увеличения нагрузки:Горизонтальное масштабирование — добавление новых серверов в пул ресурсов.Вертикальное масштабирование — увеличение мощности существующих серверов (добавление CPU, RAM и так далее).Таким образом, ключевая цель требований к масштабируемости — обеспечить, чтобы система оставалась стабильной и сохраняла требуемый уровень производительности при увеличении числа пользователей, объема данных, количества бизнес‑процессов, количества модулей и интеграций.Примеры требований к масштабируемостиВеб‑сайт должен масштабироваться до 1 000 000 одновременных посещений, сохраняя оптимальную производительность.ERP‑система должна быть способна увеличивать объёмы хранения и обработки данных по мере роста компании, включая потенциальное увеличение числа транзакций в 10 раз за пять лет.Система мониторинга IoT‑устройств должна поддерживать масштабирование с 10 000 до 500 000 устройств, без потери точности данных или снижения качества наблюдения.eCommerce‑платформа должна быть способна выдерживать рост трафика на 300% в периоды распродаж или сезонных пиков без ухудшения времени отклика и доступности сервиса.Как формулировать и прорабатывать требования к масштабируемостиОпределяйте ожидания с точки зрения бизнеса. Сначала оцените текущую нагрузку: количество пользователей, транзакций, объём данных. Затем определите прогноз роста. Сколько пользователей планируется поддерживать в будущем? Какие рынки предполагается расширять? Какие новые модули или функции могут быть добавлены позже?Учитывайте особенности отрасли. Например для eCommerce, игр, видеостриминга критичен рост числа пользователей. Если мы говорим про финтех или банки, ключевыми параметрами будут являться скорость и объем транзакций.Делайте требования измеримыми. Используйте конкретные параметры, такие как количество одновременных пользователей, объем данных, скорость обработки транзакций, время отклика при максимальной нагрузке.Самое главное, нужно ставить реалистичные цели. Масштабируемость должна опираться на реальные сценарии использования и прогнозируемые пики нагрузки, а не на абстрактные «пусть выдерживает любое количество пользователей».Требования к доступностиДоступность определяет вероятность того, что система будет доступна пользователю в определенный момент времени. Её можно выражать как долю успешных запросов или процент времени, когда система находится в работоспособном состоянии за заданный период.Доступность один из наиболее критичных бизнес‑показателей, но чтобы корректно определить ее, необходимо так же учитывать надежность (то есть как часто случаются сбои) и сопровождаемость (как быстро система восстанавливается.Примеры требований к доступности Дашборд должен быть доступен пользователям из РФ 99.98% времени в месяц в рабочие часы по МСК.Приложение должно обеспечивать стабильную работу на разных устройствах с уровнем надёжности 99%.Видеостриминг должен быть непрерывным 99,8% времени при нормальных сетевых условиях.Как формулировать требования к доступностиНачните с бизнес‑ограничений. Поймите, допустимо ли, чтобы приложение было недоступно 5% времени? Какой финансовый или KPI‑ущерб это создаст? Полностью безотказных систем не существует в природе, нужно определить критический порог.Уточните, для какого компонента определяется доступность. Часто разные части системы имеют разные SLA, например платежные модули, лендинги, админ‑панель.Учитывайте разные сценарии нагрузки. Доступность может снижаться при пиковых нагрузках, при стресс‑нагрузке и при деградации внешних сервисов. Поэтому необходимо зафиксировать требуемые показатели для нормального режима, режима с повышенной нагрузкой и аварийных условий.Немного подытожим. Сегодня мы рассмотрели нефункциональные требования, которые наиболее важны для бизнеса и менеджеров, поскольку влияют на деньги. Это производительность, доступность и масштабируемость.Для того чтобы было проще воспринимать информацию, я решила разбить требования на три логических блока, так что в следующей части мы рассмотрим нефункциональные требования, которые влияют на архитектуру и код — сопровождаемость, надежность и безопасность.Продолжить осваивать базу системного анализа — от фиксации требований до проектирования интерфейсов и API — можно на курсе «Системный аналитик. Basic». Это практический курс: научитесь формулировать NFR, декомпозировать и приоритизировать требования, оформлять документацию и проводить приёмку. Чтобы оставаться в курсе актуальных технологий и трендов, подписывайтесь на Telegram-канал OTUS.Теги:нефункциональные требованиядоступностьпроизводительностьмасштабируемостьХабы:Блог компании OTUSУправление разработкойАнализ и проектирование систем",911,0,0,9 мин,https://habr.com/ru/companies/otus/articles/963896/,12535,1516,3
Аудит доступности веб-приложения Приорбанка,archik,2025-11-12T18:55:34.000Z,"['Accessibility *', 'HTML *', 'CSS *', 'JavaScript *']","archik 16 часов назадАудит доступности веб-приложения ПриорбанкаУровень сложностиПростойВремя на прочтение8 минКоличество просмотров627Accessibility * HTML * CSS * JavaScript * Артур БасакWeb UI/UX EngineerЭта статья выросла из ростка моего цифрового сада.Я долго думал, аудит какого веб-приложения провести первым для своей небольшой заметки, чтобы показать наглядно подход из 5 шагов. С одной стороны, это должно быть что-то массовое, чем могут пользоваться большое количество людей с ограничениями. С другой стороны, владелец портала должен иметь достаточный бюджет для того, чтобы иметь возможность нанять высококвалифицированных веб-разработчиков, которые могут реализовать доступность.Наивно ожидать доступности от госучреждений, сайтов госполиклиник или порталов чиновников — там нет таких зарплат, как в частном секторе коммерческого ИТ. Также приложение должно быть хорошо известно обывателю и быть на слуху, даже если он им не пользуется.Кто же будет первым? Более 12 лет я являюсь клиентом Приорбанка (Беларусь, РБ). Банки — это важные сервисы, они определенно должны быть доступны людям с ограничениями. Я решил начать именно с него, это сервис который важен и для меня, поэтому в двойне интересно это сделать. Да простят меня сотрудники банка!)Методология аудитаЯ провел быстрый аудит из 5 шагов веб-приложения Приорбанка. Приложение большое, поэтому я выбрал проверить достаточно простой и популярный кейс — авторизироваться (или зарегистрироваться) и проверить свой счет на карте. Итак, пройдем быстро по всем пунктам и узнаем что получилось.1. Заявление о доступности и Skip LinksСразу отмечу отсутствие двух фундаментальных элементов:Заявление о доступности (Accessibility Statement)Ссылки для пропуска навигации (Skip Links)Подобные элементы — базовый шаг, и их отсутствие сигнализирует о поверхностном подходе. Отсутствие этих шаблонов — явные маркеры того, что подход к доступности у разработчиков и менеджеров продукта не очень серьезный, а значит можно ожидать явных проблем в этой области дальше.Подвал сайта Приорбанк2. Навигация с клавиатурыПытаюсь навигировать клавиатурой на главной странице и немного удивляюсь, что после верхнего меню попадаю на контролы управления динамической каруселью, которая рекламирует кредиты, реферальную программу и мобильное приложение. Только вот обо всем этом я не узнаю с плохим зрением, так как контент динамический и не размечен aria-live, а значит скринридер эти изменения озвучивать не будет.Кажется, что форма логина — это главная фича этого экрана, и я должен иметь возможность попасть на нее как можно раньше, но нет. Между шапкой и формой мне надо пройти рекламные сообщения.Карусель с рекламой акций банкаЕще грустнее, что фокус на многих элементах выключен. На самой главной кнопке формы «Войти» нет стилизации фокуса (:focus-visible). Очевидно, это очень плохо, так как я не вижу, где нахожусь. Проблема стара как мир: дизайнеры и разработчики не уделили время дизайну фокуса, и он работает так, как получилось.Фокус на кнопке Войти формы логинаА вот у соседней кнопки «Регистрация» фокус есть, но очень кривенький. А знаете почему? Потому что это не кнопка, а ссылка. На ссылках стилизация фокуса на сайте есть. Но странно, почему это ссылка, а не кнопка? Тут уже вопросы к проработке дизайн-системы. Кажется, что рядом с primary-кнопкой должна быть secondary. Причем ссылка обычно используется, когда ведет на физически другую страницу или маршрут, здесь же при регистрации открывается модальное окно.Фокус на кнопке Регистрация формы логинаМежду тем, закрыть модальное окно для русскоязычного пользователя непросто. Кнопка закрыть размечена английским словом «Close», хотя сайт на русском. Но в любом случае, наличие такого aria-label лучше, чем его полное отсутствие. Скорее всего, это что-то стандартное у UI Kit или просто разработчик на автомате вставил, чтобы кнопка с иконкой имела хоть какую-то подпись.Модальное окно регистрации и кнопка закрыть с подписью Close3. Контрастность цветовЧерный, желтый, белый — неплохое сочетание, достаточно контрастная комбинация.Узким местом стали несколько вторичных подписей и ссылок, а также плейсхолдеры инпутов. Тут текст серого цвета на белом, и он имеет плохой контраст. Соотношение, как видим на картинке, 2.4:1, что не попадает в WCAG Level AA.Вообще, даже без WCAG, можно просто прищуриться и попытаться разобрать текст, попробуйте, это будет сложновато.Проверка контраста цвета через CCA4. Работа со скринридером и клавиатуройВключаю плагин размытия, чтобы сэмулировать свое плохое зрение, и начинаю работать с сайтом через скринридер VoiceOver и клавиатуру. Вот тут самое интересное, потому что это реальная работа человека с ограничениями.Натыкаюсь опять на контролы управления каруселью вместо формы логина. Они никак не подписаны, сделаны ссылками, все это вводит в большое заблуждение конечного пользователя. Кажется, карусель сделана каким-то древним jQuery-плагином (вроде Owl Carousel) или руками с нуля, современные штуки, вроде Swiper, даже старых версий без настройки очень неплохо поддерживают доступность.Фокус на контролах управления каруселью в включенным VoiceOverНажимаю на кнопку «Регистрация» — открывается модальное окно, и я не попадаю в него. Фокус остается под модальным окном, и чтобы переместиться в него, надо пройти весь контент подложки. Из позитивного: когда я все же попал внутрь окна, то там фокус замкнулся (focus trap) и из окна уже не выходил. Но то, что при открытии я никак не был уведомлен об окне и тем более тут же не попал в него, — это большая проблема. Она полностью ломает навигацию.Фокус остался на кнопке Регистрация после открытия модального окнаВсе это говорит о незрелой дизайн-системе и не очень качественной UI-библиотеке. Как правило, современные фреймворки и даже нативный HTML dialog умеют захватывать фокус при открытии окна и автоматически направлять его на первый элемент внутри окна. Тут модалка сделана обычными div-контейнерами.Фокус остался на кнопке Регистрация после открытия модального окнаОшибки внутри формы не размечены динамическим контентом (aria-live) и никак не связаны с полями ввода (aria-errormessage), поэтому при попытке нажать «Продолжить» и не заполнив обязательные поля, скринридер ничего мне не сообщает об ошибках. То же самое и на форме логина.Нет уведомления от скринридера о появившихся ошибках на форме регистрацииКстати, бывших тестировщиков не бывает, нашел баг, где плейсхолдер поля ввода наложился на маску:Плейсхолдер поля ввода номера телефона наложился на маску вводаЕсть и другие проблемы для незрячих. Обнаружил несколько ссылок, которые были очень неинформативны. К примеру, номера для связи с диспетчером или обратной связью никак не связаны с подписями и озвучиваются просто как наборы цифр без контекста. Кнопка «Вверх» не имеет подписи и вводит в замешательство, так как непонятно, что она делает.Номер телефона диспетчера диктуется скринридером без контекстаКнопка подкрутки вверх не имеет подписиОк, идем наконец внутрь. Попадаю в личный кабинет и... Ну, тут тоже так быстро шапку и меню не пропустить. Мне всегда было очень интересно, как сделать макеты дебетовых карт доступными. И каково мое разочарование, когда я узнаю, что не могу попасть на них клавиатурой, потому что это не контролы. Хотя курсором мыши они кликабельны, чтобы провалиться внутрь. Ну и контент в них — это просто несвязные символы и цифры, обернутые в div и span с очень странными аттрибутами title. Для скринридера это фиаско.Компоненты дебетовых картЛадно. Идем в таблицу моих продуктов, может, тут я смогу понять, сколько денег на моей карте. Сразу же проблема с озвучиванием колонки с карточками: мне говорит, что ячейка пустая, хотя в ней отображается иконка типа карты (Visa, MasterCard, МИР и т.д.). Было бы хорошо это озвучить.Скринридер озвучивает таблицу моих продуктов - столбец тип картыОк, в последнем столбце мне удается все-таки услышать, сколько денег у меня на счету.Скринридер озвучивает сумму на карте5. Сканирование инструментамиПоследний шаг — это сканирование инструментами. Axe DevTools показал 12 ошибок на странице логина и 14 ошибок на главном дашборде личного кабинета. В основном это проблемы контраста серого на белом, неверные роли ARIA и даже картинки без alt-текста (среди них есть и не декоративные).Axe DevTools показывает 12 ошибок на странице логинаAxe DevTools показывает 14 ошибок на странице личного кабинетаКнопка профиля в шапке сделана картинкой без каких-либо подписей. Из-за того, что это не контрол, на нее нельзя даже попасть клавиатурой. Тут явно надо button, причем с aria-haspopup, так как она открывает диалоговое окно с настройками и информацией о профиле.Верстка кнопки профиля в шапке сайтаWAVE насчитал 16 ошибок на главной и 49 на дашборде (почему так много? 47 из них связаны с отсутствием href у ссылок, т.е. это просто так сверстаны кнопки).WAVE показывает 16 ошибок на странице логинаWAVE показывает 49 ошибок на странице личного кабинетаИнтересно глянуть на структуру заголовков: H1, а потом сразу H4 и H5. Причем именно второй уровень — это H5, а H4 — это заголовок третьего уровня в одном из виджетов. Такая ситуация это классика — встречается очень часто, так как на таких больших проектах над разными виджетами работают разные команды, и в семантике полный хаос и никто не думает про информационную архитектуру (IA).WAVE подсвечивает заголовки в личном кабинете пользователяLighthouse показал 81% доступности. Из интересного в дополнение к другим инструментам он подсветил, что контролы карусели на главной должны быть большего размера, чтобы на них было легче попасть курсором.Lighthouse подсвечивает проблемы с размерами контролов управления карусельюВыводыРезультаты аудита разочаровывают. Для такого крупного и финансово успешного банка, каким является «Приорбанк», уровень доступности его ключевого цифрового продукта недопустимо низок. Обнаруженные проблемы — от отсутствия базовой навигации до полной недоступности критического функционала для скринридеров — создают непреодолимые барьеры для людей с ограниченными возможностями.У банка, несомненно, есть ресурсы для формирования сильной фронтенд-команды и привлечения экспертов по доступности. Однако текущее состояние приложения говорит о том, что этот аспект пользовательского опыта либо не приоритизирован, либо реализуется бессистемно. Пользователям с инвалидностью пользоваться этим сервисом крайне неудобно, а в случае с банковскими операциями — практически невозможно.Скорее всего, проблема в том, что этот фронтенд жутко старое легаси, которое было немного разукрашено и подштопано под современный визуал. Мой анализатор показывает следующий стэк: jQuery UI, jQuery Mobile, Kendo UI, Bootstrap, Font Awesome, RequireJS, Animate.css.Стэк явно очень старый и отчасти это основная проблема. Взять тот же Font Awesome, он уже давно критикуется за свою любовь вставлять иконки тэгом <i>, что нарушает семантику и может плохо сказываться на доступности. У Приорбанка именно такие иконки, хотя Font Awesome можно настроить и на более нейтральный тэг вроде <span>.В таком легаси будет титанически сложно навести порядок. Уровень доступности jQuery UI по умолчанию очень низкий и безнадёжно устарел. Создание доступных интерфейсов с этой библиотекой требует огромных дополнительных усилий. А ведь я посмотрел очень маленький кусок приложения. С другой стороны, выглядит как отличный кейс для ИИ.Основные проблемы:Отсутствие базовых элементов доступности: нет заявления о доступности и skip linksПроблемы с навигацией клавиатурой: неправильный порядок табуляции, отсутствие видимого фокуса на ключевых элементахНизкий контраст: вторичный текст и плейсхолдеры не соответствуют WCAG Level AAПроблемы со скринридерами: модальные окна не захватывают фокус, отсутствие aria-live для динамического контента, неинформативные ссылки, плохая поддержка валидации форм скринридеромСемантические ошибки: неправильная структура заголовков, использование изображений вместо кнопок, отсутствие альтернативного текстаРекомендации:Внедрить skip links и заявление о доступности, что само собой будет служить уже ориентиром и дорожной картой по улучшения этого функционалаПереработать дизайн фокуса для всех интерактивных элементовУлучшить контрастность вторичного текста и плейсхолдеровИсправить работу модальных окон (использовать нативный dialog или правильно управлять фокусом)Добавить aria-live регионы для динамического контента и другую ARIA-разметку подкрепляя это тестированием на основных популярных скринридерах VoiceOver, JAWS, NVDAИсправить семантику: правильная структура заголовков, использование кнопок вместо изображений для интерактивных элементовДобавить альтернативный текст для всех значимых изображений и скрыть декоративные через роль role=""presentation""P.S.Не хочется только ругать свой банк, я уверен, что внутри есть множество продуктов, где доступность это одно из требований.К примеру, посмотрите на банкомат Приора, где есть панели с рельефно-точечным тактильным шрифтом для незрячих пользователей.Фото панели банкомата со шрифтом БрайляМеня всегда радуют такие штуки. Хотя, искренне не знаю, каков процент тех, кто знает шрифт Брайля из тех кто плохо видит или не видит совсем. На мой взгляд аудио-дикторы и Voice UI будут более востребованы в будущем.P.P.S.Подумываю сделать серию таких вот легких аудитов, без какого-то rocket science с очень простыми и очевидными советами как что-то улучшить. Если тема интересна, то обязательно пишите кто мог бы быть следующим кандидатом на аудит.Теги:accessibilityauditfrontendfrontend-разработкафронтендфронтенд-разработкадоступность сайтадоступностьwcagХабы:AccessibilityHTMLCSSJavaScript",627,0,0,8 мин,https://habr.com/ru/articles/965834/,13562,1838,4
"Практичные Python-привычки, которые реально повышают качество кода",robomania,2025-11-13T06:15:35.000Z,['Python *'],"robomania 5 часов назадПрактичные Python-привычки, которые реально повышают качество кодаВремя на прочтение2 минКоличество просмотров1.3KPython * Из песочницыНедавно я начал систематизировать практики которые обычно используются и помогают экономить время. Я хочу поделиться некоторыми из них. 1. Явное состояние и мемоизация   Скрытые состояния в замыканиях и декораторах часто приводят к трудноуловимым багам.  from functools import wraps

def memoize(func):
    cache = {}
    @wraps(func)
    def wrapper(*args):
        if args not in cache:
            cache[args] = func(*args)
        return cache[args]
    return wrapper Использование @wraps сохраняет имя функции, docstring и метаданные — критично для дебага и интеграции с Flask.   2. Асинхронность для продакшн   Асинхронность часто ухудшает код, если использовать её неправильно.  import asyncio, aiohttp

TOTAL_REQUESTS = 1_000_000
sem = asyncio.Semaphore(1000)

async def fetch(session, url):
    async with sem:
        try:
            async with session.get(url) as resp:
                return await resp.text()
        except aiohttp.ClientError:
            return None
          
async def main():
    urls = [f""https://api.site/{i}"" for i in range(TOTAL_REQUESTS)]
    async with aiohttp.ClientSession() as session:
        for i in range(0, len(urls), 10_000):
            chunk = urls[i:i+10_000]
            tasks = [fetch(session, url) for url in chunk]
            await asyncio.gather(*tasks) Контроль через Semaphore + чанки предотвращает OOM и блокировки API. Используйте create_task() для управления жизненным циклом корутин.   3. Ошибки и raise   Не ловите всё подряд и используйте новые возможности языка.  # Python 3.11+
user.is_admin or raise PermissionError(""Not allowed!"") raise как выражение и except* (Python 3.11) делают обработку ошибок лаконичной и безопасной:  # Python 3.11+
try:
    await asyncio.gather(fail1(), fail2())
except* ValueError as ve:  # Только ValueError
    print(f""ValueErrors: {ve.exceptions}"")
except* TypeError as te:   # Только TypeError
    print(f""TypeErrors: {te.exceptions}"")4. Типизация и валидация  from pydantic import validate_call
from typing import Annotated
from pydantic.types import Gt, Ge, Le 

@validate_call
def calculate_discount(
        price: Annotated[float, Gt(0)],
        discount: Annotated[float, Ge(0), Le(100)]
) -> float:
    return price * (1 - discount / 100) Constraints прямо в аннотациях делают сигнатуры самодокументируемыми и безопасными без дублирования проверок.  5. Ленивая загрузка и кеширование  @cached_property и functools.cache экономят время при дорогих вычислениях.  from functools import cached_property
import time

class UserReport:
    def __init__(self, user_id):
        self.user_id = user_id        
    @cached_property
    def total_spent(self):
        print(""Querying database..."")
        time.sleep(2)  # expensive call
        return 199.99
      
r = UserReport(123)
print(r.total_spent)     # computed once
print(r.total_spent)     # cached instantly
del r.__dict__[""total_spent""]
print(r.total_spent)     # recomputed after cache resetРезультат хранится в dict объекта, можно сбросить при необходимости. Отлично подходит для API-запросов и конфигураций.  6. Python 3.14+Новые возможности языка ускоряют работу и упрощают код:uuid7() — уникальные и сортируемые по времени ключиContextVar как контекстный менеджерt-strings (t""..."") для отложенных шаблоновsubTests для granular тестированияfrom string.templatelib import Template

def render(template: Template):
    parts = []
    for item in template:
        if isinstance(item, str):
            parts.append(item)
    return """".join(parts)

user = ""Alice""
role = ""admin""
t = t""user: {user} — role: {role}""
s = render(t)Жалко что не добавили .format() для t -строкЗаключениеДаже небольшие изменения в подходе к разработке дают ощутимый эффект на качество кода и скорость разработки.   Теги:pythonпривычки программистаХабы:Python",1300,0,0,2 мин,https://habr.com/ru/articles/965908/,3960,448,1
Распознаём позу человека во Flutter Web с MediaPipe,alexeyinkin,2025-11-12T19:46:55.000Z,"['Dart *', 'Flutter *']","alexeyinkin 15 часов назадРаспознаём позу человека во Flutter Web с MediaPipeУровень сложностиСреднийВремя на прочтение20 минКоличество просмотров268Dart * Flutter * ТуториалПереводАвтор оригинала: Alexey InkinДавайте распознаем позу по видео с вебкамеры вот так:Финальное приложение, которое мы сделаем в этой статье.Для этого есть библиотека MediaPipe, которая может распознавать много всего в картинках, текстах и звуках. Среди прочего там есть модель для распознания положения тела на изображениях.Здесь можно попробовать официальное демо.Ещё есть CodePen, чтобы быстро попробовать код на JavaScript:https://codepen.io/mediapipe‑preview/pen/abRLMxNНо эта модель работает только в Android, iOS, на Python и JavaScript, но не во Flutter напрямую.Кто‑то сделал пакет flutter_mediapipe, но он заброшен уже 4 года и не поддерживает веб.Поэтому давайте подключим официальную реализацию на JavaScript в качестве собственного веб‑плагина для Flutter.Готовое демо моего приложения — здесь (только Chrome)Скачайте исходники здесь (потому что я буду пропускать некоторые вещи):https://github.com/alexeyinkin/flutter‑mediapipeСоздаём плагинПлагин — это специальный вид пакета Dart, который подключает разные имплементации в зависимости от того, под какую платформу собираем приложение.Вот отличный официальный учебник, как писать плагины:https://docs.flutter.dev/packages‑and‑plugins/developing‑packagesЕсть ещё прекрасное введение в написание именно веб‑плагинов, от автора официального пакета url_launcher. Там рассказано, как они добавили поддержку веба в этот пакет, когда Flutter только начал поддерживать веб:Часть 1 объясняет базовый подход — такой же, как был в плагинах для Android и iOS: так называемый method channel, чтобы делегировать что‑то нативному коду на этих платформах.Часть 2 упрощает это и убирает method channel, потому что веб‑плагины и так написаны на Dart, а значит, можно вызывать все методы имплементации напрямую.Обе статьи обращаются только к стандартному API браузера и не обращаются к произвольному JavaScript, взятому где‑то ещё. Поэтому в этой статье я расскажу, как обращаться к произвольному JavaScript, основываясь на всём, что вы узнали в тех статьях.Используя архитектуру из последней статьи по url_launcher, я сделал три пакета Dart:flutter_mediapipe_vision — главный пакет. Все приложения, которые хотят распознавать позы на изображениях, подключают его как зависимость. И только его. Он уже подтягивает в проект другие пакеты как зависимости и вызывает методы конкретной имплементации. Ненужные имплементации Flutter сам стрясёт с помощью tree‑shaking.flutter_mediapipe_vision_platform_interface описывает интерфейс, которому каждая имплементация должна соответствовать. Этот пакет не делает ничего полезного, только подменяет имплементацию, чтобы первый пакет об этом ничего не знал.flutter_mediapipe_vision_web — реализация для веба, главный фокус этой статьи. Он зависит от второго пакета, потому что содержит имплементацию интерфейса, описанного там, и ничего не знает о первом пакете. Первый же пакет зависит от него и подтягивает его рекурсивно во все приложения, где используется.flutter_mediapipe_visionКаким должен быть интерфейс конечного пользователя? Статические функции хорошо работают с подменяемыми имплементациями:class FlutterMediapipeVision {
  static Future<void> ensureInitialized() async {
    await FlutterMediapipeVisionPlatform.instance.ensureInitialized();
  }

  static Future<PoseLandmarkerResult> detect(Uint8List bytes) async {
    return await FlutterMediapipeVisionPlatform.instance.detect(bytes);
  }
}Этот класс преобразует вызовы статических функций в вызовы методов конкретной имплементации.Первая функция инициализирует модель. Её можно назвать как угодно, но ensureInitialized() — это удобно. Вы же помните WidgetsFlutterBinding.ensureInitialized()Вторая функция получает байты изображения (каждого кадра) и вызывает detect() на модели. Эта функция называется именно так во всех имплементациях MediaPipe.Обратите внимание на возвращаемый тип. Скоро мы его опишем.flutter_mediapipe_vision_platform_interfaceТипы данныхНачнём с типов. В библиотеке JavaScript, которую мы подключим, есть типы для распознанных точек и общего результата. Однако, наш плагин должен возвращать что‑то независимое от платформы, поэтому нужно описать собственные типы.Это точка, распознанная в позе:class NormalizedLandmark {
  final double x;
  final double y;

  const NormalizedLandmark({required this.x, required this.y});

  Offset get offset => Offset(x, y);
}Она называется normalized, потому что x и y будут от 0 до 1, если они помещаются в кадре. Они также могут быть меньше нуля или больше единицы, если изображение обрезано и модель думает, что эта конкретная точка находится за кадром — как мой локоть здесь:Некоторые точки находятся вне кадра, и модель пытается отгадать, где мой локоть.А почему бы нам не использовать Offset из dart:ui? Библиотека ещё возвращает z — расстояние от камеры, и ещё несколько интересных вещей, которые нам пока не нужны, но хорошо иметь возможность их потом добавить. Поэтому просто Offset нам не хватит.Кроме того, этот тип NormalizedLandmark есть во всех имплементациях: TypeScript, Java, и др. Поэтому лучше, когда всё согласуется.Дальше — результат распознания всего изображения:class PoseLandmarkerResult {
  final List<List<NormalizedLandmark>> landmarks;

  const PoseLandmarkerResult.empty() : landmarks = const [];
  const PoseLandmarkerResult({required this.landmarks});
}Библиотека возвращает список распознанных поз (первое измерение списка). Каждая поза — список точек с фиксированными индексами (второе измерение списка):Индексы точек в PoseLandmarkerResult.Базовый класс имплементацийС этими типами мы теперь можем описать класс, который каждая имплементация будет наследовать:abstract class FlutterMediapipeVisionPlatform extends PlatformInterface {
  FlutterMediapipeVisionPlatform() : super(token: _token);

  static final Object _token = Object();

  static FlutterMediapipeVisionPlatform _instance =
    FlutterMediapipeVisionMethodChannel();

  static FlutterMediapipeVisionPlatform get instance => _instance;

  static set instance(FlutterMediapipeVisionPlatform instance) {
    PlatformInterface.verify(instance, _token);
    _instance = instance;
  }

  Future<void> ensureInitialized() {
    throw UnimplementedError();
  }

  Future<PoseLandmarkerResult> detect(Uint8List bytes) {
    throw UnimplementedError();
  }
}Тут много всего.Самое главное — мы определяем функции бизнес‑логики ensureInitialized и detect.Дальше — нужен какой‑то _instance по умолчанию, который мы создаём. Подробнее о нём — чуть позже.И наконец, есть объект _token. Вот зачем он нужен. Разработчики Flutter могут что‑то поменять в базовом классе PlatformInterface, и это не должно ломать наш код. Поэтому они сказали, что этот класс можно только наследовать, но не реализовывать. Мы здесь используем extends, поэтому нам проблемы не грозят. Но вообще мы не можем контролировать, кто ещё сделает имплементацию нашего плагина под ещё какую‑нибудь платформу (или даже подменит нашу имплементацию для веба), и мы не влияем на то, будет ли у них extends или implements. Если они сделают implements, их код может какое‑то время работать, а потом внезапно сломается с каким‑нибудь обновлением для какой‑нибудь одной платформы. Поэтому мы делаем такую штуку, которая сломает их код раньше. Мы используем объект _token, у которого единственная работа — быть всегда одним и тем же (как у Зюганова). Если чья‑то реализация сделает implements, она не сможет предъявить тот самый _token, поэтому set instance выдаст ошибку.Так, а что там за instance по умолчанию?const MethodChannel _channel
  = MethodChannel('ainkin.com/flutter_mediapipe_vision');

class FlutterMediapipeVisionMethodChannel
  extends FlutterMediapipeVisionPlatform {
  @override  Future<void> ensureInitialized() async {
    await _channel.invokeMethod<void>('ensureInitialized');
  }

  @override  Future<PoseLandmarkerResult> detect(Uint8List bytes) async {
    final native = await _channel.invokeMethod<void>('detect');
    throw UnimplementedError('TODO: Convert.');
  }
}В стародавние времена, когда Flutter поддерживал только Android и iOS, единственным способом вызвать что‑то нативное было создать объект MethodChannel и вызывать на нём «методы» с помощью invokeMethod(name). Flutter смотрел на название канала и название метода и дёргал нужный метод в нативном коде. Не было никаких подменяемых instance, потому что вся подмена работала при сборке приложения под конкретную платформу.Для обратной совместимости, если Flutter не проист наш плагин сделать ничего необычного, мы по умолчанию должны продолжать делать то же самое. Поэтому мы закладываем это в instance по умолчанию.Однако, мы не будем сейчас поддерживать другие платформы, кроме веба. Поэтому нам не нужна такая имплементация. Нам несложно вызвать потенциальный нативный ensureInitializee() и подождать, пока он вернёт управление. Но мы пока не можем сделать ничего осмысленного в detect(), потому что это потребует какой‑то контракт на обмен данными с нативным кодом, а мы пока не хотим с этим разбираться. Поэтому выбросим ошибку.flutter_mediapipe_vision_webНачнём плагин с вот этого:class FlutterMediapipeVisionWeb extends FlutterMediapipeVisionPlatform {
  static void registerWith(Registrar registrar) {
    FlutterMediapipeVisionPlatform.instance = FlutterMediapipeVisionWeb();
  }

  Future<void>? _initFuture;

  @override
  Future<void> ensureInitialized() =>
    _initFuture ?? (_initFuture = _initOnce());

  Future<void> _initOnce() async {
    // ...
  }

  @override
  Future<PoseLandmarkerResult> detect(Uint8List bytes) async {
    // ...
  }
}registerWith() — это волшебная функция, которую Flutter вызовет где‑то на ранней стадии после запуска, если приложение собрано для веба. Создаём наш instance и устанавливаем его для использования во всех вызовах, зависящих от платформы.Добро пожаловать в веб!Код Dart транспилируется в JavaScript или WASM. В любом случае у него есть прямой доступ ко всему в браузере, как у любого кода на JavaScript — через глобальную переменную globalContext, определённую в dart:js_interop. Поэтому нет почти никакой разницы между объектами Dart и JavaScript, они все однородные для браузера, в котором приложение работает.Загружаем код MediaPipeЯ позаимствовал этот кусок из Firebase и немного упростил. Обидно, что нужно самим писать код подгрузки JavaScript. Лучше бы во Flutter сделали для нас что‑то, что можно вызвать одной строкой.Этот код загружает скрипт из src и помещает объект его модуля в глобальную переменную, определяемую windowVar:  Future<void> _injectSrcScript(String src, String windowVar) async {
    final web.HTMLScriptElement script =
      web.document.createElement('script') as web.HTMLScriptElement;
  
    script.type = 'text/javascript';
    script.crossOrigin = 'anonymous';
  
    final stringUrl = src;
    script.text =
      '''
      window.my_trigger_$windowVar = async (callback) => {
        console.debug(""Initializing MediaPipe $windowVar"");
        callback(await import(""$stringUrl""));
      };
      ''';
  
    web.console.log('Appending a script'.toJS);
    web.document.head!.appendChild(script);
  
    Completer completer = Completer();
    globalContext.callMethod(
      'my_trigger_$windowVar'.toJS,
      (JSAny module) {
        globalContext[windowVar] = module;
        globalContext.delete('my_trigger_$windowVar'.toJS);
        completer.complete();
      }.toJS,
    );
    await completer.future;
  }Значение windowVar может быть любым, если оно не конфликтует с другими глобальными переменными. Начнём initOnce() с загрузки кода MediaPipe:const _windowVar = 'flutter_mediapipe_vision';
// ...

  Future<void> _initOnce() async {
    await _injectSrcScript(
      'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js',
      _windowVar,
    );
    // ...Это загрузит последнюю версию. Ещё можно было скачать её один раз и положить в assets нашего пакета, но пока хватит и этого.Когда этот код завершится, модуль MediaPipe будет в глобальной переменной, и к нему можно будет обращаться через globalContext[_windowVar]. Можно сразу же вызывать нужные нам функции:globalContext[_windowVar]['PoseLandmarker'].callMethod(
  'createFromOptions',
  ...
);Но лучше обеспечить безопасность типов.Накладывание интерфейсов Dart на объекты JavaScriptПомните наш класс NormalizedLandmark? На стороне JavaScript ему соответствует обычный объект со свойствами x и y, к которым можно обратиться из Dart через landmark['x'] и landmark['y'], потому что у JSObject есть оператор []. Но так можно легко допустить ошибки. К счастью, мы можем наложить на этот объект интерфейс Dart:extension type NormalizedLandmark._(JSObject _) implements JSObject {
  external num get x;
  external num get y;
}И теперь, если мы приведём этот объект точки, полученный из MediaPipe, к этому классу, то компилятор гарантирует, что обращения к свойствам будут правильными:final landmark = unsafeLandmark as NormalizedLandmark;
print(landmark.x);Extension typesНо что это за интерфейс такой? Это конструкция extension type, которая буквально и означает наложение на любой объект интерфейса, который в ней определён — без оборачивания в объект‑адаптер. Это абстракция времени компиляции, которая не существует во время выполнения. Можете прочитать про неё в документации Dart здесь.Давайте чуть отвлечёмся от главной задачи и разберём extension types подробнее, чтобы потом вернуться к JavaScript с этим новым знанием.Документация Dart по extension types содержит такой пример, который сужает интерфейс int до одного оператора:extension type IdNumber(int id) {
  // Wraps the 'int' type's '<' operator:
  operator <(IdNumber other) => id < other.id;
  // Doesn't declare the '+' operator, for example,
  // because addition does not make sense for ID numbers.
}
// ...
final safeId = IdNumber(42);Этот код говорит, что:Мы используем какой‑то IdNumber, чтобы работать с какими‑то ID.Это не класс, существующий во время выполнения, потому что это слишком дорого, поэтому extension type.Вместо класса мы используем int для хранения этих ID, потому что int — это самый дешёвый способ хранения чисел. Поэтому после названия типа идёт (int id) — это показывает, что именно мы оборачиваем.Этот интерфейс лишает наш int всех методов, операторов и свойств, которые мы не объявим далее.Мы описываем operator <, и это всё, что можно делать с нашим ID.Конструктор этого типа не описан как функция‑член, потому что конструктор как функция‑член нужен только настоящим типам, которым потенциально может понадобиться несколько конструкторов, потому что в них может проделываться какая‑то работа и мы можем захотеть проделывать её по‑разному. Но у extension type конструктор — это просто оборачивание во время компиляции, которое не превращается ни в какой код, поэтому у него всегда ровно один конструктор, и не было смысла делать для него синтаксис функции‑члена. Поэтому для него сделали такой синтаксис: (int id) сразу после названия типа.Ладно, а как это всё применимо к нашему примеру? Он, вроде бы, совсем другой:extension type NormalizedLandmark._(JSObject _) implements JSObject {
  external num get x;
  external num get y;
}В нашем примере:Мы оборачиваем JSObject и сразу же реализуем тот же интерфейс JSObject. Это значит, что мы не лишаем объект никаких членов этого интерфейса и будем только добавлять. Это нужно, потому что вскоре у нас будет JSArray<NormalizedLandmark>, а JSArray может принимать только JSObject и его подклассы, поэтому это наследование нужно сохранить.Параметр конструктора называется _, потому что в отличие от примера с ID мы не перенаправляем никакие вызовы на этот объект, а значит, ему не нужно никакое имя.Конструктор мы сделали приватным с помощью ._ Поэтому мы можем только приводить объекты к этому типу с помощью as, но не создавать их напрямую.Помечаем геттеры как external. Так мы говорим компилятору, что эти свойства уже есть в объекте JavaScript, и они просто будут работать.Когда мы оборачиваем объекты, исходящие от JavaScript или WASM, в extension type, это называется «interop type„.“»Создаём все interop typesНам нужно гораздо больше таких типов, чтобы создать объект, обнаруживающий позы на картинках, вызывать его методы и возвращать результат.Можно написать эти типы вручную, глядя на исходники MediaPipe на TypeScript:fileset_resolver.ts.templatelandmark.d.tspose_landmarker.tspose_landmarker_options.d.tspose_landmarker_result.tstask_runner_options.d.tsvision_task_options.d.tsВ теории все interop types в Dart можно сгенерировать из исходников TypeScript, но я пока с этим не разбирался. Написать их вручную — хорошая практика для начала.Вот, что я выцепил из TypeScript — только те методы и свойства, которые нам понадобятся.Результат функции detect:extension type PoseLandmarkerResult._(JSObject _) implements JSObject {
  external JSArray<JSArray<NormalizedLandmark>> get landmarks;
}Объект, распознающий позы:extension type PoseLandmarker._(JSObject _) implements JSObject {
  external JSPromise<PoseLandmarker> createFromOptions(
    WasmFileset fileset,
    PoseLandmarkerOptions options,
  );

  external void detect(HTMLImageElement img, JSFunction callback);
}Объект параметров для него:extension type PoseLandmarkerOptions._(JSObject _) implements JSObject {
  external PoseLandmarkerOptions({
    BaseOptions baseOptions,
    int numPoses,
    String runningMode,
  });

  external BaseOptions get baseOptions;
  external int get numPoses;
  external String get runningMode;
}Вложенный объект в нём:extension type BaseOptions._(JSObject _) implements JSObject {
  external BaseOptions({String modelAssetPath});
  external String get modelAssetPath;
}WasmFileset, что бы это ни значило:extension type WasmFileset._(JSObject _) implements JSObject {}Fileset resolver:extension type FilesetResolver._(JSObject _) implements JSObject {
  external JSPromise<WasmFileset> forVisionTasks(String basePath);
}И наконец, главный объект модуля MediaPipe:import 'fileset_resolver.dart' as fsr;
import 'pose_landmarker.dart' as plm;

extension type MediaPipe._(JSObject _) implements JSObject {
  external fsr.FilesetResolver get FilesetResolver;
  external plm.PoseLandmarker get PoseLandmarker;
}Инициализируем модельПродолжим писать функцию инициализации плагина:MediaPipe get mp => globalContext[_windowVar] as MediaPipe;

PoseLandmarker? _landmarker;Future<void> _initOnce() async {
  await _injectSrcScript(
    'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js',
    _windowVar,
  );

  final fs = await mp.FilesetResolver.forVisionTasks(
    'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm',
  ).toDart;

  final options = PoseLandmarkerOptions(
    baseOptions: BaseOptions(
      modelAssetPath:
        ""packages/flutter_mediapipe_vision_platform_interface/assets/""
        ""assets/models/pose_landmarker_lite.task"",
    ),
    numPoses: 5,
    runningMode: ""IMAGE"",
  );
  
  _landmarker = await mp.PoseLandmarker.createFromOptions(fs, options).toDart;
}Файл модели ещё нужно скачать отдельно (я выбрал версию lite) вот здесь:https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarkerПоскольку модель одна и та же для MediaPipe на всех платформах, лучше всего положить её в общий пакет, а не в веб‑пакет. Лучше всего подойдёт flutter_mediapipe_vision_platform_interface, потому что все имплементации от него зависят, хотя формально модель не относится к интерфейсу.В общем, когда функция завершится, мы получим объект, обнаруживающий позы, в поле _landmarker.Распознаём позыВсю работу делает этот метод:  @override  Future<PoseLandmarkerResult> detect(Uint8List bytes) async {
    final el = await _createImageFromBytes(bytes);
    // ...
  }Начинаем с создания HTMLImageElement из переданных байт, потому что функция detect в MediaPipe принимает именно его. Вот так:Future<web.HTMLImageElement> _createImageFromBytes(Uint8List bytes) async {
  final completer = Completer();

  final blob = web.Blob(
    [bytes.toJS].toJS,
    web.BlobPropertyBag(type: _detectImageFormat(bytes)),
  );
  final imageUrl = web.URL.createObjectURL(blob);
  final el = web.document.createElement('img') as web.HTMLImageElement;

  el.onload = () {
    web.URL.revokeObjectURL(imageUrl);
    completer.complete();
  }.toJS;
  el. {
    web.URL.revokeObjectURL(imageUrl);
    completer.completeError('Cannot load the image.');
  }.toJS;

  el.src = imageUrl;
  await completer.future;
  return el;
}В JavaScript конструктор объекта Blob (binary long object) принимает двумерный массив байт. Поэтому сначала превращаем на Uint8List в массив JavaScript, вызывая его геттер .toJS. Он есть у многих типов Dart, чтобы превратить их во что‑то, подходящее для передачи в функции JavaScript. Потому что код Dart хоть и транспилируется в JavaScript, но всё‑таки иногда он транспилируется в свои особые объекты с несовместимым интерфейсом, а иногда компилятору просто нужно такое приведение для формальной правильности типов. После этого оборачиваем этот массив в ещё один List и преобразуем его тоже в массив JavaScript, чтобы получить нужный двумерный массив.Формат картинки определяем по первым байтам, я здесь пропущу функцию _detectImageFormat.Дальше нам нужен какой‑то URL, который мы установим в наш тег img, потому что только так картинки можно поместить в элемент HTML.Для этого используется так называемый blob URL. Так мы говорим браузеру: «Эй, нам надо показать эти байты в объекте img. Дай нам, пожалуйста, какой‑нибудь виртуальный URL, чтобы он на них ссылался.»Браузер сохраняет эти байты куда‑то к себе во внутреннюю таблицу и даёт нам «талончик», по которому страница может обращаться к этой картинке. Он выглядит примерно так:blob:http://localhost:40000/fd108f07-5e55-43d1-b5cd-691b973c03d6Это внутренняя штука для даннй сессии браузера. Интересно, что по такому адресу можно даже открыть картинку в новой вкладке:Картинку можно открыть в новой вкладке по blob URL.Короче, мы создаём объект img и устанавливаем наш URL в его src. Теперь нужно дождаться, когда картинка загрузится. Для этого нужно указать два обработчика:  el.onload = () {
    web.URL.revokeObjectURL(imageUrl);
    completer.complete();
  }.toJS;
  el.onerror = () {
    web.URL.revokeObjectURL(imageUrl);
    completer.completeError('Cannot load the image.');
  }.toJS;Они оба завершают completer, так что функция может вернуть готовый к работе img или выбросить ошибку. Ещё они освобождают URL, чтобы не тратить память браузера. В конце концов, мы будем делать это для каждого кадра.Ещё обратите внимание, что когда мы передаём функцию Dart как колбэк JavaScript, нужно преобразовать её в функцию JavaScript с помощью геттера toJS.Когда мы получили элемент img, можно передавать его в функцию detect:import 'src/interop/pose_landmarker_result.dart' as js_plr;
// ...
  @override
  Future<PoseLandmarkerResult> detect(Uint8List bytes) async {
    PoseLandmarkerResult r = PoseLandmarkerResult.empty();
    final el = await _createImageFromBytes(bytes);
  
    _landmarker!.detect(
      el,
      (js_plr.PoseLandmarkerResult? result) {
        r = result?.toDart ?? PoseLandmarkerResult.empty();
      }.toJS,
    );
  
    return r;
  }Обратите внимание, что функция detect в MediaPipe не возвращает результат, а передаёт его в колбэк. Это позволяет ей освободить память, когда вызов завершится. На практике объект переживает этот колбэк, но на это нельзя полагаться. Нужно вытащить из этого объекта всё, что нам нужно, пока работает колбэк, и положить это в наш кросс‑платформенный объект результата, который мы определили во втором пакете.Вот и весь код наших пакетов!Связываем пакеты вместеПакет с веб‑имплементацией должен объявить в своём pubspec.yaml, что он содержит имплементацию плагина, чтобы Flutter знал, какой метод вызвать при запуске приложения, чтобы подменить имплементацию на эту:flutter:
  plugin:
    platforms:
      web:
        pluginClass: FlutterMediapipeVisionWeb
        fileName: flutter_mediapipe_vision_web.dartПакет с интерфейсом должен объявить asset с моделью, чтобы она попадала в сборки приожений, которые используют пакет:flutter:
  assets:
    - assets/models/pose_landmarker_lite.taskА публичный пакет для пользователей должен заэндорсить пакет с веб‑имплементацией:flutter:
  plugin:
    platforms:
      web:
        default_package: flutter_mediapipe_vision_webПриложениеПоказываем видео с камерыПервым делом нужно просто показать видео с камеры на экране. Для этого создадим контроллер камеры и покажем видео в виджете CameraPreview:import 'package:camera/camera.dart';
import 'package:flutter/material.dart';

late CameraController cameraController;

Future<void> main() async {
  WidgetsFlutterBinding.ensureInitialized();
  await FlutterMediapipeVision.ensureInitialized();

  cameraController = CameraController(
    (await availableCameras()).first,
    ResolutionPreset.low,
    enableAudio: false,
  );
  await cameraController.initialize();

  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      home: Scaffold(
        appBar: AppBar(title: Text('MediaPipe demo')),
        body: Center(
          child: CameraPreview(cameraController),
        ),
      ),
    );
  }
}Это минимальное приложение, чтобы показывать видео с камеры на экране. Тут есть недостатки. Например, приложение запрашивает разрешение на видео в самом начале, ещё ничего не показывая и не объясняя пользователю, и на это время всё блокируется. И никак не обрабатывает отказ. Но оно делает самое главное:Минимальное приложение для показа видео с камеры.Получаем и анализируем кадрыДавайте напишем контроллер для распознания:class InferenceController extends ChangeNotifier {
  final CameraController cameraController;

  PoseLandmarkerResult get lastResult => _lastResult;
  PoseLandmarkerResult _lastResult = PoseLandmarkerResult.empty();

  InferenceController({required this.cameraController});

  Future<void> start() async {
    while (true) {
      await _tick();
    }
  }

  Future<void> _tick() async {
    final file = await cameraController.takePicture();
    final bytes = await file.readAsBytes();
  
    _lastResult = await FlutterMediapipeVision.detect(bytes);
    notifyListeners();
  }
}Когда вызывается start(), он работает бесконечно. С этим будут проблемы на мобильных устройствах, когда приложение может быть вытеснено из памяти, но этого достаточно для минимальной веб‑версии.В цикле мы получаем кадры с помощью cameraController.takePicture(), потом передаём их байты в плагин и сохраняем результат анализа.Давайте создадим этот контроллер в main():late InferenceController inferenceController; // ИЗМЕНЕНО

Future<void> main() async {
  WidgetsFlutterBinding.ensureInitialized();
  await FlutterMediapipeVision.ensureInitialized();

  final cameraController = CameraController(
    (await availableCameras()).first,
    ResolutionPreset.low,
    enableAudio: false,
  );

  await cameraController.initialize();

  // ДОБАВЛЕНО:
  inferenceController = InferenceController(cameraController: cameraController);

  unawaited(inferenceController.start());
  runApp(const MyApp());
}Показываем скелет поверх видеоДля этого сделаем виджет CameraOverlayWidget:class CameraOverlayWidget extends StatelessWidget {
  final InferenceController inferenceController;

  const CameraOverlayWidget({required this.inferenceController});

  @override
  Widget build(BuildContext context) {
    return ListenableBuilder(
      listenable: inferenceController,
      child: CameraPreview(inferenceController.cameraController),
      builder: (context, child) {
        return CustomPaint(
          foregroundPainter: CameraOverlayPainter(
            inferenceController: inferenceController,
          ),
          willChange: true,
          child: child,
        );
      }
    );
  }
}Этот виджет слушает контроллер распознания и перестраивается при каждом уведомлении о его обновлении. Обратите внимание, что мы создаём виджет CameraPreview вне функции builder и передаём его как child в ListenableBuilder. Это исключает CameraPreview из перестройки на каждом кадре и экономит ресурсы.Виджет CustomPaint применяет foregroundPainter, чтобы рисовать поверх child.Давайте сделаем этот CameraOverlayPainter:class CameraOverlayPainter extends CustomPainter {
  final InferenceController inferenceController;

  static final _paint = Paint()
    ..color = Colors.white
    ..isAntiAlias = true
    ..style = PaintingStyle.fill
    ..strokeWidth = 5;

  static const _pointRadius = 5.0;

  CameraOverlayPainter({required this.inferenceController});

  @override  void paint(Canvas canvas, Size size) {
    _paintPose(canvas, size);
  }

  void _paintPose(Canvas canvas, Size size) {
    final pose = inferenceController.lastResult.landmarks.firstOrNull;
    if (pose == null) {
      return;
    }

    final leftShoulder = pose[Points.leftShoulder].offset.timesSize(size);
    final rightShoulder = pose[Points.rightShoulder].offset.timesSize(size);
    // Same for every point.

    _paintLine(canvas, leftShoulder, rightShoulder);
    // Same for every line.

    _paintPoint(canvas, leftShoulder);
    _paintPoint(canvas, rightShoulder);
    // Same for every point.
  }

  void _paintPoint(Canvas canvas, Offset offset) {
    canvas.drawCircle(offset, _pointRadius, _paint);
  }

  void _paintLine(Canvas canvas, Offset pt1, Offset pt2) {
    canvas.drawLine(pt1, pt2, _paint);
  }

  @override
  bool shouldRepaint(covariant CustomPainter oldDelegate) {
    return true;
  }
}

extension on Offset {
  Offset timesSize(Size size) => Offset(dx * size.width, dy * size.height);
}

abstract final class Points {
  static const leftShoulder = 11;
  static const rightShoulder = 12;
  // Same for every point.
}Этот класс выбирает из распознанного результата главные точки, которые нас интересуют, и соединяет их линиями. Все координаты от 0 до 1, поэтому умножаем их на size — текущий размер виджета. Поскольку виджет поверх видео и имеет такой же размер, все координаты правильны.Наш финальный результат:Скелет, нарисованный поверх видео с камеры. Всё готово.Вот задеплоенное демо ещё раз:https://alexeyinkin.github.io/flutter‑mediapipe/Совместимость с браузерамиВ Chrome всё работает.В Firefox 144 не работает, потому что в пакете camera есть баг, который я скоро опишу и отправлю.В Safari не работает просто так, без всяких симптомов. Если знаете, почему — скажите.ДальшеВ следующей статье доработаем архитектуру и используем распознанные точки, чтобы распознать движения более высокого уровня.Подпишитесь в Telegram, чтобы не пропустить: ainkin_comРусские переводы реже и с задержкой здесь: ainkin_com_ruТеги:mediapipecomputer visionХабы:DartFlutter",268,0,0,20 мин,https://habr.com/ru/articles/965852/,30767,3498,2
Проблемы контурных карт: анализ графики Europa Universalis 5,5a5ha,2025-11-12T17:04:17.000Z,"['Игры и игровые консоли', 'CGI (графика) *', 'Разработка игр *', '3D-графика *']","5a5ha 18 часов назадПроблемы контурных карт: анализ графики Europa Universalis 5Уровень сложностиПростойВремя на прочтение8 минКоличество просмотров4KИгры и игровые консолиCGI (графика) * Разработка игр * 3D-графика * ОбзорОткрывая глобальную стратегию, обычно вы ожидаете лёгкую нагрузку для видеокарты, т.к. такие игры никогда не славились выдающейся графикой. Однако, из-за ряда решений, которые скорее всего были приняты для упрощения разработки, мы получаем довольно плохую производительность. Видя низкий фпс, мне стало интересно, а собственно, что здесь занимает столько ресурсов?Для анализа взята релизная версия (1.0.0) с рендером Vulkan (который разработчики позиционируют как основной), GPU Capture сделана при помощи Nvidia Nsight, разрешение экрана 2560×1440.ВступлениеЧтобы поместить в перспективу то, о чём я буду говорить дальше, вкратце опишу почему слишком много треугольников это плохо. Если вы в целом знакомы с конвейером рендера, этат раздел можно пропустить.У нас есть 5 основных этапа вызова отрисовки для opaque (непрозрачных) объектов: привязка меша, вершинный шейдер, сборка примтивов + растеризатор, тест глубины и стенсила, пиксельный шейдер. Без лодов (т.е. без привязки другого меша на первом этапе), вершинный шейдер и сборка примитивов будут одинаково работать, занимай ваш объект хоть весь экран, хоть один пиксель. Растеризатор превращает информацию треугольников в пиксели, и здесь я остановлюсь поподробнее. Когда растеризуется меш, растеризатор гарантирует, что один конечный пиксель будет соответствовать одному семплу одного треугольника. Для этого производится тест покрытия: берётся точка (семпл) (в данной ситуации центре пикселя, но в зависимости от MSAA точек может быть несколько в разных местах), и если она находится внутри треугольника, то треугольник проходит тест, и его данные берутся для пиксельного шейдера. Если несколько треугольников проходят тест, то берётся тот, который ближе к камере (имеет меньшую глубину). Если семпл приходится на грань между треугольниками, то выбирается левый или верхний треугольник относительно точки (top-left fill rule). При этом, растеризатор не знает ничего про другие меши в других вызоывах отрисовки, так что вся эта работа может быть впустую, если в дальнейшем это место перекроет другой меш.После растеризатора происходит тест глубины, который сравнивает, ближе ли текущий пиксель к камере, чем значения из буфера (возможен более ранний тест глубины, но в EU5 его нет). Если ближе, значит этот пиксель виден, запускается пиксельный шейдер, который перезаписывает буфер цвета. При этом, пиксельный шейдер запускается не просто ради одного пикселя, а для блока 2х2. Это связанно с тем, как шейдер определяет, какой лод текстуры использовать: при помощи операций ddx() и ddy(), которые возвращают частную производную (дельту) переданной переменной по отношению к соседнему пикселю. Т.е., если вызвать ddx(x), когда у текущего пикселя x = 0.1, а у соеднего 0.4, ddx вернёт разницу 0.3. Понимая, как быстро меняются uv координаты треугольника, можно вычислить, текстура какого размера будет совпадать по своей плотности пикселей с необходимым колличеством пикселей на экране. При этом есть несколько нюансов: мы не можем выбрать с каким конкретно соседом сравнивать значение. Если ddx вызывает верхний правый шейдер в блоке 2x2, то вернётся дельта из верхнего левого (в некоторых реализациях при горизонтальной проверке сравнивается верхний и нижний ряд, а их результат усредняется). в ситуации с одним пикселем на треугольник, результат операции ddx может давать  некорректные значения. Поэтому, из-за увелечения треугольников на экране, качество изображения может ухудшаться, а не улучшаться.если растеризированный треугольник состоит только из одного пикселя, то мы всё равно вызовем пиксельный шейдер 4 раза, и отбросим лишние результаты. Поэтому стоимость отрисовки треугольника из одного пикселя может быть равна стоимости отрисовки треугольника из 3 пикселей. Детальная 3D-картаАнализируемый кадрСначала идёт процесс отрисовки теней. В нём нет ничего особенно интересного, за исключением того, что даже такие маленькие объекты, вроде забора огорода или бобров, являются шадоукастерами даже на большом расстоянии. Возможно, на более близком зуме их можно разглядеть, однако тут это будет невозможно. Также, можно было бы плоские участки земли исключать из шадоупаса (т. к. тень от неё в принципе никуда не может упасть). Земля занимает значительное колличесто пикселей шадоумапы, которые по сути рассчитались просто так. Можно сделать предположение, что в шадоупас просто забита вся возможная геометрия в видимой части экрана. Тень от забора огородаОтрисовка террейнаПервый вызов отрисовки, после теней, это вызов отрисовки террейна. Учитывая, что у террейна большой шейдер, имело бы смысл отсортировать непрозрачную геометрию по близости к камере, а затем вызвать отрисовку террейна. Учитывая, что подавляющее число объектов на карте являются непрозрачными, это могло бы быть довольно хорошей оптимизацией. Однако, террейн рисуется всегда и везде, а у вызовов отрисовки объектов не прослеживается чёткого порядка. Вместо разделения террейна на биомы или регионы, парадоксы пошли по пути убершейдера, который применяется везде. Последствия этого очевидны: вместо специализированного набора данных для каждого региона, мы всегда держим в памяти данные всего мира. Данный шейдер использует 145 (!) текстур слоёв террейна. Они имеют формат RGBA_BC3_UNORM_SRGB или RGBA_BC3_UNORM (для нормалей) с разрешением 512х512. В основном это диффузные текстуры + нормали. При этом, прямо скажем, не все эти текстуры блистают детализацией. К примеру:Едва различимые градиенты земли.Если взглянуть на самый большой буфер pdx_hlsl_cbPdxTerrain2MaterialConstants 55.3 KB, то можно увидеть индексы для 69 материалов, после которых идут какие-то константы материалов для 199 биомов, в некоторых биомах могут быть индексы до 15 материалов. После текстур материалов идёт ряд глобальных текстур, которые должны быть общими вне зависимости от биома. Самая первая из них, ProvinceColorIndirectionTexture_Texture, имеет формат R8G8_UNORM с разрешением 16384×8192. Одна только эта текстура занимает 256mb VRAM.ProvinceColorIndirectionTexture_TextureЕщё один пример нерационально��о использования VRAM: текстура VirtualHeightmapPhysicalTexture, является атлассом текстур высот. Однако, даже половина его не заполнена. R16_UNORM 8192×8192 занимает 128mb.VirtualHeightmapPhysicalTextureСуммарно все «глобальные» текстуры занимают  838.336mb. Шейдер занимает 4253 SPIR-V кода или 1282 GLSL.Меши рек имеют ~2 до ~4 тысяч вершин, среди которой есть много лишней геометрии, особенно на прямых.Отрисовка рекПример излишней геометрииЕсли смотреть на вызовы отрисовки остальной геометрии, там тоже всё довольно печально. Система лодов выглядит сломанной. К примеру, рядом друг с другом идут команды отрисовки детализированных домов и чуть менее детализированных. Но даже менее детализированные имеют намного больше вершин, чем колличество занимаемых ими пикселей.Lod0Lod1Занимаемые пиксели домами. Излишняя геометрия добавляет шум в конечное изображение. Если дом по центру ещё можно как-то различить, то городская площадь левее превращается в набор цветных клякс.Самые крупные вызовы отрисовки достались деревьям. Во первых, они имеют большой фрагментный шейдер (SPIRV 2578 строк или GLSL 779), в который, например, входит логика для наложения тумана войны. То есть, вместо специального шейдера для тумана, каждый объект на карте индивидуально для себя рассчитывает наложение тумана.Меш деревьевПервый вызов отрисовки деревьев: vkCmdDrawIndexed() с instanceCount 1979. Однако, этот вызов рисует только часть деревьев.Суммарно, на отрисовку всех деревьев тратится 11 вызовов отрисовки (это ещё ~половина экрана без леса), в каждом из которых от ~2 до ~4 тысяч мешей, каждый меш это 5-7 квадов. Вы уже можете догадаться что это значит.Деревья стоят очень плотно друг к другу, и нет никакого препасса глубины. Это выливается просто в адский Overdraw, видеокарте приходится раз за разом рассчитывать тонны пикселей, которые даже не попадут в финальное изображение. При этом, стоимость расчёта каждого пикселя высокая, из-за большого пиксельного шейдера. В итоге экран заполненный деревьями, по сравнению с пустыней, роняет фпс примерно в два раза.Также, тут стоит упомянуть, что в качестве дизера используется Bayer4x4, который к тому-же статичен от кадра к кадру, что не позволяет сгладить ошибку за счёт накопления информации между кадрами. Он обладает очень низким качеством, и паттерн очень чётко виден, если вы поставите DLSS в режим производительности. Bayer4x4Сравнение дизера Bayer4x4 vs Rdither vs IGN. Код лежит на shadertoy, там же можно взглянуть на динамический вариант. Подробнее про дизер можно почитать тут.После отрисовки основного массива деревьев рисуются различные животные. Олени, лоси, коровы, овцы, волки и... бобры?Прекрасный в своей детализации бобрБобра видите? Нет? А он есть.Напоминаю, что даже если пиксельный шейдер полностью скипнут тестом глубины, перед этим вершинный шейдер всё равно должен отработать для всех вершин меша. Кроме стандартных матриц трансформации, данный вертексный шейдер также читает две текстуры высоты, текстуру шума и тумана войны. И да, бобр к тому же ещё и анимирован. Это даёт около 256 строк GLSL кода или 805 SPIR-V.СпойлерВот он, бобр, занимает неполных 11х4 пикселей. Для этого используется 485 вершин, то есть более 10 вершин на пиксель. Если честно, это уже начинает напонимать другой проект Paradox Interactive — Cities Skylines 2, где у людей были детализированные зубы во рту.Глобальная картаВ целом, глобальная карта работает довольно быстро, так что претензии в этом разделе будут менее значительными с точки зрения производительности.Анализируемый кадрНачало сразу многообещающее, относительно небольшой шейдер (447 GLSL, 1625 SPIR-V), большая часть визуала  отрисована  одним квадом. Это очень хороший результат, учитывая какой процент пикселей сразу принимают конечный вид.Дальше идут сотни вызовов отрисовки текста, по одному на название страны. После них начинают рисоваться границы, по мешу на страну.До отрисовки меша границ Валахии. Обратите внимание, что шейдер карты уже нарисовал границы слегка более тёмным цветом.После отрисовки меша границ Валахии.Вершины мешей границ лежат в нескольких больших буферах, поэтому буду приводить цифры треугольников: 4 621 треугольник у Валахии. В режиме wireframe в чёрных областях заметна излишняя геометрия. Можно сказать, что этот меш сделан с приличным запасом для приближения.При этом, при резких поворотах границы куча треугольников начинают пересекать друг друга, создавая много лишней геометрии. Эта проблема хроническая, и присутствует также у рек.Даже европейский малыш, который занимает 5х7 пикселей может похвастаться 353 треугольниками.При этом, граница это место между территориями двух (или более) государств. Так что пиксели границ в среднем рисуются два и более раз.Два вызова vkCmdDrawIndexed - две отрисовки меша границ.Вернёмся к шейдеру карты. Как он нарисовал границы? При помощи текстуры BorderDistanceFieldTexture_Texture  R8_UNORM  4096x2048 (8mb VRAM). SDF текстуры (поля дистанций) являются хорошим выбором для отрисовки различных чётких линий, т.к. дистанция правильно интерпалируется между пикселями.BorderDistanceFieldTexture_TextureРазрешения этой текстуры, конечно, начинает не хватать в области священной римской империи. Поэтому меня возникает вопрос, почему бы не разделить мир на 8 регионов, каждому из них не назначить по одной такой SDF текстуре? Вершинные буферы в данной ситуации занимают больше VRAM, мне удалось насчитать ~76.84 MB  + ~4.8mb буфера индексов. При этом, дело не только в памяти, но и в колличестве вызовов отрисовки и использованных треугольниках. Чисто механически, видеокарта может совершать в вершинных шейдерах столько же работы, что и в пиксельных. Самое узкое бутылочное горлышко в растеризаторе и сборке примитивов, видеокарты просто не рассчитаны на слишком большое колличество треугольников. Так что, вариант с несколькими SDF текстурами займёт меньше памяти и ресурсов.ВыводСтудия последовательно шла по пути упрошения логики рендера ради ускорения разработки. Это видно по одному большому убершейдеру со всеми данными мира, вместо набора специализированных шейдеров и данных, по отсутствию правильного порядка вызовов отрисовки, по неконтролируемому овердроу, по примитивному алгоритму извлечения мешей из линий (границы и реки), по частично сломанной системе LOD и по отсутсвию ограничений шадоукастеров.Последствия очевидны и измеримы. Хронически низкая эффективность пиксельных шейдеров из-за излишней геометрии, большие требования к видео памяти, тяжёлые пиксельные шейдеры без защиты от overdraw, вершинные шейдеры выполняются даже для крошечных или невидимых на экране объектов. Платить за это приходится производительностью в реальном времени.Есть и плюсы. Режим политической карты ощутимо легче географического по затратам. Переходы между режимами глобальной карты выглядят хорошо. Технически наибольший эффект для производительности даст возврат к более специализированным шейдерам, сортировка непрозрачных вызовов front-to-back, исправление LOD и агрессивная фильтрация/ограничение мелких шадоукастеров. Если у вас слабое железо, советую избегать детальной карты, особенно леса.Теги:eu5рендеркадр3d-графикакомпьютерная графикаанализ кадраeuropa universalis 5Хабы:Игры и игровые консолиCGI (графика)Разработка игр3D-графика",4000,0,0,8 мин,https://habr.com/ru/articles/963470/,13471,1789,4
