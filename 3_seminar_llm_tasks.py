# -*- coding: utf-8 -*-
"""3_seminar_llm_tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_WOUVJiYkXeT_TRypxM6E1XfURh55Al

# Семинар по Главе 7: Large Language Models  
## План семинара и задачи для самостоятельной работы

Этот блокнот содержит **подробно сформулированные задачи**, направленные на закрепление ключевых идей главы 7 книги *Jurafsky & Martin — Speech and Language Processing (2025)*.

Структура задач:
- **5 лёгких задач**, проверяющих базовое понимание концепций (архитектура, предсказание токенов, prompting, sampling).
- **3 задачи посложнее**, связанные с моделированием, анализом распределений, измеримыми метриками и интерпретациями.

Каждая задача содержит постановку, теоретический контекст и измеримый результат.

## Задача 1 (лёгкая). Классификация архитектур моделей

**Тема:** Encoder, Decoder, Encoder–Decoder (Fig. 7.3).

### Контекст
В главе описаны три архитектуры:  
- Encoder-only (BERT) — создаёт представление входа.  
- Decoder-only (GPT) — генерирует текст авто-регрессионно.  
- Encoder–Decoder (T5) — преобразует вход в выход иной природы.

### Задание
Отнесите 6 описаний моделей к одной из архитектур и кратко объясните выбор:

1. Классификация «позитив/негатив»  
2. Продолжение текста  
3. Машинный перевод  
4. Заполнение пропущенного слова  
5. Ответ в диалоге  
6. Кодирование документа

### Результат
Таблица: *описание → архитектура + объяснение*.

## Задача 2 (лёгкая). Условное порождение текста

**Тема:** Условная вероятность следующего токена.

### Дано распределение:
| токен | вероятность |
|------|-------------|
| flowers | 0.61 |
| plants | 0.25 |
| trees | 0.07 |
| animals | 0.02 |
| garden | 0.05 |

### Задание
1. Укажите greedy-токен.  
2. Смоделируйте один шаг sampling (ручная симуляция).  
3. Объясните, какую информацию о мире модель использует.

### Результат
3 пункта: greedy, sample, объяснение (2–4 предложения).

## Задача 3 (лёгкая). Temperature sampling

**Тема:** Влияние параметра τ.

### Логиты:
```
flowers: 2.1
plants: 1.8
trees: 0.5
animals: -0.4
```

### Задание
1. Преобразуйте логиты в вероятности при τ = 1.  
2. Опишите, как изменится распределение при τ = 0.5 и τ = 2.  
3. Укажите, при какой τ модель будет вести себя почти как greedy.

### Результат
• Вероятности для τ=1.  
• Словесное описание.  
• Вывод.

## Задача 4 (лёгкая). Prompt engineering

**Тема:** Zero-shot vs few-shot prompting.

### Дано:
Zero-shot:  
> Classify the sentiment: "I love the new camera".

Two-shot:  
> "Terrible movie" → Negative  
> "Amazing performance" → Positive  
> Now classify: "I love the new camera".

### Задание
1. Объясните преимущество few-shot.  
2. Предскажите ответы модели в обоих случаях.  
3. Объясните механизм in‑context learning.

### Результат
3 абзаца по 3–5 предложений.

## Задача 5 (лёгкая). Перплексия

**Тема:** Perplexity как метрика.

### Вероятности:
P₁ = 0.20, P₂ = 0.50, P₃ = 0.10

### Задание
1. Вычислите PPL.  
2. Объясните влияние малых вероятностей.  
3. Сравните две модели: A=[0.3,0.3,0.3], B=[0.6,0.2,0.1].

### Результат
• одно число;  
• объяснение;  
• вывод.

## Задача 6 (сложная). 2‑шаговая autoregressive генерация

**Тема:** Авто-регрессия (Fig. 7.2).

### Дано:
Контекст: “The professor said that he”

Шаг 1 распределение:  
is 0.55, was 0.33, will 0.07, should 0.05  

Если выбран вариант “was”, шаг 2:  
happy 0.40, wrong 0.25, here 0.20, confused 0.15

### Задание
1. Выполните два шага генерации.  
2. Выпишите финальное предложение.  
3. Объясните скрытые ассоциации модели.

### Результат
Текст + аналитика (5–7 предложений).

## Задача 7 (сложная). Температура и энтропия

**Тема:** Температурное сглаживание + энтропия.

### Логиты:
cat 3.0, dog 2.7, car 1.2, apple -0.5

### Задание
1. Вычислите вероятности softmax при τ=1.  
2. При τ=0.7 и τ=1.5.  
3. Оцените энтропию распределений.  
4. Проинтерпретируйте влияние τ на разнообразие текста.

### Результат
Таблица + энтропии + интерпретация.

## Задача 8 (сложная). Этический анализ LLM

**Тема:** Риски (семантические, этические, приватность).

### Сценарии:
1. Медицинский совет.  
2. Сообщение личных данных.  
3. Генерация стереотипов.

### Задание
Для каждого:
- определите риск (hallucination / privacy / stereotyping);  
- объясните причину;  
- предложите меру смягчения.

### Результат
3 анализа × 6–8 предложений.
"""