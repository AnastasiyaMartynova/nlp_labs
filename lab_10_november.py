# -*- coding: utf-8 -*-
"""lab_10_november_2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZAALZexJ_Vdr30kDXFd1mCpgFmL9y6vC

Доделать: лучше предобрабоать текст с помощью нлтк, чтобы были только начальные формы глаголов и визуализировать где векторная арифметика
"""

import os
import re
from string import punctuation
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^а-яё\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()

    return text

#ПРОБЛЕМА С КОДИРОВКОЙ У ПОСЛЕДНИХ ТРЕХ ФАЙЛОВ

# corpus = []

# with open('HP1RUS.txt', 'r', encoding='utf-8') as file:
#     lines = file.readlines()[7:]
# text = ' '.join(lines)
# processed_text = preprocess_text(text)
# sentences = nltk.sent_tokenize(processed_text, language='russian')
# corpus.extend(sentences)

# with open('HP2RUS.txt', 'r', encoding='utf-8') as file:
#     lines = file.readlines()[7:]
# text = ' '.join(lines)
# processed_text = preprocess_text(text)
# sentences = nltk.sent_tokenize(processed_text, language='russian')
# corpus.extend(sentences)

# with open('HP3RUS.txt', 'r', encoding='utf-8') as file:
#     lines = file.readlines()[6:]
# text = ' '.join(lines)
# processed_text = preprocess_text(text)
# sentences = nltk.sent_tokenize(processed_text, language='russian')
# corpus.extend(sentences)

# with open('HP4RUS.txt', 'r', encoding='utf-8') as file:
#     lines = file.readlines()[3:]
# text = ' '.join(lines)
# processed_text = preprocess_text(text)
# sentences = nltk.sent_tokenize(processed_text, language='russian')
# corpus.extend(sentences)

def tokenize_corpus(corpus):
    tokenized_corpus = []

    for sentence in corpus:
        tokens = word_tokenize(sentence, language='russian')

        # Удалить стоп-слова и короткие слова
        stop_words = set(stopwords.words('russian'))
        filtered_tokens = [
            token for token in tokens
            if token not in stop_words and len(token) > 2
        ]

        if filtered_tokens:  # Добавить только непустые списки
            tokenized_corpus.append(filtered_tokens)

    return tokenized_corpus

def save_processed_data(tokenized_corpus, filename='processed_corpus.txt'):
    with open(filename, 'w', encoding='utf-8') as f:
        for tokens in tokenized_corpus:
            f.write(' '.join(tokens) + '\n')
    print("сохранено")
    print(filename)

with open('HP1RUS.txt', 'r', encoding='utf-8') as infile, \
     open('Data.txt', 'w', encoding='utf-8') as outfile:
    lines = infile.readlines()
    outfile.writelines(lines[7:])

with open('Data.txt', 'r', encoding='utf-8') as file:
    text = file.read()

processed_text = preprocess_text(text)
sentences = nltk.sent_tokenize(processed_text, language='russian')

print(len(sentences)) #вывод 1, поскольку это все 1 строка в обработанном тексте

print(sentences)

tokenized_corpus = tokenize_corpus(sentences)

save_processed_data(tokenized_corpus)

# Всего токенов
print(sum(len(tokens) for tokens in tokenized_corpus))

pip install scikit-learn

pip install gensim

from gensim.models import Word2Vec
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import numpy as np

model = Word2Vec(
    sentences=tokenized_corpus,
    vector_size=100,
    window=5,             # размер окна контекста
    min_count=3,          # минимальная частота слова
    sg=1,                 # алгоритм Skip-Gram (при 1) или CBOW (0)
    workers=4,            # количество ядер процессора
    epochs=20
)

print(len(model.wv.key_to_index)) # Размер словаря слов

model.save("harry_potter_rus.model")

def print_similar_words(model, word, topn=10):
    if word in model.wv:
        similar = model.wv.most_similar(word, topn=topn)
        print(f"\nБлижайшие слова к '{word}':")
        for i, (similar_word, similarity) in enumerate(similar, 1):
            print(f"{i:2d}. {similar_word:<15} {similarity:.4f}")

characters = ['гарри', 'гермиона', 'рон', 'волдеморт', 'снейп', 'мальфой']
for character in characters:
    print_similar_words(model, character)

test_words = ['гарри', 'гермиона', 'рон', 'малфой', 'волдеморт', 'дамблдор']
for word in test_words:
    if word in model.wv:
        print(f"Слово '{word}' есть в словаре")
    else:
        print(f"Слова '{word}' нет в словаре")

# Векторная арифметика
def word_analogy(model, positive, negative, topn=5):
    result = model.wv.most_similar(positive=positive, negative=negative, topn=topn)
    print(f"\nАналогия: {positive} - {negative} = ?")
    for i, (word, similarity) in enumerate(result, 1):
        print(f"{i:2d}. {word:<15} {similarity:.4f}")
    return result

word_analogy(model, positive=['гарри', 'гриффиндор'], negative=['малфой'])
word_analogy(model, positive=['дамблдор', 'добрый'], negative=['снейп'])
word_analogy(model, positive=['волдеморт', 'злой'], negative=['гарри'])

def visualize_words(model, words_to_visualize, figsize=(12, 10)):
    words = []
    vectors = []

    for word in words_to_visualize:
        if word in model.wv:
            words.append(word)
            vectors.append(model.wv[word])

    if len(vectors) < 2:
        print("Недостаточно слов для визуализации")
        return

    vectors = np.array(vectors)

    pca = PCA(n_components=2) # PCA для уменьшения размерности до 2D
    reduced_vectors = pca.fit_transform(vectors)

    plt.figure(figsize=figsize)
    plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], alpha=0.7)

    for i, word in enumerate(words):
        plt.annotate(word,
                    (reduced_vectors[i, 0], reduced_vectors[i, 1]),
                    xytext=(5, 5),
                    textcoords='offset points',
                    fontsize=12,
                    alpha=0.8)

    plt.title('Word2Vec')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    return reduced_vectors

key_words = [
    'гарри', 'гермиона', 'рон', 'дамблдор', 'волдеморт',
    'снейп', 'малфой', 'хогвартс', 'гриффиндор', 'слизерин',
    'заклинание', 'палочка', 'метла', 'квидич', 'магия'
]

reduced_vectors = visualize_words(model, key_words)

good_characters = ['гарри', 'гермиона', 'рон', 'дumbledор', 'хагрид']
bad_characters = ['волдеморт', 'мальфой', 'снейп']  # Снейп под вопросом
hogwarts_houses = ['гриффиндор', 'слизерин', 'когтевран', 'пуффендуй']

print("\nПоложительные персонажи:")
for char in good_characters:
    print_similar_words(model, char, topn=5)

print("\nОтрицательные персонажи:")
for char in bad_characters:
    print_similar_words(model, char, topn=5)

# Cемантические расстояния между персонажами
def calculate_similarity(model, word1, word2):
    if word1 in model.wv and word2 in model.wv:
        similarity = model.wv.similarity(word1, word2)
        print(f"Схожесть между '{word1}' и '{word2}': {similarity:.4f}")
        return similarity

calculate_similarity(model, 'гарри', 'гермиона')
calculate_similarity(model, 'гарри', 'рон')
calculate_similarity(model, 'гарри', 'малфой')
calculate_similarity(model, 'гарри', 'волдеморт')
calculate_similarity(model, 'снейп', 'дамблдор')
calculate_similarity(model, 'снейп', 'волдеморт')

# Поиск слов, которые находятся между двумя концепциями
def find_between_words(model, word1, word2, topn=10):
    if word1 in model.wv and word2 in model.wv:
        result = model.wv.most_similar(positive=[word1, word2], topn=topn)
        print(f"\nСлова между '{word1}' и '{word2}':")
        for i, (word, similarity) in enumerate(result, 1):
            print(f"{i:2d}. {word:<15} {similarity:.4f}")

find_between_words(model, 'дамблдор', 'волдеморт')

find_between_words(model, 'гарри', 'гриффиндор')

pip install plotly

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

import pandas as pd

pip install ipykernel

pip install --upgrade nbformat

import nbformat

def interactive_plotly_visualization(model, words_list, title="Word2Vec 3d"):
    # Получение векторов
    words = []
    vectors = []
    for word in words_list:
        if word in model.wv:
            words.append(word)
            vectors.append(model.wv[word])

    vectors = np.array(vectors)

    # PCA для 3 компонентов
    pca = PCA(n_components=3)
    vectors_3d = pca.fit_transform(vectors)

    # Отдельный датафрейм
    df = pd.DataFrame(vectors_3d, columns=['x', 'y', 'z'])
    df['word'] = words
    df['cluster'] = [word.split('_')[0] if '_' in word else 'other' for word in words]

    fig = px.scatter_3d(
        df, x='x', y='y', z='z',
        text='word',
        title=title,
        hover_name='word',
        size_max=18,
        opacity=0.8
    )

    fig.update_traces(
        textposition='top center',
        marker=dict(size=8, line=dict(width=2, color='DarkSlateGrey'))
    )

    fig.show()

    return df

character_words = [
    'гарри', 'гермиона', 'рон', 'дамблдор', 'волдеморт',
    'снейп', 'малфой', 'хагрид', 'невилл', 'луна'
]
interactive_plotly_visualization(model, character_words)

import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap

def similarity_heatmap(model, words_list, figsize=(12, 10)):
    # Фильтрация слов, которые есть в модели
    valid_words = [word for word in words_list if word in model.wv]

    similarity_matrix = np.zeros((len(valid_words), len(valid_words)))

    for i, word1 in enumerate(valid_words):
        for j, word2 in enumerate(valid_words):
            similarity_matrix[i, j] = model.wv.similarity(word1, word2)

    plt.figure(figsize=figsize)

    cmap = LinearSegmentedColormap.from_list('custom_red_blue', ['#1f77b4', '#ffffff', '#d62728'])

    sns.heatmap(
        similarity_matrix,
        xticklabels=valid_words,
        yticklabels=valid_words,
        annot=True,
        fmt='.2f',
        cmap=cmap,
        center=0.5,
        square=True,
        cbar_kws={'label': 'Семантическая схожесть'}
    )

    plt.title('Тепловавя карта')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    return similarity_matrix, valid_words

main_characters = ['гарри', 'гермиона', 'рон', 'дамблдорр', 'волдеморт', 'снейп', 'малфой']
similarity_matrix, words = similarity_heatmap(model, main_characters)

from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist

def word_dendrogram(model, words_list, method='ward', figsize=(15, 10)):
    valid_words = [word for word in words_list if word in model.wv]
    vectors = [model.wv[word] for word in valid_words]

    # Расстояния
    distance_matrix = pdist(vectors, metric='cosine')

    # Иерархическая кластеризация
    linkage_matrix = linkage(distance_matrix, method=method)

    plt.figure(figsize=figsize)
    dendrogram(
        linkage_matrix,
        labels=valid_words,
        orientation='right',
        leaf_font_size=12,
        color_threshold=0.7 * max(linkage_matrix[:, 2])
    )

    plt.title('Дендрограмма кластеризации слов')
    plt.xlabel('Расстояние')
    plt.tight_layout()
    plt.show()

    return linkage_matrix

thematic_words = [
    'гарри', 'гермиона', 'рон',
    'дамблдор', 'снейп', 'макгонагалл',
    'волдеморт', 'малфой', 'болот',
    'хогвартс', 'гриффиндор', 'слизерин',
    'заклинание', 'палочка', 'метла',
    'квидич', 'матч', 'гонг',
]

linkage_matrix = word_dendrogram(model, thematic_words)