{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0771ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\anmrt\\desktop\\useful shit\\nlp_labs\\venv\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anmrt\\desktop\\useful shit\\nlp_labs\\venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anmrt\\desktop\\useful shit\\nlp_labs\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anmrt\\desktop\\useful shit\\nlp_labs\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anmrt\\desktop\\useful shit\\nlp_labs\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anmrt\\desktop\\useful shit\\nlp_labs\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c2a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  input  sigmoid    tanh  relu\n",
      "0    x1   0.7427  0.7857  1.06\n",
      "1    x2   0.5150  0.0599  0.06\n",
      "2    x3   0.7595  0.8178  1.15\n",
      "3    x4   0.5987  0.3799  0.40\n",
      "4    x5   0.9448  0.9932  2.84\n"
     ]
    }
   ],
   "source": [
    "# ЗАДАЧА 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Вход векторы\n",
    "x_list = [\n",
    "    np.array([0.2, -0.4, 0.1]),\n",
    "    np.array([1.0, 0.5, -0.3]),\n",
    "    np.array([-0.1, -0.2, 0.7]),\n",
    "    np.array([0.0, 0.0, 0.0]),\n",
    "    np.array([2.0, -1.0, 0.3]),\n",
    "]\n",
    "\n",
    "# Парамеры юнита\n",
    "w = np.array([0.5, -1.2, 0.8])\n",
    "b = 0.4\n",
    "\n",
    "# Активация\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Нейронный юнит\n",
    "def unit(x, w, b, activation):\n",
    "# y = activation(w·x + b)\"\n",
    "    z = np.dot(w, x) + b\n",
    "    return activation(z)\n",
    "\n",
    "# Вычисление выходов для всех входов и всех активаций\n",
    "results = []\n",
    "\n",
    "activations = {\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"tanh\": tanh,\n",
    "    \"relu\": relu,\n",
    "}\n",
    "\n",
    "for i, x in enumerate(x_list, start=1):\n",
    "    row = {\"input\": f\"x{i}\"}\n",
    "    for name, act in activations.items():\n",
    "        row[name] = unit(x, w, b, act)\n",
    "    results.append(row)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad28505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0, Потери 0.693190\n",
      "Эпоха 2000, Потери 0.477393\n",
      "Эпоха 4000, Потери 0.477665\n",
      "Эпоха 6000, Потери 0.477390\n",
      "Эпоха 8000, Потери 0.477388\n",
      "[0. 0.], Pred: 0.6667, True: 0.0, class: 1\n",
      "[0. 1.], Pred: 0.6667, True: 1.0, class: 1\n",
      "[1. 0.], Pred: 0.6667, True: 1.0, class: 1\n",
      "[1. 1.], Pred: 0.0000, True: 0.0, class: 0\n",
      "Точность: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# ЗАДАЧА 2\n",
    "\n",
    "# XOR\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "], dtype=float)\n",
    "\n",
    "y = np.array([[0], [1], [1], [0]], dtype=float)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Веса\n",
    "W1 = np.random.randn(2, 2) * 0.1\n",
    "b1 = np.zeros((1, 2))\n",
    "W2 = np.random.randn(2, 1) * 0.1\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    eps = 1e-8\n",
    "    return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n",
    "\n",
    "lr = 0.5 # Шаг\n",
    "n_epochs = 10000\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    y_pred = sigmoid(z2)\n",
    "    \n",
    "    # Потери\n",
    "    loss = binary_cross_entropy(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward\n",
    "    grad_z2 = y_pred - y\n",
    "    grad_W2 = np.dot(a1.T, grad_z2)\n",
    "    grad_b2 = np.sum(grad_z2, axis=0, keepdims=True)\n",
    "    \n",
    "    # Градиентный спуск\n",
    "    grad_a1 = np.dot(grad_z2, W2.T)\n",
    "    grad_z1 = grad_a1 * relu_deriv(z1)\n",
    "    grad_W1 = np.dot(X.T, grad_z1)\n",
    "    grad_b1 = np.sum(grad_z1, axis=0, keepdims=True)\n",
    "    \n",
    "    # Обновление параметров\n",
    "    W2 -= lr * grad_W2\n",
    "    b2 -= lr * grad_b2\n",
    "    W1 -= lr * grad_W1\n",
    "    b1 -= lr * grad_b1\n",
    "\n",
    "    if epoch % 2000 == 0:\n",
    "        print(f\"Эпоха {epoch}, Потери {loss:.6f}\")\n",
    "\n",
    "z1 = np.dot(X, W1) + b1\n",
    "a1 = relu(z1)\n",
    "z2 = np.dot(a1, W2) + b2\n",
    "y_pred_final = sigmoid(z2)\n",
    "\n",
    "for i, (input_val, pred, true) in enumerate(zip(X, y_pred_final, y)):\n",
    "    print(f\"{input_val}, Pred: {pred[0]:.4f}, True: {true[0]}, class: {int(pred[0] > 0.5)}\")\n",
    "\n",
    "accuracy = np.mean((y_pred_final > 0.5).astype(int) == y)\n",
    "print(f\"Точность: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa2a188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0, Потеря 0.6933\n",
      "Эпоха 400, Потеря 0.6728\n",
      "Эпоха 800, Потеря 0.6708\n",
      "Эпоха 1200, Потеря 0.6498\n",
      "Эпоха 1600, Потеря 0.4795\n",
      "'great', True: 1, Pred: 1, Prob: 0.9875\n",
      "'awful', True: 0, Pred: 0, Prob: 0.2410\n",
      "'good movie', True: 1, Pred: 1, Prob: 0.9712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ЗАДАЧА 3\n",
    "vocab = [\"great\", \"excellent\", \"good\", \"bad\", \"awful\", \"movie\"]\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "d = 8  # размерность эмбеддинга\n",
    "\n",
    "np.random.seed(42)\n",
    "E = np.random.randn(len(vocab), d) * 0.1 # матрица эмбеддингов\n",
    "\n",
    "train_data = [\n",
    "    (\"great movie\", 1),\n",
    "    (\"excellent movie\", 1),\n",
    "    (\"good movie\", 1),\n",
    "    (\"bad movie\", 0),\n",
    "    (\"awful movie\", 0),\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    (\"great\", 1),\n",
    "    (\"awful\", 0),\n",
    "    (\"good movie\", 1),\n",
    "]\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def phrase_embedding(text):\n",
    "    tokens = tokenize(text)\n",
    "    vecs = []\n",
    "    for t in tokens:\n",
    "        if t in word2idx:\n",
    "            vecs.append(E[word2idx[t]])\n",
    "    if not vecs:\n",
    "        return np.zeros(d)\n",
    "    vecs = np.stack(vecs, axis=0)\n",
    "    return vecs.mean(axis=0)\n",
    "\n",
    "# Построим обучающие матрицы X, y\n",
    "X_train = np.stack([phrase_embedding(text) for text, label in train_data], axis=0)\n",
    "y_train = np.array([label for text, label in train_data])\n",
    "\n",
    "X_test = np.stack([phrase_embedding(text) for text, label in test_data], axis=0)\n",
    "y_test = np.array([label for text, label in test_data])\n",
    "\n",
    "# Параметры сети 8 -> 4 -> 2\n",
    "hidden_dim = 4\n",
    "output_dim = 2\n",
    "\n",
    "W1 = np.random.randn(d, hidden_dim) * 0.1\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    x_shift = x - np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x_shift)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    num_classes = y_pred.shape[1]\n",
    "    y_one_hot = np.eye(num_classes)[y_true]\n",
    "    eps = 1e-8\n",
    "    return -np.mean(np.sum(y_one_hot * np.log(y_pred + eps), axis=1))\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 2000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    z1 = np.dot(X_train, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    y_pred = softmax(z2)\n",
    "    \n",
    "    # Loss\n",
    "    loss = cross_entropy_loss(y_pred, y_train)\n",
    "    \n",
    "    # Backward\n",
    "    grad_z2 = y_pred - np.eye(output_dim)[y_train]\n",
    "    grad_z2 /= len(y_train)\n",
    "    \n",
    "    grad_W2 = np.dot(a1.T, grad_z2)\n",
    "    grad_b2 = np.sum(grad_z2, axis=0, keepdims=True)\n",
    "    \n",
    "    grad_a1 = np.dot(grad_z2, W2.T)\n",
    "    grad_z1 = grad_a1 * (a1 > 0)\n",
    "    grad_W1 = np.dot(X_train.T, grad_z1)\n",
    "    grad_b1 = np.sum(grad_z1, axis=0, keepdims=True)\n",
    "    \n",
    "    # Обновление\n",
    "    W2 -= lr * grad_W2\n",
    "    b2 -= lr * grad_b2\n",
    "    W1 -= lr * grad_W1\n",
    "    b1 -= lr * grad_b1\n",
    "    \n",
    "    if epoch % 400 == 0:\n",
    "        print(f\"Эпоха {epoch}, Потеря {loss:.4f}\")\n",
    "\n",
    "z1_test = np.dot(X_test, W1) + b1\n",
    "a1_test = relu(z1_test)\n",
    "z2_test = np.dot(a1_test, W2) + b2\n",
    "y_pred_test = softmax(z2_test)\n",
    "predictions = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "for i, (text, true_label) in enumerate(test_data):\n",
    "    pred_prob = y_pred_test[i][1]\n",
    "    pred_class = predictions[i]\n",
    "    print(f\"'{text}', True: {true_label}, Pred: {pred_class}, Prob: {pred_prob:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5783195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0, Потеря 2.3980, Perplexity: 11.0012\n",
      "Эпоха 400, Потеря 1.3414, Perplexity: 3.8244\n",
      "Эпоха 800, Потеря 0.6462, Perplexity: 1.9083\n",
      "Эпоха 1200, Потеря 0.3805, Perplexity: 1.4631\n",
      "Эпоха 1600, Потеря 0.2693, Perplexity: 1.3091\n",
      "Финальная потеря 0.1966\n",
      "Финальная Perplexity: 1.2173\n",
      "['i', 'like', 'neural'] -> True: 'networks', Pred: 'networks' (p=0.6236)\n",
      "['i', 'like', 'deep'] -> True: 'learning', Pred: 'learning' (p=0.9102)\n",
      "['neural', 'networks', 'are'] -> True: 'powerful', Pred: 'powerful' (p=0.9668)\n",
      "['deep', 'learning', 'is'] -> True: 'useful', Pred: 'useful' (p=0.9836)\n",
      "['i', 'like', 'useful'] -> True: 'tools', Pred: 'tools' (p=0.6932)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ЗАДАЧА 4\n",
    "\n",
    "corpus = [\n",
    "    \"i like neural networks\",\n",
    "    \"i like deep learning\",\n",
    "    \"neural networks are powerful\",\n",
    "    \"deep learning is useful\",\n",
    "    \"i like useful tools\",\n",
    "    \"networks are useful\",\n",
    "]\n",
    "\n",
    "# Построим слвоарь\n",
    "tokens = set()\n",
    "for sent in corpus:\n",
    "    tokens.update(sent.split())\n",
    "\n",
    "vocab = sorted(tokens)\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "d = 6\n",
    "np.random.seed(42)\n",
    "E = np.random.randn(vocab_size, d) * 0.1 # Эмбеддинги\n",
    "\n",
    "N = 3  # размер окна контекста\n",
    "\n",
    "def make_dataset(corpus, N):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sent in corpus:\n",
    "        words = sent.split()\n",
    "        if len(words) <= N:\n",
    "            continue\n",
    "        idxs = [word2idx[w] for w in words]\n",
    "        for t in range(N, len(idxs)):\n",
    "            context = idxs[t-N:t] # N предыдущих слов\n",
    "            target = idxs[t] # следущее слово\n",
    "            X.append(context)\n",
    "            y.append(target)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_ctx, y_tgt = make_dataset(corpus, N)\n",
    "\n",
    "def get_embedding_for_context(context_ids):\n",
    "    vecs = [E[i] for i in context_ids]\n",
    "    return np.concatenate(vecs, axis=0)\n",
    "\n",
    "# Построим матрицу признаков\n",
    "X_embed = np.stack([get_embedding_for_context(ctx) for ctx in X_ctx], axis=0)\n",
    "\n",
    "input_dim = N * d\n",
    "hidden_dim = 16\n",
    "output_dim = vocab_size\n",
    "\n",
    "W = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "b = np.zeros((1, hidden_dim))\n",
    "U = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "c = np.zeros((1, output_dim))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    x_shift = x - np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x_shift)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    y_one_hot = np.eye(output_dim)[y_true]\n",
    "    eps = 1e-8\n",
    "    return -np.mean(np.sum(y_one_hot * np.log(y_pred + eps), axis=1))\n",
    "\n",
    "def perplexity(loss):\n",
    "    return np.exp(loss)\n",
    "\n",
    "lr = 0.05\n",
    "n_epochs = 2000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    z1 = np.dot(X_embed, W) + b\n",
    "    h = relu(z1)\n",
    "    z2 = np.dot(h, U) + c\n",
    "    y_pred = softmax(z2)\n",
    "    \n",
    "    # Loss\n",
    "    loss = cross_entropy_loss(y_pred, y_tgt)\n",
    "    \n",
    "    # Backward\n",
    "    grad_z2 = y_pred - np.eye(output_dim)[y_tgt]\n",
    "    grad_z2 /= len(y_tgt)\n",
    "    \n",
    "    grad_U = np.dot(h.T, grad_z2)\n",
    "    grad_c = np.sum(grad_z2, axis=0, keepdims=True)\n",
    "    \n",
    "    grad_h = np.dot(grad_z2, U.T)\n",
    "    grad_z1 = grad_h * (h > 0)\n",
    "    grad_W = np.dot(X_embed.T, grad_z1)\n",
    "    grad_b = np.sum(grad_z1, axis=0, keepdims=True)\n",
    "    \n",
    "    # Обновление\n",
    "    W -= lr * grad_W\n",
    "    b -= lr * grad_b\n",
    "    U -= lr * grad_U\n",
    "    c -= lr * grad_c\n",
    "    \n",
    "    if epoch % 400 == 0:\n",
    "        ppl = perplexity(loss)\n",
    "        print(f\"Эпоха {epoch}, Потеря {loss:.4f}, Perplexity: {ppl:.4f}\")\n",
    "\n",
    "z1_final = np.dot(X_embed, W) + b\n",
    "h_final = relu(z1_final)\n",
    "z2_final = np.dot(h_final, U) + c\n",
    "y_pred_final = softmax(z2_final)\n",
    "final_loss = cross_entropy_loss(y_pred_final, y_tgt)\n",
    "final_ppl = perplexity(final_loss)\n",
    "\n",
    "print(f\"Финальная потеря {final_loss:.4f}\")\n",
    "print(f\"Финальная Perplexity: {final_ppl:.4f}\")\n",
    "for i in range(min(5, len(X_ctx))):\n",
    "    context_words = [idx2word[idx] for idx in X_ctx[i]]\n",
    "    target_word = idx2word[y_tgt[i]]\n",
    "    pred_word_idx = np.argmax(y_pred_final[i])\n",
    "    pred_word = idx2word[pred_word_idx]\n",
    "    prob = y_pred_final[i][pred_word_idx]\n",
    "    print(f\"{context_words} -> True: '{target_word}', Pred: '{pred_word}' (p={prob:.4f})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "804a12b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Задача 5 - Проверка градиентов:\n",
      "Максимальная разница для W1: 0.00000000\n",
      "Максимальная разница для b1: 0.00000000\n",
      "Максимальная разница для W2: 0.00000000\n",
      "Максимальная разница для b2: 0.00000002\n",
      "Общая максимальная разница: 0.00000002\n"
     ]
    }
   ],
   "source": [
    "# ЗАДАЧА 5\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Вход и целевая метка\n",
    "x = np.array([[0.3, -0.1, 0.0, 1.0, 2.0]])\n",
    "y = np.array([1])\n",
    "\n",
    "input_dim = 5\n",
    "hidden_dim = 4\n",
    "output_dim = 3\n",
    "\n",
    "# Инициализация параметров\n",
    "def init_params():\n",
    "    W1 = np.random.uniform(-0.1, 0.1, size=(input_dim, hidden_dim))\n",
    "    b1 = np.random.uniform(-0.1, 0.1, size=(1, hidden_dim))\n",
    "    W2 = np.random.uniform(-0.1, 0.1, size=(hidden_dim, output_dim))\n",
    "    b2 = np.random.uniform(-0.1, 0.1, size=(1, output_dim))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    x_shift = x - np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x_shift)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    y_one_hot = np.eye(output_dim)[y_true]\n",
    "    eps = 1e-8\n",
    "    return -np.mean(np.sum(y_one_hot * np.log(y_pred + eps), axis=1))\n",
    "\n",
    "def forward(x, W1, b1, W2, b2):\n",
    "    z1 = np.dot(x, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    y_pred = softmax(z2)\n",
    "    cache = {'z1': z1, 'a1': a1, 'z2': z2, 'y_pred': y_pred}\n",
    "    return y_pred, cache\n",
    "\n",
    "def backward(x, y, W1, b1, W2, b2, cache):\n",
    "    z1, a1, z2, y_pred = cache['z1'], cache['a1'], cache['z2'], cache['y_pred']\n",
    "    \n",
    "    # Градиент по выходному слою\n",
    "    grad_z2 = y_pred - np.eye(output_dim)[y]\n",
    "    grad_z2 /= len(y)\n",
    "    \n",
    "    grad_W2 = np.dot(a1.T, grad_z2)\n",
    "    grad_b2 = np.sum(grad_z2, axis=0, keepdims=True)\n",
    "    \n",
    "    # Градиент по скрытому слою\n",
    "    grad_a1 = np.dot(grad_z2, W2.T)\n",
    "    grad_z1 = grad_a1 * relu_deriv(z1)\n",
    "    grad_W1 = np.dot(x.T, grad_z1)\n",
    "    grad_b1 = np.sum(grad_z1, axis=0, keepdims=True)\n",
    "    \n",
    "    return grad_W1, grad_b1, grad_W2, grad_b2\n",
    "\n",
    "def numerical_gradient(param, f, eps=1e-5):\n",
    "    grad = np.zeros_like(param)\n",
    "    it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        original_value = param[idx]\n",
    "\n",
    "        param[idx] = original_value + eps\n",
    "        loss_plus = f(param)\n",
    "\n",
    "        param[idx] = original_value - eps\n",
    "        loss_minus = f(param)\n",
    "\n",
    "        grad[idx] = (loss_plus - loss_minus) / (2 * eps)\n",
    "        param[idx] = original_value\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "# Инициализация и вычисление аналитических градиентов\n",
    "W1, b1, W2, b2 = init_params()\n",
    "y_pred, cache = forward(x, W1, b1, W2, b2)\n",
    "loss = cross_entropy_loss(y_pred, y)\n",
    "grad_W1_analytical, grad_b1_analytical, grad_W2_analytical, grad_b2_analytical = backward(x, y, W1, b1, W2, b2, cache)\n",
    "\n",
    "# Проверка градиентов численным методом\n",
    "def f_W1(W1_perturbed):\n",
    "    y_pred, _ = forward(x, W1_perturbed, b1, W2, b2)\n",
    "    return cross_entropy_loss(y_pred, y)\n",
    "\n",
    "def f_b1(b1_perturbed):\n",
    "    y_pred, _ = forward(x, W1, b1_perturbed, W2, b2)\n",
    "    return cross_entropy_loss(y_pred, y)\n",
    "\n",
    "def f_W2(W2_perturbed):\n",
    "    y_pred, _ = forward(x, W1, b1, W2_perturbed, b2)\n",
    "    return cross_entropy_loss(y_pred, y)\n",
    "\n",
    "def f_b2(b2_perturbed):\n",
    "    y_pred, _ = forward(x, W1, b1, W2, b2_perturbed)\n",
    "    return cross_entropy_loss(y_pred, y)\n",
    "\n",
    "grad_W1_numerical = numerical_gradient(W1.copy(), f_W1)\n",
    "grad_b1_numerical = numerical_gradient(b1.copy(), f_b1)\n",
    "grad_W2_numerical = numerical_gradient(W2.copy(), f_W2)\n",
    "grad_b2_numerical = numerical_gradient(b2.copy(), f_b2)\n",
    "\n",
    "# Сравнение\n",
    "max_diff_W1 = np.max(np.abs(grad_W1_analytical - grad_W1_numerical))\n",
    "max_diff_b1 = np.max(np.abs(grad_b1_analytical - grad_b1_numerical))\n",
    "max_diff_W2 = np.max(np.abs(grad_W2_analytical - grad_W2_numerical))\n",
    "max_diff_b2 = np.max(np.abs(grad_b2_analytical - grad_b2_numerical))\n",
    "\n",
    "max_diff_overall = max(max_diff_W1, max_diff_b1, max_diff_W2, max_diff_b2)\n",
    "\n",
    "print(\"Задача 5 - Проверка градиентов:\")\n",
    "print(f\"Максимальная разница для W1: {max_diff_W1:.8f}\")\n",
    "print(f\"Максимальная разница для b1: {max_diff_b1:.8f}\")\n",
    "print(f\"Максимальная разница для W2: {max_diff_W2:.8f}\")\n",
    "print(f\"Максимальная разница для b2: {max_diff_b2:.8f}\")\n",
    "print(f\"Общая максимальная разница: {max_diff_overall:.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
